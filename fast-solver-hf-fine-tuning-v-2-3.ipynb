{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Дообучение encoder'а для классификации токенов.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  numpy==1.26.4 \\\n  pandas==2.2.3 \\\n  tqdm==4.67.1 \\\n  transformers==4.51.3 \\\n  evaluate==0.4.5 \\\n  torch==2.6.0+cu124 \\\n  seqeval==1.2.2\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport gc\nimport math\nfrom typing import List, Dict, Any, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport evaluate\nfrom transformers import (\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForTokenClassification,\n    TrainerCallback,\n    PrinterCallback,\n    EarlyStoppingCallback,\n)\nfrom transformers.modeling_outputs import ModelOutput\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\n\ndef set_seed(seed: int = 42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                try:\n                    parts.append(f\"{k.replace('eval_', '')} {float(v):.4f}\")\n                except Exception:\n                    pass\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    try:\n                        extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n                    except Exception:\n                        pass\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\nclass WeightedTokenCETrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n        self._warned_label_tiling = False\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n\n        if logits.size(0) != labels.size(0):\n            ngpu = torch.cuda.device_count()\n            if ngpu > 1 and logits.size(0) == labels.size(0) * ngpu:\n                labels = labels.repeat_interleave(ngpu, dim=0)\n                if not self._warned_label_tiling:\n                    print(f\"[Warning] DataParallel удвоил batch для logits. \"\n                          f\"Повторяем labels x{ngpu}. logits: {tuple(logits.shape)}, labels: {tuple(labels.shape)}\")\n                    self._warned_label_tiling = True\n            else:\n                raise ValueError(f\"Batch size mismatch: logits {tuple(logits.shape)} vs labels {tuple(labels.shape)}\")\n\n        loss_fct = torch.nn.CrossEntropyLoss(\n            weight=(self.class_weights.to(logits.device) if self.class_weights is not None else None),\n            ignore_index=-100,\n        )\n        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n\nclass TokenClassification:\n    \"\"\"\n    Класс-обёртка для обучения и инференса моделей токен-классификации (NER/POS и т.д.)\n    на базе Hugging Face Transformers. Поддерживает обучение со «скользящим окном»,\n    выравнивание меток слов с субтокенами, расчёт весов классов по словам, раннюю остановку,\n    агрегирование логитов по перекрывающимся окнам и извлечение эмбеддингов слов.\n    \"\"\"\n\n    def __init__(\n        self,\n        checkpoint: str,\n        label2id: Dict[str, int],\n        tokens_column_name: str,\n        tags_column_name: str\n    ):\n        \"\"\"\n        Инициализирует модель, токенайзер и инфраструктуру для обучения/инференса.\n\n        :param checkpoint: имя/путь модели в Hugging Face (например, 'bert-base-cased').\n        :param label2id: словарь отображения строковых меток в целочисленные id.\n        :param tokens_column_name: имя колонки DataFrame с токенами (словами).\n        :param tags_column_name: имя колонки DataFrame с метками (строки или уже id).\n        :return: None\n        :raises: ValueError при некорректных входных параметрах (например, пустой label2id).\n        \"\"\"\n        self.id2label = {v: k for k, v in label2id.items()}\n        self.label2id = label2id\n\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            checkpoint,\n            num_labels=len(self.id2label),\n            id2label=self.id2label,\n            label2id=self.label2id,\n            ignore_mismatched_sizes=True\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.tokens_column_name = tokens_column_name\n        self.tags_column_name = tags_column_name\n\n        # Градиентный чекпоинтинг (если поддерживается моделью)\n        try:\n            self.model.gradient_checkpointing_enable()\n        except Exception:\n            pass\n\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.data_collator = DataCollatorForTokenClassification(tokenizer=self.tokenizer)\n        self.progress_callback: Optional[TrainerCallback] = None\n\n    # ------------------------------\n    # Вспомогательные хелперы\n    # ------------------------------\n    @staticmethod\n    def _labels_are_strings(labels_col_list) -> bool:\n        \"\"\"\n        Определяет, представлены ли метки строками (а не id).\n\n        :param labels_col_list: итерируемая коллекция списков меток (по документам).\n        :return: True, если метки строковые; False, если уже id или все пусто.\n        :raises: None\n        \"\"\"\n        for tags in labels_col_list:\n            if isinstance(tags, (list, tuple)) and len(tags) > 0:\n                return isinstance(tags[0], str)\n        return False\n\n    def _label_to_id(self, tag: str) -> int:\n        \"\"\"\n        Преобразует строковую метку в id согласно self.label2id.\n\n        :param tag: строковая метка.\n        :return: целочисленный id метки.\n        :raises ValueError: если метка отсутствует в label2id.\n        \"\"\"\n        if tag not in self.label2id:\n            raise ValueError(\n                f\"Unknown label encountered: '{tag}'. \"\n                f\"Known labels: {sorted(self.label2id.keys())}\"\n            )\n        return int(self.label2id[tag])\n\n    def _assert_tokens_labels_same_len(self, tokens_seq, labels_seq):\n        \"\"\"\n        Проверяет совпадение длины списков токенов и меток для каждого документа.\n\n        :param tokens_seq: iterable со списками токенов (по документам).\n        :param labels_seq: iterable со списками меток (по документам).\n        :return: None\n        :raises ValueError: если типы неверны или длины не совпадают.\n        \"\"\"\n        for i, (toks, labs) in enumerate(zip(tokens_seq, labels_seq)):\n            if not isinstance(toks, (list, tuple)) or not isinstance(labs, (list, tuple)):\n                raise ValueError(\n                    f\"Row {i}: tokens/labels must be lists, got \"\n                    f\"{type(toks).__name__} and {type(labs).__name__}\"\n                )\n            if len(toks) != len(labs):\n                raise ValueError(\n                    f\"Row {i}: tokens and labels length mismatch: \"\n                    f\"{len(toks)} vs {len(labs)}\"\n                )\n\n    @staticmethod\n    def _to_token_list(obj):\n        \"\"\"\n        Приводит значение ячейки к списку токенов.\n\n        :param obj: значение колонки токенов (list/tuple/np.ndarray/None/другое).\n        :return: список токенов (или пустой список при неподдерживаемом типе).\n        :raises: None\n        \"\"\"\n        if obj is None:\n            return []\n        if isinstance(obj, (list, tuple)):\n            return list(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return []\n\n    def _get_effective_max_length(self) -> int:\n        \"\"\"\n        Возвращает безопасную максимальную длину контекста:\n        min(model.config.max_position_embeddings, tokenizer.model_max_length),\n        игнорируя «бесконечные» значения токенайзера.\n\n        :return: целочисленное значение безопасной максимальной длины.\n        :raises: None\n        \"\"\"\n        m_conf = int(getattr(self.model.config, \"max_position_embeddings\", 512) or 512)\n        m_tok = int(getattr(self.tokenizer, \"model_max_length\", 512) or 512)\n        if m_tok > 100000:\n            return m_conf\n        return min(m_conf, m_tok)\n\n    @staticmethod\n    def _sanitize_stride(stride: int, max_length: int) -> int:\n        \"\"\"\n        Ограничивает stride до [0, max_length - 2], учитывая спецтокены.\n\n        :param stride: желаемый страйд перекрытия.\n        :param max_length: безопасная максимальная длина контекста.\n        :return: целочисленное безопасное значение stride.\n        :raises: None\n        \"\"\"\n        stride = int(max(0, stride))\n        return int(min(stride, max(0, max_length - 2)))\n\n    # ------------------------------\n    # Алгоритмика\n    # ------------------------------\n    @staticmethod\n    def _align_labels_with_word_ids(labels_ids: List[int], word_ids: List[Optional[int]]) -> List[int]:\n        \"\"\"\n        Выравнивает метки слов по субтокенам: первый субтокен слова получает метку,\n        последующие субтокены — -100 (игнор в CrossEntropy).\n\n        :param labels_ids: список меток по словам (id), длина = числу слов.\n        :param word_ids: список индексов слов для каждого субтокена (tokenizer.word_ids()).\n        :return: список меток по длине субтокенов, с -100 для игнорируемых позиций.\n        :raises: None\n        \"\"\"\n        new_labels = []\n        prev_word_id = None\n        L = len(labels_ids)\n        for wid in word_ids:\n            if wid is None or wid < 0 or wid >= L:\n                new_labels.append(-100)\n            else:\n                if wid != prev_word_id:\n                    new_labels.append(labels_ids[wid])\n                else:\n                    new_labels.append(-100)\n            prev_word_id = wid\n        return new_labels\n\n    def _tokenize_and_align_chunk(\n        self,\n        docs_tokens: List[List[str]],\n        docs_labels_ids: List[List[int]],\n        max_length: int,\n        stride: int\n    ) -> Dataset:\n        \"\"\"\n        Токенизирует документы с разбиением на окна и выравниванием меток.\n\n        :param docs_tokens: списки токенов по документам.\n        :param docs_labels_ids: списки меток (id) по документам.\n        :param max_length: безопасная максимальная длина контекста.\n        :param stride: перекрытие между окнами.\n        :return: HF Dataset с полями input_ids, attention_mask, labels.\n        :raises: None\n        \"\"\"\n        enc = self.tokenizer(\n            docs_tokens,\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True\n        )\n        mapping = enc.pop(\"overflow_to_sample_mapping\")\n        num_chunks = len(enc[\"input_ids\"])\n\n        all_labels = []\n        for i in range(num_chunks):\n            doc_idx = int(mapping[i])\n            word_ids = enc.word_ids(batch_index=i)\n            aligned = self._align_labels_with_word_ids(docs_labels_ids[doc_idx], word_ids)\n            all_labels.append(aligned)\n\n        return Dataset.from_dict({\n            \"input_ids\": enc[\"input_ids\"],\n            \"attention_mask\": enc[\"attention_mask\"],\n            \"labels\": all_labels\n        })\n\n    def _count_total_chunks(\n        self,\n        docs_tokens: List[List[str]],\n        max_length: int,\n        stride: int,\n        batch_docs: int = 64\n    ) -> int:\n        \"\"\"\n        Подсчитывает число чанков (окон) после токенизации набора документов.\n\n        :param docs_tokens: списки токенов по документам.\n        :param max_length: безопасная максимальная длина контекста.\n        :param stride: перекрытие между окнами.\n        :param batch_docs: размер батча документов при токенизации.\n        :return: общее число чанков (int).\n        :raises: None\n        \"\"\"\n        total = 0\n        for i in range(0, len(docs_tokens), batch_docs):\n            batch = docs_tokens[i:i + batch_docs]\n            enc = self.tokenizer(\n                batch,\n                is_split_into_words=True,\n                return_overflowing_tokens=True,\n                max_length=max_length,\n                stride=stride,\n                truncation=True\n            )\n            total += len(enc[\"input_ids\"])\n        return total\n\n    def _compute_class_weights_over_words(self, docs_labels_ids: List[List[int]]) -> np.ndarray:\n        \"\"\"\n        Считает веса классов по словам (без влияния overlap-окон).\n\n        :param docs_labels_ids: списки меток (id) по документам.\n        :return: массив весов классов shape=(num_labels,), dtype=float32.\n        :raises: None\n        \"\"\"\n        num_labels = len(self.id2label)\n        counts = np.zeros(num_labels, dtype=np.int64)\n        for labs in docs_labels_ids:\n            if isinstance(labs, (list, tuple)) and len(labs) > 0:\n                arr = np.asarray(labs, dtype=np.int64)\n                arr = arr[(arr >= 0) & (arr < num_labels)]\n                if arr.size > 0:\n                    counts += np.bincount(arr, minlength=num_labels)\n        N = counts.sum()\n        weights = np.zeros(num_labels, dtype=np.float32)\n        if N > 0:\n            nonzero = counts > 0\n            weights[nonzero] = N / (num_labels * counts[nonzero].astype(np.float32))\n        return weights\n\n    @staticmethod\n    def _normalize_clip_weights(w: np.ndarray, clip: float = 5.0) -> np.ndarray:\n        \"\"\"\n        Нормирует и клипует веса классов: клип сверху до clip и нормировка\n        положительных весов к среднему ~1.0.\n\n        :param w: исходные веса классов.\n        :param clip: верхняя граница клипа (None/<=0 — без клипа).\n        :return: нормированные веса dtype=float32.\n        :raises: None\n        \"\"\"\n        w = np.asarray(w, dtype=np.float32)\n        if clip is not None and clip > 0:\n            w = np.minimum(w, clip)\n        mask = w > 0\n        mean = float(np.mean(w[mask])) if np.any(mask) else 1.0\n        if mean > 0:\n            w = w / mean\n        return w\n\n    def _setup_compute_metrics(self):\n        \"\"\"\n        Создаёт и сохраняет функцию метрик для seqeval (self.compute_metrics).\n\n        Метрики:\n        - precision/recall/f1/accuracy — агрегированные;\n        - f1_{entity} — по каждой сущности.\n\n        :return: None\n        :raises: None\n        \"\"\"\n        metric = evaluate.load(\"seqeval\")\n\n        def compute_seqeval_metrics(p):\n            if isinstance(p, (tuple, list)):\n                predictions, labels = p\n            else:\n                predictions, labels = p.predictions, p.label_ids\n\n            predictions = np.argmax(predictions, axis=2)\n\n            true_predictions = [\n                [self.id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            true_labels = [\n                [self.id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n\n            results = metric.compute(predictions=true_predictions, references=true_labels)\n\n            out = {\n                \"precision\": results.get(\"overall_precision\", 0.0),\n                \"recall\": results.get(\"overall_recall\", 0.0),\n                \"f1\": results.get(\"overall_f1\", 0.0),\n                \"accuracy\": results.get(\"overall_accuracy\", 0.0),\n            }\n            for ent, vals in results.items():\n                if isinstance(vals, dict) and \"f1\" in vals:\n                    out[f\"f1_{ent}\"] = float(vals[\"f1\"])\n            return out\n\n        self.compute_metrics = compute_seqeval_metrics\n\n    def _prepare_dataset_with_sliding_window(self, df: pd.DataFrame, max_length: int, stride: int) -> Dataset:\n        \"\"\"\n        Готовит HF Dataset для оценки/валидации со «скользящим окном».\n\n        :param df: DataFrame с колонками токенов и меток.\n        :param max_length: безопасная максимальная длина контекста.\n        :param stride: перекрытие между окнами.\n        :return: HF Dataset с полями input_ids, attention_mask, labels.\n        :raises ValueError: при неверных типах или несовпадении длины токенов и меток.\n        \"\"\"\n        docs_tokens = df[self.tokens_column_name].tolist()\n        docs_labels = df[self.tags_column_name].tolist()\n\n        if self._labels_are_strings(docs_labels):\n            docs_labels = [[self._label_to_id(tag) for tag in tags] for tags in docs_labels]\n\n        filtered_tokens, filtered_labels = [], []\n        for i, (toks, labs) in enumerate(zip(docs_tokens, docs_labels)):\n            if not isinstance(toks, (list, tuple)) or not isinstance(labs, (list, tuple)):\n                raise ValueError(\n                    f\"Row {i}: tokens/labels must be lists, got \"\n                    f\"{type(toks).__name__} and {type(labs).__name__}\"\n                )\n            if len(toks) == 0 and len(labs) == 0:\n                continue\n            if len(toks) != len(labs):\n                raise ValueError(\n                    f\"Row {i}: tokens and labels length mismatch: \"\n                    f\"{len(toks)} vs {len(labs)}\"\n                )\n            filtered_tokens.append(list(toks))\n            filtered_labels.append(list(labs))\n\n        if len(filtered_tokens) == 0:\n            return Dataset.from_dict({\"input_ids\": [], \"attention_mask\": [], \"labels\": []})\n\n        return self._tokenize_and_align_chunk(filtered_tokens, filtered_labels, max_length, stride)\n\n    # ------------------------------\n    # Обучение\n    # ------------------------------\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        test_size: float = 0.2,\n        learning_rate: float = 2e-5,\n        fp16: bool = True,\n        stride: int = 128,\n        logging_steps: int = 50,\n        eval_steps: int = 100,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        fit_chunk_size_docs: Optional[int] = None,\n        early_stopping_patience: Optional[int] = 3,\n        early_stopping_threshold: float = 0.0,\n    ):\n        \"\"\"\n        Обучает модель токен-классификации на данных.\n\n        :param train_data: DataFrame с колонками токенов и меток.\n        :param epochs: число эпох (проходов) по обучающему набору документов.\n        :param per_device_train_batch_size: размер батча на устройство при обучении.\n        :param gradient_accumulation_steps: число шагов аккумуляции градиента.\n        :param test_size: доля/размер валидации; при слишком малом наборе eval отключается автоматически.\n        :param learning_rate: скорость обучения (LR).\n        :param fp16: использовать ли fp16 (если bf16 не используется и доступен CUDA).\n        :param stride: перекрытие между окнами для токенизации длинных документов.\n        :param logging_steps: частота логирования в шагах.\n        :param eval_steps: частота валидации/сохранения (если есть eval).\n        :param output_dir: директория для артефактов обучения.\n        :param seed: seed для воспроизводимости.\n        :param fit_chunk_size_docs: сколько документов обучать за один «кусок» перед сменой train_dataset (None = все).\n        :param early_stopping_patience: количество подряд неулучшающихся точек валидации до остановки;\n                                       если None или <= 0 — ранняя остановка не используется.\n        :param early_stopping_threshold: минимальное относительное улучшение метрики, требуемое для сброса счётчика patience.\n        :return: self (для чейнинга).\n        :raises ValueError: при несогласованных данных (тип/длина токенов и меток).\n        \"\"\"\n        set_seed(seed)\n\n        max_length = self._get_effective_max_length()\n        stride = self._sanitize_stride(stride, max_length)\n\n        df_all = train_data.copy()\n        if self._labels_are_strings(df_all[self.tags_column_name].tolist()):\n            df_all[self.tags_column_name] = df_all[self.tags_column_name].apply(\n                lambda tags: [self._label_to_id(tag) for tag in tags]\n            )\n\n        self._assert_tokens_labels_same_len(\n            df_all[self.tokens_column_name].tolist(),\n            df_all[self.tags_column_name].tolist()\n        )\n\n        # Робастный train/val split\n        n_total = len(df_all)\n        use_eval = False\n        test_size_abs = 0\n        if n_total >= 2 and test_size and float(test_size) > 0:\n            if isinstance(test_size, float):\n                test_size_abs = int(round(n_total * float(test_size)))\n            else:\n                test_size_abs = int(test_size)\n            if test_size_abs <= 0:\n                test_size_abs = 1\n            if test_size_abs >= n_total:\n                test_size_abs = n_total - 1\n            use_eval = test_size_abs > 0\n\n        if use_eval:\n            df_train, df_eval = train_test_split(df_all, test_size=test_size_abs, random_state=seed, shuffle=True)\n        else:\n            df_train = df_all\n            df_eval = df_all.iloc[0:0]\n\n        eval_dataset = None\n        if len(df_eval) > 0:\n            eval_dataset = self._prepare_dataset_with_sliding_window(df_eval, max_length, stride)\n\n        train_docs_tokens = df_train[self.tokens_column_name].tolist()\n        train_docs_labels = df_train[self.tags_column_name].tolist()\n\n        class_weights = self._compute_class_weights_over_words(train_docs_labels)\n        class_weights = self._normalize_clip_weights(class_weights, clip=5.0)\n\n        self._setup_compute_metrics()\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\" if eval_dataset is not None else \"no\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\" if eval_dataset is not None else \"no\",\n            save_steps=eval_steps,\n            load_best_model_at_end=bool(eval_dataset is not None),\n            metric_for_best_model=\"eval_f1\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=bool(fp16 and torch.cuda.is_available()),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            disable_tqdm=True,\n            dataloader_pin_memory=True,\n            gradient_checkpointing=True,\n        )\n\n        data_collator = self.data_collator\n\n        def steps_for_size(n_samples: int, bsz: int, accum: int) -> int:\n            return max(0, math.ceil(math.ceil(n_samples / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(n_docs: int, chunk_docs: int):\n            for i in range(0, n_docs, chunk_docs):\n                yield slice(i, min(i + chunk_docs, n_docs))\n\n        n_docs = len(train_docs_tokens)\n        chunk_docs = int(fit_chunk_size_docs) if (fit_chunk_size_docs and fit_chunk_size_docs > 0) else n_docs\n\n        total_steps = 0\n        rng = np.random.default_rng(seed)\n        doc_indices = np.arange(n_docs)\n        for _ in range(epochs):\n            rng.shuffle(doc_indices)\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = doc_indices[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                n_samples = self._count_total_chunks(toks_chunk, max_length, stride, batch_docs=64)\n                total_steps += steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n\n        if n_docs > 0:\n            init_chunk_ds = self._tokenize_and_align_chunk(\n                [train_docs_tokens[0]], [train_docs_labels[0]], max_length, stride\n            )\n        else:\n            init_chunk_ds = Dataset.from_dict({\"input_ids\": [], \"attention_mask\": [], \"labels\": []})\n\n        self.trainer = WeightedTokenCETrainer(\n            model=self.model,\n            args=args,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics if eval_dataset is not None else None,\n            train_dataset=init_chunk_ds,\n            eval_dataset=eval_dataset,\n            tokenizer=self.tokenizer,\n            class_weights=class_weights\n        )\n        try:\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n\n        # Ранняя остановка (если есть eval и запрошена)\n        if eval_dataset is not None and (early_stopping_patience is not None) and (early_stopping_patience > 0):\n            early_cb = EarlyStoppingCallback(\n                early_stopping_patience=int(early_stopping_patience),\n                early_stopping_threshold=float(early_stopping_threshold),\n            )\n            self.trainer.add_callback(early_cb)\n\n        self.progress_callback = cb\n\n        steps_done = 0\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            order = np.arange(n_docs)\n            rng.shuffle(order)\n\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = order[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                labs_chunk = [train_docs_labels[i] for i in idx]\n\n                ds_chunk = self._tokenize_and_align_chunk(toks_chunk, labs_chunk, max_length, stride)\n                self.trainer.train_dataset = ds_chunk\n\n                n_samples = len(ds_chunk)\n                chunk_steps = steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                del ds_chunk\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n        return self\n\n    # ------------------------------\n    # Инференс\n    # ------------------------------\n    def _predict_single_document(self, tokens: List[str], stride: int) -> List[str]:\n        \"\"\"\n        Предсказывает метки для одного документа со «скользящим окном».\n\n        :param tokens: список слов (токенов) документа.\n        :param stride: перекрытие между окнами.\n        :return: список строковых меток той же длины, что и tokens.\n        :raises: None\n        \"\"\"\n        if not isinstance(tokens, (list, tuple)) or len(tokens) == 0:\n            return []\n\n        max_length = self._get_effective_max_length()\n        stride = self._sanitize_stride(stride, max_length)\n\n        tokenized_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n        )\n        tokenized_inputs.pop(\"overflow_to_sample_mapping\", None)\n\n        default_id = int(min(self.id2label.keys())) if len(self.id2label) else 0\n        default_label = self.id2label.get(default_id, str(default_id))\n\n        if not isinstance(tokenized_inputs.get(\"input_ids\", None), list) or len(tokenized_inputs[\"input_ids\"]) == 0:\n            return [default_label] * len(tokens)\n\n        chunk_dataset = Dataset.from_dict(tokenized_inputs)\n        outputs = self.trainer.predict(chunk_dataset)\n\n        if hasattr(outputs, \"predictions\"):\n            preds = outputs.predictions\n        else:\n            preds = outputs[\"predictions\"]\n\n        num_original_words = len(tokens)\n\n        # Основной путь: 3D логиты (num_chunks, seq_len, num_labels)\n        if isinstance(preds, np.ndarray) and preds.ndim == 3:\n            num_labels = preds.shape[-1]\n            word_logits = np.zeros((num_original_words, num_labels), dtype=np.float32)\n            word_counts = np.zeros((num_original_words,), dtype=np.float32)\n\n            for i in range(preds.shape[0]):\n                chunk_logits = preds[i]\n                try:\n                    chunk_word_ids = tokenized_inputs.word_ids(batch_index=i)\n                except Exception:\n                    continue\n                if chunk_word_ids is None:\n                    continue\n\n                for token_pos, word_id in enumerate(chunk_word_ids):\n                    if word_id is None:\n                        continue\n                    if token_pos == 0 or chunk_word_ids[token_pos - 1] != word_id:\n                        if 0 <= word_id < num_original_words:\n                            word_logits[word_id] += chunk_logits[token_pos]\n                            word_counts[word_id] += 1.0\n\n            mask = word_counts > 0\n            if np.any(mask):\n                word_logits[mask] /= word_counts[mask, None]\n\n            pred_ids = np.full(num_original_words, default_id, dtype=np.int32)\n            if np.any(mask):\n                pred_ids[mask] = word_logits[mask].argmax(-1)\n\n            filled = [self.id2label.get(int(x), str(int(x))) for x in pred_ids]\n            return filled\n\n        # Fallback: если preds не 3D\n        if isinstance(preds, np.ndarray):\n            if preds.ndim == 2:\n                predictions = preds\n            elif preds.ndim == 1:\n                predictions = preds[None, :]\n            else:\n                predictions = preds.reshape(len(tokenized_inputs[\"input_ids\"]), -1)\n        else:\n            predictions = np.asarray(preds)\n\n        final_predictions = np.full(num_original_words, -1, dtype=np.int32)\n        num_chunks = predictions.shape[0]\n        for i in range(num_chunks):\n            chunk_preds = predictions[i]\n            try:\n                chunk_word_ids = tokenized_inputs.word_ids(batch_index=i)\n            except Exception:\n                continue\n            if chunk_word_ids is None:\n                continue\n\n            chunk_len = len(chunk_preds)\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if token_pos >= chunk_len:\n                    break\n                if word_id is None:\n                    continue\n                if 0 <= word_id < num_original_words and final_predictions[word_id] == -1:\n                    final_predictions[word_id] = int(chunk_preds[token_pos])\n\n        filled = [\n            self.id2label.get(pid, default_label) if pid != -1 else default_label\n            for pid in final_predictions\n        ]\n        return filled\n\n    def predict(self, df: pd.DataFrame, stride: int = 128) -> List[List[str]]:\n        \"\"\"\n        Предсказывает метки для всех документов из DataFrame.\n\n        :param df: DataFrame с колонкой токенов.\n        :param stride: перекрытие между окнами.\n        :return: список документов, каждый — список строковых меток по словам.\n        :raises RuntimeError: если модель не обучена и документы непустые.\n        \"\"\"\n        all_final_labels = []\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Предсказание (sliding window)\"):\n            try:\n                original_tokens = row.get(self.tokens_column_name, None)\n            except Exception:\n                original_tokens = None\n\n            tokens = self._to_token_list(original_tokens)\n\n            if len(tokens) == 0:\n                all_final_labels.append([])\n                continue\n\n            if self.trainer is None or self.trainer.model is None:\n                raise RuntimeError(\"Модель не обучена. Вызовите .fit() перед .predict() для непустых документов.\")\n\n            document_labels = self._predict_single_document(tokens, stride)\n            all_final_labels.append(document_labels)\n        return all_final_labels\n\n    # ------------------------------\n    # Эмбеддинги\n    # ------------------------------\n    def _get_embeddings_single_document(self, tokens: List[str], stride: int, device: torch.device) -> np.ndarray:\n        \"\"\"\n        Извлекает эмбеддинги слов для одного документа.\n\n        :param tokens: список слов документа.\n        :param stride: перекрытие между окнами.\n        :param device: устройство модели (CPU/GPU).\n        :return: массив формы (num_words, hidden_size), dtype=float32.\n        :raises: None\n        \"\"\"\n        max_length = self._get_effective_max_length()\n        stride = self._sanitize_stride(stride, max_length)\n        num_original_words = len(tokens)\n\n        chunk_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        chunk_inputs.pop(\"overflow_to_sample_mapping\")\n\n        with torch.no_grad():\n            base_model = getattr(self.trainer.model, self.trainer.model.base_model_prefix)\n            outputs = base_model(**chunk_inputs)\n\n        chunk_embeddings = outputs.last_hidden_state\n\n        hidden_size = int(self.model.config.hidden_size)\n        final_word_embeddings = torch.zeros(num_original_words, hidden_size, device=device)\n        word_counts = torch.zeros(num_original_words, device=device)\n\n        for i in range(len(chunk_embeddings)):\n            chunk_embeds = chunk_embeddings[i]\n            chunk_word_ids = chunk_inputs.word_ids(batch_index=i)\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if word_id is not None:\n                    final_word_embeddings[word_id] += chunk_embeds[token_pos]\n                    if token_pos == 0 or chunk_word_ids[token_pos - 1] != word_id:\n                        word_counts[word_id] += 1\n\n        average_embeddings = final_word_embeddings / (word_counts.unsqueeze(1) + 1e-8)\n        return average_embeddings.detach().cpu().numpy()\n\n    def get_embeddings(self, df: pd.DataFrame, stride: int = 128) -> List[np.ndarray]:\n        \"\"\"\n        Извлекает эмбеддинги слов для каждого документа в DataFrame.\n\n        :param df: DataFrame с колонкой токенов.\n        :param stride: перекрытие между окнами.\n        :return: список массивов эмбеддингов, по одному на документ (num_words, hidden_size).\n        :raises RuntimeError: если модель не обучена.\n        \"\"\"\n        if self.trainer is None or self.trainer.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        self.trainer.model.eval()\n        device = self.trainer.model.device\n        all_final_embeddings = []\n\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Генерация эмбеддингов (sliding window)\"):\n            original_tokens = row[self.tokens_column_name]\n            if not original_tokens:\n                all_final_embeddings.append(np.zeros((0, int(self.model.config.hidden_size)), dtype=np.float32))\n                continue\n\n            document_embeddings = self._get_embeddings_single_document(original_tokens, stride, device)\n            all_final_embeddings.append(document_embeddings)\n\n        return all_final_embeddings","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Данные: строковые метки (будут конвертированы в id внутри .fit)\ntokens_col, tags_col = \"tokens\", \"tags\"\nlabel2id = {\n    \"O\": 0,\n    \"B-PER\": 1, \"I-PER\": 2,\n    \"B-LOC\": 3, \"I-LOC\": 4,\n    \"B-ORG\": 5, \"I-ORG\": 6,\n}\n\ndf_train = pd.DataFrame([\n    {tokens_col: [\"John\", \"Doe\", \"lives\", \"in\", \"Berlin\"], tags_col: [\"B-PER\",\"I-PER\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"Mary\", \"works\", \"at\", \"Google\"], tags_col: [\"B-PER\",\"O\",\"O\",\"B-ORG\"]},\n    {tokens_col: [\"Alice\", \"is\", \"from\", \"Paris\"], tags_col: [\"B-PER\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"IBM\", \"is\", \"in\", \"Armonk\"], tags_col: [\"B-ORG\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"Bob\", \"moved\", \"to\", \"London\"],  tags_col: [\"B-PER\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"Google\", \"is\", \"in\", \"California\"], tags_col: [\"B-ORG\",\"O\",\"O\",\"B-LOC\"]},\n])\n\n# Инициализация (минимум, что требует класс)\nCKPT = \"prajjwal1/bert-tiny\"\ntc = TokenClassification(\n    checkpoint=CKPT,\n    label2id=label2id,\n    tokens_column_name=tokens_col,\n    tags_column_name=tags_col\n)\n\n# Обучение с максимальной параметризацией\ntc.fit(\n    train_data=df_train,\n    epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    test_size=0.33,             # включаем валидацию\n    learning_rate=3e-5,\n    fp16=True,                  # если есть CUDA — включит fp16\n    stride=64,                  # скользящее окно\n    logging_steps=1,\n    eval_steps=2,\n    output_dir=\"./tokcls_max_param\",\n    seed=123,\n    fit_chunk_size_docs=2,      # обучаемся «кусками» по 2 документа\n    early_stopping_patience=2,  # ранняя остановка после 2 неулучшений\n    early_stopping_threshold=0.0,\n)\n\n# Метрики от Trainer (включая per-entity F1: eval_f1_PER/LOC/ORG и т.д., если встретились)\nmetrics = tc.trainer.evaluate()\nprint(\"Eval metrics (subset):\", {k: float(v) for k, v in metrics.items() if isinstance(v, (int, float))})\n\n# Предсказание на части данных (с отдельным stride на инференсе)\ndf_infer = df_train.iloc[:3]\npreds = tc.predict(df_infer, stride=32)\nfor i, (tokens, pred) in enumerate(zip(df_infer[tokens_col], preds), 1):\n    print(f\"Doc {i}:\")\n    print(list(zip(tokens, pred)))\n\n# Эмбеддинги слов (каждый документ -> массив [num_words, hidden_size])\nembs = tc.get_embeddings(df_infer, stride=32)\nprint(\"Embeddings shapes:\", [e.shape for e in embs])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntokens_col, tags_col = \"tokens\", \"tags\"\nlabel2id = {\"O\": 0, \"B-PER\": 1}  # минимальный набор меток\n\n# Данные уже в id (минимальная разметка)\ndf_small = pd.DataFrame([\n    {tokens_col: [\"John\", \"works\"], tags_col: [1, 0]},  # [\"B-PER\",\"O\"]\n    {tokens_col: [\"Mary\", \"smiles\"], tags_col: [1, 0]},  # [\"B-PER\",\"O\"]\n])\n\n# Инициализация\nCKPT = \"prajjwal1/bert-tiny\"\ntc = TokenClassification(\n    checkpoint=CKPT,\n    label2id=label2id,\n    tokens_column_name=tokens_col,\n    tags_column_name=tags_col\n)\n\n# Обучение — все параметры по умолчанию\ntc.fit(train_data=df_small)\n\n# Базовый предикт — тоже по умолчанию\npreds = tc.predict(df_small)\nprint(\"Preds:\", preds)\n\n# При необходимости — эмбеддинги (тоже с параметрами по умолчанию)\nembs = tc.get_embeddings(df_small)\nprint(\"Embeddings shape for doc 0:\", embs[0].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  pillow==11.1.0 \\\n  numpy==1.26.4 \\\n  pandas==2.2.3 \\\n  tqdm==4.67.1 \\\n  transformers==4.51.3 \\\n  evaluate==0.4.5 \\\n  wav2clip==0.1.0 \\\n  torch==2.6.0+cu124 \\\n  torchaudio==2.6.0+cu124\n\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport math\nimport random\nimport gc\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, Generator, List, Optional, Union\n\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers.trainer_callback import TrainerCallback, PrinterCallback, EarlyStoppingCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport evaluate\n\n# =========================\n# Утилиты\n# =========================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef to_pil(x: Union[str, np.ndarray, 'Image.Image']) -> 'Image.Image':\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\ndef safe_load(component_cls, checkpoint: str, local_cache_dir: str = \"./model_cache\",\n              local_files_only: Optional[bool] = None, **kwargs):\n    if local_files_only is None:\n        local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n    name = getattr(component_cls, \"__name__\", \"\")\n    if \"Tokenizer\" in name:\n        kwargs.setdefault(\"use_fast\", True)\n    return component_cls.from_pretrained(\n        checkpoint, cache_dir=local_cache_dir, local_files_only=local_files_only, **kwargs\n    )\n\n\n# =========================\n# Токенизатор батчевый\n# =========================\n\nclass BatchTokenizer:\n    def __init__(\n        self,\n        tokenizer,\n        max_length: int = 512,\n        cache_size: int = 10000,\n        batch_size: int = 256,\n        use_fast: bool = True,\n        device: str = \"cpu\"\n    ):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.use_fast = use_fast\n        self.device = device\n        self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n        self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n        if self.is_fast:\n            print(\"✓ Используется Fast Tokenizer\")\n\n    def _tokenize_single(self, text: str) -> tuple:\n        result = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        return tuple((k, v.squeeze(0).cpu().numpy()) for k, v in result.items())\n\n    def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n        if use_cache and len(texts) < 100:\n            results = [dict(self._cache(text)) for text in texts]\n            keys = results[0].keys()\n            batch_dict = {}\n            for key in keys:\n                dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                batch_dict[key] = torch.tensor(np.stack([r[key] for r in results]), dtype=dtype)\n            return batch_dict\n        else:\n            result = self.tokenizer(\n                texts,\n                padding=True,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n    def tokenize_dataset_lazy(\n        self,\n        texts: List[str],\n        batch_size: Optional[int] = None\n    ) -> Generator[Dict[str, torch.Tensor], None, None]:\n        batch_size = batch_size or self.batch_size\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            yield self.tokenize_batch(batch, use_cache=False)\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n\n# =========================\n# Универсальный датасет\n# =========================\n\nclass MultiComboDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        label2id: Dict[Any, int],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer: Optional[BatchTokenizer] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        pretokenize: bool = False,\n        pretokenize_batch_size: int = 256,\n        max_cache_size: int = 100000,\n        tokenizer_returns_tensors: bool = False,\n        cache_dir: Optional[str] = None\n    ):\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.label2id = label2id\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.text_tokenizer = text_tokenizer\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n        self.tokenized_cache: Dict[int, Dict[str, torch.Tensor]] = {}\n        self.cache_hits = 0\n        self.cache_misses = 0\n\n        if pretokenize and self.text_tokenizer and self.text_columns:\n            self._pretokenize_texts(\n                batch_size=pretokenize_batch_size,\n                max_cache_size=min(max_cache_size, len(self.df))\n            )\n\n    def _join_text(self, row: pd.Series) -> str:\n        sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n        return sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n    def _pretokenize_texts(self, batch_size: int, max_cache_size: int):\n        print(\"Предварительная токенизация текстов...\")\n        indices = list(range(min(len(self.df), max_cache_size)))\n        all_texts = [self._join_text(self.df.iloc[i]) for i in indices]\n\n        for start in range(0, len(indices), batch_size):\n            batch_idx = indices[start:start + batch_size]\n            batch_txt = all_texts[start:start + batch_size]\n            tokenized = self.text_tokenizer.tokenize_batch(batch_txt, use_cache=False)\n\n            for j, idx in enumerate(batch_idx):\n                token_dict: Dict[str, torch.Tensor] = {}\n                for k, v in tokenized.items():\n                    t = v[j]\n                    token_dict[k] = t.clone().long() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else t.clone()\n                self.tokenized_cache[idx] = token_dict\n\n        print(f\"✓ Предварительно токенизировано {len(self.tokenized_cache)} текстов\")\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        row = self.df.iloc[idx]\n        item: Dict[str, Any] = {}\n        if self.target_col in row:\n            item[\"labels\"] = int(self.label2id[row[self.target_col]])\n        else:\n            item[\"labels\"] = 0\n\n        if self.text_columns:\n            if idx in self.tokenized_cache:\n                cached = self.tokenized_cache[idx]\n                text_tokens: Dict[str, torch.Tensor] = {}\n                for k, v in cached.items():\n                    text_tokens[k] = v.long() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else v\n                item[\"text_tokens\"] = text_tokens\n                self.cache_hits += 1\n            elif self.text_tokenizer is not None:\n                text = self._join_text(row)\n                tokenized = self.text_tokenizer.tokenize_batch([text], use_cache=True)\n                text_tokens = {k: (v[0].long() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else v[0])\n                               for k, v in tokenized.items()}\n                item[\"text_tokens\"] = text_tokens\n                self.cache_misses += 1\n                if len(self.tokenized_cache) < 100000:\n                    self.tokenized_cache[idx] = {k: t.clone() for k, t in text_tokens.items()}\n            elif self.text_tokenizer_fn is not None:\n                text_data = {c: str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns}\n                result = self.text_tokenizer_fn(text_data, self.special_tokens)\n                if isinstance(result, dict) and 'input_ids' in result:\n                    item[\"text_tokens\"] = result\n                    self.tokenizer_returns_tensors = True\n                else:\n                    item[\"text\"] = result\n            else:\n                item[\"text\"] = self._join_text(row)\n\n        def _as_list(v):\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                return []\n            if isinstance(v, (list, tuple)):\n                return list(v)\n            return [v]\n\n        if self.image_columns:\n            imgs = []\n            for c in self.image_columns:\n                if c in row:\n                    imgs.extend(_as_list(row[c]))\n            item[\"images\"] = imgs\n\n        if self.audio_columns:\n            auds = []\n            for c in self.audio_columns:\n                if c in row:\n                    auds.extend(_as_list(row[c]))\n            item[\"audios\"] = auds\n\n        return item\n\n    def get_cache_stats(self) -> Dict[str, Any]:\n        total = self.cache_hits + self.cache_misses\n        hit_rate = self.cache_hits / total if total > 0 else 0.0\n        return {\n            \"cache_size\": len(self.tokenized_cache),\n            \"cache_hits\": self.cache_hits,\n            \"cache_misses\": self.cache_misses,\n            \"hit_rate\": hit_rate\n        }\n\n    def clear_cache(self):\n        self.tokenized_cache.clear()\n        self.cache_hits = 0\n        self.cache_misses = 0\n        if self.text_tokenizer:\n            self.text_tokenizer.clear_cache()\n\n\n# =========================\n# Универсальный бэкенд\n# =========================\n\nclass BaseBackend(nn.Module):\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n    text_tokenizer_fn: Optional[Callable] = None\n    batch_tokenizer: Optional[BatchTokenizer] = None\n    special_tokens: Dict[str, str] = {}\n    tokenizer_returns_tensors: bool = False\n    local_cache_dir: str = \"./model_cache\"\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        raise NotImplementedError\n\n    def freeze_all(self):\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n    def set_text_tokenizer(self, tokenizer_fn: Optional[Callable], special_tokens: Optional[Dict[str, str]] = None,\n                           returns_tensors: bool = False):\n        self.text_tokenizer_fn = tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = returns_tensors\n\n    def set_batch_tokenizer(self, tokenizer, max_length: int = 512,\n                            cache_size: int = 10000, batch_size: int = 256):\n        self.batch_tokenizer = BatchTokenizer(\n            tokenizer=tokenizer,\n            max_length=max_length,\n            cache_size=cache_size,\n            batch_size=batch_size,\n            use_fast=True\n        )\n\n\nclass UniversalMultiBackend(BaseBackend):\n    name = \"universal\"\n    \n    class _ParamDeviceProxy(nn.Module):\n        def __init__(self, base, device: torch.device):\n            super().__init__()\n            self.base = base if isinstance(base, nn.Module) else None\n            self._callable = base if not isinstance(base, nn.Module) else None\n            self._dummy = nn.Parameter(torch.empty(0), requires_grad=False)\n            with torch.no_grad():\n                self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n    \n        def forward(self, *args, **kwargs):\n            target = self.base if self.base is not None else self._callable\n            return target(*args, **kwargs)\n    \n        def to(self, device, *args, **kwargs):\n            self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n            return super().to(device, *args, **kwargs)\n    \n    def _preferred_device(self) -> torch.device:\n        if torch.cuda.is_available():\n            return torch.device(\"cuda\")\n        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n            return torch.device(\"mps\")\n        return torch.device(\"cpu\")\n    \n    def _wrap_if_parameterless(self, model, device: torch.device):\n        try:\n            it = model.parameters() if hasattr(model, \"parameters\") else iter(())\n            next(it)\n            return model\n        except StopIteration:\n            return self._ParamDeviceProxy(model, device)\n        except Exception:\n            return self._ParamDeviceProxy(model, device)\n\n    def __init__(\n        self,\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        freeze: bool = True,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        use_batch_tokenizer: bool = True,\n        tokenizer_cache_size: int = 10000,\n        tokenizer_batch_size: int = 256,\n        local_cache_dir: str = \"./model_cache\"\n    ):\n        super().__init__()\n        self.supported = set()\n        self.out_dim_per_modality = {}\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.tokenizer_batch_size = tokenizer_batch_size\n        self.local_cache_dir = local_cache_dir\n\n        self.text_model = None\n        self.text_processor = None\n        self.text_config = text_model_config or {}\n        if text_model_config:\n            self._init_text_model(text_model_config)\n            self.supported.add(\"text\")\n\n        self.image_model = None\n        self.image_processor = None\n        self.image_config = image_model_config or {}\n        if image_model_config:\n            self._init_image_model(image_model_config)\n            self.supported.add(\"image\")\n\n        self.audio_model = None\n        self.audio_processor = None\n        self.audio_config = audio_model_config or {}\n        if audio_model_config:\n            self._init_audio_model(audio_model_config)\n            self.supported.add(\"audio\")\n\n        if freeze:\n            self.freeze_all()\n\n    def _ensure_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        if x is None:\n            return None\n        if x.dim() == 1:\n            return x.unsqueeze(0)\n        if x.dim() > 2:\n            return x.view(x.size(0), -1)\n        return x\n\n    def _normalize_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        x = self._ensure_2d(x)\n        return F.normalize(x, dim=-1, eps=1e-12) if x is not None and x.numel() > 0 else x\n    \n    def _init_text_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoTokenizer, CLIPModel, CLIPTokenizer, ClapModel, ClapProcessor\n    \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n    \n        print(f\"Загрузка текстовой модели {checkpoint}...\")\n    \n        if model_type == 'clip':\n            self.text_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(CLIPTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.projection_dim\n        elif model_type == 'clap':\n            self.text_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            proc = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = getattr(proc, 'tokenizer', None) or safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = getattr(self.text_model.config, \"projection_dim\", 512)\n        else:\n            self.text_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.text_model  = self._wrap_if_parameterless(self.text_model, dev)\n    \n        if self.use_batch_tokenizer and self.text_processor is not None:\n            self.set_batch_tokenizer(\n                self.text_processor,\n                max_length=config.get('max_length', 512),\n                cache_size=self.tokenizer_cache_size,\n                batch_size=self.tokenizer_batch_size\n            )\n    \n        self.text_config['max_length'] = config.get('max_length', 512)\n        self.text_config['dim'] = dim\n        self.text_config['model_type'] = model_type\n        self.out_dim_per_modality['text'] = dim\n\n    def _init_image_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoImageProcessor, CLIPModel, CLIPImageProcessor\n    \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n    \n        print(f\"Загрузка визуальной модели {checkpoint}...\")\n    \n        if model_type == 'clip':\n            self.image_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(CLIPImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.projection_dim\n        else:\n            self.image_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(AutoImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.image_model = self._wrap_if_parameterless(self.image_model, dev)\n    \n        self.image_config['max_images'] = config.get('max_images', 1)\n        self.image_config['image_agg'] = config.get('image_agg', 'concat')\n        self.image_config['dim'] = dim\n        self.image_config['model_type'] = model_type\n    \n        self.out_dim_per_modality['image'] = (dim * self.image_config['max_images']) if self.image_config['image_agg'] == 'concat' else dim\n    \n    def _init_audio_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoProcessor, ClapModel, ClapProcessor\n    \n        model_type = config.get('model_type', 'auto').lower()\n        checkpoint = config.get('checkpoint', None)\n    \n        print(f\"Загрузка аудио модели (type={model_type})...\")\n    \n        if model_type == 'wav2clip':\n            import wav2clip as w2c\n            self._w2c = w2c\n    \n            w2c_model = None\n            if hasattr(w2c, \"get_model\"):\n                w2c_model = w2c.get_model()\n            elif hasattr(w2c, \"model\"):\n                m = w2c.model\n                w2c_model = m() if callable(m) else m\n            else:\n                raise RuntimeError(\"wav2clip не содержит get_model()/model. Обновите пакет wav2clip.\")\n    \n            self.audio_model = w2c_model\n    \n            try:\n                if isinstance(self.audio_model, torch.nn.Module) and torch.cuda.is_available():\n                    self.audio_model = self.audio_model.to(\"cuda\")\n            except Exception:\n                pass\n    \n            self.audio_processor = None\n            dim = 512\n            sr = config.get('sr', 16000)\n\n        elif model_type == 'clap':\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для CLAP\")\n            self.audio_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = getattr(self.audio_model.config, \"projection_dim\", 512)\n            sr = getattr(self.audio_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.audio_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n    \n        else:\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для аудио-моделей, кроме wav2clip\")\n            self.audio_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(AutoProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.audio_model.config.hidden_size\n            fe = getattr(self.audio_processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 16000) if fe is not None else 16000\n\n        dev = self._preferred_device()\n        self.audio_model = self._wrap_if_parameterless(self.audio_model, dev)\n    \n        self.audio_config['sr'] = config.get('sr', sr)\n        self.audio_config['max_audios'] = config.get('max_audios', 1)\n        self.audio_config['audio_agg'] = config.get('audio_agg', 'concat')\n        self.audio_config['dim'] = dim\n        self.audio_config['model_type'] = model_type\n    \n        self.out_dim_per_modality['audio'] = (\n            dim * self.audio_config['max_audios']\n            if self.audio_config['audio_agg'] == 'concat' else dim\n        )\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        labels = []\n        for b in batch:\n            labels.append(torch.tensor(b.get(\"labels\", 0), dtype=torch.long))\n        labels = torch.stack(labels)\n\n        backend_inputs: Dict[str, Any] = {}\n        batch_size = len(batch)\n\n        if self.text_model is not None:\n            if \"text_tokens\" in batch[0]:\n                text_inputs = {}\n                for key in batch[0][\"text_tokens\"].keys():\n                    if torch.is_tensor(batch[0][\"text_tokens\"][key]):\n                        text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n                    else:\n                        dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                        text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n                backend_inputs[\"text_inputs\"] = text_inputs\n            else:\n                texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n                if self.batch_tokenizer:\n                    text_inputs = self.batch_tokenizer.tokenize_batch(texts, use_cache=True)\n                else:\n                    text_inputs = self.text_processor(\n                        texts, padding=True, truncation=True,\n                        max_length=self.text_config.get('max_length', 512),\n                        return_tensors=\"pt\"\n                    )\n                backend_inputs[\"text_inputs\"] = {k: v for k, v in text_inputs.items()}\n\n        if self.image_model is not None:\n            images_lists = [b.get(\"images\", []) for b in batch]\n            flat_images, img_counts = [], []\n            for lst in images_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [img for img in lst if img is not None]\n                img_counts.append(len(lst))\n                for img in lst:\n                    flat_images.append(to_pil(img))\n\n            if len(flat_images) > 0:\n                img_proc = self.image_processor(images=flat_images, return_tensors=\"pt\")\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": img_proc[\"pixel_values\"]}\n            else:\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": None}\n\n            backend_inputs[\"image_counts\"] = torch.tensor(img_counts, dtype=torch.long)\n\n        if self.audio_model is not None:\n            audios_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios, aud_counts = [], []\n            for lst in audios_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_config['sr']))\n                    elif isinstance(a, np.ndarray):\n                        aa = np.asarray(a, dtype=np.float32)\n                        if aa.ndim > 1:\n                            aa = np.squeeze(aa)\n                        if aa.ndim > 1:\n                            aa = aa.reshape(-1)\n                        flat_audios.append(aa)\n            if self.audio_config.get('model_type') == 'wav2clip':\n                backend_inputs[\"audio_inputs\"] = {\"raw_audios\": flat_audios}\n            elif len(flat_audios) > 0:\n                if self.audio_config.get('model_type') == 'clap':\n                    aud_proc = self.audio_processor(\n                        audios=flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_features\": aud_proc[\"input_features\"]}\n                else:\n                    aud_proc = self.audio_processor(\n                        flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_values\": aud_proc[\"input_values\"]}\n            else:\n                backend_inputs[\"audio_inputs\"] = {\"input_features\": None, \"input_values\": None, \"raw_audios\": []}\n        \n            backend_inputs[\"audio_counts\"] = torch.tensor(aud_counts, dtype=torch.long)\n\n        backend_inputs[\"batch_size\"] = batch_size\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _aggregate_embeddings(\n        self,\n        embs: Optional[torch.Tensor],\n        counts: List[int],\n        max_k: int,\n        dim_hint: int,\n        agg_type: str,\n        batch_size: int,\n        device: torch.device\n    ) -> torch.Tensor:\n        if embs is None or (torch.is_tensor(embs) and embs.numel() == 0):\n            feat_dim = int(dim_hint) if dim_hint is not None else 0\n            out_dim = feat_dim * max_k if agg_type == 'concat' else feat_dim\n            return torch.zeros((batch_size, out_dim), device=device, dtype=torch.float32)\n    \n        if not torch.is_tensor(embs):\n            embs = torch.as_tensor(embs, device=device, dtype=torch.float32)\n        if embs.dim() == 1:\n            embs = embs.unsqueeze(0)\n        elif embs.dim() > 2:\n            embs = embs.view(embs.size(0), -1)\n    \n        N, D = embs.size()\n        out_dim = (D * max_k) if agg_type == 'concat' else D\n        out = torch.zeros((batch_size, out_dim), device=device, dtype=embs.dtype)\n    \n        offset = 0\n        for i, c in enumerate(counts):\n            if c <= 0 or offset >= N:\n                continue\n            take_n = min(c, N - offset)\n            sample = embs[offset:offset + take_n]\n            offset += take_n\n    \n            if agg_type == 'concat':\n                take = sample[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n    \n        return F.normalize(out, dim=-1, eps=1e-12) if out.size(1) > 0 else out\n\n    @torch.no_grad()\n    def _wav2clip_embed(self, arr: np.ndarray, device: torch.device) -> torch.Tensor:\n        arr = np.asarray(arr, dtype=np.float32)\n        if arr.ndim > 1:\n            arr = np.squeeze(arr)\n        if arr.ndim > 1:\n            arr = arr.reshape(-1)\n        if arr.size < 512:\n            arr = np.pad(arr, (0, 512 - arr.size), mode=\"constant\")\n\n        try:\n            emb = self._w2c.embed_audio(arr, self.audio_model)\n            emb = np.asarray(emb)\n        except Exception:\n            x = torch.from_numpy(arr).float().unsqueeze(0).to(device)\n            y = self.audio_model(x)\n            if isinstance(y, (tuple, list)):\n                y = y[0]\n            if torch.is_tensor(y):\n                if y.dim() == 2 and y.size(0) == 1:\n                    y = y.squeeze(0)\n                emb = y.detach().cpu().numpy()\n            else:\n                emb = np.asarray(y)\n\n        if emb.ndim > 1:\n            emb = emb.reshape(-1)\n        return torch.as_tensor(emb, device=device, dtype=torch.float32)\n    \n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        results: Dict[str, torch.Tensor] = {}\n        batch_size = int(backend_inputs.get(\"batch_size\", 1))\n\n        if self.text_model is not None and \"text_inputs\" in backend_inputs:\n            text_inputs = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n            if hasattr(self.text_model, \"get_text_features\"):\n                text_z = self.text_model.get_text_features(**text_inputs)\n            else:\n                outputs = self.text_model(**text_inputs)\n                text_z = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n            results[\"text\"] = self._normalize_2d(text_z)\n\n        if self.image_model is not None and \"image_inputs\" in backend_inputs:\n            pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n            counts = backend_inputs[\"image_counts\"].tolist()\n            total_images_needed = sum(counts)\n\n            img_flat = None\n            actual_img_dim = self.image_config.get(\"dim\", 768)\n\n            if pi is not None and pi.numel() > 0 and total_images_needed > 0:\n                pi = pi.to(device)\n                if pi.size(0) > total_images_needed:\n                    pi = pi[:total_images_needed]\n\n                if hasattr(self.image_model, \"get_image_features\"):\n                    img_flat = self.image_model.get_image_features(pixel_values=pi)\n                else:\n                    outputs = self.image_model(pixel_values=pi)\n                    img_flat = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state[:, 0]\n\n                img_flat = self._normalize_2d(img_flat)\n                actual_img_dim = img_flat.size(1) if img_flat is not None else actual_img_dim\n\n            img_z = self._aggregate_embeddings(\n                img_flat, counts,\n                self.image_config[\"max_images\"],\n                actual_img_dim,\n                self.image_config[\"image_agg\"],\n                len(counts),\n                device\n            )\n\n            if actual_img_dim != self.image_config.get(\"dim\"):\n                self.image_config[\"dim\"] = actual_img_dim\n                self.out_dim_per_modality[\"image\"] = (\n                    actual_img_dim * self.image_config[\"max_images\"]\n                    if self.image_config[\"image_agg\"] == \"concat\" else actual_img_dim\n                )\n\n            results[\"image\"] = img_z\n\n        if self.audio_model is not None and \"audio_inputs\" in backend_inputs:\n            counts = backend_inputs[\"audio_counts\"].tolist()\n            total_audios_needed = sum(counts)\n\n            aud_flat = None\n            actual_aud_dim = self.audio_config.get(\"dim\", 768)\n            model_type = self.audio_config.get(\"model_type\")\n\n            if total_audios_needed > 0:\n                if model_type == \"clap\":\n                    af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n                    if af is not None and af.numel() > 0:\n                        af = af.to(device)\n                        if af.size(0) > total_audios_needed:\n                            af = af[:total_audios_needed]\n                        with torch.cuda.amp.autocast(enabled=False):\n                            aud_flat = self.audio_model.get_audio_features(input_features=af.float())\n                        aud_flat = self._normalize_2d(aud_flat.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n                elif model_type == \"wav2clip\":\n                    raw_list = backend_inputs[\"audio_inputs\"].get(\"raw_audios\", [])\n                    if len(raw_list) > total_audios_needed:\n                        raw_list = raw_list[:total_audios_needed]\n                    if len(raw_list) > 0:\n                        embs = [self._wav2clip_embed(arr, device) for arr in raw_list]\n                        aud_flat = torch.stack(embs, dim=0)\n                        aud_flat = self._normalize_2d(aud_flat)\n                        actual_aud_dim = aud_flat.size(1)\n\n                else:\n                    av = backend_inputs[\"audio_inputs\"][\"input_values\"]\n                    if av is not None and av.numel() > 0:\n                        av = av.to(device)\n                        if av.size(0) > total_audios_needed:\n                            av = av[:total_audios_needed]\n                        av = av.clamp_(-1.0, 1.0)\n                        with torch.cuda.amp.autocast(enabled=False):\n                            outputs = self.audio_model(input_values=av.float())\n                            feats = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n                        aud_flat = self._normalize_2d(feats.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n            aud_z = self._aggregate_embeddings(\n                aud_flat, counts,\n                self.audio_config[\"max_audios\"],\n                actual_aud_dim,\n                self.audio_config[\"audio_agg\"],\n                len(counts),\n                device\n            )\n\n            if aud_flat is not None and actual_aud_dim != self.audio_config.get(\"dim\"):\n                self.audio_config[\"dim\"] = actual_aud_dim\n                self.out_dim_per_modality[\"audio\"] = (\n                    actual_aud_dim * self.audio_config[\"max_audios\"]\n                    if self.audio_config[\"audio_agg\"] == \"concat\" else actual_aud_dim\n                )\n\n            results[\"audio\"] = aud_z\n\n        if results:\n            bs_list = [v.size(0) for v in results.values()]\n            if len(set(bs_list)) != 1:\n                raise RuntimeError(f\"Inconsistent batch sizes across modalities: {bs_list}\")\n\n        return results\n\n\n# =========================\n# Классификатор\n# =========================\n\nclass SingleBackboneClassifier(nn.Module):\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_labels: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError('Для fusion=\"mean\" размеры модальностей должны совпадать')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                has_trainable = any(p.requires_grad for p in m.parameters()) if hasattr(m, \"parameters\") else False\n            except Exception:\n                has_trainable = False\n            if not has_trainable:\n                continue\n            try:\n                cfg = getattr(m, \"config\", None)\n                if cfg is not None and hasattr(cfg, \"use_cache\"):\n                    cfg.use_cache = False\n            except Exception:\n                pass\n            try:\n                if hasattr(m, \"gradient_checkpointing_enable\"):\n                    try:\n                        if gradient_checkpointing_kwargs is not None:\n                            m.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                        else:\n                            m.gradient_checkpointing_enable()\n                    except TypeError:\n                        m.gradient_checkpointing_enable()\n            except Exception:\n                pass\n\n    def gradient_checkpointing_disable(self):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                if hasattr(m, \"gradient_checkpointing_disable\"):\n                    m.gradient_checkpointing_disable()\n            except Exception:\n                pass\n    \n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        batch_size = None\n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() == 3:\n                    t = t.mean(dim=1)\n                elif t.dim() > 3:\n                    t = t.view(t.size(0), -1)\n                feats.append(t)\n                if batch_size is None:\n                    batch_size = t.size(0)\n        if batch_size is not None:\n            feats = [f[:batch_size] for f in feats]\n        if self.fusion == \"concat\":\n            out = torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            out = torch.stack(feats, dim=0).mean(dim=0)\n        return out\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        if not z:\n            raise ValueError(\"Backend не вернул эмбеддинги\")\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\n# =========================\n# Trainer с весами классов\n# =========================\n\nclass WeightedCETrainer(Trainer):\n    def __init__(self, *args, num_labels=None, train_labels=None, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        if class_weights is not None:\n            self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32)\n        elif train_labels is not None and num_labels is not None:\n            y = np.asarray(train_labels).astype(int)\n            counts = np.bincount(y, minlength=num_labels)\n            n = counts.sum()\n            w = np.zeros(num_labels, dtype=np.float32)\n            nz = counts > 0\n            w[nz] = n / (num_labels * counts[nz].astype(np.float32))\n            self.class_weights = torch.tensor(w, dtype=torch.float32)\n        else:\n            self.class_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        if logits.size(0) != labels.size(0):\n            raise ValueError(f\"Batch size mismatch: logits batch={logits.size(0)} vs labels batch={labels.size(0)}\")\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = F.cross_entropy(logits, labels.long(), weight=weight)\n        return (loss, outputs) if return_outputs else loss\n\n\n# =========================\n# Прогресс-логгер\n# =========================\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n        self.tqdm = tqdm\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            self.tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\n# =========================\n# Основной пайплайн\n# =========================\n\nclass SingleModelMultiComboClassification:\n    \"\"\"\n    Универсальный пайплайн для мульти-модальной классификации (text / image / audio) поверх моделей Hugging Face.\n    Поддерживает автоматический выбор бэкенда (CLIP/CLAP/Auto), батчевую токенизацию текста, кэширование,\n    чанковую тренировку, раннюю остановку, взвешивание классов, предсказание и извлечение эмбеддингов.\n\n    Основные возможности:\n    - Автоматическая сборка бэкенда: CLIP для связки text+image, CLAP для text+audio, либо произвольные Auto-модели.\n    - Работа с тремя модальностями: text, image, audio (любой поднабор).\n    - Батчевая токенизация текста с кэшем и опциональной предварительной токенизацией датасета.\n    - Чанковая тренировка очень больших датасетов без перегрузки памяти.\n    - Сбалансированная (взвешенная) кросс-энтропия на основе частот классов в тренировочных данных.\n    - Прогресс-бар, ранняя остановка и выбор лучшей модели по метрике.\n    - Предсказания и извлечение эмбеддингов (в том числе по модальностям).\n\n    :param modalities: Список используемых модальностей из {\"text\", \"image\", \"audio\"}.\n    :param num_labels: Число классов в задаче классификации.\n    :param target_column_name: Имя колонки с целевой меткой в DataFrame.\n    :param text_columns: Список текстовых колонок (используются, если выбрана модальность \"text\"). Значения будут\n                         конкатенированы через special_tokens[\"sep\"] при подготовке примеров.\n    :param image_columns: Список колонок с изображениями (пути к файлам, PIL.Image, np.ndarray или списки таких объектов),\n                          используется, если выбрана модальность \"image\".\n    :param audio_columns: Список колонок с аудио (пути к файлам или массивы np.ndarray; моно, float32),\n                          используется, если выбрана модальность \"audio\". Для чтения из файлов требуется torchaudio.\n    :param text_tokenizer_fn: Кастомная функция токенизации текста (если не используется встроенный BatchTokenizer).\n                              Сигнатура: fn(text_dict: Dict[str, str], special_tokens: Dict[str, str]) -> Union[Dict[str, Tensor], str].\n                              Если возвращает dict с ключами вроде 'input_ids', считается, что функция сразу возвращает тензоры токенов;\n                              иначе строку для последующей стандартной токенизации.\n    :param special_tokens: Спец. токены/разделители для подготовки текста. По умолчанию {\"sep\": \" [SEP] \"}.\n    :param tokenizer_returns_tensors: Флаг, сигнализирующий, что custom text_tokenizer_fn возвращает уже тензоры\n                                      (dict c 'input_ids', 'attention_mask' и т.д.). Влияет на коллатор.\n    :param backend: Режим сборки бэкенда. \"auto\" — подобрать оптимальные модели по модальностям;\n                    \"clip\" — CLIP для текста и изображений; \"clap\" — CLAP для текста и аудио; любое иное — ручные конфиги.\n    :param clip_checkpoint: Чекпойнт CLIP (используется при auto/clip), по умолчанию \"openai/clip-vit-base-patch32\".\n    :param clap_checkpoint: Чекпойнт CLAP (используется при auto/clap), по умолчанию \"laion/clap-htsat-unfused\".\n    :param text_model_config: Конфиг текстовой модели. Минимум: {\"checkpoint\": \"...\", \"model_type\": \"...\"}.\n                              Дополнительно: \"max_length\" и т.д. Примеры model_type: \"clip\", \"clap\", \"bert\", \"auto\".\n    :param image_model_config: Конфиг визуальной модели. Минимум: {\"checkpoint\": \"...\", \"model_type\": \"...\"}.\n                               Дополнительно: \"max_images\", \"image_agg\" (\"concat\"|\"mean\") и т.д. Примеры model_type: \"clip\", \"vit\", \"auto\".\n    :param audio_model_config: Конфиг аудио-модели. Минимум: {\"checkpoint\": \"...\", \"model_type\": \"...\"} (кроме \"wav2clip\").\n                               Дополнительно: \"max_audios\", \"audio_agg\" (\"concat\"|\"mean\"), \"sr\". Примеры model_type: \"clap\", \"wav2clip\", \"auto\".\n    :param fusion: Способ слияния модальностей в классификаторе: \"concat\" или \"mean\".\n                   При \"mean\" размеры эмбеддингов всех модальностей должны совпадать.\n    :param freeze_backbone: Если True — бэкенды заморожены (тренируется только классификационная \"голова\").\n    :param clip_max_length: Максимальная длина текста для CLIP-токенизатора (по умолчанию 77).\n    :param max_images_per_sample: Сколько изображений брать на сэмпл (усреднение или конкатенация задаются в image_model_config[\"image_agg\"]).\n    :param max_audios_per_sample: Сколько аудио брать на сэмпл (аналично image, параметр audio_model_config[\"audio_agg\"]).\n    :param use_batch_tokenizer: Использовать BatchTokenizer для текста (ускоряет токенизацию и кэширует результаты).\n    :param pretokenize_data: Предварительно токенизировать текст датасета (в памяти) для ускорения обучения/инференса.\n    :param pretokenize_batch_size: Батч-размер при предварительной токенизации.\n    :param tokenizer_cache_size: Размер LRU-кэша в BatchTokenizer.\n    :param max_pretokenize_samples: Максимум сэмплов для предварительной токенизации на чанк/датасет.\n    :param local_cache_dir: Локальная директория кэша моделей/процессоров HF.\n\n    :return: None\n\n    :raises ValueError: Если бэкенд не поддерживает выбранные модальности (внутренняя проверка соответствия).\n                        Также возможны ошибки конфигов, например fusion=\"mean\" при несовпадающих размерах эмбеддингов.\n    :raises OSError: Ошибки загрузки моделей/процессоров из Hugging Face Hub (сетевые/офлайн проблемы, отсутствующие чекпойнты).\n    :raises RuntimeError: Проблемы с устройством/драйвером (CUDA/MPS), несовместимость версий зависимостей и т.п.\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        num_labels: int,\n        target_column_name: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1,\n        use_batch_tokenizer: bool = True,\n        pretokenize_data: bool = True,\n        pretokenize_batch_size: int = 256,\n        tokenizer_cache_size: int = 10000,\n        max_pretokenize_samples: int = 100000,\n        local_cache_dir: str = \"./model_cache\"\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.num_labels = num_labels\n        self.target_column_name = target_column_name\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n\n        self.label2id: Optional[Dict[Any, int]] = None\n        self.id2label: Optional[Dict[int, str]] = None\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[PbarConsoleLogger] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        mods = set(self.modalities)\n        name = self.backend_name\n\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n                }\n                self.image_model_config = self.image_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n                }\n            elif mods == {\"text\", \"audio\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n                }\n                self.audio_model_config = self.audio_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n                }\n            else:\n                if \"text\" in mods and self.text_model_config is None:\n                    self.text_model_config = {'checkpoint': 'bert-base-multilingual-cased', 'model_type': 'bert', 'max_length': 512}\n                if \"image\" in mods and self.image_model_config is None:\n                    self.image_model_config = {'checkpoint': 'google/vit-base-patch16-224', 'model_type': 'vit', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'}\n                if \"audio\" in mods and self.audio_model_config is None:\n                    self.audio_model_config = {'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000}\n\n        elif name == \"clip\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n            }\n            self.image_model_config = self.image_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n            }\n        elif name == \"clap\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n            }\n            self.audio_model_config = self.audio_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n            }\n        else:\n            pass\n\n        self.backend = UniversalMultiBackend(\n            text_model_config=self.text_model_config if \"text\" in mods else None,\n            image_model_config=self.image_model_config if \"image\" in mods else None,\n            audio_model_config=self.audio_model_config if \"audio\" in mods else None,\n            freeze=self.freeze_backbone,\n            text_tokenizer_fn=self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors,\n            use_batch_tokenizer=self.use_batch_tokenizer,\n            tokenizer_cache_size=self.tokenizer_cache_size,\n            tokenizer_batch_size=self.pretokenize_batch_size,\n            local_cache_dir=self.local_cache_dir\n        )\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}\")\n\n    def _setup_metrics(self, metric_name: str):\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            metric = evaluate.load(\"f1\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids, average=\"weighted\")\n        elif metric_name == \"accuracy\":\n            metric = evaluate.load(\"accuracy\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids)\n        else:\n            raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"f1\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1,\n        gradient_checkpointing: bool = False,\n        fit_chunk_size: Optional[int] = None,\n        clear_cache_every_n_chunks: int = 10,\n        early_stopping_patience: Optional[int] = 3,\n        early_stopping_threshold: float = 0.0\n    ):\n        \"\"\"\n        Обучает классификатор на заданном DataFrame с поддержкой валидации, ранней остановки и чанковой тренировки.\n        Внутренне использует WeightedCETrainer (кросс-энтропия с весами классов по обратной частоте в train_data),\n        логгер прогресса и, при необходимости, предварительную токенизацию батчей текста.\n    \n        Если test_data не передан, train_data разделяется на train/eval по test_size (стратификации нет).\n        При очень больших датасетах обучение проводится чанками (fit_chunk_size), чтобы ограничить потребление памяти.\n    \n        :param train_data: Обучающий DataFrame. Должен содержать столбец target_column_name, а также столбцы по выбранным модальностям:\n                           - text_columns для \"text\" (строки, допускаются NaN),\n                           - image_columns для \"image\" (строки путей к файлам, PIL.Image, np.ndarray или списки этих типов),\n                           - audio_columns для \"audio\" (пути к аудиофайлам или np.ndarray моно-сигнала; для путей требуется torchaudio).\n        :param epochs: Количество эпох обучения.\n        :param test_size: Доля данных на валидацию, если test_data не задан.\n        :param test_data: Отдельный DataFrame для валидации. Если указан, параметр test_size игнорируется.\n        :param per_device_train_batch_size: Батч-размер на устройство для обучения.\n        :param gradient_accumulation_steps: Шаги аккумулирования градиента (эффективный батч = batch_size * steps).\n        :param learning_rate: Начальная скорость обучения (оптимизатор и шедулер создаются внутри Trainer).\n        :param metric_name: Метрика ранней остановки/выбора лучшей модели: \"f1\" (weighted) или \"accuracy\".\n        :param fp16: Использовать ли полуточность (только при наличии CUDA).\n        :param logging_steps: Периодичность логирования в шагах.\n        :param eval_steps: Периодичность валидации/сохранения модели в шагах.\n        :param output_dir: Директория для чекпойнтов/логов.\n        :param seed: Начальное зерно для воспроизводимости.\n        :param hidden: Размер скрытого слоя классификационной головы.\n        :param dropout: Дропаут в классификационной голове.\n        :param gradient_checkpointing: Включить gradient checkpointing в бэкендах (если они обучаемые).\n        :param fit_chunk_size: Размер чанка для поэтапной тренировки (None — весь датасет за эпоху без разбиения).\n        :param clear_cache_every_n_chunks: Каждые N чанков очищать кэш токенизации (для экономии памяти).\n        :param early_stopping_patience: Патенс ранней остановки (кол-во валидационных проверок без улучшения).\n                                        Если None или <=0 — ранняя остановка отключена.\n        :param early_stopping_threshold: Порог минимального улучшения метрики для сброса патенса.\n    \n        :return: self (для чейнинга вызовов).\n    \n        :raises ValueError: \n            - Отсутствуют обязательные колонки по модальностям в train_data (внутренняя проверка _validate_data).\n            - Невозможно собрать классификатор с fusion=\"mean\" при разных размерах эмбеддингов модальностей.\n        :raises RuntimeError: Ошибки, возникающие внутри transformers.Trainer (например, рассогласование батчей/логитов),\n                              проблемы с устройством (CUDA OOM, MPS), ошибки чтения аудио/изображений.\n        :raises OSError: Ошибки чтения файлов данных (изображения/аудио) или кэша моделей.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        classes = sorted(train_data[self.target_column_name].unique().tolist())\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n        if test_data is None:\n            df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n        else:\n            df_train, df_eval = train_data, test_data\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds_eval = MultiComboDataset(\n            df=df_eval,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_eval) < 50000),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_eval), self.max_pretokenize_samples),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n        counts = np.bincount(y_train_all, minlength=self.num_labels)\n        n_all = counts.sum()\n        class_weights = np.zeros(self.num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        class_weights[nonzero] = n_all / (self.num_labels * counts[nonzero].astype(np.float32))\n\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        self._setup_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def data_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        n_train = len(df_train)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        dummy_idx = np.arange(min(len(df_train), 1))\n        ds_train_init = (\n            MultiComboDataset(\n                df=df_train.iloc[dummy_idx],\n                target_col=self.target_column_name,\n                label2id=self.label2id,\n                text_columns=self.text_columns,\n                image_columns=self.image_columns,\n                audio_columns=self.audio_columns,\n                text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                special_tokens=self.special_tokens,\n                pretokenize=False,\n                tokenizer_returns_tensors=self.tokenizer_returns_tensors\n            ) if len(dummy_idx) > 0 else ds_eval\n        )\n\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            num_labels=self.num_labels,\n            train_labels=y_train_all,\n            class_weights=class_weights\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        if (ds_eval is not None) and (early_stopping_patience is not None) and (early_stopping_patience > 0):\n            esc = EarlyStoppingCallback(\n                early_stopping_patience=int(early_stopping_patience),\n                early_stopping_threshold=float(early_stopping_threshold)\n            )\n            self.trainer.add_callback(esc)\n\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        steps_done = 0\n        chunk_counter = 0\n\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n\n            for slc in chunk_slices(shuffled, chunk_size):\n                chunk_df = df_train.iloc[slc]\n                should_pretokenize = (\n                    self.pretokenize_data and has_bt and len(slc) < self.max_pretokenize_samples and len(slc) > 100\n                )\n\n                ds_chunk = MultiComboDataset(\n                    df=chunk_df,\n                    target_col=self.target_column_name,\n                    label2id=self.label2id,\n                    text_columns=self.text_columns,\n                    image_columns=self.image_columns,\n                    audio_columns=self.audio_columns,\n                    text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                    text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                    special_tokens=self.special_tokens,\n                    pretokenize=should_pretokenize,\n                    pretokenize_batch_size=self.pretokenize_batch_size,\n                    max_cache_size=min(len(slc), self.max_pretokenize_samples),\n                    tokenizer_returns_tensors=self.tokenizer_returns_tensors\n                )\n\n                self.trainer.train_dataset = ds_chunk\n\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk, chunk_df\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                chunk_counter += 1\n                if chunk_counter % clear_cache_every_n_chunks == 0:\n                    if hasattr(ds_chunk, 'clear_cache'):\n                        ds_chunk.clear_cache()\n                        print(f\"✓ Очищен кэш токенизации после {chunk_counter} чанков\")\n\n                del ds_chunk, chunk_df\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n\n        if getattr(self.backend, \"batch_tokenizer\", None):\n            self.backend.batch_tokenizer.clear_cache()\n\n        return self\n\n    def predict(\n        self,\n        df: pd.DataFrame,\n        return_label_str: bool = False,\n        return_proba: bool = False,\n        batch_size: Optional[int] = None\n    ) -> np.ndarray:\n        \"\"\"\n        Выполняет инференс на новом DataFrame и возвращает предсказания.\n        Если в df отсутствует столбец target_column_name, он будет добавлен фиктивными значениями.\n    \n        :param df: DataFrame с теми же колонками по выбранным модальностям, что и при обучении.\n                   - text_columns: строки,\n                   - image_columns: пути к изображениям, PIL.Image, np.ndarray или списки таких элементов,\n                   - audio_columns: пути к аудиофайлам или np.ndarray (моно, float32).\n        :param return_label_str: Если True — вернуть массив строковых меток (id2label), иначе — индексы классов.\n        :param return_proba: Если True — вернуть распределения вероятностей (softmax) формы [N, num_labels].\n                             При включении этого флага игнорируется return_label_str.\n        :param batch_size: Переопределяет per_device_eval_batch_size на время инференса (опционально).\n    \n        :return: \n            - Если return_proba=True: np.ndarray формы [N, num_labels] — вероятности классов.\n            - Иначе: \n                - Если return_label_str=True: np.ndarray формы [N] со строковыми метками,\n                - Иначе: np.ndarray формы [N] с индексами предсказанных классов.\n    \n        :raises RuntimeError: Если модель не обучена (trainer отсутствует).\n        :raises ValueError: Ошибки приведения данных (например, несоответствие ожидаемым колонкам/типам),\n                            внутренние ошибки коллатора/бэкенда (рассогласование размеров батчей).\n        :raises OSError: Ошибки чтения исходных файлов (изображения/аудио).\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n\n        print(f\"Preparing dataset for prediction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_c), 10000),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n\n        print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds)\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        if hasattr(ds, 'clear_cache'):\n            ds.clear_cache()\n\n        if return_proba:\n            logits = preds.predictions\n            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n            return probabilities\n\n        y_pred = np.argmax(preds.predictions, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    def get_embeddings(\n        self,\n        df: pd.DataFrame,\n        batch_size: int = 32,\n        return_per_modality: bool = False\n    ):\n        \"\"\"\n        Извлекает эмбеддинги для входного DataFrame. Возвращает склеенный (fused) эмбеддинг,\n        а при необходимости — также эмбеддинги по каждой модальности.\n        Схема слияния (concat/mean) и размеры зависят от настроек бэкенда и параметра fusion.\n    \n        Если в df отсутствует столбец target_column_name, он будет добавлен фиктивными значениями.\n    \n        :param df: DataFrame с данными по модальностям (аналогично predict()).\n        :param batch_size: Батч-размер при извлечении эмбеддингов.\n        :param return_per_modality: Если True — вернуть дополнительно словарь с эмбеддингами по модальностям.\n    \n        :return:\n            - Если return_per_modality=False:\n                np.ndarray формы [N, D_fused], где D_fused — размерность эмбеддинга после слияния.\n            - Если return_per_modality=True:\n                (fused, per_mod) — кортеж:\n                    - fused: np.ndarray [N, D_fused],\n                    - per_mod: Dict[str, np.ndarray], где ключ — модальность (\"text\"/\"image\"/\"audio\"),\n                               значение — эмбеддинги этой модальности формы [N, D_mod].\n    \n        :raises RuntimeError: Если модель не обучена или не готова (trainer/model отсутствуют).\n        :raises ValueError: Если бэкенд не вернул эмбеддинги (например, неверные настройки модальностей/данных).\n        :raises OSError: Ошибки чтения исходных файлов (изображения/аудио).\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n\n        print(f\"Preparing dataset for embeddings extraction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=False,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device: torch.device):\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        print(\"Concatenating embeddings...\")\n        fused_arr = np.vstack(fused_list)\n\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Функции для создания фиктивных данных.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef make_rand_image(h=512, w=512):\n    return (np.random.rand(h, w, 3) * 255).astype(\"uint8\")\n\ndef make_sine_audio(sr=48000, seconds=1.0, freq=440.0):\n    t = np.linspace(0, seconds, int(sr * seconds), endpoint=False)\n    return (0.1 * np.sin(2 * np.pi * freq * t)).astype(np.float32)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Данные (строковые метки → красиво отобразятся в predict(return_label_str=True))\ndf_clip_clap = pd.DataFrame([\n    {\"text\": \"A man riding a bike\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 440.0), \"label\": \"sports\"},\n    {\"text\": \"A cat lying on sofa\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 330.0), \"label\": \"lifestyle\"},\n    {\"text\": \"Stock market is volatile\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 550.0), \"label\": \"business\"},\n    {\"text\": \"Runner on the track\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 660.0), \"label\": \"sports\"},\n    {\"text\": \"New cafe opens downtown\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 220.0), \"label\": \"lifestyle\"},\n    {\"text\": \"Company reports revenue\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 770.0), \"label\": \"business\"},\n])\n\npipe1 = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\", \"audio\"],\n    num_labels=3,                          # == числу уникальных меток\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    image_columns=[\"image\"],\n    audio_columns=[\"audio\"],\n    # Явные конфиги бэкендов\n    text_model_config={\n        \"checkpoint\": \"openai/clip-vit-base-patch32\",\n        \"model_type\": \"clip\",\n        \"max_length\": 77\n    },\n    image_model_config={\n        \"checkpoint\": \"openai/clip-vit-base-patch32\",\n        \"model_type\": \"clip\",\n        \"max_images\": 1,\n        \"image_agg\": \"mean\"\n    },\n    audio_model_config={\n        \"checkpoint\": \"laion/clap-htsat-unfused\",\n        \"model_type\": \"clap\",\n        \"sr\": 48000,\n        \"max_audios\": 1,\n        \"audio_agg\": \"mean\"\n    },\n    fusion=\"mean\",                         # у всех 512 → mean\n    freeze_backbone=False,                  # linear probing\n    use_batch_tokenizer=True,              # быстрый токенизатор\n    pretokenize_data=True,                 # предварительная токенизация\n    pretokenize_batch_size=128,\n    tokenizer_cache_size=5000,\n    max_pretokenize_samples=100000,\n    local_cache_dir=\"./model_cache\"\n)\n\npipe1.fit(\n    train_data=df_clip_clap,\n    epochs=2,\n    test_size=0.33,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=1e-4,\n    metric_name=\"f1\",\n    fp16=True,                 # если есть CUDA\n    logging_steps=1,\n    eval_steps=2,\n    output_dir=\"./mc_max_clip_clap\",\n    seed=42,\n    hidden=512,\n    dropout=0.2,\n    gradient_checkpointing=True,\n    fit_chunk_size=2,          # чанки по 2 сэмпла\n    clear_cache_every_n_chunks=5,\n    early_stopping_patience=2,         # ранняя остановка\n    early_stopping_threshold=0.0\n)\n\n# Предсказания — вероятности\nprobas1 = pipe1.predict(df_clip_clap.iloc[:3], return_proba=True)\nprint(\"Probas shape:\", probas1.shape)\n\n# Предсказания — строковые метки\nlabels1 = pipe1.predict(df_clip_clap.iloc[:3], return_label_str=True)\nprint(\"Labels:\", labels1)\n\n# Эмбеддинги (fused + по модальностям)\nfused1, per1 = pipe1.get_embeddings(df_clip_clap.iloc[:3], batch_size=2, return_per_modality=True)\nprint(\"Fused shape:\", fused1.shape)\nfor m, arr in per1.items():\n    print(f\"{m} emb shape:\", arr.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef build_binary_multimodal_df(n_per_class: int = 50, sr: int = 16000) -> pd.DataFrame:\n    # Генерация текстов «pets»\n    pet_animals = [\"кошка\", \"собака\", \"щенок\", \"кот\", \"котёнок\", \"пёс\", \"питомец\", \"котик\", \"пёсик\", \"котяра\"]\n    pet_actions = [\"сидит\", \"лежит\", \"играет\", \"смотрит\", \"прячется\", \"спит\", \"тянется\", \"мурлычет\", \"исследует\", \"охотится\"]\n    pet_places = [\"на подоконнике\", \"на диване\", \"на ковре\", \"на кухне\", \"в коробке\", \"на кресле\", \"у окна\", \"в саду\", \"на полу\", \"на стуле\"]\n\n    # Генерация текстов «news»\n    news_subjects = [\"власти города\", \"жители района\", \"аналитики\", \"эксперты\", \"журналисты\", \"компания\", \"департамент\", \"учёные\", \"инженеры\", \"ведомство\"]\n    news_verbs = [\"обсудили\", \"обновили\", \"сообщили\", \"рассказали\", \"анонсировали\", \"заявили\", \"подтвердили\", \"планируют\", \"запустили\", \"увеличили\"]\n    news_topics = [\"экономику\", \"политику\", \"транспорт\", \"погоду\", \"технологии\", \"культуру\", \"спорт\", \"здравоохранение\", \"образование\", \"экологию\"]\n\n    rows = []\n\n    # PETS класс\n    pet_freqs = [260.0, 280.0, 300.0, 320.0, 340.0, 360.0]\n    for _ in range(n_per_class):\n        text = f\"{random.choice(pet_animals).capitalize()} {random.choice(pet_actions)} {random.choice(pet_places)}\"\n        img = make_rand_image()\n        aud = make_sine_audio(sr, 1.0, random.choice(pet_freqs))\n        rows.append({\"text\": text, \"image\": img, \"audio\": aud, \"label\": \"pets\"})\n\n    # NEWS класс\n    news_freqs = [560.0, 580.0, 600.0, 620.0, 640.0, 660.0]\n    for _ in range(n_per_class):\n        text = f\"{random.choice(news_subjects).capitalize()} {random.choice(news_verbs)} новости про {random.choice(news_topics)}\"\n        img = make_rand_image()\n        aud = make_sine_audio(sr, 1.0, random.choice(news_freqs))\n        rows.append({\"text\": text, \"image\": img, \"audio\": aud, \"label\": \"news\"})\n\n    random.shuffle(rows)\n    df = pd.DataFrame(rows)\n    return df\n\ntrain_data = build_binary_multimodal_df(n_per_class=12)\ndf_train, df_eval = train_test_split(\n    train_data, test_size=0.3, random_state=42, shuffle=True,\n    stratify=train_data['label'])\n\npipe3 = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\", \"audio\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    image_columns=[\"image\"],\n    audio_columns=[\"audio\"],\n    text_model_config={\n        \"checkpoint\": \"DeepPavlov/rubert-base-cased\",\n        \"model_type\": \"bert\",\n        \"max_length\": 256\n    },\n    image_model_config={\n        \"checkpoint\": \"google/vit-base-patch16-224\",\n        \"model_type\": \"vit\",\n        \"max_images\": 1,\n        \"image_agg\": \"mean\"\n    },\n    audio_model_config={\n        \"checkpoint\": \"facebook/wav2vec2-base-960h\",\n        \"model_type\": \"auto\",   # AutoModel + AutoProcessor\n        \"sr\": 16000,\n        \"max_audios\": 1,\n        \"audio_agg\": \"mean\"\n    },\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=True,\n    pretokenize_data=True,\n    pretokenize_batch_size=128,\n    tokenizer_cache_size=5000,\n    local_cache_dir=\"./model_cache\"\n)\n\npipe3.fit(\n    train_data=df_train,\n    test_data=df_eval,\n    epochs=2,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-4,\n    metric_name=\"f1\",\n    fp16=True,\n    logging_steps=1,\n    eval_steps=1,\n    output_dir=\"./mc_max_rubert_vit_w2v2\",\n    seed=2025,\n    hidden=512,\n    dropout=0.2,\n    gradient_checkpointing=True,\n    fit_chunk_size=8,\n    clear_cache_every_n_chunks=3,\n    early_stopping_patience=2,\n    early_stopping_threshold=0.0\n)\n\nprint(\"Pred (labels):\", pipe3.predict(df_rubert_vit_w2v, return_label_str=True))\nfused3, per3 = pipe3.get_embeddings(df_rubert_vit_w2v, batch_size=2, return_per_modality=True)\nprint(\"Fused:\", fused3.shape, \"| text:\", per3[\"text\"].shape, \"| image:\", per3[\"image\"].shape, \"| audio:\", per3[\"audio\"].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 3.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_min = pd.DataFrame([\n    {\"text\": \"Привет, как дела?\", \"label\": \"greet\"},\n    {\"text\": \"Сегодня отличная погода\", \"label\": \"weather\"},\n    {\"text\": \"До встречи!\", \"label\": \"greet\"},\n])\n\npipe_min = SingleModelMultiComboClassification(\n    modalities=[\"text\"],\n    num_labels=2,                           # ровно столько, сколько уникальных меток в df_min\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    # Достаточно указать только текстовую модель\n    text_model_config={\n        \"checkpoint\": \"DeepPavlov/rubert-base-cased\",\n        \"model_type\": \"bert\",\n        \"max_length\": 128\n    },\n    fusion=\"concat\",                        # неважно для single-modality\n    freeze_backbone=True\n)\n\n# Минимальный fit: всё по умолчанию (без ранней остановки, без чанков)\npipe_min.fit(train_data=df_min)\n\nprint(\"Pred (ids):\", pipe_min.predict(df_min))\nprint(\"Pred (labels):\", pipe_min.predict(df_min, return_label_str=True))\nemb_min = pipe_min.get_embeddings(df_min)\nprint(\"Embeddings shape:\", emb_min.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение регрессора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация регрессора.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  pillow==11.1.0 \\\n  numpy==1.26.4 \\\n  pandas==2.2.3 \\\n  tqdm==4.67.1 \\\n  transformers==4.51.3 \\\n  evaluate==0.4.5 \\\n  wav2clip==0.1.0 \\\n  torch==2.6.0+cu124 \\\n  torchaudio==2.6.0+cu124\n\n\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport math\nimport random\nimport gc\nfrom functools import lru_cache\nfrom typing import Any, Callable, Dict, Generator, List, Optional, Union\n\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers.trainer_callback import TrainerCallback, PrinterCallback, EarlyStoppingCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport evaluate\n\n\n# =========================\n# Утилиты\n# =========================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef to_pil(x: Union[str, np.ndarray, 'Image.Image']) -> 'Image.Image':\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\ndef safe_load(component_cls, checkpoint: str, local_cache_dir: str = \"./model_cache\",\n              local_files_only: Optional[bool] = None, **kwargs):\n    if local_files_only is None:\n        local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n    name = getattr(component_cls, \"__name__\", \"\")\n    if \"Tokenizer\" in name:\n        kwargs.setdefault(\"use_fast\", True)\n    return component_cls.from_pretrained(\n        checkpoint, cache_dir=local_cache_dir, local_files_only=local_files_only, **kwargs\n    )\n\n\n# =========================\n# Токенизатор батчевый\n# =========================\n\nclass BatchTokenizer:\n    def __init__(\n        self,\n        tokenizer,\n        max_length: int = 512,\n        cache_size: int = 10000,\n        batch_size: int = 256,\n        use_fast: bool = True,\n        device: str = \"cpu\"\n    ):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.use_fast = use_fast\n        self.device = device\n        self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n        self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n        if self.is_fast:\n            print(\"✓ Используется Fast Tokenizer\")\n\n    def _tokenize_single(self, text: str) -> tuple:\n        result = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        return tuple((k, v.squeeze(0).cpu().numpy()) for k, v in result.items())\n\n    def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n        if use_cache and len(texts) < 100:\n            results = [dict(self._cache(text)) for text in texts]\n            keys = results[0].keys()\n            batch_dict = {}\n            for key in keys:\n                dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                batch_dict[key] = torch.tensor(np.stack([r[key] for r in results]), dtype=dtype)\n            return batch_dict\n        else:\n            result = self.tokenizer(\n                texts,\n                padding=True,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n    def tokenize_dataset_lazy(\n        self,\n        texts: List[str],\n        batch_size: Optional[int] = None\n    ) -> Generator[Dict[str, torch.Tensor], None, None]:\n        batch_size = batch_size or self.batch_size\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            yield self.tokenize_batch(batch, use_cache=False)\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n\n# =========================\n# Универсальный датасет\n# =========================\n\nclass MultiComboDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer: Optional[BatchTokenizer] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        pretokenize: bool = False,\n        pretokenize_batch_size: int = 256,\n        max_cache_size: int = 100000,\n        tokenizer_returns_tensors: bool = False,\n        cache_dir: Optional[str] = None\n    ):\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.text_tokenizer = text_tokenizer\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n        self.tokenized_cache: Dict[int, Dict[str, torch.Tensor]] = {}\n        self.cache_hits = 0\n        self.cache_misses = 0\n\n        if pretokenize and self.text_tokenizer and self.text_columns:\n            self._pretokenize_texts(\n                batch_size=pretokenize_batch_size,\n                max_cache_size=min(max_cache_size, len(self.df))\n            )\n\n    def _join_text(self, row: pd.Series) -> str:\n        sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n        return sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n    def _pretokenize_texts(self, batch_size: int, max_cache_size: int):\n        print(\"Предварительная токенизация текстов...\")\n        indices = list(range(min(len(self.df), max_cache_size)))\n        all_texts = [self._join_text(self.df.iloc[i]) for i in indices]\n\n        for start in range(0, len(indices), batch_size):\n            batch_idx = indices[start:start + batch_size]\n            batch_txt = all_texts[start:start + batch_size]\n            tokenized = self.text_tokenizer.tokenize_batch(batch_txt, use_cache=False)\n\n            for j, idx in enumerate(batch_idx):\n                token_dict: Dict[str, torch.Tensor] = {}\n                for k, v in tokenized.items():\n                    t = v[j]\n                    token_dict[k] = t.clone().long() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else t.clone()\n                self.tokenized_cache[idx] = token_dict\n\n        print(f\"✓ Предварительно токенизировано {len(self.tokenized_cache)} текстов\")\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        row = self.df.iloc[idx]\n        item: Dict[str, Any] = {}\n\n        # Регрессия: метка всегда float\n        if self.target_col in row:\n            try:\n                item[\"labels\"] = float(row[self.target_col])\n            except Exception:\n                item[\"labels\"] = 0.0\n        else:\n            item[\"labels\"] = 0.0\n\n        # Текст\n        if self.text_columns:\n            if idx in self.tokenized_cache:\n                cached = self.tokenized_cache[idx]\n                text_tokens: Dict[str, torch.Tensor] = {}\n                for k, v in cached.items():\n                    text_tokens[k] = v.long() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else v\n                item[\"text_tokens\"] = text_tokens\n                self.cache_hits += 1\n            elif self.text_tokenizer is not None:\n                text = self._join_text(row)\n                tokenized = self.text_tokenizer.tokenize_batch([text], use_cache=True)\n                text_tokens = {k: (v[0].long() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else v[0])\n                               for k, v in tokenized.items()}\n                item[\"text_tokens\"] = text_tokens\n                self.cache_misses += 1\n                if len(self.tokenized_cache) < 100000:\n                    self.tokenized_cache[idx] = {k: t.clone() for k, t in text_tokens.items()}\n            elif self.text_tokenizer_fn is not None:\n                text_data = {c: str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns}\n                result = self.text_tokenizer_fn(text_data, self.special_tokens)\n                if isinstance(result, dict) and 'input_ids' in result:\n                    item[\"text_tokens\"] = result\n                    self.tokenizer_returns_tensors = True\n                else:\n                    item[\"text\"] = result\n            else:\n                item[\"text\"] = self._join_text(row)\n\n        # Утилита для нормализации поля в список\n        def _as_list(v):\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                return []\n            if isinstance(v, (list, tuple)):\n                return list(v)\n            return [v]\n\n        # Картинки\n        if self.image_columns:\n            imgs = []\n            for c in self.image_columns:\n                if c in row:\n                    imgs.extend(_as_list(row[c]))\n            item[\"images\"] = imgs\n\n        # Аудио\n        if self.audio_columns:\n            auds = []\n            for c in self.audio_columns:\n                if c in row:\n                    auds.extend(_as_list(row[c]))\n            item[\"audios\"] = auds\n\n        return item\n\n    def get_cache_stats(self) -> Dict[str, Any]:\n        total = self.cache_hits + self.cache_misses\n        hit_rate = self.cache_hits / total if total > 0 else 0.0\n        return {\n            \"cache_size\": len(self.tokenized_cache),\n            \"cache_hits\": self.cache_hits,\n            \"cache_misses\": self.cache_misses,\n            \"hit_rate\": hit_rate\n        }\n\n    def clear_cache(self):\n        self.tokenized_cache.clear()\n        self.cache_hits = 0\n        self.cache_misses = 0\n        if self.text_tokenizer:\n            self.text_tokenizer.clear_cache()\n\n\n# =========================\n# Универсальный бэкенд\n# =========================\n\nclass BaseBackend(nn.Module):\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n    text_tokenizer_fn: Optional[Callable] = None\n    batch_tokenizer: Optional[BatchTokenizer] = None\n    special_tokens: Dict[str, str] = {}\n    tokenizer_returns_tensors: bool = False\n    local_cache_dir: str = \"./model_cache\"\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        raise NotImplementedError\n\n    def freeze_all(self):\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n    def set_text_tokenizer(self, tokenizer_fn: Optional[Callable], special_tokens: Optional[Dict[str, str]] = None,\n                           returns_tensors: bool = False):\n        self.text_tokenizer_fn = tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = returns_tensors\n\n    def set_batch_tokenizer(self, tokenizer, max_length: int = 512,\n                            cache_size: int = 10000, batch_size: int = 256):\n        self.batch_tokenizer = BatchTokenizer(\n            tokenizer=tokenizer,\n            max_length=max_length,\n            cache_size=cache_size,\n            batch_size=batch_size,\n            use_fast=True\n        )\n\n\nclass UniversalMultiBackend(BaseBackend):\n    name = \"universal\"\n    \n    class _ParamDeviceProxy(nn.Module):\n        def __init__(self, base, device: torch.device):\n            super().__init__()\n            self.base = base if isinstance(base, nn.Module) else None\n            self._callable = base if not isinstance(base, nn.Module) else None\n            self._dummy = nn.Parameter(torch.empty(0), requires_grad=False)\n            with torch.no_grad():\n                self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n    \n        def forward(self, *args, **kwargs):\n            target = self.base if self.base is not None else self._callable\n            return target(*args, **kwargs)\n    \n        def to(self, device, *args, **kwargs):\n            self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n            return super().to(device, *args, **kwargs)\n\n    def _model_device(self, model, default: torch.device) -> torch.device:\n        try:\n            return next(model.parameters()).device\n        except StopIteration:\n            pass\n        try:\n            buf = next(model.buffers())\n            return buf.device\n        except StopIteration:\n            pass\n        return default\n    \n    def _preferred_device(self) -> torch.device:\n        if torch.cuda.is_available():\n            return torch.device(\"cuda\")\n        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n            return torch.device(\"mps\")\n        return torch.device(\"cpu\")\n    \n    def _wrap_if_parameterless(self, model, device: torch.device):\n        try:\n            it = model.parameters() if hasattr(model, \"parameters\") else iter(())\n            next(it)\n            return model\n        except StopIteration:\n            return self._ParamDeviceProxy(model, device)\n        except Exception:\n            return self._ParamDeviceProxy(model, device)\n\n    def __init__(\n        self,\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        freeze: bool = True,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        use_batch_tokenizer: bool = True,\n        tokenizer_cache_size: int = 10000,\n        tokenizer_batch_size: int = 256,\n        local_cache_dir: str = \"./model_cache\"\n    ):\n        super().__init__()\n        self.supported = set()\n        self.out_dim_per_modality = {}\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.tokenizer_batch_size = tokenizer_batch_size\n        self.local_cache_dir = local_cache_dir\n\n        self.text_model = None\n        self.text_processor = None\n        self.text_config = text_model_config or {}\n        if text_model_config:\n            self._init_text_model(text_model_config)\n            self.supported.add(\"text\")\n\n        self.image_model = None\n        self.image_processor = None\n        self.image_config = image_model_config or {}\n        if image_model_config:\n            self._init_image_model(image_model_config)\n            self.supported.add(\"image\")\n\n        self.audio_model = None\n        self.audio_processor = None\n        self.audio_config = audio_model_config or {}\n        if audio_model_config:\n            self._init_audio_model(audio_model_config)\n            self.supported.add(\"audio\")\n\n        if freeze:\n            self.freeze_all()\n\n    def _ensure_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        if x is None:\n            return None\n        if x.dim() == 1:\n            return x.unsqueeze(0)\n        if x.dim() > 2:\n            return x.view(x.size(0), -1)\n        return x\n\n    def _normalize_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        x = self._ensure_2d(x)\n        return F.normalize(x, dim=-1) if x is not None and x.numel() > 0 else x\n    \n    def _init_text_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoTokenizer, CLIPModel, CLIPTokenizer, ClapModel, ClapProcessor\n    \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n    \n        print(f\"Загрузка текстовой модели {checkpoint}...\")\n    \n        if model_type == 'clip':\n            self.text_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(CLIPTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.projection_dim\n        elif model_type == 'clap':\n            self.text_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            proc = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = getattr(proc, 'tokenizer', None) or safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = getattr(self.text_model.config, \"projection_dim\", 512)\n        else:\n            self.text_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.text_model  = self._wrap_if_parameterless(self.text_model, dev)\n    \n        if self.use_batch_tokenizer and self.text_processor is not None:\n            self.set_batch_tokenizer(\n                self.text_processor,\n                max_length=config.get('max_length', 512),\n                cache_size=self.tokenizer_cache_size,\n                batch_size=self.tokenizer_batch_size\n            )\n    \n        self.text_config['max_length'] = config.get('max_length', 512)\n        self.text_config['dim'] = dim\n        self.text_config['model_type'] = model_type\n        self.out_dim_per_modality['text'] = dim\n\n    def _init_image_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoImageProcessor, CLIPModel, CLIPImageProcessor\n    \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n    \n        print(f\"Загрузка визуальной модели {checkpoint}...\")\n    \n        if model_type == 'clip':\n            self.image_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(CLIPImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.projection_dim\n        else:\n            self.image_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(AutoImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.image_model = self._wrap_if_parameterless(self.image_model, dev)\n    \n        self.image_config['max_images'] = config.get('max_images', 1)\n        self.image_config['image_agg'] = config.get('image_agg', 'concat')\n        self.image_config['dim'] = dim\n        self.image_config['model_type'] = model_type\n    \n        self.out_dim_per_modality['image'] = (dim * self.image_config['max_images']) if self.image_config['image_agg'] == 'concat' else dim\n    \n    def _init_audio_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoProcessor, ClapModel, ClapProcessor\n    \n        model_type = config.get('model_type', 'auto').lower()\n        checkpoint = config.get('checkpoint', None)\n    \n        print(f\"Загрузка аудио модели (type={model_type})...\")\n    \n        if model_type == 'wav2clip':\n            import wav2clip as w2c\n            self._w2c = w2c\n    \n            w2c_model = None\n            if hasattr(w2c, \"get_model\"):\n                w2c_model = w2c.get_model()\n            elif hasattr(w2c, \"model\"):\n                m = w2c.model\n                w2c_model = m() if callable(m) else m\n            else:\n                raise RuntimeError(\"wav2clip не содержит get_model()/model. Обновите пакет wav2clip.\")\n    \n            self.audio_model = w2c_model\n    \n            try:\n                if isinstance(self.audio_model, torch.nn.Module) and torch.cuda.is_available():\n                    self.audio_model = self.audio_model.to(\"cuda\")\n            except Exception:\n                pass\n    \n            self.audio_processor = None\n            dim = 512\n            sr = config.get('sr', 16000)\n\n        elif model_type == 'clap':\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для CLAP\")\n            self.audio_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = getattr(self.audio_model.config, \"projection_dim\", 512)\n            sr = getattr(self.audio_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.audio_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n    \n        else:\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для аудио-моделей, кроме wav2clip\")\n            self.audio_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(AutoProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.audio_model.config.hidden_size\n            fe = getattr(self.audio_processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 16000) if fe is not None else 16000\n\n        dev = self._preferred_device()\n        self.audio_model = self._wrap_if_parameterless(self.audio_model, dev)\n    \n        self.audio_config['sr'] = config.get('sr', sr)\n        self.audio_config['max_audios'] = config.get('max_audios', 1)\n        self.audio_config['audio_agg'] = config.get('audio_agg', 'concat')\n        self.audio_config['dim'] = dim\n        self.audio_config['model_type'] = model_type\n    \n        self.out_dim_per_modality['audio'] = (\n            dim * self.audio_config['max_audios']\n            if self.audio_config['audio_agg'] == 'concat' else dim\n        )\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        labels = []\n        for b in batch:\n            labels.append(torch.tensor(b.get(\"labels\", 0.0), dtype=torch.float32))\n        labels = torch.stack(labels)\n\n        backend_inputs: Dict[str, Any] = {}\n        batch_size = len(batch)\n\n        # Текст\n        if self.text_model is not None:\n            if \"text_tokens\" in batch[0]:\n                text_inputs = {}\n                for key in batch[0][\"text_tokens\"].keys():\n                    if torch.is_tensor(batch[0][\"text_tokens\"][key]):\n                        text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n                    else:\n                        dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                        text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n                backend_inputs[\"text_inputs\"] = text_inputs\n            else:\n                texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n                if self.batch_tokenizer:\n                    text_inputs = self.batch_tokenizer.tokenize_batch(texts, use_cache=True)\n                else:\n                    text_inputs = self.text_processor(\n                        texts, padding=True, truncation=True,\n                        max_length=self.text_config.get('max_length', 512),\n                        return_tensors=\"pt\"\n                    )\n                backend_inputs[\"text_inputs\"] = {k: v for k, v in text_inputs.items()}\n\n        # Изображения\n        if self.image_model is not None:\n            images_lists = [b.get(\"images\", []) for b in batch]\n            flat_images, img_counts = [], []\n            for lst in images_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [img for img in lst if img is not None]\n                img_counts.append(len(lst))\n                for img in lst:\n                    flat_images.append(to_pil(img))\n\n            if len(flat_images) > 0:\n                img_proc = self.image_processor(images=flat_images, return_tensors=\"pt\")\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": img_proc[\"pixel_values\"]}\n            else:\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": None}\n\n            backend_inputs[\"image_counts\"] = torch.tensor(img_counts, dtype=torch.long)\n\n        # Аудио\n        if self.audio_model is not None:\n            audios_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios, aud_counts = [], []\n            for lst in audios_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_config['sr']))\n                    elif isinstance(a, np.ndarray):\n                        aa = np.asarray(a, dtype=np.float32)\n                        if aa.ndim > 1:\n                            aa = np.squeeze(aa)\n                        if aa.ndim > 1:\n                            aa = aa.reshape(-1)\n                        flat_audios.append(aa)\n            if self.audio_config.get('model_type') == 'wav2clip':\n                backend_inputs[\"audio_inputs\"] = {\"raw_audios\": flat_audios}\n            elif len(flat_audios) > 0:\n                if self.audio_config.get('model_type') == 'clap':\n                    aud_proc = self.audio_processor(\n                        audios=flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_features\": aud_proc[\"input_features\"]}\n                else:\n                    aud_proc = self.audio_processor(\n                        flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_values\": aud_proc[\"input_values\"]}\n            else:\n                backend_inputs[\"audio_inputs\"] = {\"input_features\": None, \"input_values\": None, \"raw_audios\": []}\n        \n            backend_inputs[\"audio_counts\"] = torch.tensor(aud_counts, dtype=torch.long)\n\n        backend_inputs[\"batch_size\"] = batch_size\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _aggregate_embeddings(\n        self,\n        embs: Optional[torch.Tensor],\n        counts: List[int],\n        max_k: int,\n        dim_hint: int,\n        agg_type: str,\n        batch_size: int,\n        device: torch.device\n    ) -> torch.Tensor:\n        if embs is None or (torch.is_tensor(embs) and embs.numel() == 0):\n            feat_dim = int(dim_hint) if dim_hint is not None else 0\n            out_dim = feat_dim * max_k if agg_type == 'concat' else feat_dim\n            return torch.zeros((batch_size, out_dim), device=device, dtype=torch.float32)\n    \n        if not torch.is_tensor(embs):\n            embs = torch.as_tensor(embs, device=device, dtype=torch.float32)\n        if embs.dim() == 1:\n            embs = embs.unsqueeze(0)\n        elif embs.dim() > 2:\n            embs = embs.view(embs.size(0), -1)\n    \n        N, D = embs.size()\n        out_dim = (D * max_k) if agg_type == 'concat' else D\n        out = torch.zeros((batch_size, out_dim), device=device, dtype=embs.dtype)\n    \n        offset = 0\n        for i, c in enumerate(counts):\n            if c <= 0 or offset >= N:\n                continue\n            take_n = min(c, N - offset)\n            sample = embs[offset:offset + take_n]\n            offset += take_n\n    \n            if agg_type == 'concat':\n                take = sample[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n    \n        return F.normalize(out, dim=-1) if out.size(1) > 0 else out\n\n    @torch.no_grad()\n    def _wav2clip_embed(self, arr: np.ndarray, device: torch.device) -> torch.Tensor:\n        arr = np.asarray(arr, dtype=np.float32)\n        if arr.ndim > 1:\n            arr = np.squeeze(arr)\n        if arr.ndim > 1:\n            arr = arr.reshape(-1)\n        if arr.size < 512:\n            arr = np.pad(arr, (0, 512 - arr.size), mode=\"constant\")\n    \n        try:\n            emb = self._w2c.embed_audio(arr, self.audio_model)\n            emb = np.asarray(emb)\n        except Exception:\n            model_dev = self._model_device(self.audio_model, default=device)\n            x = torch.from_numpy(arr).float().unsqueeze(0).to(model_dev)\n            y = self.audio_model(x)\n            if isinstance(y, (tuple, list)):\n                y = y[0]\n            if torch.is_tensor(y):\n                if y.dim() == 2 and y.size(0) == 1:\n                    y = y.squeeze(0)\n                emb = y.detach().cpu().numpy()\n            else:\n                emb = np.asarray(y)\n    \n        if emb.ndim > 1:\n            emb = emb.reshape(-1)\n        return torch.as_tensor(emb, device=device, dtype=torch.float32)\n    \n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        results: Dict[str, torch.Tensor] = {}\n        batch_size = int(backend_inputs.get(\"batch_size\", 1))\n    \n        # Текст\n        if self.text_model is not None and \"text_inputs\" in backend_inputs:\n            text_inputs = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n            if hasattr(self.text_model, \"get_text_features\"):\n                text_z = self.text_model.get_text_features(**text_inputs)\n            else:\n                outputs = self.text_model(**text_inputs)\n                text_z = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n            results[\"text\"] = self._normalize_2d(text_z)\n    \n        # Изображения\n        if self.image_model is not None and \"image_inputs\" in backend_inputs:\n            pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n            counts = backend_inputs[\"image_counts\"].tolist()\n            total_images_needed = sum(counts)\n    \n            img_flat = None\n            actual_img_dim = self.image_config.get(\"dim\", 768)\n    \n            if pi is not None and pi.numel() > 0 and total_images_needed > 0:\n                pi = pi.to(device)\n                if pi.size(0) > total_images_needed:\n                    pi = pi[:total_images_needed]\n    \n                if hasattr(self.image_model, \"get_image_features\"):\n                    img_flat = self.image_model.get_image_features(pixel_values=pi)\n                else:\n                    outputs = self.image_model(pixel_values=pi)\n                    img_flat = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state[:, 0]\n    \n                img_flat = self._normalize_2d(img_flat)\n                actual_img_dim = img_flat.size(1) if img_flat is not None else actual_img_dim\n    \n            img_z = self._aggregate_embeddings(\n                img_flat, counts,\n                self.image_config[\"max_images\"],\n                actual_img_dim,\n                self.image_config[\"image_agg\"],\n                len(counts),\n                device\n            )\n    \n            if actual_img_dim != self.image_config.get(\"dim\"):\n                self.image_config[\"dim\"] = actual_img_dim\n                self.out_dim_per_modality[\"image\"] = (\n                    actual_img_dim * self.image_config[\"max_images\"]\n                    if self.image_config[\"image_agg\"] == \"concat\" else actual_img_dim\n                )\n    \n            results[\"image\"] = img_z\n    \n        # Аудио\n        if self.audio_model is not None and \"audio_inputs\" in backend_inputs:\n            counts = backend_inputs[\"audio_counts\"].tolist()\n            total_audios_needed = sum(counts)\n    \n            aud_flat = None\n            actual_aud_dim = self.audio_config.get(\"dim\", 768)\n            model_type = self.audio_config.get(\"model_type\")\n    \n            if total_audios_needed > 0:\n                if model_type == \"clap\":\n                    af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n                    if af is not None and af.numel() > 0:\n                        af = af.to(device)\n                        if af.size(0) > total_audios_needed:\n                            af = af[:total_audios_needed]\n                        with torch.cuda.amp.autocast(enabled=False):\n                            aud_flat = self.audio_model.get_audio_features(input_features=af.float())\n                        aud_flat = self._normalize_2d(aud_flat.float())\n                        actual_aud_dim = aud_flat.size(1)\n    \n                elif model_type == \"wav2clip\":\n                    raw_list = backend_inputs[\"audio_inputs\"].get(\"raw_audios\", [])\n                    if len(raw_list) > total_audios_needed:\n                        raw_list = raw_list[:total_audios_needed]\n                    if len(raw_list) > 0:\n                        embs = [self._wav2clip_embed(arr, device) for arr in raw_list]\n                        aud_flat = torch.stack(embs, dim=0)\n                        aud_flat = self._normalize_2d(aud_flat)\n                        actual_aud_dim = aud_flat.size(1)\n    \n                else:\n                    av = backend_inputs[\"audio_inputs\"][\"input_values\"]\n                    if av is not None and av.numel() > 0:\n                        av = av.to(device)\n                        if av.size(0) > total_audios_needed:\n                            av = av[:total_audios_needed]\n                        av = av.clamp_(-1.0, 1.0)\n                        with torch.cuda.amp.autocast(enabled=False):\n                            outputs = self.audio_model(input_values=av.float())\n                            feats = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n                        aud_flat = self._normalize_2d(feats.float())\n                        actual_aud_dim = aud_flat.size(1)\n    \n            aud_z = self._aggregate_embeddings(\n                aud_flat, counts,\n                self.audio_config[\"max_audios\"],\n                actual_aud_dim,\n                self.audio_config[\"audio_agg\"],\n                len(counts),\n                device\n            )\n    \n            if aud_flat is not None and actual_aud_dim != self.audio_config.get(\"dim\"):\n                self.audio_config[\"dim\"] = actual_aud_dim\n                self.out_dim_per_modality[\"audio\"] = (\n                    actual_aud_dim * self.audio_config[\"max_audios\"]\n                    if self.audio_config[\"audio_agg\"] == \"concat\" else actual_aud_dim\n                )\n    \n            results[\"audio\"] = aud_z\n    \n        # Строгая проверка согласованности размеров батча между модальностями\n        if results:\n            bs_list = [v.size(0) for v in results.values()]\n            if len(set(bs_list)) != 1:\n                raise RuntimeError(f\"Inconsistent batch sizes across modalities: {bs_list}\")\n    \n        return results\n\n\n# =========================\n# Классификатор (голова регрессии)\n# =========================\n\nclass SingleBackboneClassifier(nn.Module):\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_labels: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError('Для fusion=\"mean\" размеры модальностей должны совпадать')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                has_trainable = any(p.requires_grad for p in m.parameters()) if hasattr(m, \"parameters\") else False\n            except Exception:\n                has_trainable = False\n            if not has_trainable:\n                continue\n            try:\n                cfg = getattr(m, \"config\", None)\n                if cfg is not None and hasattr(cfg, \"use_cache\"):\n                    cfg.use_cache = False\n            except Exception:\n                pass\n            try:\n                if hasattr(m, \"gradient_checkpointing_enable\"):\n                    try:\n                        if gradient_checkpointing_kwargs is not None:\n                            m.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                        else:\n                            m.gradient_checkpointing_enable()\n                    except TypeError:\n                        m.gradient_checkpointing_enable()\n            except Exception:\n                pass\n\n    def gradient_checkpointing_disable(self):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                if hasattr(m, \"gradient_checkpointing_disable\"):\n                    m.gradient_checkpointing_disable()\n            except Exception:\n                pass\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        batch_size = None\n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() == 3:\n                    t = t.mean(dim=1)\n                elif t.dim() > 3:\n                    t = t.view(t.size(0), -1)\n                feats.append(t)\n                if batch_size is None:\n                    batch_size = t.size(0)\n        if batch_size is not None:\n            feats = [f[:batch_size] for f in feats]\n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        if not z:\n            raise ValueError(\"Backend не вернул эмбеддинги\")\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\n# =========================\n# Trainer для регрессии (MSE)\n# =========================\n\nclass MSETrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        labels = inputs.pop(\"labels\").to(torch.float32)\n        outputs = model(**inputs)\n        logits = outputs.logits\n        preds = logits if logits.dim() == 1 else (logits.squeeze(-1) if logits.size(-1) == 1 else logits)\n        labels = labels.view_as(preds)\n        loss = F.mse_loss(preds, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# =========================\n# Прогресс-логгер\n# =========================\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n        self.tqdm = tqdm\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            self.tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\n# =========================\n# Основной пайплайн (регрессия)\n# =========================\n\nclass SingleModelMultiComboRegression:\n    \"\"\"\n    Высокоуровневый пайплайн мультимодальной регрессии (text / image / audio) поверх моделей Hugging Face\n    и wav2clip. Поддерживает автоматическую сборку бэкенда под набор модальностей, батчевую токенизацию,\n    предварительную токенизацию датасета, чанковую тренировку, раннюю остановку, извлечение эмбеддингов.\n\n    Основное:\n    - Комбинации модальностей: [\"text\"], [\"image\"], [\"audio\"], [\"text\",\"image\"], [\"text\",\"audio\"], [\"text\",\"image\",\"audio\"].\n    - Модели:\n        • Text: Auto (BERT/DistilBERT и т.п.), CLIP text, CLAP text.\n        • Image: Auto (ViT и т.п.), CLIP image.\n        • Audio: CLAP audio, Auto (Wav2Vec2), wav2clip (без HF процессора).\n    - Слияние эмбеддингов: \"concat\" или \"mean\" (для \"mean\" размеры эмбеддингов должны совпадать).\n    - Тренируется регрессионная «голова» поверх (замороженных по умолчанию) энкодеров.\n    - Метрики: rmse, mae, r2 (считаются внутри numpy; библиотека evaluate не требуется).\n    - Предикт: возвращает массив float значений формы [N] или [N, num_labels].\n    - Эмбеддинги: fused (и опционально по модальностям).\n\n    :param modalities: Список используемых модальностей из {\"text\",\"image\",\"audio\"}.\n    :param num_labels: Размерность целевой переменной (обычно 1). Определяет выходную размерность регрессионной «головы».\n    :param target_column_name: Имя столбца-цели в DataFrame (float значения).\n    :param text_columns: Список текстовых колонок. Их значения конкатенируются через special_tokens[\"sep\"].\n    :param image_columns: Список колонок с изображениями (пути, PIL.Image.Image, np.ndarray или списки таких объектов).\n    :param audio_columns: Список колонок с аудио (пути к файлам или np.ndarray float32 моно; для путей нужен torchaudio).\n    :param text_tokenizer_fn: Опциональная пользовательская функция препроцессинга текста.\n                              Если возвращает dict с 'input_ids' — пайплайн считает, что это уже тензоры токенов.\n                              Если возвращает строку — затем применяется стандартная токенизация процессором.\n    :param special_tokens: Спец. токены (разделители), по умолчанию {\"sep\": \" [SEP] \"}.\n    :param tokenizer_returns_tensors: Флаг для совместимости с внешними токенизаторами (если text_tokenizer_fn отдает тензоры).\n    :param backend: Режим сборки бэкенда: \"auto\" | \"clip\" | \"clap\" | другой (для явных конфигов).\n    :param clip_checkpoint: Дефолтный чекпойнт CLIP (используется, если выбран соответствующий путь).\n    :param clap_checkpoint: Дефолтный чекпойнт CLAP.\n    :param text_model_config: Конфиг текстовой модели, например:\n                             {\"checkpoint\": \"...\", \"model_type\": \"clip|clap|auto|bert\", \"max_length\": int}.\n    :param image_model_config: Конфиг визуальной модели, например:\n                             {\"checkpoint\": \"...\", \"model_type\": \"clip|vit|auto\", \"max_images\": int, \"image_agg\": \"concat|mean\"}.\n    :param audio_model_config: Конфиг аудио-модели, например:\n                             {\"checkpoint\": \"...\", \"model_type\": \"clap|auto|wav2clip\", \"sr\": int, \"max_audios\": int, \"audio_agg\": \"concat|mean\"}.\n    :param fusion: Слияние модальностей: \"concat\" (по умолчанию) или \"mean\". Для \"mean\" размерности эмбеддингов должны совпадать.\n    :param freeze_backbone: Заморозить энкодеры модальностей (True по умолчанию).\n    :param clip_max_length: Макс. длина для CLIP-текста (обычно 77).\n    :param max_images_per_sample: Максимум изображений на сэмпл (используется при агрегации \"concat\" или \"mean\").\n    :param max_audios_per_sample: Максимум аудио на сэмпл (аналогично изображению).\n    :param use_batch_tokenizer: Использовать батчевую токенизацию текста (LRU-кэш, ускорение).\n    :param pretokenize_data: Включить предварительную токенизацию датасета (ускоряет обучение/предикт).\n    :param pretokenize_batch_size: Батч-размер при предварительной токенизации.\n    :param tokenizer_cache_size: Размер LRU-кэша в батчевом токенизаторе.\n    :param max_pretokenize_samples: Лимит записей для предварительной токенизации (на чанк/раздел).\n    :param local_cache_dir: Локальная директория кэша моделей/процессоров HF.\n\n    :return: None\n\n    :raises ValueError: \n        - Если выбранные модальности пайплайна не поддерживаются собранным бэкендом.\n        - Если fusion=\"mean\" при различающихся размерностях эмбеддингов.\n        - Если отсутствуют обязательные колонки для выбранных модальностей в данных.\n    :raises OSError: Ошибки загрузки моделей/процессоров из HF Hub/локального кэша (сетевые/офлайн).\n    :raises RuntimeError: Ошибки устройств (CUDA/MPS), рассогласование размеров батчей между модальностями и т.п.\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        num_labels: int,\n        target_column_name: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1,\n        use_batch_tokenizer: bool = True,\n        pretokenize_data: bool = True,\n        pretokenize_batch_size: int = 256,\n        tokenizer_cache_size: int = 10000,\n        max_pretokenize_samples: int = 100000,\n        local_cache_dir: str = \"./model_cache\"\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.num_labels = num_labels\n        self.target_column_name = target_column_name\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[PbarConsoleLogger] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        mods = set(self.modalities)\n        name = self.backend_name\n\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n                }\n                self.image_model_config = self.image_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n                }\n            elif mods == {\"text\", \"audio\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n                }\n                self.audio_model_config = self.audio_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n                }\n            else:\n                if \"text\" in mods and self.text_model_config is None:\n                    self.text_model_config = {'checkpoint': 'bert-base-multilingual-cased', 'model_type': 'bert', 'max_length': 512}\n                if \"image\" in mods and self.image_model_config is None:\n                    self.image_model_config = {'checkpoint': 'google/vit-base-patch16-224', 'model_type': 'vit', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'}\n                if \"audio\" in mods and self.audio_model_config is None:\n                    self.audio_model_config = {'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000}\n\n        elif name == \"clip\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n            }\n            self.image_model_config = self.image_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n            }\n        elif name == \"clap\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n            }\n            self.audio_model_config = self.audio_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n            }\n        else:\n            pass\n\n        self.backend = UniversalMultiBackend(\n            text_model_config=self.text_model_config if \"text\" in mods else None,\n            image_model_config=self.image_model_config if \"image\" in mods else None,\n            audio_model_config=self.audio_model_config if \"audio\" in mods else None,\n            freeze=self.freeze_backbone,\n            text_tokenizer_fn=self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors,\n            use_batch_tokenizer=self.use_batch_tokenizer,\n            tokenizer_cache_size=self.tokenizer_cache_size,\n            tokenizer_batch_size=self.pretokenize_batch_size,\n            local_cache_dir=self.local_cache_dir\n        )\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}\")\n\n    def _setup_metrics(self, metric_name: str):\n        name = metric_name.lower()\n        if name not in (\"rmse\", \"mae\", \"r2\"):\n            raise ValueError('metric_name для регрессии должен быть \"rmse\", \"mae\" или \"r2\"')\n\n        def compute(p):\n            preds = p.predictions\n            y = p.label_ids\n            preds = preds.squeeze(-1) if preds.ndim == 2 and preds.shape[-1] == 1 else preds\n            y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n            axis = 0 if preds.ndim == 2 else None\n            if name == \"rmse\":\n                err = preds - y\n                mse = np.mean(err**2, axis=axis)\n                rmse = np.sqrt(mse)\n                return {\"rmse\": float(np.mean(rmse))}\n            elif name == \"mae\":\n                mae = np.mean(np.abs(preds - y), axis=axis)\n                return {\"mae\": float(np.mean(mae))}\n            else:\n                y_mean = np.mean(y, axis=axis, keepdims=True) if preds.ndim == 2 else np.mean(y)\n                ss_res = np.sum((y - preds) ** 2, axis=axis)\n                ss_tot = np.sum((y - y_mean) ** 2, axis=axis)\n                r2 = 1.0 - (ss_res / (ss_tot + 1e-12))\n                return {\"r2\": float(np.mean(r2))}\n\n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"rmse\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result_reg\",\n        seed: int = 42,\n        hidden: int = 256,\n        dropout: float = 0.1,\n        gradient_checkpointing: bool = False,\n        fit_chunk_size: Optional[int] = None,\n        clear_cache_every_n_chunks: int = 10,\n        early_stopping_patience: Optional[int] = 3,\n        early_stopping_threshold: float = 0.0\n    ):\n        \"\"\"\n        Обучает регрессионную голову поверх (по умолчанию) замороженных энкодеров модальностей.\n        Поддерживает предварительную токенизацию, чанковую тренировку, раннюю остановку,\n        градиентный чекпоинтинг, автоматический прогресс-бар.\n    \n        Данные:\n        - train_data/test_data — DataFrame со столбцом target_column_name (float).\n        - По каждой модальности — соответствующие колонки (text/image/audio).\n    \n        Тренировка:\n        - Trainer = MSETrainer (MSE лосс).\n        - Метрика для лучшей модели: eval_<metric_name>.\n        - evaluation_strategy=\"steps\" (каждые eval_steps), сохранение чекпоинтов каждые eval_steps.\n        - Чанковая тренировка: train_data делится на чанки (fit_chunk_size) для экономии памяти.\n    \n        :param train_data: Обучающий DataFrame со столбцом target_column_name.\n        :param epochs: Количество эпох.\n        :param test_size: Доля валидации (если test_data не задан).\n        :param test_data: Отдельный валидационный DataFrame.\n        :param per_device_train_batch_size: Батч на устройство.\n        :param gradient_accumulation_steps: Аккумуляция градиента (эффективный батч = batch_size * steps).\n        :param learning_rate: Базовый learning rate.\n        :param metric_name: \"rmse\" | \"mae\" | \"r2\".\n        :param fp16: Включить ли fp16 (True только при наличии CUDA).\n        :param logging_steps: Шаг логирования Trainer (учтите, что при одном шаге обучения train_loss может не залогироваться).\n        :param eval_steps: Периодичность валидации/сохранения.\n        :param output_dir: Каталог для результатов и чекпоинтов.\n        :param seed: Сид.\n        :param hidden: Размер скрытого слоя регрессионной головы.\n        :param dropout: Дропаут в голове.\n        :param gradient_checkpointing: Включить градиентный чекпоинтинг (пробрасывается в энкодеры, если они поддерживают).\n        :param fit_chunk_size: Размер чанка тренировочных данных (None — без разбиения).\n        :param clear_cache_every_n_chunks: Каждые N чанков очищать кэш токенизации.\n        :param early_stopping_patience: Патенс ранней остановки (None или <=0 — отключено).\n        :param early_stopping_threshold: Минимальное улучшение метрики для сброса патенса.\n    \n        :return: self\n    \n        :raises ValueError: Некорректные конфигурации (например, отсутствуют обязательные колонки).\n        :raises OSError: Ошибки чтения/загрузки данных/моделей.\n        :raises RuntimeError: Ошибки устройств (CUDA/MPS), рассогласование размеров батчей при слиянии модальностей.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds_eval = MultiComboDataset(\n            df=df_eval,\n            target_col=self.target_column_name,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_eval) < 50000),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_eval), self.max_pretokenize_samples),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        if gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n\n        self._setup_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.0,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def regression_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            out = self.backend.collate(batch_list)\n            if \"labels\" in out:\n                out[\"labels\"] = out[\"labels\"].to(torch.float32)\n            return out\n\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        n_train = len(df_train)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        dummy_idx = np.arange(min(len(df_train), 1))\n        ds_train_init = (\n            MultiComboDataset(\n                df=df_train.iloc[dummy_idx],\n                target_col=self.target_column_name,\n                text_columns=self.text_columns,\n                image_columns=self.image_columns,\n                audio_columns=self.audio_columns,\n                text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                special_tokens=self.special_tokens,\n                pretokenize=False,\n                tokenizer_returns_tensors=self.tokenizer_returns_tensors\n            ) if len(dummy_idx) > 0 else ds_eval\n        )\n\n        self.trainer = MSETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=regression_collator,\n            compute_metrics=self.compute_metrics\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        if (ds_eval is not None) and (early_stopping_patience is not None) and (early_stopping_patience > 0):\n            esc = EarlyStoppingCallback(\n                early_stopping_patience=int(early_stopping_patience),\n                early_stopping_threshold=float(early_stopping_threshold)\n            )\n            self.trainer.add_callback(esc)\n\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        steps_done = 0\n        chunk_counter = 0\n\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n            for slc in chunk_slices(shuffled, chunk_size):\n                chunk_df = df_train.iloc[slc]\n                ds_chunk = MultiComboDataset(\n                    df=chunk_df,\n                    target_col=self.target_column_name,\n                    text_columns=self.text_columns,\n                    image_columns=self.image_columns,\n                    audio_columns=self.audio_columns,\n                    text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                    text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                    special_tokens=self.special_tokens,\n                    pretokenize=(self.pretokenize_data and has_bt and len(slc) < self.max_pretokenize_samples and len(slc) > 100),\n                    pretokenize_batch_size=self.pretokenize_batch_size,\n                    max_cache_size=min(len(slc), self.max_pretokenize_samples),\n                    tokenizer_returns_tensors=self.tokenizer_returns_tensors\n                )\n                self.trainer.train_dataset = ds_chunk\n\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk, chunk_df\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                chunk_counter += 1\n                if chunk_counter % clear_cache_every_n_chunks == 0:\n                    if hasattr(ds_chunk, 'clear_cache'):\n                        ds_chunk.clear_cache()\n                        print(f\"✓ Очищен кэш токенизации после {chunk_counter} чанков\")\n\n                del ds_chunk, chunk_df\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n\n        if getattr(self.backend, \"batch_tokenizer\", None):\n            self.backend.batch_tokenizer.clear_cache()\n\n        return self\n\n    def predict(\n        self,\n        df: pd.DataFrame,\n        batch_size: Optional[int] = None\n    ) -> np.ndarray:\n        \"\"\"\n        Выполняет предсказание для регрессии.\n    \n        :param df: DataFrame с колонками модальностей (если нет целевой колонки, будет заполнена нулями).\n        :param batch_size: Переопределить per_device_eval_batch_size на время инференса.\n    \n        :return: np.ndarray формы [N] (если num_labels=1) или [N, num_labels] (для многомерной регрессии).\n    \n        :raises RuntimeError: Если модель не обучена (trainer отсутствует).\n        :raises OSError: Ошибки чтения исходных данных (пути к изображениям/аудио).\n        :raises ValueError: Внутренние ошибки коллатора/бэкенда (например, несовпадение размеров батчей между модальностями).\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = 0.0\n\n        print(f\"Preparing dataset for prediction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_c), 10000),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n\n        print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds)\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        if hasattr(ds, 'clear_cache'):\n            ds.clear_cache()\n\n        y = preds.predictions\n        y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n        return y\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        \"\"\"\n        Извлекает эмбеддинги для входных данных.\n    \n        :param df: DataFrame с колонками модальностей (если нет целевой колонки, будет заполнена нулями).\n        :param batch_size: Батч-размер во время извлечения эмбеддингов.\n        :param return_per_modality: Если True — вернуть помимо fused также по-модальные эмбеддинги.\n    \n        :return:\n            - Если return_per_modality=False:\n                np.ndarray fused формы [N, D_fused], где D_fused — размерность после слияния (concat/mean).\n            - Если return_per_modality=True:\n                (fused, per_mod) — кортеж:\n                    fused: np.ndarray [N, D_fused],\n                    per_mod: Dict[str, np.ndarray] с ключами \"text\"/\"image\"/\"audio\", значениями [N, D_mod].\n    \n        :raises RuntimeError: Если модель не обучена (нет trainer/model).\n        :raises OSError: Ошибки чтения исходных данных (пути к изображениям/аудио).\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = 0.0\n\n        print(f\"Preparing dataset for embeddings extraction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=False,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device: torch.device):\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        print(\"Concatenating embeddings...\")\n        fused_arr = np.vstack(fused_list)\n\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Данные\nn = 8\ndf = pd.DataFrame({\n    \"title\": [f\"some short text {i}\" for i in range(n)],\n    \"audio\": [(np.sin(np.linspace(0, 2*np.pi, 48000)).astype(np.float32) * 0.1) for _ in range(n)],\n    \"target\": np.random.randn(n).astype(np.float32)\n})\n\n# Инициализация пайплайна (text=CLIP, audio=CLAP)\npipe = SingleModelMultiComboRegression(\n    modalities=[\"text\",\"audio\"],\n    num_labels=1,\n    target_column_name=\"target\",\n    text_columns=[\"title\"],\n    audio_columns=[\"audio\"],\n    text_model_config={\n        \"checkpoint\": \"openai/clip-vit-base-patch32\",\n        \"model_type\": \"clip\",\n        \"max_length\": 77\n    },\n    audio_model_config={\n        \"checkpoint\": \"laion/clap-htsat-unfused\",\n        \"model_type\": \"clap\",\n        \"sr\": 48000,\n        \"max_audios\": 2,\n        \"audio_agg\": \"concat\"\n    },\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=True,\n    pretokenize_data=True,\n    pretokenize_batch_size=64,\n    tokenizer_cache_size=10000,\n    max_pretokenize_samples=50000,\n    local_cache_dir=\"./model_cache\"\n)\n\n# Тренировка\npipe.fit(\n    train_data=df,\n    epochs=2,\n    test_size=0.25,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-4,\n    metric_name=\"rmse\",\n    fp16=True,                       # будет True, только если есть CUDA\n    logging_steps=10,\n    eval_steps=10,\n    output_dir=\"./out_clip_clap\",\n    seed=42,\n    hidden=512,\n    dropout=0.1,\n    gradient_checkpointing=True,     # включить GC там, где поддерживается\n    fit_chunk_size=None,\n    clear_cache_every_n_chunks=2,\n    early_stopping_patience=3,\n    early_stopping_threshold=0.0\n)\n\n# Предсказания\ny_pred = pipe.predict(df, batch_size=4)\nprint(\"Pred shape:\", y_pred.shape)\n\n# Эмбеддинги\nemb_fused, emb_per = pipe.get_embeddings(df, batch_size=4, return_per_modality=True)\nprint(\"Fused:\", emb_fused.shape, \"| text:\", emb_per[\"text\"].shape, \"| audio:\", emb_per[\"audio\"].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Данные\nn = 9\ndf = pd.DataFrame({\n    \"text\": [f\"example text #{i}\" for i in range(n)],\n    \"image\": [(np.random.rand(224,224,3) * 255).astype(np.uint8) for _ in range(n)],\n    \"audio\": [(np.random.randn(16000).astype(np.float32)*0.05) for _ in range(n)],\n    \"target\": np.random.randn(n).astype(np.float32)\n})\n\npipe = SingleModelMultiComboRegression(\n    modalities=[\"text\",\"image\",\"audio\"],\n    num_labels=1,\n    target_column_name=\"target\",\n    text_columns=[\"text\"],\n    image_columns=[\"image\"],\n    audio_columns=[\"audio\"],\n    text_model_config={\n        \"checkpoint\": \"distilbert-base-uncased\",\n        \"model_type\": \"auto\",\n        \"max_length\": 128\n    },\n    image_model_config={\n        \"checkpoint\": \"google/vit-base-patch16-224\",\n        \"model_type\": \"vit\",\n        \"max_images\": 2,\n        \"image_agg\": \"mean\"\n    },\n    audio_model_config={\n        \"checkpoint\": \"facebook/wav2vec2-base-960h\",\n        \"model_type\": \"auto\",\n        \"sr\": 16000,\n        \"max_audios\": 2,\n        \"audio_agg\": \"mean\"\n    },\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=True,\n    pretokenize_data=True,\n    pretokenize_batch_size=32,\n    tokenizer_cache_size=20000,\n    max_pretokenize_samples=100000,\n    local_cache_dir=\"./model_cache\"\n)\n\npipe.fit(\n    train_data=df,\n    epochs=2,\n    test_size=0.3,\n    per_device_train_batch_size=3,\n    gradient_accumulation_steps=1,\n    learning_rate=1e-4,\n    metric_name=\"r2\",\n    fp16=True,\n    logging_steps=5,\n    eval_steps=5,\n    output_dir=\"./out_auto_triplet\",\n    seed=123,\n    hidden=384,\n    dropout=0.1,\n    gradient_checkpointing=True,\n    fit_chunk_size=6,                # демонстрация чанковой тренировки\n    clear_cache_every_n_chunks=2,\n    early_stopping_patience=2\n)\n\ny = pipe.predict(df, batch_size=3)\nprint(\"Pred shape:\", y.shape)\n\nfused, per_mod = pipe.get_embeddings(df, batch_size=3, return_per_modality=True)\nprint(\"Fused:\", fused.shape, \"| text:\", per_mod[\"text\"].shape, \"| image:\", per_mod[\"image\"].shape, \"| audio:\", per_mod[\"audio\"].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 3.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Данные\nn = 5\ndf = pd.DataFrame({\n    \"audio\": [(np.sin(np.linspace(0, 6.28, 16000)).astype(np.float32)*0.05) for _ in range(n)],\n    \"target\": np.random.randn(n).astype(np.float32)\n})\n\npipe = SingleModelMultiComboRegression(\n    modalities=[\"audio\"],\n    num_labels=1,\n    target_column_name=\"target\",\n    audio_columns=[\"audio\"],\n    audio_model_config={\"model_type\": \"wav2clip\", \"sr\": 16000, \"max_audios\": 1, \"audio_agg\": \"mean\"},\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=False,      # текст не используется\n    pretokenize_data=False\n)\n\npipe.fit(df, epochs=1, test_size=0.33, per_device_train_batch_size=2, eval_steps=1, logging_steps=1, output_dir=\"./out_w2c\")\n\npred = pipe.predict(df)\nprint(\"Pred shape:\", pred.shape)\n\nemb = pipe.get_embeddings(df)\nprint(\"Embeddings:\", emb.shape)    # ожидаемо [N, 512]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}