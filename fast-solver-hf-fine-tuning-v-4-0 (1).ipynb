{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Дообучение encoder'а для классификации токенов.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  numpy==1.26.4 \\\n  pandas==2.2.3 \\\n  tqdm==4.67.1 \\\n  transformers==4.51.3 \\\n  evaluate==0.4.5 \\\n  torch==2.6.0+cu124 \\\n  seqeval==1.2.2\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport gc\nimport math\nfrom typing import List, Dict, Any, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport evaluate\nfrom transformers import (\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForTokenClassification,\n    TrainerCallback,\n    PrinterCallback,\n    EarlyStoppingCallback,\n)\nfrom transformers.modeling_outputs import ModelOutput\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\n\ndef set_seed(seed: int = 42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                try:\n                    parts.append(f\"{k.replace('eval_', '')} {float(v):.4f}\")\n                except Exception:\n                    pass\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    try:\n                        extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n                    except Exception:\n                        pass\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\nclass WeightedTokenCETrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n        self._warned_label_tiling = False\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n\n        if logits.size(0) != labels.size(0):\n            ngpu = torch.cuda.device_count()\n            if ngpu > 1 and logits.size(0) == labels.size(0) * ngpu:\n                labels = labels.repeat_interleave(ngpu, dim=0)\n                if not self._warned_label_tiling:\n                    print(f\"[Warning] DataParallel удвоил batch для logits. \"\n                          f\"Повторяем labels x{ngpu}. logits: {tuple(logits.shape)}, labels: {tuple(labels.shape)}\")\n                    self._warned_label_tiling = True\n            else:\n                raise ValueError(f\"Batch size mismatch: logits {tuple(logits.shape)} vs labels {tuple(labels.shape)}\")\n\n        loss_fct = torch.nn.CrossEntropyLoss(\n            weight=(self.class_weights.to(logits.device) if self.class_weights is not None else None),\n            ignore_index=-100,\n        )\n        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n\nclass TokenClassification:\n    \"\"\"\n    Класс-обёртка для обучения и инференса моделей токен-классификации (NER/POS и т.д.)\n    на базе Hugging Face Transformers. Поддерживает обучение со «скользящим окном»,\n    выравнивание меток слов с субтокенами, расчёт весов классов по словам, раннюю остановку,\n    агрегирование логитов по перекрывающимся окнам и извлечение эмбеддингов слов.\n    \"\"\"\n\n    def __init__(\n        self,\n        checkpoint: str,\n        label2id: Dict[str, int],\n        tokens_column_name: str,\n        tags_column_name: str\n    ):\n        \"\"\"\n        Инициализирует модель, токенайзер и инфраструктуру для обучения/инференса.\n\n        :param checkpoint: имя/путь модели в Hugging Face (например, 'bert-base-cased').\n        :param label2id: словарь отображения строковых меток в целочисленные id.\n        :param tokens_column_name: имя колонки DataFrame с токенами (словами).\n        :param tags_column_name: имя колонки DataFrame с метками (строки или уже id).\n        :return: None\n        :raises: ValueError при некорректных входных параметрах (например, пустой label2id).\n        \"\"\"\n        self.id2label = {v: k for k, v in label2id.items()}\n        self.label2id = label2id\n\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            checkpoint,\n            num_labels=len(self.id2label),\n            id2label=self.id2label,\n            label2id=self.label2id,\n            ignore_mismatched_sizes=True\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.tokens_column_name = tokens_column_name\n        self.tags_column_name = tags_column_name\n\n        # Градиентный чекпоинтинг (если поддерживается моделью)\n        try:\n            self.model.gradient_checkpointing_enable()\n        except Exception:\n            pass\n\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.data_collator = DataCollatorForTokenClassification(tokenizer=self.tokenizer)\n        self.progress_callback: Optional[TrainerCallback] = None\n\n    # ------------------------------\n    # Вспомогательные хелперы\n    # ------------------------------\n    @staticmethod\n    def _labels_are_strings(labels_col_list) -> bool:\n        \"\"\"\n        Определяет, представлены ли метки строками (а не id).\n\n        :param labels_col_list: итерируемая коллекция списков меток (по документам).\n        :return: True, если метки строковые; False, если уже id или все пусто.\n        :raises: None\n        \"\"\"\n        for tags in labels_col_list:\n            if isinstance(tags, (list, tuple)) and len(tags) > 0:\n                return isinstance(tags[0], str)\n        return False\n\n    def _label_to_id(self, tag: str) -> int:\n        \"\"\"\n        Преобразует строковую метку в id согласно self.label2id.\n\n        :param tag: строковая метка.\n        :return: целочисленный id метки.\n        :raises ValueError: если метка отсутствует в label2id.\n        \"\"\"\n        if tag not in self.label2id:\n            raise ValueError(\n                f\"Unknown label encountered: '{tag}'. \"\n                f\"Known labels: {sorted(self.label2id.keys())}\"\n            )\n        return int(self.label2id[tag])\n\n    def _assert_tokens_labels_same_len(self, tokens_seq, labels_seq):\n        \"\"\"\n        Проверяет совпадение длины списков токенов и меток для каждого документа.\n\n        :param tokens_seq: iterable со списками токенов (по документам).\n        :param labels_seq: iterable со списками меток (по документам).\n        :return: None\n        :raises ValueError: если типы неверны или длины не совпадают.\n        \"\"\"\n        for i, (toks, labs) in enumerate(zip(tokens_seq, labels_seq)):\n            if not isinstance(toks, (list, tuple)) or not isinstance(labs, (list, tuple)):\n                raise ValueError(\n                    f\"Row {i}: tokens/labels must be lists, got \"\n                    f\"{type(toks).__name__} and {type(labs).__name__}\"\n                )\n            if len(toks) != len(labs):\n                raise ValueError(\n                    f\"Row {i}: tokens and labels length mismatch: \"\n                    f\"{len(toks)} vs {len(labs)}\"\n                )\n\n    @staticmethod\n    def _to_token_list(obj):\n        \"\"\"\n        Приводит значение ячейки к списку токенов.\n\n        :param obj: значение колонки токенов (list/tuple/np.ndarray/None/другое).\n        :return: список токенов (или пустой список при неподдерживаемом типе).\n        :raises: None\n        \"\"\"\n        if obj is None:\n            return []\n        if isinstance(obj, (list, tuple)):\n            return list(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return []\n\n    def _get_effective_max_length(self) -> int:\n        \"\"\"\n        Возвращает безопасную максимальную длину контекста:\n        min(model.config.max_position_embeddings, tokenizer.model_max_length),\n        игнорируя «бесконечные» значения токенайзера.\n\n        :return: целочисленное значение безопасной максимальной длины.\n        :raises: None\n        \"\"\"\n        m_conf = int(getattr(self.model.config, \"max_position_embeddings\", 512) or 512)\n        m_tok = int(getattr(self.tokenizer, \"model_max_length\", 512) or 512)\n        if m_tok > 100000:\n            return m_conf\n        return min(m_conf, m_tok)\n\n    @staticmethod\n    def _sanitize_stride(stride: int, max_length: int) -> int:\n        \"\"\"\n        Ограничивает stride до [0, max_length - 2], учитывая спецтокены.\n\n        :param stride: желаемый страйд перекрытия.\n        :param max_length: безопасная максимальная длина контекста.\n        :return: целочисленное безопасное значение stride.\n        :raises: None\n        \"\"\"\n        stride = int(max(0, stride))\n        return int(min(stride, max(0, max_length - 2)))\n\n    # ------------------------------\n    # Алгоритмика\n    # ------------------------------\n    @staticmethod\n    def _align_labels_with_word_ids(labels_ids: List[int], word_ids: List[Optional[int]]) -> List[int]:\n        \"\"\"\n        Выравнивает метки слов по субтокенам: первый субтокен слова получает метку,\n        последующие субтокены — -100 (игнор в CrossEntropy).\n\n        :param labels_ids: список меток по словам (id), длина = числу слов.\n        :param word_ids: список индексов слов для каждого субтокена (tokenizer.word_ids()).\n        :return: список меток по длине субтокенов, с -100 для игнорируемых позиций.\n        :raises: None\n        \"\"\"\n        new_labels = []\n        prev_word_id = None\n        L = len(labels_ids)\n        for wid in word_ids:\n            if wid is None or wid < 0 or wid >= L:\n                new_labels.append(-100)\n            else:\n                if wid != prev_word_id:\n                    new_labels.append(labels_ids[wid])\n                else:\n                    new_labels.append(-100)\n            prev_word_id = wid\n        return new_labels\n\n    def _tokenize_and_align_chunk(\n        self,\n        docs_tokens: List[List[str]],\n        docs_labels_ids: List[List[int]],\n        max_length: int,\n        stride: int\n    ) -> Dataset:\n        \"\"\"\n        Токенизирует документы с разбиением на окна и выравниванием меток.\n\n        :param docs_tokens: списки токенов по документам.\n        :param docs_labels_ids: списки меток (id) по документам.\n        :param max_length: безопасная максимальная длина контекста.\n        :param stride: перекрытие между окнами.\n        :return: HF Dataset с полями input_ids, attention_mask, labels.\n        :raises: None\n        \"\"\"\n        enc = self.tokenizer(\n            docs_tokens,\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True\n        )\n        mapping = enc.pop(\"overflow_to_sample_mapping\")\n        num_chunks = len(enc[\"input_ids\"])\n\n        all_labels = []\n        for i in range(num_chunks):\n            doc_idx = int(mapping[i])\n            word_ids = enc.word_ids(batch_index=i)\n            aligned = self._align_labels_with_word_ids(docs_labels_ids[doc_idx], word_ids)\n            all_labels.append(aligned)\n\n        return Dataset.from_dict({\n            \"input_ids\": enc[\"input_ids\"],\n            \"attention_mask\": enc[\"attention_mask\"],\n            \"labels\": all_labels\n        })\n\n    def _count_total_chunks(\n        self,\n        docs_tokens: List[List[str]],\n        max_length: int,\n        stride: int,\n        batch_docs: int = 64\n    ) -> int:\n        \"\"\"\n        Подсчитывает число чанков (окон) после токенизации набора документов.\n\n        :param docs_tokens: списки токенов по документам.\n        :param max_length: безопасная максимальная длина контекста.\n        :param stride: перекрытие между окнами.\n        :param batch_docs: размер батча документов при токенизации.\n        :return: общее число чанков (int).\n        :raises: None\n        \"\"\"\n        total = 0\n        for i in range(0, len(docs_tokens), batch_docs):\n            batch = docs_tokens[i:i + batch_docs]\n            enc = self.tokenizer(\n                batch,\n                is_split_into_words=True,\n                return_overflowing_tokens=True,\n                max_length=max_length,\n                stride=stride,\n                truncation=True\n            )\n            total += len(enc[\"input_ids\"])\n        return total\n\n    def _compute_class_weights_over_words(self, docs_labels_ids: List[List[int]]) -> np.ndarray:\n        \"\"\"\n        Считает веса классов по словам (без влияния overlap-окон).\n\n        :param docs_labels_ids: списки меток (id) по документам.\n        :return: массив весов классов shape=(num_labels,), dtype=float32.\n        :raises: None\n        \"\"\"\n        num_labels = len(self.id2label)\n        counts = np.zeros(num_labels, dtype=np.int64)\n        for labs in docs_labels_ids:\n            if isinstance(labs, (list, tuple)) and len(labs) > 0:\n                arr = np.asarray(labs, dtype=np.int64)\n                arr = arr[(arr >= 0) & (arr < num_labels)]\n                if arr.size > 0:\n                    counts += np.bincount(arr, minlength=num_labels)\n        N = counts.sum()\n        weights = np.zeros(num_labels, dtype=np.float32)\n        if N > 0:\n            nonzero = counts > 0\n            weights[nonzero] = N / (num_labels * counts[nonzero].astype(np.float32))\n        return weights\n\n    @staticmethod\n    def _normalize_clip_weights(w: np.ndarray, clip: float = 5.0) -> np.ndarray:\n        \"\"\"\n        Нормирует и клипует веса классов: клип сверху до clip и нормировка\n        положительных весов к среднему ~1.0.\n\n        :param w: исходные веса классов.\n        :param clip: верхняя граница клипа (None/<=0 — без клипа).\n        :return: нормированные веса dtype=float32.\n        :raises: None\n        \"\"\"\n        w = np.asarray(w, dtype=np.float32)\n        if clip is not None and clip > 0:\n            w = np.minimum(w, clip)\n        mask = w > 0\n        mean = float(np.mean(w[mask])) if np.any(mask) else 1.0\n        if mean > 0:\n            w = w / mean\n        return w\n\n    def _setup_compute_metrics(self):\n        \"\"\"\n        Создаёт и сохраняет функцию метрик для seqeval (self.compute_metrics).\n\n        Метрики:\n        - precision/recall/f1/accuracy — агрегированные;\n        - f1_{entity} — по каждой сущности.\n\n        :return: None\n        :raises: None\n        \"\"\"\n        metric = evaluate.load(\"seqeval\")\n\n        def compute_seqeval_metrics(p):\n            if isinstance(p, (tuple, list)):\n                predictions, labels = p\n            else:\n                predictions, labels = p.predictions, p.label_ids\n\n            predictions = np.argmax(predictions, axis=2)\n\n            true_predictions = [\n                [self.id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            true_labels = [\n                [self.id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n\n            results = metric.compute(predictions=true_predictions, references=true_labels)\n\n            out = {\n                \"precision\": results.get(\"overall_precision\", 0.0),\n                \"recall\": results.get(\"overall_recall\", 0.0),\n                \"f1\": results.get(\"overall_f1\", 0.0),\n                \"accuracy\": results.get(\"overall_accuracy\", 0.0),\n            }\n            for ent, vals in results.items():\n                if isinstance(vals, dict) and \"f1\" in vals:\n                    out[f\"f1_{ent}\"] = float(vals[\"f1\"])\n            return out\n\n        self.compute_metrics = compute_seqeval_metrics\n\n    def _prepare_dataset_with_sliding_window(self, df: pd.DataFrame, max_length: int, stride: int) -> Dataset:\n        \"\"\"\n        Готовит HF Dataset для оценки/валидации со «скользящим окном».\n\n        :param df: DataFrame с колонками токенов и меток.\n        :param max_length: безопасная максимальная длина контекста.\n        :param stride: перекрытие между окнами.\n        :return: HF Dataset с полями input_ids, attention_mask, labels.\n        :raises ValueError: при неверных типах или несовпадении длины токенов и меток.\n        \"\"\"\n        docs_tokens = df[self.tokens_column_name].tolist()\n        docs_labels = df[self.tags_column_name].tolist()\n\n        if self._labels_are_strings(docs_labels):\n            docs_labels = [[self._label_to_id(tag) for tag in tags] for tags in docs_labels]\n\n        filtered_tokens, filtered_labels = [], []\n        for i, (toks, labs) in enumerate(zip(docs_tokens, docs_labels)):\n            if not isinstance(toks, (list, tuple)) or not isinstance(labs, (list, tuple)):\n                raise ValueError(\n                    f\"Row {i}: tokens/labels must be lists, got \"\n                    f\"{type(toks).__name__} and {type(labs).__name__}\"\n                )\n            if len(toks) == 0 and len(labs) == 0:\n                continue\n            if len(toks) != len(labs):\n                raise ValueError(\n                    f\"Row {i}: tokens and labels length mismatch: \"\n                    f\"{len(toks)} vs {len(labs)}\"\n                )\n            filtered_tokens.append(list(toks))\n            filtered_labels.append(list(labs))\n\n        if len(filtered_tokens) == 0:\n            return Dataset.from_dict({\"input_ids\": [], \"attention_mask\": [], \"labels\": []})\n\n        return self._tokenize_and_align_chunk(filtered_tokens, filtered_labels, max_length, stride)\n\n    # ------------------------------\n    # Обучение\n    # ------------------------------\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        test_size: float = 0.2,\n        learning_rate: float = 2e-5,\n        fp16: bool = True,\n        stride: int = 128,\n        logging_steps: int = 50,\n        eval_steps: int = 100,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        fit_chunk_size_docs: Optional[int] = None,\n        early_stopping_patience: Optional[int] = 3,\n        early_stopping_threshold: float = 0.0,\n    ):\n        \"\"\"\n        Обучает модель токен-классификации на данных.\n\n        :param train_data: DataFrame с колонками токенов и меток.\n        :param epochs: число эпох (проходов) по обучающему набору документов.\n        :param per_device_train_batch_size: размер батча на устройство при обучении.\n        :param gradient_accumulation_steps: число шагов аккумуляции градиента.\n        :param test_size: доля/размер валидации; при слишком малом наборе eval отключается автоматически.\n        :param learning_rate: скорость обучения (LR).\n        :param fp16: использовать ли fp16 (если bf16 не используется и доступен CUDA).\n        :param stride: перекрытие между окнами для токенизации длинных документов.\n        :param logging_steps: частота логирования в шагах.\n        :param eval_steps: частота валидации/сохранения (если есть eval).\n        :param output_dir: директория для артефактов обучения.\n        :param seed: seed для воспроизводимости.\n        :param fit_chunk_size_docs: сколько документов обучать за один «кусок» перед сменой train_dataset (None = все).\n        :param early_stopping_patience: количество подряд неулучшающихся точек валидации до остановки;\n                                       если None или <= 0 — ранняя остановка не используется.\n        :param early_stopping_threshold: минимальное относительное улучшение метрики, требуемое для сброса счётчика patience.\n        :return: self (для чейнинга).\n        :raises ValueError: при несогласованных данных (тип/длина токенов и меток).\n        \"\"\"\n        set_seed(seed)\n\n        max_length = self._get_effective_max_length()\n        stride = self._sanitize_stride(stride, max_length)\n\n        df_all = train_data.copy()\n        if self._labels_are_strings(df_all[self.tags_column_name].tolist()):\n            df_all[self.tags_column_name] = df_all[self.tags_column_name].apply(\n                lambda tags: [self._label_to_id(tag) for tag in tags]\n            )\n\n        self._assert_tokens_labels_same_len(\n            df_all[self.tokens_column_name].tolist(),\n            df_all[self.tags_column_name].tolist()\n        )\n\n        # Робастный train/val split\n        n_total = len(df_all)\n        use_eval = False\n        test_size_abs = 0\n        if n_total >= 2 and test_size and float(test_size) > 0:\n            if isinstance(test_size, float):\n                test_size_abs = int(round(n_total * float(test_size)))\n            else:\n                test_size_abs = int(test_size)\n            if test_size_abs <= 0:\n                test_size_abs = 1\n            if test_size_abs >= n_total:\n                test_size_abs = n_total - 1\n            use_eval = test_size_abs > 0\n\n        if use_eval:\n            df_train, df_eval = train_test_split(df_all, test_size=test_size_abs, random_state=seed, shuffle=True)\n        else:\n            df_train = df_all\n            df_eval = df_all.iloc[0:0]\n\n        eval_dataset = None\n        if len(df_eval) > 0:\n            eval_dataset = self._prepare_dataset_with_sliding_window(df_eval, max_length, stride)\n\n        train_docs_tokens = df_train[self.tokens_column_name].tolist()\n        train_docs_labels = df_train[self.tags_column_name].tolist()\n\n        class_weights = self._compute_class_weights_over_words(train_docs_labels)\n        class_weights = self._normalize_clip_weights(class_weights, clip=5.0)\n\n        self._setup_compute_metrics()\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\" if eval_dataset is not None else \"no\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\" if eval_dataset is not None else \"no\",\n            save_steps=eval_steps,\n            load_best_model_at_end=bool(eval_dataset is not None),\n            metric_for_best_model=\"eval_f1\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=bool(fp16 and torch.cuda.is_available()),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            disable_tqdm=True,\n            dataloader_pin_memory=True,\n            gradient_checkpointing=True,\n        )\n\n        data_collator = self.data_collator\n\n        def steps_for_size(n_samples: int, bsz: int, accum: int) -> int:\n            return max(0, math.ceil(math.ceil(n_samples / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(n_docs: int, chunk_docs: int):\n            for i in range(0, n_docs, chunk_docs):\n                yield slice(i, min(i + chunk_docs, n_docs))\n\n        n_docs = len(train_docs_tokens)\n        chunk_docs = int(fit_chunk_size_docs) if (fit_chunk_size_docs and fit_chunk_size_docs > 0) else n_docs\n\n        total_steps = 0\n        rng = np.random.default_rng(seed)\n        doc_indices = np.arange(n_docs)\n        for _ in range(epochs):\n            rng.shuffle(doc_indices)\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = doc_indices[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                n_samples = self._count_total_chunks(toks_chunk, max_length, stride, batch_docs=64)\n                total_steps += steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n\n        if n_docs > 0:\n            init_chunk_ds = self._tokenize_and_align_chunk(\n                [train_docs_tokens[0]], [train_docs_labels[0]], max_length, stride\n            )\n        else:\n            init_chunk_ds = Dataset.from_dict({\"input_ids\": [], \"attention_mask\": [], \"labels\": []})\n\n        self.trainer = WeightedTokenCETrainer(\n            model=self.model,\n            args=args,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics if eval_dataset is not None else None,\n            train_dataset=init_chunk_ds,\n            eval_dataset=eval_dataset,\n            tokenizer=self.tokenizer,\n            class_weights=class_weights\n        )\n        try:\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n\n        # Ранняя остановка (если есть eval и запрошена)\n        if eval_dataset is not None and (early_stopping_patience is not None) and (early_stopping_patience > 0):\n            early_cb = EarlyStoppingCallback(\n                early_stopping_patience=int(early_stopping_patience),\n                early_stopping_threshold=float(early_stopping_threshold),\n            )\n            self.trainer.add_callback(early_cb)\n\n        self.progress_callback = cb\n\n        steps_done = 0\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            order = np.arange(n_docs)\n            rng.shuffle(order)\n\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = order[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                labs_chunk = [train_docs_labels[i] for i in idx]\n\n                ds_chunk = self._tokenize_and_align_chunk(toks_chunk, labs_chunk, max_length, stride)\n                self.trainer.train_dataset = ds_chunk\n\n                n_samples = len(ds_chunk)\n                chunk_steps = steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                del ds_chunk\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n        return self\n\n    # ------------------------------\n    # Инференс\n    # ------------------------------\n    def _predict_single_document(self, tokens: List[str], stride: int) -> List[str]:\n        \"\"\"\n        Предсказывает метки для одного документа со «скользящим окном».\n\n        :param tokens: список слов (токенов) документа.\n        :param stride: перекрытие между окнами.\n        :return: список строковых меток той же длины, что и tokens.\n        :raises: None\n        \"\"\"\n        if not isinstance(tokens, (list, tuple)) or len(tokens) == 0:\n            return []\n\n        max_length = self._get_effective_max_length()\n        stride = self._sanitize_stride(stride, max_length)\n\n        tokenized_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n        )\n        tokenized_inputs.pop(\"overflow_to_sample_mapping\", None)\n\n        default_id = int(min(self.id2label.keys())) if len(self.id2label) else 0\n        default_label = self.id2label.get(default_id, str(default_id))\n\n        if not isinstance(tokenized_inputs.get(\"input_ids\", None), list) or len(tokenized_inputs[\"input_ids\"]) == 0:\n            return [default_label] * len(tokens)\n\n        chunk_dataset = Dataset.from_dict(tokenized_inputs)\n        outputs = self.trainer.predict(chunk_dataset)\n\n        if hasattr(outputs, \"predictions\"):\n            preds = outputs.predictions\n        else:\n            preds = outputs[\"predictions\"]\n\n        num_original_words = len(tokens)\n\n        # Основной путь: 3D логиты (num_chunks, seq_len, num_labels)\n        if isinstance(preds, np.ndarray) and preds.ndim == 3:\n            num_labels = preds.shape[-1]\n            word_logits = np.zeros((num_original_words, num_labels), dtype=np.float32)\n            word_counts = np.zeros((num_original_words,), dtype=np.float32)\n\n            for i in range(preds.shape[0]):\n                chunk_logits = preds[i]\n                try:\n                    chunk_word_ids = tokenized_inputs.word_ids(batch_index=i)\n                except Exception:\n                    continue\n                if chunk_word_ids is None:\n                    continue\n\n                for token_pos, word_id in enumerate(chunk_word_ids):\n                    if word_id is None:\n                        continue\n                    if token_pos == 0 or chunk_word_ids[token_pos - 1] != word_id:\n                        if 0 <= word_id < num_original_words:\n                            word_logits[word_id] += chunk_logits[token_pos]\n                            word_counts[word_id] += 1.0\n\n            mask = word_counts > 0\n            if np.any(mask):\n                word_logits[mask] /= word_counts[mask, None]\n\n            pred_ids = np.full(num_original_words, default_id, dtype=np.int32)\n            if np.any(mask):\n                pred_ids[mask] = word_logits[mask].argmax(-1)\n\n            filled = [self.id2label.get(int(x), str(int(x))) for x in pred_ids]\n            return filled\n\n        # Fallback: если preds не 3D\n        if isinstance(preds, np.ndarray):\n            if preds.ndim == 2:\n                predictions = preds\n            elif preds.ndim == 1:\n                predictions = preds[None, :]\n            else:\n                predictions = preds.reshape(len(tokenized_inputs[\"input_ids\"]), -1)\n        else:\n            predictions = np.asarray(preds)\n\n        final_predictions = np.full(num_original_words, -1, dtype=np.int32)\n        num_chunks = predictions.shape[0]\n        for i in range(num_chunks):\n            chunk_preds = predictions[i]\n            try:\n                chunk_word_ids = tokenized_inputs.word_ids(batch_index=i)\n            except Exception:\n                continue\n            if chunk_word_ids is None:\n                continue\n\n            chunk_len = len(chunk_preds)\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if token_pos >= chunk_len:\n                    break\n                if word_id is None:\n                    continue\n                if 0 <= word_id < num_original_words and final_predictions[word_id] == -1:\n                    final_predictions[word_id] = int(chunk_preds[token_pos])\n\n        filled = [\n            self.id2label.get(pid, default_label) if pid != -1 else default_label\n            for pid in final_predictions\n        ]\n        return filled\n\n    def predict(self, df: pd.DataFrame, stride: int = 128) -> List[List[str]]:\n        \"\"\"\n        Предсказывает метки для всех документов из DataFrame.\n\n        :param df: DataFrame с колонкой токенов.\n        :param stride: перекрытие между окнами.\n        :return: список документов, каждый — список строковых меток по словам.\n        :raises RuntimeError: если модель не обучена и документы непустые.\n        \"\"\"\n        all_final_labels = []\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Предсказание (sliding window)\"):\n            try:\n                original_tokens = row.get(self.tokens_column_name, None)\n            except Exception:\n                original_tokens = None\n\n            tokens = self._to_token_list(original_tokens)\n\n            if len(tokens) == 0:\n                all_final_labels.append([])\n                continue\n\n            if self.trainer is None or self.trainer.model is None:\n                raise RuntimeError(\"Модель не обучена. Вызовите .fit() перед .predict() для непустых документов.\")\n\n            document_labels = self._predict_single_document(tokens, stride)\n            all_final_labels.append(document_labels)\n        return all_final_labels\n\n    # ------------------------------\n    # Эмбеддинги\n    # ------------------------------\n    def _get_embeddings_single_document(self, tokens: List[str], stride: int, device: torch.device) -> np.ndarray:\n        \"\"\"\n        Извлекает эмбеддинги слов для одного документа.\n\n        :param tokens: список слов документа.\n        :param stride: перекрытие между окнами.\n        :param device: устройство модели (CPU/GPU).\n        :return: массив формы (num_words, hidden_size), dtype=float32.\n        :raises: None\n        \"\"\"\n        max_length = self._get_effective_max_length()\n        stride = self._sanitize_stride(stride, max_length)\n        num_original_words = len(tokens)\n\n        chunk_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        chunk_inputs.pop(\"overflow_to_sample_mapping\")\n\n        with torch.no_grad():\n            base_model = getattr(self.trainer.model, self.trainer.model.base_model_prefix)\n            outputs = base_model(**chunk_inputs)\n\n        chunk_embeddings = outputs.last_hidden_state\n\n        hidden_size = int(self.model.config.hidden_size)\n        final_word_embeddings = torch.zeros(num_original_words, hidden_size, device=device)\n        word_counts = torch.zeros(num_original_words, device=device)\n\n        for i in range(len(chunk_embeddings)):\n            chunk_embeds = chunk_embeddings[i]\n            chunk_word_ids = chunk_inputs.word_ids(batch_index=i)\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if word_id is not None:\n                    final_word_embeddings[word_id] += chunk_embeds[token_pos]\n                    if token_pos == 0 or chunk_word_ids[token_pos - 1] != word_id:\n                        word_counts[word_id] += 1\n\n        average_embeddings = final_word_embeddings / (word_counts.unsqueeze(1) + 1e-8)\n        return average_embeddings.detach().cpu().numpy()\n\n    def get_embeddings(self, df: pd.DataFrame, stride: int = 128) -> List[np.ndarray]:\n        \"\"\"\n        Извлекает эмбеддинги слов для каждого документа в DataFrame.\n\n        :param df: DataFrame с колонкой токенов.\n        :param stride: перекрытие между окнами.\n        :return: список массивов эмбеддингов, по одному на документ (num_words, hidden_size).\n        :raises RuntimeError: если модель не обучена.\n        \"\"\"\n        if self.trainer is None or self.trainer.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        self.trainer.model.eval()\n        device = self.trainer.model.device\n        all_final_embeddings = []\n\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Генерация эмбеддингов (sliding window)\"):\n            original_tokens = row[self.tokens_column_name]\n            if not original_tokens:\n                all_final_embeddings.append(np.zeros((0, int(self.model.config.hidden_size)), dtype=np.float32))\n                continue\n\n            document_embeddings = self._get_embeddings_single_document(original_tokens, stride, device)\n            all_final_embeddings.append(document_embeddings)\n\n        return all_final_embeddings","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Данные: строковые метки (будут конвертированы в id внутри .fit)\ntokens_col, tags_col = \"tokens\", \"tags\"\nlabel2id = {\n    \"O\": 0,\n    \"B-PER\": 1, \"I-PER\": 2,\n    \"B-LOC\": 3, \"I-LOC\": 4,\n    \"B-ORG\": 5, \"I-ORG\": 6,\n}\n\ndf_train = pd.DataFrame([\n    {tokens_col: [\"John\", \"Doe\", \"lives\", \"in\", \"Berlin\"], tags_col: [\"B-PER\",\"I-PER\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"Mary\", \"works\", \"at\", \"Google\"], tags_col: [\"B-PER\",\"O\",\"O\",\"B-ORG\"]},\n    {tokens_col: [\"Alice\", \"is\", \"from\", \"Paris\"], tags_col: [\"B-PER\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"IBM\", \"is\", \"in\", \"Armonk\"], tags_col: [\"B-ORG\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"Bob\", \"moved\", \"to\", \"London\"],  tags_col: [\"B-PER\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"Google\", \"is\", \"in\", \"California\"], tags_col: [\"B-ORG\",\"O\",\"O\",\"B-LOC\"]},\n])\n\n# Инициализация (минимум, что требует класс)\nCKPT = \"prajjwal1/bert-tiny\"\ntc = TokenClassification(\n    checkpoint=CKPT,\n    label2id=label2id,\n    tokens_column_name=tokens_col,\n    tags_column_name=tags_col\n)\n\n# Обучение с максимальной параметризацией\ntc.fit(\n    train_data=df_train,\n    epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    test_size=0.33,             # включаем валидацию\n    learning_rate=3e-5,\n    fp16=True,                  # если есть CUDA — включит fp16\n    stride=64,                  # скользящее окно\n    logging_steps=1,\n    eval_steps=2,\n    output_dir=\"./tokcls_max_param\",\n    seed=123,\n    fit_chunk_size_docs=2,      # обучаемся «кусками» по 2 документа\n    early_stopping_patience=2,  # ранняя остановка после 2 неулучшений\n    early_stopping_threshold=0.0,\n)\n\n# Метрики от Trainer (включая per-entity F1: eval_f1_PER/LOC/ORG и т.д., если встретились)\nmetrics = tc.trainer.evaluate()\nprint(\"Eval metrics (subset):\", {k: float(v) for k, v in metrics.items() if isinstance(v, (int, float))})\n\n# Предсказание на части данных (с отдельным stride на инференсе)\ndf_infer = df_train.iloc[:3]\npreds = tc.predict(df_infer, stride=32)\nfor i, (tokens, pred) in enumerate(zip(df_infer[tokens_col], preds), 1):\n    print(f\"Doc {i}:\")\n    print(list(zip(tokens, pred)))\n\n# Эмбеддинги слов (каждый документ -> массив [num_words, hidden_size])\nembs = tc.get_embeddings(df_infer, stride=32)\nprint(\"Embeddings shapes:\", [e.shape for e in embs])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntokens_col, tags_col = \"tokens\", \"tags\"\nlabel2id = {\"O\": 0, \"B-PER\": 1}  # минимальный набор меток\n\n# Данные уже в id (минимальная разметка)\ndf_small = pd.DataFrame([\n    {tokens_col: [\"John\", \"works\"], tags_col: [1, 0]},  # [\"B-PER\",\"O\"]\n    {tokens_col: [\"Mary\", \"smiles\"], tags_col: [1, 0]},  # [\"B-PER\",\"O\"]\n])\n\n# Инициализация\nCKPT = \"prajjwal1/bert-tiny\"\ntc = TokenClassification(\n    checkpoint=CKPT,\n    label2id=label2id,\n    tokens_column_name=tokens_col,\n    tags_column_name=tags_col\n)\n\n# Обучение — все параметры по умолчанию\ntc.fit(train_data=df_small)\n\n# Базовый предикт — тоже по умолчанию\npreds = tc.predict(df_small)\nprint(\"Preds:\", preds)\n\n# При необходимости — эмбеддинги (тоже с параметрами по умолчанию)\nembs = tc.get_embeddings(df_small)\nprint(\"Embeddings shape for doc 0:\", embs[0].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  pillow==11.1.0 \\\n  numpy==1.26.4 \\\n  pandas==2.2.3 \\\n  tqdm==4.67.1 \\\n  transformers==4.51.3 \\\n  evaluate==0.4.5 \\\n  wav2clip==0.1.0 \\\n  torch==2.6.0+cu124 \\\n  torchaudio==2.6.0+cu124\n# !pip install evaluate wav2clip\n\nimport os\nimport time\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport math\nimport random\nimport gc\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, Generator, List, Optional, Union\n\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers.trainer_callback import TrainerCallback, PrinterCallback, EarlyStoppingCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport evaluate\n\n# =========================\n# Утилиты\n# =========================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef to_pil(x: Union[str, np.ndarray, 'Image.Image']) -> 'Image.Image':\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\ndef safe_load(component_cls, checkpoint: str, local_cache_dir: str = \"./model_cache\",\n              local_files_only: Optional[bool] = None, **kwargs):\n    if local_files_only is None:\n        local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n    name = getattr(component_cls, \"__name__\", \"\")\n    if \"Tokenizer\" in name:\n        kwargs.setdefault(\"use_fast\", True)\n    return component_cls.from_pretrained(\n        checkpoint, cache_dir=local_cache_dir, local_files_only=local_files_only, **kwargs\n    )\n\n\n# =========================\n# Токенизатор батчевый\n# =========================\n\nclass BatchTokenizer:\n    def __init__(\n        self,\n        tokenizer,\n        max_length: int = 512,\n        cache_size: int = 10000,\n        batch_size: int = 256,\n        use_fast: bool = True,\n        device: str = \"cpu\",\n        padding_strategy: str = \"max_length\"  # \"max_length\" или \"dynamic\"\n    ):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.use_fast = use_fast\n        self.device = device\n        self.padding_strategy = padding_strategy\n        if self.padding_strategy not in {\"max_length\", \"dynamic\"}:\n            raise ValueError(\"padding_strategy должен быть 'max_length' или 'dynamic'\")\n        self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n        self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n        if self.is_fast:\n            print(\"✓ Используется Fast Tokenizer\")\n\n    def _tokenize_single(self, text: str) -> tuple:\n        # В кэше храним только версии с фиксированной длиной — иначе нельзя будет склеивать батчи\n        result = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        return tuple((k, v.squeeze(0).cpu().numpy()) for k, v in result.items())\n\n    def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n        # Динамический паддинг: всегда токенизируем списком целиком (без кэша по-элементно)\n        if self.padding_strategy == \"dynamic\":\n            result = self.tokenizer(\n                texts,\n                padding=True,  # паддинг до «longest» в батче\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n        # Фиксированный паддинг — как раньше, но явно padding=\"max_length\"\n        if use_cache and len(texts) < 100:\n            results = [dict(self._cache(text)) for text in texts]\n            keys = results[0].keys()\n            batch_dict = {}\n            for key in keys:\n                dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                batch_dict[key] = torch.tensor(np.stack([r[key] for r in results]), dtype=dtype)\n            return batch_dict\n        else:\n            result = self.tokenizer(\n                texts,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n    def tokenize_dataset_lazy(\n        self,\n        texts: List[str],\n        batch_size: Optional[int] = None\n    ) -> Generator[Dict[str, torch.Tensor], None, None]:\n        batch_size = batch_size or self.batch_size\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            yield self.tokenize_batch(batch, use_cache=False)\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n\n# =========================\n# Универсальный датасет\n# =========================\n\nclass MultiComboDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        label2id: Dict[Any, int],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer: Optional[Any] = None,         # BatchTokenizer\n        text_tokenizer_fn: Optional[Callable] = None, # custom fn -> dict of tensors\n        special_tokens: Optional[Dict[str, Any]] = None,\n        pretokenize: bool = True,\n        pretokenize_batch_size: int = 1024,\n        tokenizer_returns_tensors: bool = True,\n        deduplicate_texts: bool = True,\n        max_cache_size=None\n    ):\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.label2id = label2id\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        # Один путь: BatchTokenizer; другой: custom fn\n        self.batch_tokenizer = text_tokenizer\n        self.text_tokenizer_fn = text_tokenizer_fn\n\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n        self._N = len(self.df)\n\n        # labels сразу в тензор [N]\n        if self.target_col in self.df.columns:\n            y = self.df[self.target_col].map(self.label2id).fillna(0).astype(int).values\n        else:\n            # если на инференсе метки нет — пусть будут 0\n            y = np.zeros(self._N, dtype=np.int64)\n        self._labels = torch.tensor(y, dtype=torch.long)\n\n        # Предсобранные списки изображений/аудио (чтобы не дёргать pandas в __getitem__)\n        self._image_lists = None\n        if self.image_columns:\n            self._image_lists = self._collect_multi_values(self.df, self.image_columns)\n\n        self._audio_lists = None\n        if self.audio_columns:\n            self._audio_lists = self._collect_multi_values(self.df, self.audio_columns)\n\n        # Предтокенизированные банки: dict(key -> torch.Tensor [N, ...])\n        self._tok_bank: Optional[Dict[str, torch.Tensor]] = None\n\n        # Предтокенизация текста (ускоряет обучение на порядки)\n        self._has_text = bool(self.text_columns)\n\n        # dynamic-паддинг несовместим с предтокенизацией (формы в батчах будут разные)\n        if pretokenize and self.batch_tokenizer is not None and getattr(self.batch_tokenizer, \"padding_strategy\", \"max_length\") == \"dynamic\":\n            print(\"⚠ Предтокенизация отключена: выбран dynamic-паддинг для текста.\")\n            pretokenize = False\n\n        if self._has_text and pretokenize:\n            t0 = time.time()\n            if self.batch_tokenizer is not None and self.text_tokenizer_fn is None:\n                # BatchTokenizer путь\n                self._pretokenize_with_batch_tokenizer(pretokenize_batch_size)\n            elif self.text_tokenizer_fn is not None:\n                # Custom fn путь (equal-split и т.п.)\n                self._pretokenize_with_custom_fn(pretokenize_batch_size, deduplicate_texts=deduplicate_texts)\n            else:\n                # Ни BatchTokenizer, ни custom fn — оставляем без токенов (коллатор потом сам токенизирует из строк)\n                pass\n            t1 = time.time()\n            if self._tok_bank is not None:\n                shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n                print(f\"✓ Предтокенизация завершена: {self._N} образцов за {t1 - t0:.2f}s | keys={list(self._tok_bank.keys())}, shapes={shapes}\")\n\n    def __len__(self) -> int:\n        return self._N\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item: Dict[str, Any] = {}\n        item[\"labels\"] = int(self._labels[idx])\n\n        # Текст: если есть предтокенизированные банки — просто слайсим\n        if self._tok_bank is not None:\n            item[\"text_tokens\"] = {k: v[idx] for k, v in self._tok_bank.items()}\n        elif self._has_text:\n            # Фоллбек: отдаём строку (коллатор бэкенда сам батчево токенизирует)\n            item[\"text\"] = self._join_text(self.df.iloc[idx])  # быстрый fallback; лучше всегда pretokenize\n\n        # Изображения/аудио — просто отдаём подготовленные списки\n        if self._image_lists is not None:\n            item[\"images\"] = self._image_lists[idx]\n        if self._audio_lists is not None:\n            item[\"audios\"] = self._audio_lists[idx]\n\n        return item\n\n    # --------------------------\n    # Вспомогательные методы\n    # --------------------------\n\n    def clear_cache(self):\n        # Освободить предтокенизированные банки (для экономии RAM между чанками)\n        self._tok_bank = None\n        torch.cuda.empty_cache()\n\n    def get_cache_stats(self) -> Dict[str, Any]:\n        has = self._tok_bank is not None\n        sizes = {k: tuple(v.shape) for k, v in (self._tok_bank or {}).items()}\n        return {\"has_pretokenized\": has, \"shapes\": sizes, \"N\": self._N}\n\n    def _join_text(self, row: pd.Series) -> str:\n        sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n        return sep.join([(\"\" if pd.isna(row[c]) else str(row[c])) for c in self.text_columns])\n\n    @staticmethod\n    def _as_list(v):\n        if v is None or (isinstance(v, float) and math.isnan(v)):\n            return []\n        if isinstance(v, (list, tuple)):\n            return list(v)\n        return [v]\n\n    def _collect_multi_values(self, df: pd.DataFrame, columns: List[str]) -> List[List[Any]]:\n        out = []\n        as_list = self._as_list\n        for _, row in df.iterrows():\n            lst: List[Any] = []\n            for c in columns:\n                if c in row:\n                    lst.extend([x for x in as_list(row[c]) if x is not None])\n            out.append(lst)\n        return out\n\n    # --------------------------\n    # Предтокенизация: BatchTokenizer\n    # --------------------------\n    def _pretokenize_with_batch_tokenizer(self, batch_size: int):\n        texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n        banks: Dict[str, List[torch.Tensor]] = {}\n\n        for start in range(0, self._N, batch_size):\n            batch = texts[start:start + batch_size]\n            tok = self.batch_tokenizer.tokenize_batch(batch, use_cache=False)  # dict[str, torch.Tensor [B, L]]\n            # Нормализуем типы\n            for k in tok:\n                if k in (\"input_ids\", \"attention_mask\", \"token_type_ids\"):\n                    tok[k] = tok[k].long()\n                else:\n                    tok[k] = tok[k].to(torch.float32)\n\n            # Сохраняем\n            for k, v in tok.items():\n                banks.setdefault(k, []).append(v)\n\n        # Склеиваем по первой оси\n        self._tok_bank = {k: torch.cat(v_parts, dim=0).contiguous() for k, v_parts in banks.items()}\n\n    # --------------------------\n    # Предтокенизация: custom fn\n    # --------------------------\n    def _pretokenize_with_custom_fn(self, batch_size: int, deduplicate_texts: bool = True):\n        # Подготовим «сырые» тексты как списки (без pandas в горячем цикле)\n        cols = self.text_columns\n        col_arrays = [self.df[c].astype(str).where(~self.df[c].isna(), other=\"\").tolist() for c in cols]\n\n        # Детектируем форму по первому примеру\n        first_td = {c: col_arrays[i][0] for i, c in enumerate(cols)}\n        first_tok = self.text_tokenizer_fn(first_td, self.special_tokens)\n        if not isinstance(first_tok, dict):\n            raise ValueError(\"custom text_tokenizer_fn должна возвращать dict тензоров\")\n\n        # Проверим одинаковую длину для всех ключей\n        shapes = {k: tuple(t.shape) for k, t in first_tok.items()}\n        if any(len(s) == 0 for s in shapes.values()):\n            raise ValueError(\"text_tokenizer_fn должна возвращать тензоры с размерностью хотя бы [L]\")\n\n        # Предвыделим банки\n        bank: Dict[str, torch.Tensor] = {}\n        for k, t in first_tok.items():\n            dtype = t.dtype if torch.is_tensor(t) else torch.long\n            bank[k] = torch.empty((self._N, *t.shape), dtype=dtype)\n\n        # Заполним первую строку\n        for k, t in first_tok.items():\n            bank[k][0].copy_(t if torch.is_tensor(t) else torch.tensor(t))\n\n        # Дедупликация (опционально)\n        cache: Dict[tuple, Dict[str, torch.Tensor]] = {}\n        if deduplicate_texts:\n            key0 = tuple(first_td.get(c, \"\") for c in cols)\n            cache[key0] = {k: (v.clone() if v.is_floating_point() else v.clone()) for k, v in first_tok.items()}\n\n        # Основной цикл: батчами формируем text_data и токенизируем per-sample (но единожды)\n        for start in range(1, self._N, batch_size):\n            end = min(self._N, start + batch_size)\n            for i in range(start, end):\n                # Сформировать text_data для i-й строки\n                td = {c: col_arrays[j][i] for j, c in enumerate(cols)}\n                if deduplicate_texts:\n                    key = tuple(td.get(c, \"\") for c in cols)\n                    got = cache.get(key)\n                    if got is None:\n                        tok = self.text_tokenizer_fn(td, self.special_tokens)\n                        cache[key] = tok\n                    else:\n                        tok = got\n                else:\n                    tok = self.text_tokenizer_fn(td, self.special_tokens)\n\n                # Записать в банки\n                for k, t in tok.items():\n                    if not torch.is_tensor(t):\n                        t = torch.tensor(t)\n                    bank[k][i].copy_(t)\n\n        # Сохраняем банки\n        self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n\n\n# =========================\n# Универсальный бэкенд\n# =========================\n\nclass BaseBackend(nn.Module):\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n    text_tokenizer_fn: Optional[Callable] = None\n    batch_tokenizer: Optional[BatchTokenizer] = None\n    special_tokens: Dict[str, str] = {}\n    tokenizer_returns_tensors: bool = False\n    local_cache_dir: str = \"./model_cache\"\n    text_padding_strategy: str = \"max_length\"  # стратегия паддинга текста\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        raise NotImplementedError\n\n    def freeze_all(self):\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n    def set_text_tokenizer(self, tokenizer_fn: Optional[Callable], special_tokens: Optional[Dict[str, str]] = None,\n                           returns_tensors: bool = False):\n        self.text_tokenizer_fn = tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = returns_tensors\n\n    def set_batch_tokenizer(self, tokenizer, max_length: int = 512,\n                            cache_size: int = 10000, batch_size: int = 256,\n                            padding_strategy: str = \"max_length\"):\n        self.text_padding_strategy = padding_strategy\n        self.batch_tokenizer = BatchTokenizer(\n            tokenizer=tokenizer,\n            max_length=max_length,\n            cache_size=cache_size,\n            batch_size=batch_size,\n            use_fast=True,\n            padding_strategy=padding_strategy\n        )\n\n\nclass UniversalMultiBackend(BaseBackend):\n    name = \"universal\"\n\n    class _ParamDeviceProxy(nn.Module):\n        def __init__(self, base, device: torch.device):\n            super().__init__()\n            self.base = base if isinstance(base, nn.Module) else None\n            self._callable = base if not isinstance(base, nn.Module) else None\n            self._dummy = nn.Parameter(torch.empty(0), requires_grad=False)\n            with torch.no_grad():\n                self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n\n        def forward(self, *args, **kwargs):\n            target = self.base if self.base is not None else self._callable\n            return target(*args, **kwargs)\n\n        def to(self, device, *args, **kwargs):\n            self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n            return super().to(device, *args, **kwargs)\n\n    def _preferred_device(self) -> torch.device:\n        if torch.cuda.is_available():\n            return torch.device(\"cuda\")\n        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n            return torch.device(\"mps\")\n        return torch.device(\"cpu\")\n\n    def _wrap_if_parameterless(self, model, device: torch.device):\n        try:\n            it = model.parameters() if hasattr(model, \"parameters\") else iter(())\n            next(it)\n            return model\n        except StopIteration:\n            return self._ParamDeviceProxy(model, device)\n        except Exception:\n            return self._ParamDeviceProxy(model, device)\n\n    def __init__(\n        self,\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        freeze: bool = True,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        use_batch_tokenizer: bool = True,\n        tokenizer_cache_size: int = 10000,\n        tokenizer_batch_size: int = 256,\n        local_cache_dir: str = \"./model_cache\",\n        text_padding_strategy: str = \"max_length\"  # стратегия паддинга текста\n    ):\n        super().__init__()\n        self.supported = set()\n        self.out_dim_per_modality = {}\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.tokenizer_batch_size = tokenizer_batch_size\n        self.local_cache_dir = local_cache_dir\n        self.text_padding_strategy = text_padding_strategy\n\n        self.text_model = None\n        self.text_processor = None\n        self.text_config = text_model_config or {}\n        if text_model_config:\n            self._init_text_model(text_model_config)\n            self.supported.add(\"text\")\n\n        self.image_model = None\n        self.image_processor = None\n        self.image_config = image_model_config or {}\n        if image_model_config:\n            self._init_image_model(image_model_config)\n            self.supported.add(\"image\")\n\n        self.audio_model = None\n        self.audio_processor = None\n        self.audio_config = audio_model_config or {}\n        if audio_model_config:\n            self._init_audio_model(audio_model_config)\n            self.supported.add(\"audio\")\n\n        if freeze:\n            self.freeze_all()\n\n    def _ensure_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        if x is None:\n            return None\n        if x.dim() == 1:\n            return x.unsqueeze(0)\n        if x.dim() > 2:\n            return x.view(x.size(0), -1)\n        return x\n\n    def _normalize_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        x = self._ensure_2d(x)\n        return F.normalize(x, dim=-1, eps=1e-12) if x is not None and x.numel() > 0 else x\n\n    def _init_text_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoTokenizer, CLIPModel, CLIPTokenizer, ClapModel, ClapProcessor\n\n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n\n        print(f\"Загрузка текстовой модели {checkpoint}...\")\n\n        if model_type == 'clip':\n            self.text_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(CLIPTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.projection_dim\n        elif model_type == 'clap':\n            self.text_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            proc = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = getattr(proc, 'tokenizer', None) or safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = getattr(self.text_model.config, \"projection_dim\", 512)\n        else:\n            self.text_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.text_model  = self._wrap_if_parameterless(self.text_model, dev)\n\n        if self.use_batch_tokenizer and self.text_processor is not None:\n            self.set_batch_tokenizer(\n                self.text_processor,\n                max_length=config.get('max_length', 512),\n                cache_size=self.tokenizer_cache_size,\n                batch_size=self.tokenizer_batch_size,\n                padding_strategy=self.text_padding_strategy\n            )\n\n        self.text_config['max_length'] = config.get('max_length', 512)\n        self.text_config['dim'] = dim\n        self.text_config['model_type'] = model_type\n        self.out_dim_per_modality['text'] = dim\n\n    def _init_image_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoImageProcessor, CLIPModel, CLIPImageProcessor\n\n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n\n        print(f\"Загрузка визуальной модели {checkpoint}...\")\n\n        if model_type == 'clip':\n            self.image_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(CLIPImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.projection_dim\n        else:\n            self.image_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(AutoImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.image_model = self._wrap_if_parameterless(self.image_model, dev)\n\n        self.image_config['max_images'] = config.get('max_images', 1)\n        self.image_config['image_agg'] = config.get('image_agg', 'concat')\n        self.image_config['dim'] = dim\n        self.image_config['model_type'] = model_type\n\n        self.out_dim_per_modality['image'] = (dim * self.image_config['max_images']) if self.image_config['image_agg'] == 'concat' else dim\n\n    def _init_audio_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoProcessor, ClapModel, ClapProcessor\n\n        model_type = config.get('model_type', 'auto').lower()\n        checkpoint = config.get('checkpoint', None)\n\n        print(f\"Загрузка аудио модели (type={model_type})...\")\n\n        if model_type == 'wav2clip':\n            import wav2clip as w2c\n            self._w2c = w2c\n\n            w2c_model = None\n            if hasattr(w2c, \"get_model\"):\n                w2c_model = w2c.get_model()\n            elif hasattr(w2c, \"model\"):\n                m = w2c.model\n                w2c_model = m() if callable(m) else m\n            else:\n                raise RuntimeError(\"wav2clip не содержит get_model()/model. Обновите пакет wav2clip.\")\n\n            self.audio_model = w2c_model\n\n            try:\n                if isinstance(self.audio_model, torch.nn.Module) and torch.cuda.is_available():\n                    self.audio_model = self.audio_model.to(\"cuda\")\n            except Exception:\n                pass\n\n            self.audio_processor = None\n            dim = 512\n            sr = config.get('sr', 16000)\n\n        elif model_type == 'clap':\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для CLAP\")\n            self.audio_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = getattr(self.audio_model.config, \"projection_dim\", 512)\n            sr = getattr(self.audio_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.audio_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n\n        else:\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для аудио-моделей, кроме wav2clip\")\n            self.audio_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(AutoProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.audio_model.config.hidden_size\n            fe = getattr(self.audio_processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 16000) if fe is not None else 16000\n\n        dev = self._preferred_device()\n        self.audio_model = self._wrap_if_parameterless(self.audio_model, dev)\n\n        self.audio_config['sr'] = config.get('sr', sr)\n        self.audio_config['max_audios'] = config.get('max_audios', 1)\n        self.audio_config['audio_agg'] = config.get('audio_agg', 'concat')\n        self.audio_config['dim'] = dim\n        self.audio_config['model_type'] = model_type\n\n        self.out_dim_per_modality['audio'] = (\n            dim * self.audio_config['max_audios']\n            if self.audio_config['audio_agg'] == 'concat' else dim\n        )\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        labels = []\n        for b in batch:\n            labels.append(torch.tensor(b.get(\"labels\", 0), dtype=torch.long))\n        labels = torch.stack(labels)\n\n        backend_inputs: Dict[str, Any] = {}\n        batch_size = len(batch)\n\n        if self.text_model is not None:\n            if \"text_tokens\" in batch[0]:\n                text_inputs = {}\n                for key in batch[0][\"text_tokens\"].keys():\n                    if torch.is_tensor(batch[0][\"text_tokens\"][key]):\n                        text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n                    else:\n                        dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                        text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n                backend_inputs[\"text_inputs\"] = text_inputs\n            else:\n                texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n                if self.batch_tokenizer:\n                    text_inputs = self.batch_tokenizer.tokenize_batch(texts, use_cache=True)\n                else:\n                    pad = \"max_length\" if getattr(self, \"text_padding_strategy\", \"max_length\") == \"max_length\" else True\n                    text_inputs = self.text_processor(\n                        texts, padding=pad, truncation=True,\n                        max_length=self.text_config.get('max_length', 512),\n                        return_tensors=\"pt\"\n                    )\n                backend_inputs[\"text_inputs\"] = {k: v for k, v in text_inputs.items()}\n\n        if self.image_model is not None:\n            images_lists = [b.get(\"images\", []) for b in batch]\n            flat_images, img_counts = [], []\n            for lst in images_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [img for img in lst if img is not None]\n                img_counts.append(len(lst))\n                for img in lst:\n                    flat_images.append(to_pil(img))\n\n            if len(flat_images) > 0:\n                img_proc = self.image_processor(images=flat_images, return_tensors=\"pt\")\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": img_proc[\"pixel_values\"]}\n            else:\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": None}\n\n            backend_inputs[\"image_counts\"] = torch.tensor(img_counts, dtype=torch.long)\n\n        if self.audio_model is not None:\n            audios_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios, aud_counts = [], []\n            for lst in audios_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_config['sr']))\n                    elif isinstance(a, np.ndarray):\n                        aa = np.asarray(a, dtype=np.float32)\n                        if aa.ndim > 1:\n                            aa = np.squeeze(aa)\n                        if aa.ndim > 1:\n                            aa = aa.reshape(-1)\n                        flat_audios.append(aa)\n            if self.audio_config.get('model_type') == 'wav2clip':\n                backend_inputs[\"audio_inputs\"] = {\"raw_audios\": flat_audios}\n            elif len(flat_audios) > 0:\n                if self.audio_config.get('model_type') == 'clap':\n                    aud_proc = self.audio_processor(\n                        audios=flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_features\": aud_proc[\"input_features\"]}\n                else:\n                    aud_proc = self.audio_processor(\n                        flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_values\": aud_proc[\"input_values\"]}\n            else:\n                backend_inputs[\"audio_inputs\"] = {\"input_features\": None, \"input_values\": None, \"raw_audios\": []}\n\n            backend_inputs[\"audio_counts\"] = torch.tensor(aud_counts, dtype=torch.long)\n\n        backend_inputs[\"batch_size\"] = batch_size\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _aggregate_embeddings(\n        self,\n        embs: Optional[torch.Tensor],\n        counts: List[int],\n        max_k: int,\n        dim_hint: int,\n        agg_type: str,\n        batch_size: int,\n        device: torch.device\n    ) -> torch.Tensor:\n        if embs is None or (torch.is_tensor(embs) and embs.numel() == 0):\n            feat_dim = int(dim_hint) if dim_hint is not None else 0\n            out_dim = feat_dim * max_k if agg_type == 'concat' else feat_dim\n            return torch.zeros((batch_size, out_dim), device=device, dtype=torch.float32)\n\n        if not torch.is_tensor(embs):\n            embs = torch.as_tensor(embs, device=device, dtype=torch.float32)\n        if embs.dim() == 1:\n            embs = embs.unsqueeze(0)\n        elif embs.dim() > 2:\n            embs = embs.view(embs.size(0), -1)\n\n        N, D = embs.size()\n        out_dim = (D * max_k) if agg_type == 'concat' else D\n        out = torch.zeros((batch_size, out_dim), device=device, dtype=embs.dtype)\n\n        offset = 0\n        for i, c in enumerate(counts):\n            if c <= 0 or offset >= N:\n                continue\n            take_n = min(c, N - offset)\n            sample = embs[offset:offset + take_n]\n            offset += take_n\n\n            if agg_type == 'concat':\n                take = sample[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n\n        return F.normalize(out, dim=-1, eps=1e-12) if out.size(1) > 0 else out\n\n    @torch.no_grad()\n    def _wav2clip_embed(self, arr: np.ndarray, device: torch.device) -> torch.Tensor:\n        arr = np.asarray(arr, dtype=np.float32)\n        if arr.ndim > 1:\n            arr = np.squeeze(arr)\n        if arr.ndim > 1:\n            arr = arr.reshape(-1)\n        if arr.size < 512:\n            arr = np.pad(arr, (0, 512 - arr.size), mode=\"constant\")\n\n        try:\n            emb = self._w2c.embed_audio(arr, self.audio_model)\n            emb = np.asarray(emb)\n        except Exception:\n            x = torch.from_numpy(arr).float().unsqueeze(0).to(device)\n            y = self.audio_model(x)\n            if isinstance(y, (tuple, list)):\n                y = y[0]\n            if torch.is_tensor(y):\n                if y.dim() == 2 and y.size(0) == 1:\n                    y = y.squeeze(0)\n                emb = y.detach().cpu().numpy()\n            else:\n                emb = np.asarray(y)\n\n        if emb.ndim > 1:\n            emb = emb.reshape(-1)\n        return torch.as_tensor(emb, device=device, dtype=torch.float32)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        results: Dict[str, torch.Tensor] = {}\n        batch_size = int(backend_inputs.get(\"batch_size\", 1))\n\n        if self.text_model is not None and \"text_inputs\" in backend_inputs:\n            text_inputs = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n            if hasattr(self.text_model, \"get_text_features\"):\n                text_z = self.text_model.get_text_features(**text_inputs)\n            else:\n                outputs = self.text_model(**text_inputs)\n                text_z = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n            results[\"text\"] = self._normalize_2d(text_z)\n\n        if self.image_model is not None and \"image_inputs\" in backend_inputs:\n            pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n            counts = backend_inputs[\"image_counts\"].tolist()\n            total_images_needed = sum(counts)\n\n            img_flat = None\n            actual_img_dim = self.image_config.get(\"dim\", 768)\n\n            if pi is not None and pi.numel() > 0 and total_images_needed > 0:\n                pi = pi.to(device)\n                if pi.size(0) > total_images_needed:\n                    pi = pi[:total_images_needed]\n\n                if hasattr(self.image_model, \"get_image_features\"):\n                    img_flat = self.image_model.get_image_features(pixel_values=pi)\n                else:\n                    outputs = self.image_model(pixel_values=pi)\n                    img_flat = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state[:, 0]\n\n                img_flat = self._normalize_2d(img_flat)\n                actual_img_dim = img_flat.size(1) if img_flat is not None else actual_img_dim\n\n            img_z = self._aggregate_embeddings(\n                img_flat, counts,\n                self.image_config[\"max_images\"],\n                actual_img_dim,\n                self.image_config[\"image_agg\"],\n                len(counts),\n                device\n            )\n\n            if actual_img_dim != self.image_config.get(\"dim\"):\n                self.image_config[\"dim\"] = actual_img_dim\n                self.out_dim_per_modality[\"image\"] = (\n                    actual_img_dim * self.image_config[\"max_images\"]\n                    if self.image_config[\"image_agg\"] == \"concat\" else actual_img_dim\n                )\n\n            results[\"image\"] = img_z\n\n        if self.audio_model is not None and \"audio_inputs\" in backend_inputs:\n            counts = backend_inputs[\"audio_counts\"].tolist()\n            total_audios_needed = sum(counts)\n\n            aud_flat = None\n            actual_aud_dim = self.audio_config.get(\"dim\", 768)\n            model_type = self.audio_config.get(\"model_type\")\n\n            if total_audios_needed > 0:\n                if model_type == \"clap\":\n                    af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n                    if af is not None and af.numel() > 0:\n                        af = af.to(device)\n                        if af.size(0) > total_audios_needed:\n                            af = af[:total_audios_needed]\n                        with torch.cuda.amp.autocast(enabled=False):\n                            aud_flat = self.audio_model.get_audio_features(input_features=af.float())\n                        aud_flat = self._normalize_2d(aud_flat.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n                elif model_type == \"wav2clip\":\n                    raw_list = backend_inputs[\"audio_inputs\"].get(\"raw_audios\", [])\n                    if len(raw_list) > total_audios_needed:\n                        raw_list = raw_list[:total_audios_needed]\n                    if len(raw_list) > 0:\n                        embs = [self._wav2clip_embed(arr, device) for arr in raw_list]\n                        aud_flat = torch.stack(embs, dim=0)\n                        aud_flat = self._normalize_2d(aud_flat)\n                        actual_aud_dim = aud_flat.size(1)\n\n                else:\n                    av = backend_inputs[\"audio_inputs\"][\"input_values\"]\n                    if av is not None and av.numel() > 0:\n                        av = av.to(device)\n                        if av.size(0) > total_audios_needed:\n                            av = av[:total_audios_needed]\n                        av = av.clamp_(-1.0, 1.0)\n                        with torch.cuda.amp.autocast(enabled=False):\n                            outputs = self.audio_model(input_values=av.float())\n                            feats = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n                        aud_flat = self._normalize_2d(feats.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n            aud_z = self._aggregate_embeddings(\n                aud_flat, counts,\n                self.audio_config[\"max_audios\"],\n                actual_aud_dim,\n                self.audio_config[\"audio_agg\"],\n                len(counts),\n                device\n            )\n\n            if aud_flat is not None and actual_aud_dim != self.audio_config.get(\"dim\"):\n                self.audio_config[\"dim\"] = actual_aud_dim\n                self.out_dim_per_modality[\"audio\"] = (\n                    actual_aud_dim * self.audio_config[\"max_audios\"]\n                    if self.audio_config[\"audio_agg\"] == \"concat\" else actual_aud_dim\n                )\n\n            results[\"audio\"] = aud_z\n\n        if results:\n            bs_list = [v.size(0) for v in results.values()]\n            if len(set(bs_list)) != 1:\n                raise RuntimeError(f\"Inconsistent batch sizes across modalities: {bs_list}\")\n\n        return results\n\n\n# =========================\n# Классификатор\n# =========================\n\nclass SingleBackboneClassifier(nn.Module):\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_labels: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError('Для fusion=\"mean\" размеры модальностей должны совпадать')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                has_trainable = any(p.requires_grad for p in m.parameters()) if hasattr(m, \"parameters\") else False\n            except Exception:\n                has_trainable = False\n            if not has_trainable:\n                continue\n            try:\n                cfg = getattr(m, \"config\", None)\n                if cfg is not None and hasattr(cfg, \"use_cache\"):\n                    cfg.use_cache = False\n            except Exception:\n                pass\n            try:\n                if hasattr(m, \"gradient_checkpointing_enable\"):\n                    try:\n                        if gradient_checkpointing_kwargs is not None:\n                            m.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                        else:\n                            m.gradient_checkpointing_enable()\n                    except TypeError:\n                        m.gradient_checkpointing_enable()\n            except Exception:\n                pass\n\n    def gradient_checkpointing_disable(self):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                if hasattr(m, \"gradient_checkpointing_disable\"):\n                    m.gradient_checkpointing_disable()\n            except Exception:\n                pass\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        batch_size = None\n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() == 3:\n                    t = t.mean(dim=1)\n                elif t.dim() > 3:\n                    t = t.view(t.size(0), -1)\n                feats.append(t)\n                if batch_size is None:\n                    batch_size = t.size(0)\n        if batch_size is not None:\n            feats = [f[:batch_size] for f in feats]\n        if self.fusion == \"concat\":\n            out = torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            out = torch.stack(feats, dim=0).mean(dim=0)\n        return out\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        if not z:\n            raise ValueError(\"Backend не вернул эмбеддинги\")\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\n# =========================\n# Trainer с весами классов\n# =========================\n\nclass WeightedCETrainer(Trainer):\n    def __init__(self, *args, num_labels=None, train_labels=None, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        if class_weights is not None:\n            self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32)\n        elif train_labels is not None and num_labels is not None:\n            y = np.asarray(train_labels).astype(int)\n            counts = np.bincount(y, minlength=num_labels)\n            n = counts.sum()\n            w = np.zeros(num_labels, dtype=np.float32)\n            nz = counts > 0\n            w[nz] = n / (num_labels * counts[nz].astype(np.float32))\n            self.class_weights = torch.tensor(w, dtype=torch.float32)\n        else:\n            self.class_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        if logits.size(0) != labels.size(0):\n            raise ValueError(f\"Batch size mismatch: logits batch={logits.size(0)} vs labels batch={labels.size(0)}\")\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = F.cross_entropy(logits, labels.long(), weight=weight)\n        return (loss, outputs) if return_outputs else loss\n\n\n# =========================\n# Прогресс-логгер\n# =========================\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n        self.tqdm = tqdm\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            self.tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\n# =========================\n# Основной пайплайн\n# =========================\n\nclass SingleModelMultiComboClassification:\n    \"\"\"\n    Универсальный пайплайн для мульти-модальной классификации (text / image / audio) поверх моделей Hugging Face.\n    Поддерживает автоматический выбор бэкенда (CLIP/CLAP/Auto), батчевую токенизацию текста, кэширование,\n    чанковую тренировку, раннюю остановку, взвешивание классов, предсказание и извлечение эмбеддингов.\n\n    Основные возможности:\n    - Автоматическая сборка бэкенда: CLIP для связки text+image, CLAP для text+audio, либо произвольные Auto-модели.\n    - Работа с тремя модальностями: text, image, audio (любой поднабор).\n    - Батчевая токенизация текста с кэшем и опциональной предварительной токенизацией датасета.\n    - Чанковая тренировка очень больших датасетов без перегрузки памяти.\n    - Сбалансированная (взвешенная) кросс-энтропия на основе частот классов в тренировочных данных.\n    - Прогресс-бар, ранняя остановка и выбор лучшей модели по метрике.\n    - Предсказания и извлечение эмбеддингов (в том числе по модальностям).\n\n    :param modalities: Список используемых модальностей из {\"text\", \"image\", \"audio\"}.\n    :param num_labels: Число классов в задаче классификации.\n    :param target_column_name: Имя колонки с целевой меткой в DataFrame.\n    :param text_columns: Список текстовых колонок (используются, если выбрана модальность \"text\"). Значения будут\n                         конкатенированы через special_tokens[\"sep\"] при подготовке примеров.\n    :param image_columns: Список колонок с изображениями (пути к файлам, PIL.Image, np.ndarray или списки таких объектов),\n                          используется, если выбрана модальность \"image\".\n    :param audio_columns: Список колонок с аудио (пути к файлам или массивы np.ndarray; моно, float32),\n                          используется, если выбрана модальность \"audio\". Для чтения из файлов требуется torchaudio.\n    :param text_tokenizer_fn: Кастомная функция токенизации текста (если не используется встроенный BatchTokenizer).\n                              Сигнатура: fn(text_dict: Dict[str, str], special_tokens: Dict[str, str]) -> Union[Dict[str, Tensor], str].\n                              Если возвращает dict с ключами вроде 'input_ids', считается, что функция сразу возвращает тензоры токенов;\n                              иначе строку для последующей стандартной токенизации.\n    :param special_tokens: Спец. токены/разделители для подготовки текста. По умолчанию {\"sep\": \" [SEP] \"}.\n    :param tokenizer_returns_tensors: Флаг, сигнализирующий, что custom text_tokenizer_fn возвращает уже тензоры\n                                      (dict c 'input_ids', 'attention_mask' и т.д.). Влияет на коллатор.\n    :param backend: Режим сборки бэкенда. \"auto\" — подобрать оптимальные модели по модальностям;\n                    \"clip\" — CLIP для текста и изображений; \"clap\" — CLAP для текста и аудио; любое иное — ручные конфиги.\n    :param clip_checkpoint: Чекпойнт CLIP (используется при auto/clip), по умолчанию \"openai/clip-vit-base-patch32\".\n    :param clap_checkpoint: Чекпойнт CLAP (используется при auto/clap), по умолчанию \"laion/clap-htsat-unfused\".\n    :param text_model_config: Конфиг текстовой модели. Минимум: {\"checkpoint\": \"...\", \"model_type\": \"...\"}.\n                              Дополнительно: \"max_length\" и т.д. Примеры model_type: \"clip\", \"clap\", \"bert\", \"auto\".\n    :param image_model_config: Конфиг визуальной модели. Минимум: {\"checkpoint\": \"...\", \"model_type\": \"...\"}.\n                               Дополнительно: \"max_images\", \"image_agg\" (\"concat\"|\"mean\") и т.д. Примеры model_type: \"clip\", \"vit\", \"auto\".\n    :param audio_model_config: Конфиг аудио-модели. Минимум: {\"checkpoint\": \"...\", \"model_type\": \"...\"} (кроме \"wav2clip\").\n                               Дополнительно: \"max_audios\", \"audio_agg\" (\"concat\"|\"mean\"), \"sr\". Примеры model_type: \"clap\", \"wav2clip\", \"auto\".\n    :param fusion: Способ слияния модальностей в классификаторе: \"concat\" или \"mean\".\n                   При \"mean\" размеры эмбеддингов всех модальностей должны совпадать.\n    :param freeze_backbone: Если True — бэкенды заморожены (тренируется только классификационная \"голова\").\n    :param clip_max_length: Максимальная длина текста для CLIP-токенизатора (по умолчанию 77).\n    :param max_images_per_sample: Сколько изображений брать на сэмпл (усреднение или конкатенация задаются в image_model_config[\"image_agg\"]).\n    :param max_audios_per_sample: Сколько аудио брать на сэмпл (аналично image, параметр audio_model_config[\"audio_agg\"]).\n    :param use_batch_tokenizer: Использовать BatchTokenizer для текста (ускоряет токенизацию и кэширует результаты).\n    :param pretokenize_data: Предварительно токенизировать текст датасета (в памяти) для ускорения обучения/инференса.\n    :param pretokenize_batch_size: Батч-размер при предварительной токенизации.\n    :param tokenizer_cache_size: Размер LRU-кэша в BatchTokenizer.\n    :param max_pretokenize_samples: Максимум сэмплов для предварительной токенизации на чанк/датасет.\n    :param local_cache_dir: Локальная директория кэша моделей/процессоров HF.\n    :param text_padding: \"max_length\" — паддинг до фиксированной длины; \"dynamic\" — паддинг до максимальной длины в батче.\n                         При \"dynamic\" предтокенизация текста автоматически отключается.\n\n    :return: None\n\n    :raises ValueError: Если бэкенд не поддерживает выбранные модальности (внутренняя проверка соответствия).\n                        Также возможны ошибки конфигов, например fusion=\"mean\" при несовпадающих размерах эмбеддингов.\n    :raises OSError: Ошибки загрузки моделей/процессоров из Hugging Face Hub (сетевые/офлайн проблемы, отсутствующие чекпойнты).\n    :raises RuntimeError: Проблемы с устройством/драйвером (CUDA/MPS), несовместимость версий зависимостей и т.п.\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        num_labels: int,\n        target_column_name: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1,\n        use_batch_tokenizer: bool = True,\n        pretokenize_data: bool = True,\n        pretokenize_batch_size: int = 256,\n        tokenizer_cache_size: int = 10000,\n        max_pretokenize_samples: int = 100000,\n        local_cache_dir: str = \"./model_cache\",\n        text_padding: str = \"max_length\"  # \"max_length\" или \"dynamic\"\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.num_labels = num_labels\n        self.target_column_name = target_column_name\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n        self.text_padding = text_padding\n\n        self.label2id: Optional[Dict[Any, int]] = None\n        self.id2label: Optional[Dict[int, str]] = None\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[PbarConsoleLogger] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        mods = set(self.modalities)\n        name = self.backend_name\n\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n                }\n                self.image_model_config = self.image_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n                }\n            elif mods == {\"text\", \"audio\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n                }\n                self.audio_model_config = self.audio_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n                }\n            else:\n                if \"text\" in mods and self.text_model_config is None:\n                    self.text_model_config = {'checkpoint': 'bert-base-multilingual-cased', 'model_type': 'bert', 'max_length': 512}\n                if \"image\" in mods and self.image_model_config is None:\n                    self.image_model_config = {'checkpoint': 'google/vit-base-patch16-224', 'model_type': 'vit', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'}\n                if \"audio\" in mods and self.audio_model_config is None:\n                    self.audio_model_config = {'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000}\n\n        elif name == \"clip\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n            }\n            self.image_model_config = self.image_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n            }\n        elif name == \"clap\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n            }\n            self.audio_model_config = self.audio_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n            }\n        else:\n            pass\n\n        self.backend = UniversalMultiBackend(\n            text_model_config=self.text_model_config if \"text\" in mods else None,\n            image_model_config=self.image_model_config if \"image\" in mods else None,\n            audio_model_config=self.audio_model_config if \"audio\" in mods else None,\n            freeze=self.freeze_backbone,\n            text_tokenizer_fn=self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors,\n            use_batch_tokenizer=self.use_batch_tokenizer,\n            tokenizer_cache_size=self.tokenizer_cache_size,\n            tokenizer_batch_size=self.pretokenize_batch_size,\n            local_cache_dir=self.local_cache_dir,\n            text_padding_strategy=self.text_padding\n        )\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}\")\n\n    def _setup_metrics(self, metric_name: str):\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            metric = evaluate.load(\"f1\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids, average=\"weighted\")\n        elif metric_name == \"accuracy\":\n            metric = evaluate.load(\"accuracy\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids)\n        else:\n            raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"f1\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1,\n        gradient_checkpointing: bool = False,\n        fit_chunk_size: Optional[int] = None,\n        clear_cache_every_n_chunks: int = 10\n    ):\n        \"\"\"\n        Обучает классификатор на заданном DataFrame с поддержкой валидации, ранней остановки и чанковой тренировки.\n        Внутренне использует WeightedCETrainer (кросс-энтропия с весами классов по обратной частоте в train_data),\n        логгер прогресса и, при необходимости, предварительную токенизацию батчей текста.\n\n        Если test_data не передан, train_data разделяется на train/eval по test_size (стратификации нет).\n        При очень больших датасетах обучение проводится чанками (fit_chunk_size), чтобы ограничить потребление памяти.\n\n        :param train_data: Обучающий DataFrame. Должен содержать столбец target_column_name, а также столбцы по выбранным модальностям:\n                           - text_columns для \"text\" (строки, допускаются NaN),\n                           - image_columns для \"image\" (строки путей к файлам, PIL.Image, np.ndarray или списки этих типов),\n                           - audio_columns для \"audio\" (пути к аудиофайлам или np.ndarray моно-сигнала; для путей требуется torchaudio).\n        :param epochs: Количество эпох обучения.\n        :param test_size: Доля данных на валидацию, если test_data не задан.\n        :param test_data: Отдельный DataFrame для валидации. Если указан, параметр test_size игнорируется.\n        :param per_device_train_batch_size: Батч-размер на устройство для обучения.\n        :param gradient_accumulation_steps: Шаги аккумулирования градиента (эффективный батч = batch_size * steps).\n        :param learning_rate: Начальная скорость обучения (оптимизатор и шедулер создаются внутри Trainer).\n        :param metric_name: Метрика ранней остановки/выбора лучшей модели: \"f1\" (weighted) или \"accuracy\".\n        :param fp16: Использовать ли полуточность (только при наличии CUDA).\n        :param logging_steps: Периодичность логирования в шагах.\n        :param eval_steps: Периодичность валидации/сохранения модели в шагах.\n        :param output_dir: Директория для чекпойнтов/логов.\n        :param seed: Начальное зерно для воспроизводимости.\n        :param hidden: Размер скрытого слоя классификационной головы.\n        :param dropout: Дропаут в классификационной голове.\n        :param gradient_checkpointing: Включить gradient checkpointing в бэкендах (если они обучаемые).\n        :param fit_chunk_size: Размер чанка для поэтапной тренировки (None — весь датасет за эпоху без разбиения).\n        :param clear_cache_every_n_chunks: Каждые N чанков очищать кэш токенизации (для экономии памяти).\n\n        :return: self (для чейнинга вызовов).\n\n        :raises ValueError:\n            - Отсутствуют обязательные колонки по модальностям в train_data (внутренняя проверка _validate_data).\n            - Невозможно собрать классификатор с fusion=\"mean\" при разных размерах эмбеддингов модальностей.\n        :raises RuntimeError: Ошибки, возникающие внутри transformers.Trainer (например, рассогласование батчей/логитов),\n                              проблемы с устройством (CUDA OOM, MPS), ошибки чтения аудио/изображений.\n        :raises OSError: Ошибки чтения исходных файлов данных (изображения/аудио) или кэша моделей.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        classes = sorted(train_data[self.target_column_name].unique().tolist())\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n        if test_data is None:\n            df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n        else:\n            df_train, df_eval = train_data, test_data\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds_eval = MultiComboDataset(\n            df=df_eval,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_eval) < 50000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_eval), self.max_pretokenize_samples),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n        counts = np.bincount(y_train_all, minlength=self.num_labels)\n        n_all = counts.sum()\n        class_weights = np.zeros(self.num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        class_weights[nonzero] = n_all / (self.num_labels * counts[nonzero].astype(np.float32))\n\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        self._setup_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def data_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        n_train = len(df_train)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        dummy_idx = np.arange(min(len(df_train), 1))\n        ds_train_init = (\n            MultiComboDataset(\n                df=df_train.iloc[dummy_idx],\n                target_col=self.target_column_name,\n                label2id=self.label2id,\n                text_columns=self.text_columns,\n                image_columns=self.image_columns,\n                audio_columns=self.audio_columns,\n                text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                special_tokens=self.special_tokens,\n                pretokenize=False,\n                tokenizer_returns_tensors=self.tokenizer_returns_tensors\n            ) if len(dummy_idx) > 0 else ds_eval\n        )\n\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            num_labels=self.num_labels,\n            train_labels=y_train_all,\n            class_weights=class_weights\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        steps_done = 0\n        chunk_counter = 0\n\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n\n            for slc in chunk_slices(shuffled, chunk_size):\n                chunk_df = df_train.iloc[slc]\n                should_pretokenize = (\n                    self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                    and len(slc) < self.max_pretokenize_samples and len(slc) > 100\n                )\n\n                ds_chunk = MultiComboDataset(\n                    df=chunk_df,\n                    target_col=self.target_column_name,\n                    label2id=self.label2id,\n                    text_columns=self.text_columns,\n                    image_columns=self.image_columns,\n                    audio_columns=self.audio_columns,\n                    text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                    text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                    special_tokens=self.special_tokens,\n                    pretokenize=should_pretokenize,\n                    pretokenize_batch_size=self.pretokenize_batch_size,\n                    max_cache_size=min(len(slc), self.max_pretokenize_samples),\n                    tokenizer_returns_tensors=self.tokenizer_returns_tensors\n                )\n\n                self.trainer.train_dataset = ds_chunk\n\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk, chunk_df\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                chunk_counter += 1\n                if chunk_counter % clear_cache_every_n_chunks == 0:\n                    if hasattr(ds_chunk, 'clear_cache'):\n                        ds_chunk.clear_cache()\n                        print(f\"✓ Очищен кэш токенизации после {chunk_counter} чанков\")\n\n                del ds_chunk, chunk_df\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n\n        if getattr(self.backend, \"batch_tokenizer\", None):\n            self.backend.batch_tokenizer.clear_cache()\n\n        return self\n\n    def predict(\n        self,\n        df: pd.DataFrame,\n        return_label_str: bool = False,\n        return_proba: bool = False,\n        batch_size: Optional[int] = None\n    ) -> np.ndarray:\n        \"\"\"\n        Выполняет инференс на новом DataFrame и возвращает предсказания.\n        Если в df отсутствует столбец target_column_name, он будет добавлен фиктивными значениями.\n\n        :param df: DataFrame с теми же колонками по выбранным модальностям, что и при обучении.\n                   - text_columns: строки,\n                   - image_columns: пути к изображениям, PIL.Image, np.ndarray или списки таких элементов,\n                   - audio_columns: пути к аудиофайлам или np.ndarray (моно, float32).\n        :param return_label_str: Если True — вернуть массив строковых меток (id2label), иначе — индексы классов.\n        :param return_proba: Если True — вернуть распределения вероятностей (softmax) формы [N, num_labels].\n                             При включении этого флага игнорируется return_label_str.\n        :param batch_size: Переопределяет per_device_eval_batch_size на время инференса (опционально).\n\n        :return:\n            - Если return_proba=True: np.ndarray формы [N, num_labels] — вероятности классов.\n            - Иначе:\n                - Если return_label_str=True: np.ndarray формы [N] со строковыми метками,\n                - Иначе: np.ndarray формы [N] с индексами предсказанных классов.\n\n        :raises RuntimeError: Если модель не обучена (trainer отсутствует).\n        :raises ValueError: Ошибки приведения данных (например, несоответствие ожидаемым колонкам/типам),\n                            внутренние ошибки коллатора/бэкенда (рассогласование размеров батчей).\n        :raises OSError: Ошибки чтения исходных файлов (изображения/аудио).\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n\n        print(f\"Preparing dataset for prediction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_c), 10000),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n\n        print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds)\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        if hasattr(ds, 'clear_cache'):\n            ds.clear_cache()\n\n        if return_proba:\n            logits = preds.predictions\n            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n            return probabilities\n\n        y_pred = np.argmax(preds.predictions, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    def get_embeddings(\n        self,\n        df: pd.DataFrame,\n        batch_size: int = 32,\n        return_per_modality: bool = False\n    ):\n        \"\"\"\n        Извлекает эмбеддинги для входного DataFrame. Возвращает склеенный (fused) эмбеддинг,\n        а при необходимости — также эмбеддинги по каждой модальности.\n        Схема слияния (concat/mean) и размеры зависят от настроек бэкенда и параметра fusion.\n\n        Если в df отсутствует столбец target_column_name, он будет добавлен фиктивными значениями.\n\n        :param df: DataFrame с данными по модальностям (аналогично predict()).\n        :param batch_size: Батч-размер при извлечении эмбеддингов.\n        :param return_per_modality: Если True — вернуть дополнительно словарь с эмбеддингами по модальностям.\n\n        :return:\n            - Если return_per_modality=False:\n                np.ndarray формы [N, D_fused], где D_fused — размерность эмбеддинга после слияния.\n            - Если return_per_modality=True:\n                (fused, per_mod) — кортеж:\n                    - fused: np.ndarray [N, D_fused],\n                    - per_mod: Dict[str, np.ndarray], где ключ — модальность (\"text\"/\"image\"/\"audio\"),\n                               значение — эмбеддинги этой модальности формы [N, D_mod].\n\n        :raises RuntimeError: Если модель не обучена или не готова (trainer/model отсутствуют).\n        :raises ValueError: Если бэкенд не вернул эмбеддинги (например, неверные настройки модальностей/данных).\n        :raises OSError: Ошибки чтения исходных файлов (изображения/аудио).\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n\n        print(f\"Preparing dataset for embeddings extraction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=False,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device: torch.device):\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        print(\"Concatenating embeddings...\")\n        fused_arr = np.vstack(fused_list)\n\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Функции для создания фиктивных данных.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef make_rand_image(h=512, w=512):\n    return (np.random.rand(h, w, 3) * 255).astype(\"uint8\")\n\ndef make_sine_audio(sr=48000, seconds=1.0, freq=440.0):\n    t = np.linspace(0, seconds, int(sr * seconds), endpoint=False)\n    return (0.1 * np.sin(2 * np.pi * freq * t)).astype(np.float32)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-09-11T15:40:17.137607Z","iopub.execute_input":"2025-09-11T15:40:17.138192Z","iopub.status.idle":"2025-09-11T15:40:17.143503Z","shell.execute_reply.started":"2025-09-11T15:40:17.138168Z","shell.execute_reply":"2025-09-11T15:40:17.142693Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Данные (строковые метки → красиво отобразятся в predict(return_label_str=True))\ndf_clip_clap = pd.DataFrame([\n    {\"text\": \"A man riding a bike\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 440.0), \"label\": \"sports\"},\n    {\"text\": \"A cat lying on sofa\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 330.0), \"label\": \"lifestyle\"},\n    {\"text\": \"Stock market is volatile\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 550.0), \"label\": \"business\"},\n    {\"text\": \"Runner on the track\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 660.0), \"label\": \"sports\"},\n    {\"text\": \"New cafe opens downtown\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 220.0), \"label\": \"lifestyle\"},\n    {\"text\": \"Company reports revenue\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 770.0), \"label\": \"business\"},\n])\n\npipe1 = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\", \"audio\"],\n    num_labels=3,                          # == числу уникальных меток\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    image_columns=[\"image\"],\n    audio_columns=[\"audio\"],\n    # Явные конфиги бэкендов\n    text_model_config={\n        \"checkpoint\": \"openai/clip-vit-base-patch32\",\n        \"model_type\": \"clip\",\n        \"max_length\": 77\n    },\n    image_model_config={\n        \"checkpoint\": \"openai/clip-vit-base-patch32\",\n        \"model_type\": \"clip\",\n        \"max_images\": 1,\n        \"image_agg\": \"mean\"\n    },\n    audio_model_config={\n        \"checkpoint\": \"laion/clap-htsat-unfused\",\n        \"model_type\": \"clap\",\n        \"sr\": 48000,\n        \"max_audios\": 1,\n        \"audio_agg\": \"mean\"\n    },\n    fusion=\"mean\",                         # у всех 512 → mean\n    freeze_backbone=False,                  # linear probing\n    use_batch_tokenizer=True,              # быстрый токенизатор\n    pretokenize_data=True,                 # предварительная токенизация\n    pretokenize_batch_size=128,\n    tokenizer_cache_size=5000,\n    max_pretokenize_samples=100000,\n    local_cache_dir=\"./model_cache\"\n)\n\npipe1.fit(\n    train_data=df_clip_clap,\n    epochs=2,\n    test_size=0.33,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=1e-4,\n    metric_name=\"f1\",\n    fp16=True,                 # если есть CUDA\n    logging_steps=1,\n    eval_steps=2,\n    output_dir=\"./mc_max_clip_clap\",\n    seed=42,\n    hidden=512,\n    dropout=0.2,\n    gradient_checkpointing=True,\n    fit_chunk_size=2,          # чанки по 2 сэмпла\n    clear_cache_every_n_chunks=5,\n)\n\n# Предсказания — вероятности\nprobas1 = pipe1.predict(df_clip_clap.iloc[:3], return_proba=True)\nprint(\"Probas shape:\", probas1.shape)\n\n# Предсказания — строковые метки\nlabels1 = pipe1.predict(df_clip_clap.iloc[:3], return_label_str=True)\nprint(\"Labels:\", labels1)\n\n# Эмбеддинги (fused + по модальностям)\nfused1, per1 = pipe1.get_embeddings(df_clip_clap.iloc[:3], batch_size=2, return_per_modality=True)\nprint(\"Fused shape:\", fused1.shape)\nfor m, arr in per1.items():\n    print(f\"{m} emb shape:\", arr.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef build_binary_multimodal_df(n_per_class: int = 50, sr: int = 16000) -> pd.DataFrame:\n    # Генерация текстов «pets»\n    pet_animals = [\"кошка\", \"собака\", \"щенок\", \"кот\", \"котёнок\", \"пёс\", \"питомец\", \"котик\", \"пёсик\", \"котяра\"]\n    pet_actions = [\"сидит\", \"лежит\", \"играет\", \"смотрит\", \"прячется\", \"спит\", \"тянется\", \"мурлычет\", \"исследует\", \"охотится\"]\n    pet_places = [\"на подоконнике\", \"на диване\", \"на ковре\", \"на кухне\", \"в коробке\", \"на кресле\", \"у окна\", \"в саду\", \"на полу\", \"на стуле\"]\n\n    # Генерация текстов «news»\n    news_subjects = [\"власти города\", \"жители района\", \"аналитики\", \"эксперты\", \"журналисты\", \"компания\", \"департамент\", \"учёные\", \"инженеры\", \"ведомство\"]\n    news_verbs = [\"обсудили\", \"обновили\", \"сообщили\", \"рассказали\", \"анонсировали\", \"заявили\", \"подтвердили\", \"планируют\", \"запустили\", \"увеличили\"]\n    news_topics = [\"экономику\", \"политику\", \"транспорт\", \"погоду\", \"технологии\", \"культуру\", \"спорт\", \"здравоохранение\", \"образование\", \"экологию\"]\n\n    rows = []\n\n    # PETS класс\n    pet_freqs = [260.0, 280.0, 300.0, 320.0, 340.0, 360.0]\n    for _ in range(n_per_class):\n        text = f\"{random.choice(pet_animals).capitalize()} {random.choice(pet_actions)} {random.choice(pet_places)}\"\n        img = make_rand_image()\n        aud = make_sine_audio(sr, 1.0, random.choice(pet_freqs))\n        rows.append({\"text\": text, \"image\": img, \"audio\": aud, \"label\": \"pets\"})\n\n    # NEWS класс\n    news_freqs = [560.0, 580.0, 600.0, 620.0, 640.0, 660.0]\n    for _ in range(n_per_class):\n        text = f\"{random.choice(news_subjects).capitalize()} {random.choice(news_verbs)} новости про {random.choice(news_topics)}\"\n        img = make_rand_image()\n        aud = make_sine_audio(sr, 1.0, random.choice(news_freqs))\n        rows.append({\"text\": text, \"image\": img, \"audio\": aud, \"label\": \"news\"})\n\n    random.shuffle(rows)\n    df = pd.DataFrame(rows)\n    return df\n\ntrain_data = build_binary_multimodal_df(n_per_class=12)\ndf_train, df_eval = train_test_split(\n    train_data, test_size=0.3, random_state=42, shuffle=True,\n    stratify=train_data['label'])\n\npipe3 = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\", \"audio\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    image_columns=[\"image\"],\n    audio_columns=[\"audio\"],\n    text_model_config={\n        \"checkpoint\": \"DeepPavlov/rubert-base-cased\",\n        \"model_type\": \"bert\",\n        \"max_length\": 256\n    },\n    image_model_config={\n        \"checkpoint\": \"google/vit-base-patch16-224\",\n        \"model_type\": \"vit\",\n        \"max_images\": 1,\n        \"image_agg\": \"mean\"\n    },\n    audio_model_config={\n        \"checkpoint\": \"facebook/wav2vec2-base-960h\",\n        \"model_type\": \"auto\",   # AutoModel + AutoProcessor\n        \"sr\": 16000,\n        \"max_audios\": 1,\n        \"audio_agg\": \"mean\"\n    },\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=True,\n    pretokenize_data=True,\n    pretokenize_batch_size=128,\n    tokenizer_cache_size=5000,\n    local_cache_dir=\"./model_cache\"\n)\n\npipe3.fit(\n    train_data=df_train,\n    test_data=df_eval,\n    epochs=2,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-4,\n    metric_name=\"f1\",\n    fp16=True,\n    logging_steps=1,\n    eval_steps=1,\n    output_dir=\"./mc_max_rubert_vit_w2v2\",\n    seed=2025,\n    hidden=512,\n    dropout=0.2,\n    gradient_checkpointing=True,\n    fit_chunk_size=8,\n    clear_cache_every_n_chunks=3\n)\n\nprint(\"Pred (labels):\", pipe3.predict(df_eval, return_label_str=True))\nfused3, per3 = pipe3.get_embeddings(df_eval, batch_size=2, return_per_modality=True)\nprint(\"Fused:\", fused3.shape, \"| text:\", per3[\"text\"].shape, \"| image:\", per3[\"image\"].shape, \"| audio:\", per3[\"audio\"].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 3.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_min = pd.DataFrame([\n    {\"text\": \"Привет, как дела?\", \"label\": \"greet\"},\n    {\"text\": \"Сегодня отличная погода\", \"label\": \"weather\"},\n    {\"text\": \"До встречи!\", \"label\": \"greet\"},\n])\n\npipe_min = SingleModelMultiComboClassification(\n    modalities=[\"text\"],\n    num_labels=2,                           # ровно столько, сколько уникальных меток в df_min\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    # Достаточно указать только текстовую модель\n    text_model_config={\n        \"checkpoint\": \"DeepPavlov/rubert-base-cased\",\n        \"model_type\": \"bert\",\n        \"max_length\": 128\n    },\n    fusion=\"concat\",                        # неважно для single-modality\n    freeze_backbone=True\n)\n\n# Минимальный fit: всё по умолчанию (без ранней остановки, без чанков)\npipe_min.fit(train_data=df_min)\n\nprint(\"Pred (ids):\", pipe_min.predict(df_min))\nprint(\"Pred (labels):\", pipe_min.predict(df_min, return_label_str=True))\nemb_min = pipe_min.get_embeddings(df_min)\nprint(\"Embeddings shape:\", emb_min.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 4.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom huggingface_hub import login\nlogin('hf_ьдшин4хфэюц2\\хкпсзэыэкпзмцфх3мххи ихщ т игщ йхт ъйм\\укщи хъ4хрьх84им о\\фуео\\щ рэрь')  # ваш токен\n\ndf_min = pd.DataFrame([\n    {\"text\": \"Привет, как дела?\", \"label\": \"greet\"},\n    {\"text\": \"Сегодня отличная погода\", \"label\": \"weather\"},\n    {\"text\": \"До встречи!\", \"label\": \"greet\"},\n])\n\npipe_min = SingleModelMultiComboClassification(\n    modalities=[\"text\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    text_model_config={\n        \"checkpoint\": \"google/embeddinggemma-300m\",\n        \"max_length\": 2048\n    },\n    text_padding='max_length',\n    freeze_backbone=False\n)\n\npipe_min.fit(train_data=df_min)\n\nprint(\"Pred (ids):\", pipe_min.predict(df_min))\nprint(\"Pred (labels):\", pipe_min.predict(df_min, return_label_str=True))\nemb_min = pipe_min.get_embeddings(df_min)\nprint(\"Embeddings shape:\", emb_min.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение регрессора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация регрессора.","metadata":{}},{"cell_type":"code","source":"# !pip install --upgrade --no-cache-dir \\\n#   --extra-index-url https://download.pytorch.org/whl/cu124 \\\n#   pillow==11.1.0 \\\n#   numpy==1.26.4 \\\n#   pandas==2.2.3 \\\n#   tqdm==4.67.1 \\\n#   transformers==4.51.3 \\\n#   evaluate==0.4.5 \\\n#   wav2clip==0.1.0 \\\n#   torch==2.6.0+cu124 \\\n#   torchaudio==2.6.0+cu124\n!pip install evaluate wav2clip\n\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport math\nimport random\nimport gc\nfrom functools import lru_cache\nfrom typing import Any, Callable, Dict, Generator, List, Optional, Union\n\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers.trainer_callback import TrainerCallback, PrinterCallback, EarlyStoppingCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport evaluate\n\n\n# =========================\n# Утилиты\n# =========================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef to_pil(x: Union[str, np.ndarray, 'Image.Image']) -> 'Image.Image':\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\ndef safe_load(component_cls, checkpoint: str, local_cache_dir: str = \"./model_cache\",\n              local_files_only: Optional[bool] = None, **kwargs):\n    if local_files_only is None:\n        local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n    name = getattr(component_cls, \"__name__\", \"\")\n    if \"Tokenizer\" in name:\n        kwargs.setdefault(\"use_fast\", True)\n    return component_cls.from_pretrained(\n        checkpoint, cache_dir=local_cache_dir, local_files_only=local_files_only, **kwargs\n    )\n\n\n# =========================\n# Токенизатор батчевый (с поддержкой стратегии паддинга)\n# =========================\n\nclass BatchTokenizer:\n    def __init__(\n        self,\n        tokenizer,\n        max_length: int = 512,\n        cache_size: int = 10000,\n        batch_size: int = 256,\n        use_fast: bool = True,\n        device: str = \"cpu\",\n        padding_strategy: str = \"max_length\"  # \"max_length\" или \"dynamic\"\n    ):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.use_fast = use_fast\n        self.device = device\n        self.padding_strategy = padding_strategy\n        if self.padding_strategy not in {\"max_length\", \"dynamic\"}:\n            raise ValueError(\"padding_strategy должен быть 'max_length' или 'dynamic'\")\n        self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n        self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n        if self.is_fast:\n            print(\"✓ Используется Fast Tokenizer\")\n\n    def _tokenize_single(self, text: str) -> tuple:\n        # Кэшируем только фиксированный паддинг — иначе батчи не склеить\n        result = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        return tuple((k, v.squeeze(0).cpu().numpy()) for k, v in result.items())\n\n    def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n        # Динамический паддинг — токенизируем сразу список (без поэлементного кэша, иначе формы различаются)\n        if self.padding_strategy == \"dynamic\":\n            result = self.tokenizer(\n                texts,\n                padding=True,  # 'longest'\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n        # Фиксированный паддинг — используем кэш для коротких батчей\n        if use_cache and len(texts) < 100:\n            results = [dict(self._cache(text)) for text in texts]\n            keys = results[0].keys()\n            batch_dict = {}\n            for key in keys:\n                dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                batch_dict[key] = torch.tensor(np.stack([r[key] for r in results]), dtype=dtype)\n            return batch_dict\n        else:\n            result = self.tokenizer(\n                texts,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n    def tokenize_dataset_lazy(\n        self,\n        texts: List[str],\n        batch_size: Optional[int] = None\n    ) -> Generator[Dict[str, torch.Tensor], None, None]:\n        batch_size = batch_size or self.batch_size\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            yield self.tokenize_batch(batch, use_cache=False)\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n\n# =========================\n# Универсальный датасет (ускорённый)\n# =========================\n\nclass MultiComboDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer: Optional[BatchTokenizer] = None,          # BatchTokenizer\n        text_tokenizer_fn: Optional[Callable] = None,              # кастомная fn -> dict тензоров ИЛИ строка\n        text_tokenizer_fn_batched: Optional[Callable] = None,      # батчевая кастомная fn: List[dict] -> dict тензоров\n        special_tokens: Optional[Dict[str, Any]] = None,\n        pretokenize: bool = False,\n        pretokenize_batch_size: int = 256,\n        max_cache_size: int = 100000,                              # совместимость; не используется банками напрямую\n        tokenizer_returns_tensors: bool = False,\n        cache_dir: Optional[str] = None,\n        deduplicate_texts: bool = True\n    ):\n        \"\"\"\n        Быстрый датасет:\n        - Предтокенизирует текст батчами в банки тензоров (если возможно).\n        - В __getitem__ просто делает слайс по банкам (O(1)), либо возвращает сырую строку.\n        - Предсобирает списки изображений/аудио и метки.\n\n        Если выбран dynamic padding в BatchTokenizer — предтокенизация выключается.\n        \"\"\"\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.batch_tokenizer = text_tokenizer\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.text_tokenizer_fn_batched = text_tokenizer_fn_batched\n\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n        self._N = len(self.df)\n\n        # labels в тензор [N, K] или [N]\n        self._labels = self._prepare_labels(self.df, self.target_col)\n\n        # Предсобранные списки изображений/аудио (без pandas в __getitem__)\n        self._image_lists = self._collect_multi_values(self.df, self.image_columns) if self.image_columns else None\n        self._audio_lists = self._collect_multi_values(self.df, self.audio_columns) if self.audio_columns else None\n\n        # Банки предтокенизированных текстов: dict(key -> torch.Tensor [N, L])\n        self._tok_bank: Optional[Dict[str, torch.Tensor]] = None\n        self._has_text = bool(self.text_columns)\n\n        # Dynamic-паддинг несовместим с предтокенизацией (формы различаются)\n        if pretokenize and self.batch_tokenizer is not None and getattr(self.batch_tokenizer, \"padding_strategy\", \"max_length\") == \"dynamic\":\n            print(\"⚠ Предтокенизация отключена: выбран dynamic-паддинг для текста.\")\n            pretokenize = False\n\n        # Предтокенизация (батчами) — либо BatchTokenizer, либо кастомная функция (batched/single)\n        if self._has_text and pretokenize:\n            if self.batch_tokenizer is not None and self.text_tokenizer_fn is None and self.text_tokenizer_fn_batched is None:\n                self._pretokenize_with_batch_tokenizer(pretokenize_batch_size)\n            else:\n                self._pretokenize_with_custom_fn(pretokenize_batch_size, deduplicate_texts=deduplicate_texts)\n\n    # --------------------------\n    # Метки\n    # --------------------------\n    def _prepare_labels(self, df: pd.DataFrame, target_col: str) -> torch.Tensor:\n        if target_col not in df.columns:\n            return torch.zeros((len(df), 1), dtype=torch.float32)\n        labels_list = []\n        for i in range(len(df)):\n            v = df.iloc[i][target_col]\n            if isinstance(v, (list, tuple, np.ndarray)):\n                arr = np.asarray(v, dtype=np.float32)\n            else:\n                try:\n                    arr = np.asarray([float(v)], dtype=np.float32)\n                except Exception:\n                    arr = np.asarray([0.0], dtype=np.float32)\n            labels_list.append(arr)\n        # выравниваем K по максимуму\n        K = max(a.shape[0] for a in labels_list) if labels_list else 1\n        out = np.zeros((len(labels_list), K), dtype=np.float32)\n        for i, a in enumerate(labels_list):\n            out[i, :a.shape[0]] = a\n        return torch.tensor(out, dtype=torch.float32)\n\n    # --------------------------\n    # Хелперы по тексту/мультимедиа\n    # --------------------------\n    def _join_text(self, row: pd.Series) -> str:\n        sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n        return sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n    @staticmethod\n    def _as_list(v):\n        if v is None or (isinstance(v, float) and np.isnan(v)):\n            return []\n        if isinstance(v, (list, tuple)):\n            return list(v)\n        return [v]\n\n    def _collect_multi_values(self, df: pd.DataFrame, columns: List[str]) -> List[List[Any]]:\n        out = []\n        as_list = self._as_list\n        for _, row in df.iterrows():\n            lst: List[Any] = []\n            for c in columns:\n                if c in row:\n                    lst.extend([x for x in as_list(row[c]) if x is not None])\n            out.append(lst)\n        return out\n\n    # --------------------------\n    # Предтокенизация: BatchTokenizer\n    # --------------------------\n    def _pretokenize_with_batch_tokenizer(self, batch_size: int):\n        print(\"Предтокенизация с BatchTokenizer...\")\n        texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n        banks: Dict[str, List[torch.Tensor]] = {}\n\n        for start in range(0, self._N, batch_size):\n            batch = texts[start:start + batch_size]\n            tok = self.batch_tokenizer.tokenize_batch(batch, use_cache=False)  # dict[str, torch.Tensor [B, L]]\n            # типы\n            for k in tok:\n                if k in (\"input_ids\", \"attention_mask\", \"token_type_ids\"):\n                    tok[k] = tok[k].long()\n                else:\n                    tok[k] = tok[k].to(torch.float32)\n            for k, v in tok.items():\n                banks.setdefault(k, []).append(v)\n\n        self._tok_bank = {k: torch.cat(v_parts, dim=0).contiguous() for k, v_parts in banks.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"✓ Предтокенизация завершена: {self._N} образцов | keys={list(self._tok_bank.keys())}, shapes={shapes}\")\n\n    # --------------------------\n    # Предтокенизация: кастомные функции (batched / single + дедуп)\n    # --------------------------\n    def _pretokenize_with_custom_fn(self, batch_size: int, deduplicate_texts: bool = True):\n        if not self._has_text:\n            return\n\n        cols = list(self.text_columns)\n\n        # Если есть batched-функция — используем её\n        if self.text_tokenizer_fn_batched is not None:\n            print(\"Предтокенизация кастомной batched-функцией...\")\n            # первая порция для выяснения формы\n            first_end = min(self._N, max(8, batch_size))\n            batch_data = []\n            for i in range(first_end):\n                row = self.df.iloc[i]\n                d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                batch_data.append(d)\n            first_tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n            if not isinstance(first_tok, dict):\n                raise ValueError(\"text_tokenizer_fn_batched должна возвращать dict тензоров [B, L]\")\n            # выделяем банки по форме первого батча\n            bank: Dict[str, torch.Tensor] = {}\n            for k, t in first_tok.items():\n                if not torch.is_tensor(t): t = torch.tensor(t)\n                bank[k] = torch.empty((self._N, t.size(1)), dtype=t.dtype)\n                bank[k][:first_end] = t[:first_end]\n\n            # оставшаяся часть\n            for start in range(first_end, self._N, batch_size):\n                end = min(self._N, start + batch_size)\n                batch_data = []\n                for i in range(start, end):\n                    row = self.df.iloc[i]\n                    d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                    batch_data.append(d)\n                tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): t = torch.tensor(t)\n                    bank[k][start:end] = t\n\n            self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n            shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n            print(f\"✓ Предтокенизация кастомной batched-функцией завершена: shapes={shapes}\")\n            return\n\n        # Иначе — single-функция + дедуп\n        print(\"Предтокенизация кастомной single-функцией...\")\n        # подготовим строки (без pandas в горячем цикле)\n        col_arrays = [self.df[c].astype(str).where(~self.df[c].isna(), other=\"\").tolist() for c in cols]\n\n        # детектируем форму\n        first_td = {c: col_arrays[i][0] for i, c in enumerate(cols)}\n        first_tok = self.text_tokenizer_fn(first_td, self.special_tokens)\n        if not isinstance(first_tok, dict):\n            raise ValueError(\"custom text_tokenizer_fn должна возвращать dict тензоров\")\n        for k, t in first_tok.items():\n            if not torch.is_tensor(t): first_tok[k] = torch.tensor(t)\n        # банки\n        bank: Dict[str, torch.Tensor] = {k: torch.empty((self._N, *t.shape), dtype=t.dtype) for k, t in first_tok.items()}\n        for k, t in first_tok.items():\n            bank[k][0].copy_(t)\n\n        cache: Dict[tuple, Dict[str, torch.Tensor]] = {}\n        if deduplicate_texts:\n            key0 = tuple(first_td.get(c, \"\") for c in cols)\n            cache[key0] = {k: v.clone() for k, v in first_tok.items()}\n\n        for i in range(1, self._N):\n            td = {c: col_arrays[j][i] for j, c in enumerate(cols)}\n            if deduplicate_texts:\n                key = tuple(td.get(c, \"\") for c in cols)\n                tok = cache.get(key)\n                if tok is None:\n                    tok = self.text_tokenizer_fn(td, self.special_tokens)\n                    for k, t in tok.items():\n                        if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n                    cache[key] = {k: v.clone() for k, v in tok.items()}\n            else:\n                tok = self.text_tokenizer_fn(td, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n            for k, t in tok.items():\n                bank[k][i].copy_(t)\n\n        self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"✓ Предтокенизация кастомной single-функцией завершена: shapes={shapes}\")\n\n    # --------------------------\n    # Интерфейс Dataset\n    # --------------------------\n    def __len__(self) -> int:\n        return self._N\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item: Dict[str, Any] = {}\n\n        # Метки (вектор [K] или [1])\n        item[\"labels\"] = self._labels[idx]\n\n        # Текст: либо слайс из банка, либо сырая строка (коллатор потом батчево токенизирует)\n        if self.text_columns:\n            if self._tok_bank is not None:\n                item[\"text_tokens\"] = {k: v[idx] for k, v in self._tok_bank.items()}\n            else:\n                item[\"text\"] = self._join_text(self.df.iloc[idx])\n\n        # Изображения/аудио — предсобранные списки\n        if self._image_lists is not None:\n            item[\"images\"] = self._image_lists[idx]\n        if self._audio_lists is not None:\n            item[\"audios\"] = self._audio_lists[idx]\n\n        return item\n\n    # --------------------------\n    # Сервис\n    # --------------------------\n    def get_cache_stats(self) -> Dict[str, Any]:\n        has = self._tok_bank is not None\n        sizes = {k: tuple(v.shape) for k, v in (self._tok_bank or {}).items()}\n        return {\"has_pretokenized\": has, \"shapes\": sizes, \"N\": self._N}\n\n    def clear_cache(self):\n        self._tok_bank = None\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n\n# =========================\n# Универсальный бэкенд\n# =========================\n\nclass BaseBackend(nn.Module):\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n    text_tokenizer_fn: Optional[Callable] = None\n    batch_tokenizer: Optional[BatchTokenizer] = None\n    special_tokens: Dict[str, str] = {}\n    tokenizer_returns_tensors: bool = False\n    local_cache_dir: str = \"./model_cache\"\n    text_padding_strategy: str = \"max_length\"  # стратегия паддинга текста\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        raise NotImplementedError\n\n    def freeze_all(self):\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n    def set_text_tokenizer(self, tokenizer_fn: Optional[Callable], special_tokens: Optional[Dict[str, str]] = None,\n                           returns_tensors: bool = False):\n        self.text_tokenizer_fn = tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = returns_tensors\n\n    def set_batch_tokenizer(self, tokenizer, max_length: int = 512,\n                            cache_size: int = 10000, batch_size: int = 256,\n                            padding_strategy: str = \"max_length\"):\n        self.text_padding_strategy = padding_strategy\n        self.batch_tokenizer = BatchTokenizer(\n            tokenizer=tokenizer,\n            max_length=max_length,\n            cache_size=cache_size,\n            batch_size=batch_size,\n            use_fast=True,\n            padding_strategy=padding_strategy\n        )\n\n\nclass UniversalMultiBackend(BaseBackend):\n    name = \"universal\"\n\n    class _ParamDeviceProxy(nn.Module):\n        def __init__(self, base, device: torch.device):\n            super().__init__()\n            self.base = base if isinstance(base, nn.Module) else None\n            self._callable = base if not isinstance(base, nn.Module) else None\n            self._dummy = nn.Parameter(torch.empty(0), requires_grad=False)\n            with torch.no_grad():\n                self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n\n        def forward(self, *args, **kwargs):\n            target = self.base if self.base is not None else self._callable\n            return target(*args, **kwargs)\n\n        def to(self, device, *args, **kwargs):\n            self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n            return super().to(device, *args, **kwargs)\n\n    def _model_device(self, model, default: torch.device) -> torch.device:\n        try:\n            return next(model.parameters()).device\n        except StopIteration:\n            pass\n        try:\n            buf = next(model.buffers())\n            return buf.device\n        except StopIteration:\n            pass\n        return default\n\n    def _preferred_device(self) -> torch.device:\n        if torch.cuda.is_available():\n            return torch.device(\"cuda\")\n        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n            return torch.device(\"mps\")\n        return torch.device(\"cpu\")\n\n    def _wrap_if_parameterless(self, model, device: torch.device):\n        try:\n            it = model.parameters() if hasattr(model, \"parameters\") else iter(())\n            next(it)\n            return model\n        except StopIteration:\n            return self._ParamDeviceProxy(model, device)\n        except Exception:\n            return self._ParamDeviceProxy(model, device)\n\n    def __init__(\n        self,\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        freeze: bool = True,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        use_batch_tokenizer: bool = True,\n        tokenizer_cache_size: int = 10000,\n        tokenizer_batch_size: int = 256,\n        local_cache_dir: str = \"./model_cache\",\n        text_padding_strategy: str = \"max_length\"\n    ):\n        super().__init__()\n        self.supported = set()\n        self.out_dim_per_modality = {}\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.tokenizer_batch_size = tokenizer_batch_size\n        self.local_cache_dir = local_cache_dir\n        self.text_padding_strategy = text_padding_strategy\n\n        self.text_model = None\n        self.text_processor = None\n        self.text_config = text_model_config or {}\n        if text_model_config:\n            self._init_text_model(text_model_config)\n            self.supported.add(\"text\")\n\n        self.image_model = None\n        self.image_processor = None\n        self.image_config = image_model_config or {}\n        if image_model_config:\n            self._init_image_model(image_model_config)\n            self.supported.add(\"image\")\n\n        self.audio_model = None\n        self.audio_processor = None\n        self.audio_config = audio_model_config or {}\n        if audio_model_config:\n            self._init_audio_model(audio_model_config)\n            self.supported.add(\"audio\")\n\n        if freeze:\n            self.freeze_all()\n\n    def _ensure_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        if x is None:\n            return None\n        if x.dim() == 1:\n            return x.unsqueeze(0)\n        if x.dim() > 2:\n            return x.view(x.size(0), -1)\n        return x\n\n    def _normalize_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        x = self._ensure_2d(x)\n        return F.normalize(x, dim=-1, eps=1e-12) if x is not None and x.numel() > 0 else x\n\n    def _init_text_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoTokenizer, CLIPModel, CLIPTokenizer, ClapModel, ClapProcessor\n\n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n\n        print(f\"Загрузка текстовой модели {checkpoint}...\")\n\n        if model_type == 'clip':\n            self.text_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(CLIPTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.projection_dim\n        elif model_type == 'clap':\n            self.text_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            proc = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = getattr(proc, 'tokenizer', None) or safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = getattr(self.text_model.config, \"projection_dim\", 512)\n        else:\n            self.text_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.text_model  = self._wrap_if_parameterless(self.text_model, dev)\n\n        if self.use_batch_tokenizer and self.text_processor is not None:\n            self.set_batch_tokenizer(\n                self.text_processor,\n                max_length=config.get('max_length', 512),\n                cache_size=self.tokenizer_cache_size,\n                batch_size=self.tokenizer_batch_size,\n                padding_strategy=self.text_padding_strategy\n            )\n\n        self.text_config['max_length'] = config.get('max_length', 512)\n        self.text_config['dim'] = dim\n        self.text_config['model_type'] = model_type\n        self.out_dim_per_modality['text'] = dim\n\n    def _init_image_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoImageProcessor, CLIPModel, CLIPImageProcessor\n\n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n\n        print(f\"Загрузка визуальной модели {checkpoint}...\")\n\n        if model_type == 'clip':\n            self.image_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(CLIPImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.projection_dim\n        else:\n            self.image_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(AutoImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.image_model = self._wrap_if_parameterless(self.image_model, dev)\n\n        self.image_config['max_images'] = config.get('max_images', 1)\n        self.image_config['image_agg'] = config.get('image_agg', 'concat')\n        self.image_config['dim'] = dim\n        self.image_config['model_type'] = model_type\n\n        self.out_dim_per_modality['image'] = (dim * self.image_config['max_images']) if self.image_config['image_agg'] == 'concat' else dim\n\n    def _init_audio_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoProcessor, ClapModel, ClapProcessor\n\n        model_type = config.get('model_type', 'auto').lower()\n        checkpoint = config.get('checkpoint', None)\n\n        print(f\"Загрузка аудио модели (type={model_type})...\")\n\n        if model_type == 'wav2clip':\n            import wav2clip as w2c\n            self._w2c = w2c\n\n            w2c_model = None\n            if hasattr(w2c, \"get_model\"):\n                w2c_model = w2c.get_model()\n            elif hasattr(w2c, \"model\"):\n                m = w2c.model\n                w2c_model = m() if callable(m) else m\n            else:\n                raise RuntimeError(\"wav2clip не содержит get_model()/model. Обновите пакет wav2clip.\")\n\n            self.audio_model = w2c_model\n\n            try:\n                if isinstance(self.audio_model, torch.nn.Module) and torch.cuda.is_available():\n                    self.audio_model = self.audio_model.to(\"cuda\")\n            except Exception:\n                pass\n\n            self.audio_processor = None\n            dim = 512\n            sr = config.get('sr', 16000)\n\n        elif model_type == 'clap':\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для CLAP\")\n            self.audio_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = getattr(self.audio_model.config, \"projection_dim\", 512)\n            sr = getattr(self.audio_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.audio_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n\n        else:\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для аудио-моделей, кроме wav2clip\")\n            self.audio_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(AutoProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.audio_model.config.hidden_size\n            fe = getattr(self.audio_processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 16000) if fe is not None else 16000\n\n        dev = self._preferred_device()\n        self.audio_model = self._wrap_if_parameterless(self.audio_model, dev)\n\n        self.audio_config['sr'] = config.get('sr', sr)\n        self.audio_config['max_audios'] = config.get('max_audios', 1)\n        self.audio_config['audio_agg'] = config.get('audio_agg', 'concat')\n        self.audio_config['dim'] = dim\n        self.audio_config['model_type'] = model_type\n\n        self.out_dim_per_modality['audio'] = (\n            dim * self.audio_config['max_audios']\n            if self.audio_config['audio_agg'] == 'concat' else dim\n        )\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        # Метки (регрессия: вектор или скаляр)\n        labels = []\n        for b in batch:\n            labels.append(torch.as_tensor(b.get(\"labels\", 0.0), dtype=torch.float32))\n        labels = torch.stack(labels)\n\n        backend_inputs: Dict[str, Any] = {}\n        batch_size = len(batch)\n\n        # Текст\n        if self.text_model is not None:\n            if \"text_tokens\" in batch[0]:\n                text_inputs = {}\n                for key in batch[0][\"text_tokens\"].keys():\n                    if torch.is_tensor(batch[0][\"text_tokens\"][key]):\n                        text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n                    else:\n                        dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                        text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n                backend_inputs[\"text_inputs\"] = text_inputs\n            else:\n                texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n                if self.batch_tokenizer:\n                    text_inputs = self.batch_tokenizer.tokenize_batch(texts, use_cache=True)\n                else:\n                    pad = \"max_length\" if getattr(self, \"text_padding_strategy\", \"max_length\") == \"max_length\" else True\n                    text_inputs = self.text_processor(\n                        texts, padding=pad, truncation=True,\n                        max_length=self.text_config.get('max_length', 512),\n                        return_tensors=\"pt\"\n                    )\n                backend_inputs[\"text_inputs\"] = {k: v for k, v in text_inputs.items()}\n\n        # Изображения\n        if self.image_model is not None:\n            images_lists = [b.get(\"images\", []) for b in batch]\n            flat_images, img_counts = [], []\n            for lst in images_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [img for img in lst if img is not None]\n                img_counts.append(len(lst))\n                for img in lst:\n                    flat_images.append(to_pil(img))\n\n            if len(flat_images) > 0:\n                img_proc = self.image_processor(images=flat_images, return_tensors=\"pt\")\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": img_proc[\"pixel_values\"]}\n            else:\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": None}\n\n            backend_inputs[\"image_counts\"] = torch.tensor(img_counts, dtype=torch.long)\n\n        # Аудио\n        if self.audio_model is not None:\n            audios_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios, aud_counts = [], []\n            for lst in audios_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_config['sr']))\n                    elif isinstance(a, np.ndarray):\n                        aa = np.asarray(a, dtype=np.float32)\n                        if aa.ndim > 1: aa = np.squeeze(aa)\n                        if aa.ndim > 1: aa = aa.reshape(-1)\n                        flat_audios.append(aa)\n            if self.audio_config.get('model_type') == 'wav2clip':\n                backend_inputs[\"audio_inputs\"] = {\"raw_audios\": flat_audios}\n            elif len(flat_audios) > 0:\n                if self.audio_config.get('model_type') == 'clap':\n                    aud_proc = self.audio_processor(\n                        audios=flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_features\": aud_proc[\"input_features\"]}\n                else:\n                    aud_proc = self.audio_processor(\n                        flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_values\": aud_proc[\"input_values\"]}\n            else:\n                backend_inputs[\"audio_inputs\"] = {\"input_features\": None, \"input_values\": None, \"raw_audios\": []}\n\n            backend_inputs[\"audio_counts\"] = torch.tensor(aud_counts, dtype=torch.long)\n\n        backend_inputs[\"batch_size\"] = batch_size\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _aggregate_embeddings(\n        self,\n        embs: Optional[torch.Tensor],\n        counts: List[int],\n        max_k: int,\n        dim_hint: int,\n        agg_type: str,\n        batch_size: int,\n        device: torch.device\n    ) -> torch.Tensor:\n        if embs is None or (torch.is_tensor(embs) and embs.numel() == 0):\n            feat_dim = int(dim_hint) if dim_hint is not None else 0\n            out_dim = feat_dim * max_k if agg_type == 'concat' else feat_dim\n            return torch.zeros((batch_size, out_dim), device=device, dtype=torch.float32)\n\n        if not torch.is_tensor(embs):\n            embs = torch.as_tensor(embs, device=device, dtype=torch.float32)\n        if embs.dim() == 1:\n            embs = embs.unsqueeze(0)\n        elif embs.dim() > 2:\n            embs = embs.view(embs.size(0), -1)\n\n        N, D = embs.size()\n        out_dim = (D * max_k) if agg_type == 'concat' else D\n        out = torch.zeros((batch_size, out_dim), device=device, dtype=embs.dtype)\n\n        offset = 0\n        for i, c in enumerate(counts):\n            if c <= 0 or offset >= N:\n                continue\n            take_n = min(c, N - offset)\n            sample = embs[offset:offset + take_n]\n            offset += take_n\n\n            if agg_type == 'concat':\n                take = sample[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n\n        return F.normalize(out, dim=-1, eps=1e-12) if out.size(1) > 0 else out\n\n    @torch.no_grad()\n    def _wav2clip_embed(self, arr: np.ndarray, device: torch.device) -> torch.Tensor:\n        arr = np.asarray(arr, dtype=np.float32)\n        if arr.ndim > 1:\n            arr = np.squeeze(arr)\n        if arr.ndim > 1:\n            arr = arr.reshape(-1)\n        if arr.size < 512:\n            arr = np.pad(arr, (0, 512 - arr.size), mode=\"constant\")\n\n        try:\n            emb = self._w2c.embed_audio(arr, self.audio_model)\n            emb = np.asarray(emb)\n        except Exception:\n            model_dev = self._model_device(self.audio_model, default=device)\n            x = torch.from_numpy(arr).float().unsqueeze(0).to(model_dev)\n            y = self.audio_model(x)\n            if isinstance(y, (tuple, list)):\n                y = y[0]\n            if torch.is_tensor(y):\n                if y.dim() == 2 and y.size(0) == 1:\n                    y = y.squeeze(0)\n                emb = y.detach().cpu().numpy()\n            else:\n                emb = np.asarray(y)\n\n        if emb.ndim > 1:\n            emb = emb.reshape(-1)\n        return torch.as_tensor(emb, device=device, dtype=torch.float32)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        results: Dict[str, torch.Tensor] = {}\n        batch_size = int(backend_inputs.get(\"batch_size\", 1))\n\n        # Текст\n        if self.text_model is not None and \"text_inputs\" in backend_inputs:\n            text_inputs = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n            if hasattr(self.text_model, \"get_text_features\"):\n                text_z = self.text_model.get_text_features(**text_inputs)\n            else:\n                outputs = self.text_model(**text_inputs)\n                text_z = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n            results[\"text\"] = self._normalize_2d(text_z)\n\n        # Изображения\n        if self.image_model is not None and \"image_inputs\" in backend_inputs:\n            pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n            counts = backend_inputs[\"image_counts\"].tolist()\n            total_images_needed = sum(counts)\n\n            img_flat = None\n            actual_img_dim = self.image_config.get(\"dim\", 768)\n\n            if pi is not None and pi.numel() > 0 and total_images_needed > 0:\n                pi = pi.to(device)\n                if pi.size(0) > total_images_needed:\n                    pi = pi[:total_images_needed]\n\n                if hasattr(self.image_model, \"get_image_features\"):\n                    img_flat = self.image_model.get_image_features(pixel_values=pi)\n                else:\n                    outputs = self.image_model(pixel_values=pi)\n                    img_flat = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state[:, 0]\n\n                img_flat = self._normalize_2d(img_flat)\n                actual_img_dim = img_flat.size(1) if img_flat is not None else actual_img_dim\n\n            img_z = self._aggregate_embeddings(\n                img_flat, counts,\n                self.image_config[\"max_images\"],\n                actual_img_dim,\n                self.image_config[\"image_agg\"],\n                len(counts),\n                device\n            )\n\n            if actual_img_dim != self.image_config.get(\"dim\"):\n                self.image_config[\"dim\"] = actual_img_dim\n                self.out_dim_per_modality[\"image\"] = (\n                    actual_img_dim * self.image_config[\"max_images\"]\n                    if self.image_config[\"image_agg\"] == \"concat\" else actual_img_dim\n                )\n\n            results[\"image\"] = img_z\n\n        # Аудио\n        if self.audio_model is not None and \"audio_inputs\" in backend_inputs:\n            counts = backend_inputs[\"audio_counts\"].tolist()\n            total_audios_needed = sum(counts)\n\n            aud_flat = None\n            actual_aud_dim = self.audio_config.get(\"dim\", 768)\n            model_type = self.audio_config.get(\"model_type\")\n\n            if total_audios_needed > 0:\n                if model_type == \"clap\":\n                    af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n                    if af is not None and af.numel() > 0:\n                        af = af.to(device)\n                        if af.size(0) > total_audios_needed:\n                            af = af[:total_audios_needed]\n                        with torch.cuda.amp.autocast(enabled=False):\n                            aud_flat = self.audio_model.get_audio_features(input_features=af.float())\n                        aud_flat = self._normalize_2d(aud_flat.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n                elif model_type == \"wav2clip\":\n                    raw_list = backend_inputs[\"audio_inputs\"].get(\"raw_audios\", [])\n                    if len(raw_list) > total_audios_needed:\n                        raw_list = raw_list[:total_audios_needed]\n                    if len(raw_list) > 0:\n                        embs = [self._wav2clip_embed(arr, device) for arr in raw_list]\n                        aud_flat = torch.stack(embs, dim=0)\n                        aud_flat = self._normalize_2d(aud_flat)\n                        actual_aud_dim = aud_flat.size(1)\n\n                else:\n                    av = backend_inputs[\"audio_inputs\"][\"input_values\"]\n                    if av is not None and av.numel() > 0:\n                        av = av.to(device)\n                        if av.size(0) > total_audios_needed:\n                            av = av[:total_audios_needed]\n                        av = av.clamp_(-1.0, 1.0)\n                        with torch.cuda.amp.autocast(enabled=False):\n                            outputs = self.audio_model(input_values=av.float())\n                            feats = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n                        aud_flat = self._normalize_2d(feats.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n            aud_z = self._aggregate_embeddings(\n                aud_flat, counts,\n                self.audio_config[\"max_audios\"],\n                actual_aud_dim,\n                self.audio_config[\"audio_agg\"],\n                len(counts),\n                device\n            )\n\n            if aud_flat is not None and actual_aud_dim != self.audio_config.get(\"dim\"):\n                self.audio_config[\"dim\"] = actual_aud_dim\n                self.out_dim_per_modality[\"audio\"] = (\n                    actual_aud_dim * self.audio_config[\"max_audios\"]\n                    if self.audio_config[\"audio_agg\"] == \"concat\" else actual_aud_dim\n                )\n\n            results[\"audio\"] = aud_z\n\n        # Строгая проверка согласованности размеров батча между модальностями\n        if results:\n            bs_list = [v.size(0) for v in results.values()]\n            if len(set(bs_list)) != 1:\n                raise RuntimeError(f\"Inconsistent batch sizes across modalities: {bs_list}\")\n\n        return results\n\n\n# =========================\n# Классификатор (голова регрессии)\n# =========================\n\nclass SingleBackboneClassifier(nn.Module):\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_labels: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError('Для fusion=\"mean\" размеры модальностей должны совпадать')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                has_trainable = any(p.requires_grad for p in m.parameters()) if hasattr(m, \"parameters\") else False\n            except Exception:\n                has_trainable = False\n            if not has_trainable:\n                continue\n            try:\n                cfg = getattr(m, \"config\", None)\n                if cfg is not None and hasattr(cfg, \"use_cache\"):\n                    cfg.use_cache = False\n            except Exception:\n                pass\n            try:\n                if hasattr(m, \"gradient_checkpointing_enable\"):\n                    try:\n                        if gradient_checkpointing_kwargs is not None:\n                            m.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                        else:\n                            m.gradient_checkpointing_enable()\n                    except TypeError:\n                        m.gradient_checkpointing_enable()\n            except Exception:\n                pass\n\n    def gradient_checkpointing_disable(self):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                if hasattr(m, \"gradient_checkpointing_disable\"):\n                    m.gradient_checkpointing_disable()\n            except Exception:\n                pass\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        batch_size = None\n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() == 3:\n                    t = t.mean(dim=1)\n                elif t.dim() > 3:\n                    t = t.view(t.size(0), -1)\n                feats.append(t)\n                if batch_size is None:\n                    batch_size = t.size(0)\n        if batch_size is not None:\n            feats = [f[:batch_size] for f in feats]\n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        if not z:\n            raise ValueError(\"Backend не вернул эмбеддинги\")\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\n# =========================\n# Trainer для регрессии (MSE)\n# =========================\n\nclass MSETrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        labels = inputs.pop(\"labels\").to(torch.float32)\n        outputs = model(**inputs)\n        logits = outputs.logits\n        preds = logits if logits.dim() == 1 else (logits.squeeze(-1) if logits.size(-1) == 1 else logits)\n        labels = labels.view_as(preds)\n        loss = F.mse_loss(preds, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# =========================\n# Прогресс-логгер\n# =========================\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n        self.tqdm = tqdm\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            self.tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\n# =========================\n# Основной пайплайн (регрессия с несколькими таргет-колонками)\n# =========================\n\nclass SingleModelMultiComboRegression:\n    \"\"\"\n    Высокоуровневый пайплайн мультимодальной регрессии (text / image / audio) поверх моделей Hugging Face\n    и wav2clip. Поддерживает автоматическую сборку бэкенда под набор модальностей, батчевую токенизацию\n    (включая dynamic padding), предварительную токенизацию датасета (отключается при dynamic), чанковую тренировку,\n    раннюю остановку, извлечение эмбеддингов.\n\n    Теперь поддерживаются несколько столбцов-таргетов. Вы передаёте список target_column_names,\n    а число выходов (num_labels) определяется автоматически как len(target_column_names).\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        target_column_names: List[str],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1,\n        use_batch_tokenizer: bool = True,\n        pretokenize_data: bool = True,\n        pretokenize_batch_size: int = 256,\n        tokenizer_cache_size: int = 10000,\n        max_pretokenize_samples: int = 100000,\n        local_cache_dir: str = \"./model_cache\",\n        text_padding: str = \"max_length\"\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.target_column_names = list(target_column_names)\n        self.num_labels = len(self.target_column_names)\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n        self.text_padding = text_padding\n\n        self._target_vec_col = \"__target_vector__\"\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[PbarConsoleLogger] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        mods = set(self.modalities)\n        name = self.backend_name\n\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n                }\n                self.image_model_config = self.image_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n                }\n            elif mods == {\"text\", \"audio\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n                }\n                self.audio_model_config = self.audio_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n                }\n            else:\n                if \"text\" in mods and self.text_model_config is None:\n                    self.text_model_config = {'checkpoint': 'bert-base-multilingual-cased', 'model_type': 'bert', 'max_length': 512}\n                if \"image\" in mods and self.image_model_config is None:\n                    self.image_model_config = {'checkpoint': 'google/vit-base-patch16-224', 'model_type': 'vit', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'}\n                if \"audio\" in mods and self.audio_model_config is None:\n                    self.audio_model_config = {'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000}\n\n        elif name == \"clip\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n            }\n            self.image_model_config = self.image_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n            }\n        elif name == \"clap\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n            }\n            self.audio_model_config = self.audio_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n            }\n        else:\n            pass\n\n        self.backend = UniversalMultiBackend(\n            text_model_config=self.text_model_config if \"text\" in mods else None,\n            image_model_config=self.image_model_config if \"image\" in mods else None,\n            audio_model_config=self.audio_model_config if \"audio\" in mods else None,\n            freeze=self.freeze_backbone,\n            text_tokenizer_fn=self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors,\n            use_batch_tokenizer=self.use_batch_tokenizer,\n            tokenizer_cache_size=self.tokenizer_cache_size,\n            tokenizer_batch_size=self.pretokenize_batch_size,\n            local_cache_dir=self.local_cache_dir,\n            text_padding_strategy=self.text_padding\n        )\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}\")\n\n    def _setup_metrics(self, metric_name: str):\n        name = metric_name.lower()\n        if name not in (\"rmse\", \"mae\", \"r2\"):\n            raise ValueError('metric_name для регрессии должен быть \"rmse\", \"mae\" или \"r2\"')\n\n        def compute(p):\n            preds = p.predictions\n            y = p.label_ids\n            preds = preds.squeeze(-1) if preds.ndim == 2 and preds.shape[-1] == 1 else preds\n            y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n            axis = 0 if preds.ndim == 2 else None\n            if name == \"rmse\":\n                err = preds - y\n                mse = np.mean(err**2, axis=axis)\n                rmse = np.sqrt(mse)\n                return {\"rmse\": float(np.mean(rmse))}\n            elif name == \"mae\":\n                mae = np.mean(np.abs(preds - y), axis=axis)\n                return {\"mae\": float(np.mean(mae))}\n            else:\n                y_mean = np.mean(y, axis=axis, keepdims=True) if preds.ndim == 2 else np.mean(y)\n                ss_res = np.sum((y - preds) ** 2, axis=axis)\n                ss_tot = np.sum((y - y_mean) ** 2, axis=axis)\n                r2 = 1.0 - (ss_res / (ss_tot + 1e-12))\n                return {\"r2\": float(np.mean(r2))}\n\n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data_modalities(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def _validate_targets_present(self, df: pd.DataFrame):\n        missing = [c for c in self.target_column_names if c not in df.columns]\n        if missing:\n            raise ValueError(f\"В DataFrame отсутствуют целевые колонки: {missing}\")\n\n    def _attach_target_vector(self, df: pd.DataFrame, fill_zeros: bool = False) -> pd.DataFrame:\n        df_c = df.copy()\n        K = self.num_labels\n        if fill_zeros:\n            df_c[self._target_vec_col] = [np.zeros(K, dtype=np.float32) for _ in range(len(df_c))]\n        else:\n            def _row_to_vec(row):\n                vals = [row[c] for c in self.target_column_names]\n                return np.asarray(vals, dtype=np.float32)\n            df_c[self._target_vec_col] = df_c.apply(_row_to_vec, axis=1)\n        return df_c\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"rmse\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result_reg\",\n        seed: int = 42,\n        hidden: int = 256,\n        dropout: float = 0.1,\n        gradient_checkpointing: bool = False,\n        fit_chunk_size: Optional[int] = None,\n        clear_cache_every_n_chunks: int = 10\n    ):\n        self._validate_data_modalities(train_data)\n        self._validate_targets_present(train_data)\n        set_seed(seed)\n\n        df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n        if test_data is not None:\n            self._validate_targets_present(test_data)\n\n        df_train_ext = self._attach_target_vector(df_train, fill_zeros=False)\n        df_eval_ext = self._attach_target_vector(df_eval, fill_zeros=False)\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds_eval = MultiComboDataset(\n            df=df_eval_ext,\n            target_col=self._target_vec_col,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_eval_ext) < 50000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_eval_ext), self.max_pretokenize_samples),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        if gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n\n        self._setup_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.0,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def regression_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            out = self.backend.collate(batch_list)\n            if \"labels\" in out:\n                out[\"labels\"] = out[\"labels\"].to(torch.float32)\n            return out\n\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        n_train = len(df_train_ext)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        dummy_idx = np.arange(min(len(df_train_ext), 1))\n        ds_train_init = (\n            MultiComboDataset(\n                df=df_train_ext.iloc[dummy_idx],\n                target_col=self._target_vec_col,\n                text_columns=self.text_columns,\n                image_columns=self.image_columns,\n                audio_columns=self.audio_columns,\n                text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                special_tokens=self.special_tokens,\n                pretokenize=False,\n                tokenizer_returns_tensors=self.tokenizer_returns_tensors\n            ) if len(dummy_idx) > 0 else ds_eval\n        )\n\n        self.trainer = MSETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=regression_collator,\n            compute_metrics=self.compute_metrics\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        steps_done = 0\n        chunk_counter = 0\n\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n            for slc in chunk_slices(shuffled, chunk_size):\n                chunk_df = df_train_ext.iloc[slc]\n                ds_chunk = MultiComboDataset(\n                    df=chunk_df,\n                    target_col=self._target_vec_col,\n                    text_columns=self.text_columns,\n                    image_columns=self.image_columns,\n                    audio_columns=self.audio_columns,\n                    text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                    text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                    special_tokens=self.special_tokens,\n                    pretokenize=(self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                                 and len(slc) < self.max_pretokenize_samples and len(slc) > 100),\n                    pretokenize_batch_size=self.pretokenize_batch_size,\n                    max_cache_size=min(len(slc), self.max_pretokenize_samples),\n                    tokenizer_returns_tensors=self.tokenizer_returns_tensors\n                )\n                self.trainer.train_dataset = ds_chunk\n\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                chunk_counter += 1\n                if chunk_counter % clear_cache_every_n_chunks == 0:\n                    if hasattr(ds_chunk, 'clear_cache'):\n                        ds_chunk.clear_cache()\n                        print(f\"✓ Очищен кэш токенизации после {chunk_counter} чанков\")\n\n                del ds_chunk\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n\n        if getattr(self.backend, \"batch_tokenizer\", None):\n            self.backend.batch_tokenizer.clear_cache()\n\n        return self\n\n    def predict(\n        self,\n        df: pd.DataFrame,\n        batch_size: Optional[int] = None\n    ) -> np.ndarray:\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n\n        print(f\"Preparing dataset for prediction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self._target_vec_col,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_c), 10000),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n\n        print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds)\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        if hasattr(ds, 'clear_cache'):\n            ds.clear_cache()\n\n        y = preds.predictions\n        y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n        return y\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n\n        print(f\"Preparing dataset for embeddings extraction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self._target_vec_col,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=False,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device: torch.device):\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        print(\"Concatenating embeddings...\")\n        fused_arr = np.vstack(fused_list)\n\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Данные\nn = 8\ndf = pd.DataFrame({\n    \"title\": [f\"some short text {i}\" for i in range(n)],\n    \"audio\": [(np.sin(np.linspace(0, 2*np.pi, 48000)).astype(np.float32) * 0.1) for _ in range(n)],\n    \"target\": np.random.randn(n).astype(np.float32)\n})\n\n# Инициализация пайплайна (text=CLIP, audio=CLAP)\npipe = SingleModelMultiComboRegression(\n    modalities=[\"text\",\"audio\"],\n    target_column_names=[\"target\"],\n    text_columns=[\"title\"],\n    audio_columns=[\"audio\"],\n    text_model_config={\n        \"checkpoint\": \"openai/clip-vit-base-patch32\",\n        \"model_type\": \"clip\",\n        \"max_length\": 77\n    },\n    audio_model_config={\n        \"checkpoint\": \"laion/clap-htsat-unfused\",\n        \"model_type\": \"clap\",\n        \"sr\": 48000,\n        \"max_audios\": 2,\n        \"audio_agg\": \"concat\"\n    },\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=True,\n    pretokenize_data=True,\n    pretokenize_batch_size=64,\n    tokenizer_cache_size=10000,\n    max_pretokenize_samples=50000,\n    local_cache_dir=\"./model_cache\"\n)\n\n# Тренировка\npipe.fit(\n    train_data=df,\n    epochs=2,\n    test_size=0.25,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-4,\n    metric_name=\"rmse\",\n    fp16=True,                       # будет True, только если есть CUDA\n    logging_steps=10,\n    eval_steps=10,\n    output_dir=\"./out_clip_clap\",\n    seed=42,\n    hidden=512,\n    dropout=0.1,\n    gradient_checkpointing=True,     # включить GC там, где поддерживается\n    fit_chunk_size=None,\n    clear_cache_every_n_chunks=2\n)\n\n# Предсказания\ny_pred = pipe.predict(df, batch_size=4)\nprint(\"Pred shape:\", y_pred.shape)\n\n# Эмбеддинги\nemb_fused, emb_per = pipe.get_embeddings(df, batch_size=4, return_per_modality=True)\nprint(\"Fused:\", emb_fused.shape, \"| text:\", emb_per[\"text\"].shape, \"| audio:\", emb_per[\"audio\"].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Данные\nn = 9\ndf = pd.DataFrame({\n    \"text\": [f\"example text #{i}\" for i in range(n)],\n    \"image\": [(np.random.rand(224,224,3) * 255).astype(np.uint8) for _ in range(n)],\n    \"audio\": [(np.random.randn(16000).astype(np.float32)*0.05) for _ in range(n)],\n    \"target\": np.random.randn(n).astype(np.float32)\n})\n\npipe = SingleModelMultiComboRegression(\n    modalities=[\"text\",\"image\",\"audio\"],\n    target_column_names=[\"target\"],\n    text_columns=[\"text\"],\n    image_columns=[\"image\"],\n    audio_columns=[\"audio\"],\n    text_model_config={\n        \"checkpoint\": \"distilbert-base-uncased\",\n        \"model_type\": \"auto\",\n        \"max_length\": 128\n    },\n    image_model_config={\n        \"checkpoint\": \"google/vit-base-patch16-224\",\n        \"model_type\": \"vit\",\n        \"max_images\": 2,\n        \"image_agg\": \"mean\"\n    },\n    audio_model_config={\n        \"checkpoint\": \"facebook/wav2vec2-base-960h\",\n        \"model_type\": \"auto\",\n        \"sr\": 16000,\n        \"max_audios\": 2,\n        \"audio_agg\": \"mean\"\n    },\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=True,\n    pretokenize_data=True,\n    pretokenize_batch_size=32,\n    tokenizer_cache_size=20000,\n    max_pretokenize_samples=100000,\n    local_cache_dir=\"./model_cache\"\n)\n\npipe.fit(\n    train_data=df,\n    epochs=2,\n    test_size=0.3,\n    per_device_train_batch_size=3,\n    gradient_accumulation_steps=1,\n    learning_rate=1e-4,\n    metric_name=\"r2\",\n    fp16=True,\n    logging_steps=5,\n    eval_steps=5,\n    output_dir=\"./out_auto_triplet\",\n    seed=123,\n    hidden=384,\n    dropout=0.1,\n    gradient_checkpointing=True,\n    fit_chunk_size=6,                # демонстрация чанковой тренировки\n)\n\ny = pipe.predict(df, batch_size=3)\nprint(\"Pred shape:\", y.shape)\n\nfused, per_mod = pipe.get_embeddings(df, batch_size=3, return_per_modality=True)\nprint(\"Fused:\", fused.shape, \"| text:\", per_mod[\"text\"].shape, \"| image:\", per_mod[\"image\"].shape, \"| audio:\", per_mod[\"audio\"].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 3.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Данные\nn = 5\ndf = pd.DataFrame({\n    \"audio\": [(np.sin(np.linspace(0, 6.28, 16000)).astype(np.float32)*0.05) for _ in range(n)],\n    \"target\": np.random.randn(n)\n})\n\npipe = SingleModelMultiComboRegression(\n    modalities=[\"audio\"],\n    target_column_names=[\"target\"],\n    audio_columns=[\"audio\"],\n    audio_model_config={\"model_type\": \"wav2clip\", \"sr\": 16000, \"max_audios\": 1, \"audio_agg\": \"mean\"},\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=False,      # текст не используется\n    pretokenize_data=False\n)\n\npipe.fit(df, epochs=1, test_size=0.33, per_device_train_batch_size=2, eval_steps=1, logging_steps=1, output_dir=\"./out_w2c\")\n\npred = pipe.predict(df)\nprint(\"Pred shape:\", pred.shape)\n\nemb = pipe.get_embeddings(df)\nprint(\"Embeddings:\", emb.shape)    # ожидаемо [N, 512]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 4.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom huggingface_hub import login\nlogin('hf_флжптфджуртн ижщрнтур пфмудгьтпруцдждсп йжт жтжщ йр45цт зз н нй2хщй тэщш рэйффыуео зрьт тжвэвцу5фвое')  # ваш HF токен\n\n# Данные\nn = 10\ndf = pd.DataFrame({\n    \"text\": [f\"example text #{i}\" for i in range(n)],\n    \"target_1\": np.random.randn(n),\n    \"target_2\": np.random.randn(n)\n})\n\npipe = SingleModelMultiComboRegression(\n    modalities=[\"text\"],\n    target_column_names=[\"target_1\", \"target_2\"],\n    text_columns=[\"text\"],\n    text_model_config={\n        \"checkpoint\": \"google/embeddinggemma-300m\",\n        \"max_length\": 2048\n    },\n    text_padding=\"dynamic\",\n    freeze_backbone=False,\n    pretokenize_data=True\n)\n\npipe.fit(df, epochs=1, test_size=0.33, per_device_train_batch_size=2, eval_steps=1, logging_steps=1, output_dir=\"./test123\")\n\npred = pipe.predict(df)\nprint(\"Pred shape:\", pred.shape)\n\nemb = pipe.get_embeddings(df)\nprint(\"Embeddings:\", emb.shape)    # ожидаемо [N, 512]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора с RuCLIP, который работает ещё и со звуком.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  torch==2.6.0+cu124 torchaudio==2.6.0+cu124 \\\n\n!pip install --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  torch==2.6.0+cu124 torchaudio==2.6.0+cu124\n!pip install \"open-clip-torch==2.26.1\" \"transformers==4.51.3\" \"evaluate==0.4.5\"\n!pip install \"ruclip @ git+https://github.com/ai-forever/ru-clip.git@main#egg=ruclip\"\n\nimport os, math, random, gc, time\nfrom functools import lru_cache\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\nimport evaluate\nfrom transformers import TrainingArguments, Trainer\nfrom transformers.trainer_callback import TrainerCallback, PrinterCallback, EarlyStoppingCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\n# =========================\n# Утилиты\n# =========================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n\ndef to_pil(x: Union[str, np.ndarray, 'Image.Image']) -> 'Image.Image':\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr:\n        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\n# =========================\n# RuCLIP токенизатор (батч + кэш)\n# =========================\n\nclass RuCLIPBatchTokenizer:\n    def __init__(self, tokenizer, context_length: int = 77, cache_size: int = 20000):\n        self.tokenizer = tokenizer\n        self.context_length = context_length\n        self._cache = lru_cache(maxsize=cache_size)(self._tok_one)\n\n    def _tok_one(self, text: str) -> np.ndarray:\n        # open_clip токенизатор принимает список строк и возвращает LongTensor [B, L]\n        ids = self.tokenizer([text], context_length=self.context_length)\n        return ids.squeeze(0).cpu().numpy().astype(np.int64)\n\n    def tokenize_batch(self, texts: List[str]) -> torch.Tensor:\n        if len(texts) < 100:\n            arrs = [self._cache(t) for t in texts]\n            return torch.from_numpy(np.stack(arrs, axis=0)).long()\n        return self.tokenizer(texts, context_length=self.context_length).long()\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n\n# =========================\n# Датасет\n# =========================\n\nclass MultiComboDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        label2id: Dict[Any, int],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_batch_tokenizer: Optional[RuCLIPBatchTokenizer] = None,\n        pretokenize_text: bool = True,\n        pretokenize_batch_size: int = 2048,\n        ruclip_context_len: int = 77,\n        audio_sr: int = 48000\n    ):\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.label2id = label2id\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tok = text_batch_tokenizer\n        self.context_len = ruclip_context_len\n        self.audio_sr = audio_sr\n\n        self._N = len(self.df)\n        if self.target_col in self.df.columns:\n            y = self.df[self.target_col].map(self.label2id).fillna(0).astype(int).values\n        else:\n            y = np.zeros(self._N, dtype=np.int64)\n        self._labels = torch.tensor(y, dtype=torch.long)\n\n        self._image_lists = self._collect_multi(self.df, self.image_columns) if self.image_columns else None\n        self._audio_lists = self._collect_multi(self.df, self.audio_columns) if self.audio_columns else None\n\n        self._text_bank: Optional[torch.Tensor] = None\n        if self.text_columns and pretokenize_text and self.text_tok is not None:\n            texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n            chunks = []\n            for i in range(0, len(texts), pretokenize_batch_size):\n                chunks.append(self.text_tok.tokenize_batch(texts[i:i+pretokenize_batch_size]))\n            self._text_bank = torch.cat(chunks, dim=0).contiguous()\n            print(f\"✓ Предтокенизация RuCLIP: shape={tuple(self._text_bank.shape)}\")\n\n    def __len__(self): return self._N\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item: Dict[str, Any] = {\"labels\": int(self._labels[idx])}\n        if self._text_bank is not None:\n            item[\"text_tokens\"] = self._text_bank[idx]\n        elif self.text_columns:\n            item[\"text\"] = self._join_text(self.df.iloc[idx])\n        if self._image_lists is not None:\n            item[\"images\"] = self._image_lists[idx]\n        if self._audio_lists is not None:\n            item[\"audios\"] = self._audio_lists[idx]\n        return item\n\n    @staticmethod\n    def _as_list(v):\n        if v is None or (isinstance(v, float) and np.isnan(v)): return []\n        if isinstance(v, (list, tuple)): return list(v)\n        return [v]\n\n    def _collect_multi(self, df: pd.DataFrame, cols: List[str]) -> List[List[Any]]:\n        out = []\n        for _, row in df.iterrows():\n            lst = []\n            for c in cols:\n                if c in row: lst.extend([x for x in self._as_list(row[c]) if x is not None])\n            out.append(lst)\n        return out\n\n    def _join_text(self, row: pd.Series) -> str:\n        parts = []\n        for c in self.text_columns:\n            v = row.get(c, \"\")\n            if pd.isna(v): v = \"\"\n            parts.append(str(v))\n        return \" [SEP] \".join(parts)\n\n    def clear_cache(self):\n        self._text_bank = None\n        torch.cuda.empty_cache()\n\n\n# =========================\n# Бэкенд: RuCLIP (+ опционально аудио через CLAP)\n# =========================\n\nclass RuCLIPBackend(nn.Module):\n    name = \"ruclip\"\n    def __init__(\n        self,\n        ruclip_model_name: str = \"ViT-B-32\",\n        ruclip_pretrained: Optional[str] = \"hf-hub:ai-forever/ru-clip-vit-base-patch32-224\",\n        ruclip_context_len: int = 77,\n        max_images: int = 1,\n        image_agg: str = \"concat\",          # concat|mean\n        max_audios: int = 1,\n        audio_agg: str = \"concat\",          # concat|mean\n        audio_cfg: Optional[Dict[str, Any]] = None,   # None или {'type':'clap','checkpoint':..., 'sr':48000}\n        freeze: bool = True,\n        device: Optional[torch.device] = None\n    ):\n        super().__init__()\n        import open_clip\n\n        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n\n        # Детектируем «родной» RuCLIP от ai-forever (через ruclip)\n        use_ruclip_pkg = False\n        ruclip_repo = (ruclip_pretrained or \"\")  # может быть None\n        if ruclip_repo.startswith(\"hf-hub:\"):\n            repopath = ruclip_repo.replace(\"hf-hub:\", \"\")\n        else:\n            repopath = ruclip_repo\n\n        base_name = repopath.split(\"/\")[-1] if repopath else \"\"\n        # Признак: репо ai-forever/ru-clip-... или имя ruclip-...\n        if (\"ai-forever/ru-clip-\" in repopath) or base_name.startswith(\"ru-clip-\") or base_name.startswith(\"ruclip-\"):\n            use_ruclip_pkg = True\n\n        print(f\"Загрузка RuCLIP: {ruclip_model_name} / {ruclip_pretrained}\")\n\n        if use_ruclip_pkg:\n            try:\n                import ruclip\n            except Exception as e:\n                raise RuntimeError(\n                    \"Для загрузки RuCLIP из HF нужен пакет 'ruclip'. Установите: pip install ruclip==0.0.2\"\n                ) from e\n\n            # ruclip.load ожидает имя вроде \"ruclip-vit-base-patch32-224\"\n            # Если пришло \"ru-clip-...\", заменим на \"ruclip-...\"\n            ruclip_id = base_name\n            if ruclip_id.startswith(\"ru-clip-\"):\n                ruclip_id = ruclip_id.replace(\"ru-clip-\", \"ruclip-\", 1)\n\n            clip_model, processor = ruclip.load(ruclip_id, device=str(self.device))\n            self.ruclip_model = clip_model.eval()\n\n            # Processor в ruclip обычно хранит препроцесс изображений и токенизацию\n            self.ruclip_preprocess = getattr(processor, \"preprocess\", processor)\n\n            # Токенизатор: берём из processor, если есть; иначе — из open_clip\n            proc_tokenizer = getattr(processor, \"tokenizer\", None)\n            self.ruclip_tokenizer = proc_tokenizer or open_clip.get_tokenizer(ruclip_model_name)\n\n        else:\n            # Фоллбек: open-clip (поддерживает openai/laion2b/... или pretrained=None)\n            try:\n                self.ruclip_model, self.ruclip_preprocess, _ = open_clip.create_model_and_transforms(\n                    ruclip_model_name, pretrained=ruclip_pretrained\n                )\n            except Exception:\n                # Попытка через HF-путь с флагом pretrained_hf\n                if isinstance(ruclip_pretrained, str):\n                    hf_path = ruclip_pretrained.replace(\"hf-hub:\", \"\")\n                else:\n                    hf_path = ruclip_pretrained\n                try:\n                    self.ruclip_model, _, self.ruclip_preprocess = open_clip.create_model_and_transforms(\n                        ruclip_model_name, pretrained=hf_path, pretrained_hf=True\n                    )\n                except Exception:\n                    # Самый глубокий фоллбек: создать модель и взять стандартные трансформы\n                    self.ruclip_model = open_clip.create_model(ruclip_model_name, pretrained=hf_path)\n                    try:\n                        _, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n                            ruclip_model_name, pretrained=None\n                        )\n                        self.ruclip_preprocess = preprocess_val\n                    except Exception:\n                        # Минимальный фоллбек на torchvision.transforms\n                        from torchvision import transforms\n                        self.ruclip_preprocess = transforms.Compose([\n                            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n                            transforms.CenterCrop(224),\n                            transforms.ToTensor(),\n                            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n                                                 std=(0.26862954, 0.26130258, 0.27577711)),\n                        ])\n            self.ruclip_tokenizer = open_clip.get_tokenizer(ruclip_model_name)\n\n        self.context_len = ruclip_context_len\n        self.text_tok = RuCLIPBatchTokenizer(self.ruclip_tokenizer, context_length=self.context_len)\n\n        self.max_images = int(max_images); self.image_agg = image_agg\n        self.max_audios = int(max_audios); self.audio_agg = audio_agg\n\n        # Аудио: только CLAP\n        self.audio_model = None\n        self.clap_processor = None\n        self.audio_sr = 48000\n        self.audio_enabled = False\n\n        if audio_cfg is not None:\n            at = str(audio_cfg.get(\"type\", \"\")).lower()\n            if at != \"clap\":\n                raise ValueError(\"audio_cfg['type'] должен быть 'clap' или None.\")\n            from transformers import ClapModel, ClapProcessor\n            ckpt = audio_cfg.get(\"checkpoint\", \"laion/clap-htsat-unfused\")\n            print(f\"Загрузка CLAP: {ckpt}\")\n            self.audio_model = ClapModel.from_pretrained(ckpt)\n            self.clap_processor = ClapProcessor.from_pretrained(ckpt)\n            sr = getattr(self.clap_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.clap_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n            self.audio_sr = int(audio_cfg.get(\"sr\", sr))\n            self.audio_enabled = True\n\n        self.ruclip_model.to(self.device).eval()\n        if isinstance(self.audio_model, nn.Module):\n            self.audio_model.to(self.device).eval()\n\n        if freeze:\n            for p in self.ruclip_model.parameters(): p.requires_grad = False\n            if isinstance(self.audio_model, nn.Module):\n                for p in self.audio_model.parameters(): p.requires_grad = False\n\n        # Размерности эмбеддингов\n        self.ruclip_dim = self._infer_ruclip_dim()\n        self.out_dim_per_modality = {\n            \"text\": self.ruclip_dim,\n            \"image\": self.ruclip_dim if self.image_agg == \"mean\" else self.ruclip_dim * self.max_images\n        }\n        if self.audio_enabled:\n            ad = getattr(getattr(self.audio_model, \"config\", None), \"projection_dim\", 512)\n            self.audio_dim = int(ad)\n            self.out_dim_per_modality[\"audio\"] = self.audio_dim if self.audio_agg == \"mean\" else self.audio_dim * self.max_audios\n        else:\n            self.audio_dim = 0\n\n    def _infer_ruclip_dim(self) -> int:\n        # Надёжное извлечение проекционной размерности текста\n        if hasattr(self.ruclip_model, \"text_projection\"):\n            proj = self.ruclip_model.text_projection\n            if hasattr(proj, 'shape'):   # nn.Parameter\n                return int(proj.shape[0])\n            if hasattr(proj, 'weight'):  # nn.Linear\n                return int(proj.weight.shape[1])\n        # Фоллбек: прогон фиктивного токена\n        with torch.no_grad():\n            # токенизатор open-clip совместим с .to(device)\n            ids = self.ruclip_tokenizer([\"test\"], context_length=self.context_len).to(self.device)\n            z = F.normalize(self.ruclip_model.encode_text(ids), dim=-1)\n        return int(z.shape[-1])\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n\n        # Текст\n        if \"text_tokens\" in batch[0]:\n            text_ids = torch.stack([b[\"text_tokens\"] for b in batch], dim=0)\n        elif \"text\" in batch[0]:\n            texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n            text_ids = self.text_tok.tokenize_batch(texts)\n        else:\n            raise ValueError(\"Ожидается модальность 'text' для RuCLIP\")\n\n        # Картинки\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, img_counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            lst = [img for img in lst if img is not None]\n            img_counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n        if len(flat_images) > 0:\n            px = torch.stack([self.ruclip_preprocess(img) for img in flat_images], dim=0)\n        else:\n            px = torch.empty(0)\n\n        # Аудио (опционально) — CLAP\n        aud_counts = None\n        audio_pack: Dict[str, Any] = {}\n        if self.audio_enabled:\n            aud_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios, aud_counts = [], []\n            for lst in aud_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_sr))\n                    elif isinstance(a, np.ndarray):\n                        aa = np.asarray(a, dtype=np.float32)\n                        if aa.ndim > 1: aa = np.squeeze(aa)\n                        if aa.ndim > 1: aa = aa.reshape(-1)\n                        flat_audios.append(aa)\n            if len(flat_audios) > 0:\n                proc = self.clap_processor(audios=flat_audios, sampling_rate=self.audio_sr, padding=True, return_tensors=\"pt\")\n                audio_pack[\"features\"] = proc[\"input_features\"]\n            else:\n                audio_pack[\"features\"] = torch.empty(0)\n\n        return {\n            \"labels\": labels,\n            \"backend_inputs\": {\n                \"text_ids\": text_ids,\n                \"pixel_values\": px,\n                \"image_counts\": torch.tensor(img_counts, dtype=torch.long),\n                \"audio\": audio_pack if self.audio_enabled else None,\n                \"audio_counts\": torch.tensor(aud_counts, dtype=torch.long) if aud_counts is not None else None,\n                \"batch_size\": len(batch),\n            }\n        }\n\n    @torch.no_grad()\n    def _aggregate(self, embs: Optional[torch.Tensor], counts: List[int], max_k: int, agg: str, dim_hint: int) -> torch.Tensor:\n        device = self.device\n        bs = len(counts)\n        if embs is None or (torch.is_tensor(embs) and embs.numel() == 0):\n            out_dim = dim_hint * max_k if agg == \"concat\" else dim_hint\n            return torch.zeros((bs, out_dim), device=device, dtype=torch.float32)\n        if embs.dim() == 1: embs = embs.unsqueeze(0)\n        if embs.dim() > 2: embs = embs.view(embs.size(0), -1)\n        N, D = embs.size()\n        out_dim = D * max_k if agg == \"concat\" else D\n        out = torch.zeros((bs, out_dim), device=device, dtype=embs.dtype)\n        off = 0\n        for i, c in enumerate(counts):\n            if c <= 0 or off >= N: continue\n            take_n = min(c, N - off)\n            sample = embs[off:off+take_n]; off += take_n\n            if agg == \"concat\":\n                take = sample[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n        return F.normalize(out, dim=-1, eps=1e-12) if out.size(1) > 0 else out\n\n    @torch.no_grad()\n    def encode(self, backend_inputs: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n        # Текст\n        text_ids = backend_inputs[\"text_ids\"].to(self.device)\n        zt = F.normalize(self.ruclip_model.encode_text(text_ids), dim=-1)\n\n        # Картинки\n        px = backend_inputs[\"pixel_values\"]\n        img_counts = backend_inputs[\"image_counts\"].tolist()\n        total_imgs = sum(img_counts)\n        zi_flat = None\n        if isinstance(px, torch.Tensor) and px.numel() > 0 and total_imgs > 0:\n            px = px.to(self.device)\n            if px.size(0) > total_imgs:\n                px = px[:total_imgs]\n            zi_flat = F.normalize(self.ruclip_model.encode_image(px), dim=-1)\n        zi = self._aggregate(zi_flat, img_counts, self.max_images, self.image_agg, self.ruclip_dim)\n\n        out = {\"text\": zt, \"image\": zi}\n\n        # Аудио (опционально) — CLAP\n        if self.audio_enabled:\n            ac = backend_inputs[\"audio_counts\"].tolist() if backend_inputs[\"audio_counts\"] is not None else [0]*zt.size(0)\n            total_a = sum(ac)\n            za_flat = None\n            audio_pack = backend_inputs[\"audio\"] or {}\n            feats = audio_pack.get(\"features\", None)\n            if feats is not None and isinstance(feats, torch.Tensor) and feats.numel() > 0 and total_a > 0:\n                feats = feats.to(self.device)\n                if feats.size(0) > total_a:\n                    feats = feats[:total_a]\n                z = self.audio_model.get_audio_features(input_features=feats.float())\n                za_flat = F.normalize(z.float(), dim=-1)\n                self.audio_dim = int(za_flat.size(1))\n            za = self._aggregate(za_flat, ac, self.max_audios, self.audio_agg, self.audio_dim)\n            out[\"audio\"] = za\n\n        return out\n\n    def get_out_dim(self, modality: str) -> int:\n        return self.out_dim_per_modality.get(modality, 0)\n\n    def get_text_tokenizer(self) -> RuCLIPBatchTokenizer:\n        return self.text_tok\n\n\n# =========================\n# Классификатор\n# =========================\n\nclass SingleBackboneClassifier(nn.Module):\n    def __init__(self, backend: RuCLIPBackend, modalities: List[str], num_labels: int,\n                 fusion: str = \"concat\", hidden: int = 512, dropout: float = 0.1):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError('Для fusion=\"mean\" размеры модальностей должны совпадать')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() > 2: t = t.view(t.size(0), -1)\n                feats.append(t)\n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        else:\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        z = self.backend.encode(backend_inputs)\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        z = self.backend.encode(backend_inputs)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\n# =========================\n# Trainer с весами классов и прогресс\n# =========================\n\nclass WeightedCETrainer(Trainer):\n    def __init__(self, *args, num_labels=None, train_labels=None, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        if class_weights is not None:\n            self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32)\n        elif train_labels is not None and num_labels is not None:\n            y = np.asarray(train_labels).astype(int)\n            counts = np.bincount(y, minlength=num_labels)\n            n = counts.sum(); w = np.zeros(num_labels, dtype=np.float32)\n            nz = counts > 0; w[nz] = n / (num_labels * counts[nz].astype(np.float32))\n            self.class_weights = torch.tensor(w, dtype=torch.float32)\n        else:\n            self.class_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = F.cross_entropy(logits, labels.long(), weight=weight)\n        return (loss, outputs) if return_outputs else loss\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar: tqdm):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(getattr(state, \"global_step\", 0) or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in ('eval_loss','eval_runtime','eval_samples_per_second','eval_steps_per_second','epoch'):\n                parts.append(f\"{k.replace('eval_','')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_train_begin(self, args, state, control, **kwargs):\n        max_steps = int(getattr(state, \"max_steps\", 0) or 0)\n        if max_steps > 0:\n            self.pbar.reset(total=max_steps)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs: return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        # синхронизируем прогресс\n        self.on_step_end(args, state, control)\n        # печать строк валидации\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step not in self.printed_eval_steps:\n                self.printed_eval_steps.add(step)\n                excl = {'eval_loss','eval_runtime','eval_samples_per_second','eval_steps_per_second','epoch'}\n                extra = [f\"{k.replace('eval_','')}: {float(v):.6f}\" for k,v in logs.items() if k.startswith('eval_') and k not in excl]\n                from tqdm.auto import tqdm as _tqdm\n                _tqdm.write(f\"step {step} | \" + \", \".join(extra))\n\n    def on_step_end(self, args, state, control, **kwargs):\n        g = self._step(state)\n        if self.pbar.total:\n            n = min(g, self.pbar.total)\n        else:\n            n = g\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_train_end(self, args, state, control, **kwargs):\n        try:\n            g = self._step(state)\n            if self.pbar.total and g > self.pbar.n:\n                self.pbar.update(g - self.pbar.n)\n        finally:\n            self.pbar.close()\n\n\n# =========================\n# Пайплайн: RuCLIP классификация\n# =========================\n\nclass RuCLIPMultiModalClassification:\n    \"\"\"\n    Классификация с модальностями:\n      - ['text','image'] обязательно через RuCLIP\n      - ['text','image','audio'] + звук только через CLAP\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],                    # ['text','image'] или ['text','image','audio']\n        num_labels: int,\n        target_column_name: str,\n        text_columns: List[str],\n        image_columns: List[str],\n        audio_columns: Optional[List[str]] = None,\n        ruclip_model_name: str = \"ViT-B-32\",\n        ruclip_pretrained: Optional[str] = \"hf-hub:ai-forever/ru-clip-vit-base-patch32-224\",\n        ruclip_context_len: int = 77,\n        max_images_per_sample: int = 1,\n        image_agg: str = \"concat\",\n        audio_cfg: Optional[Dict[str, Any]] = None,    # {'type':'clap','checkpoint':..., 'sr':48000} или None\n        max_audios_per_sample: int = 1,\n        audio_agg: str = \"concat\",\n        freeze_backbone: bool = True\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        assert set(self.modalities).issuperset({\"text\",\"image\"}) and set(self.modalities).issubset({\"text\",\"image\",\"audio\"}), \\\n            \"Поддерживаются только ['text','image'] или ['text','image','audio']\"\n\n        self.num_labels = num_labels\n        self.target_column_name = target_column_name\n        self.text_columns = text_columns\n        self.image_columns = image_columns\n        self.audio_columns = audio_columns or []\n\n        self.backend = RuCLIPBackend(\n            ruclip_model_name=ruclip_model_name,\n            ruclip_pretrained=ruclip_pretrained,\n            ruclip_context_len=ruclip_context_len,\n            max_images=max_images_per_sample,\n            image_agg=image_agg,\n            max_audios=max_audios_per_sample,\n            audio_agg=audio_agg,\n            audio_cfg=audio_cfg,  # только CLAP\n            freeze=freeze_backbone\n        )\n\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n\n    def _validate_data(self, df: pd.DataFrame):\n        for c in self.text_columns:\n            if c not in df.columns:\n                raise ValueError(f\"Нет текстовой колонки '{c}' в DataFrame\")\n        for c in self.image_columns:\n            if c not in df.columns:\n                raise ValueError(f\"Нет колонки изображений '{c}' в DataFrame\")\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали 'audio', но audio_columns пуст\")\n            for c in self.audio_columns:\n                if c not in df.columns:\n                    raise ValueError(f\"Нет аудио колонки '{c}' в DataFrame\")\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _setup_metrics(self, metric_name: str):\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            metric = evaluate.load(\"f1\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids, average=\"weighted\")\n        elif metric_name == \"accuracy\":\n            metric = evaluate.load(\"accuracy\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids)\n        else:\n            raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n        self.compute_metrics = compute\n\n    def _make_dataset(self, df: pd.DataFrame, pretokenize_text: bool) -> MultiComboDataset:\n        return MultiComboDataset(\n            df=df,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=(self.audio_columns if \"audio\" in self.modalities else None),\n            text_batch_tokenizer=self.backend.get_text_tokenizer(),\n            pretokenize_text=pretokenize_text,\n            ruclip_context_len=self.backend.context_len,\n            audio_sr=self.backend.audio_sr\n        )\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"f1\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1,\n        fusion: str = \"concat\",\n        early_stopping_patience: Optional[int] = 3,\n        early_stopping_threshold: float = 0.0\n    ):\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        classes = sorted(train_data[self.target_column_name].unique().tolist())\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n\n        if test_data is None:\n            df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n        else:\n            df_train, df_eval = train_data, test_data\n\n        y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n        counts = np.bincount(y_train_all, minlength=self.num_labels)\n        n_all = counts.sum(); class_weights = np.zeros(self.num_labels, dtype=np.float32)\n        nz = counts > 0; class_weights[nz] = n_all / (self.num_labels * counts[nz].astype(np.float32))\n\n        ds_eval = self._make_dataset(df_eval, pretokenize_text=True)\n        ds_train = self._make_dataset(df_train, pretokenize_text=True)\n\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        self._setup_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def data_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            num_labels=self.num_labels,\n            train_labels=y_train_all,\n            class_weights=class_weights\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        if (ds_eval is not None) and (early_stopping_patience is not None) and (early_stopping_patience > 0):\n            esc = EarlyStoppingCallback(\n                early_stopping_patience=int(early_stopping_patience),\n                early_stopping_threshold=float(early_stopping_threshold)\n            )\n            self.trainer.add_callback(esc)\n\n        # Улучшенный прогресс-бар (точный total подтянется при старте обучения)\n        pbar = tqdm(total=0, desc=\"Training Progress\", unit=\"step\", leave=False, dynamic_ncols=True)\n        self.trainer.add_callback(PbarConsoleLogger(pbar))\n        try:\n            self.trainer.train()\n        finally:\n            try: pbar.close()\n            except Exception: pass\n\n        self.backend.get_text_tokenizer().clear_cache()\n        return self\n\n    def predict(self, df: pd.DataFrame, return_label_str: bool = False, return_proba: bool = False, batch_size: Optional[int] = None) -> np.ndarray:\n        if self.trainer is None: raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n        ds = self._make_dataset(df_c, pretokenize_text=len(df_c) < 10000)\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n        self.trainer.args.disable_tqdm = False\n        preds = self.trainer.predict(test_dataset=ds)\n        self.trainer.args.disable_tqdm = True\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        if return_proba:\n            logits = preds.predictions\n            exp = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n            return exp / np.sum(exp, axis=1, keepdims=True)\n\n        y_pred = np.argmax(preds.predictions, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n        ds = self._make_dataset(df_c, pretokenize_text=False)\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list, per_mod_lists = [], ({m: [] for m in self.modalities} if return_per_modality else None)\n\n        print(f\"Extracting embeddings (batch_size={batch_size})...\")\n        device = next(self.model.parameters()).device\n        self.model.eval()\n        with torch.no_grad():\n            for batch in tqdm(loader, unit=\"batch\"):\n                bi = batch[\"backend_inputs\"]\n                def move_to_device(obj):\n                    if torch.is_tensor(obj): return obj.to(device)\n                    if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n                    return obj\n                bi = move_to_device(bi)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nset_seed(123)\n\n# 1) Синтетические данные: по 2 картинки и 2 аудио на объект\nN = 12\ndf = pd.DataFrame({\n    \"label\": [\"спорт\", \"еда\", \"техника\"] * (N // 3) + ([\"спорт\"] * (N % 3)),\n    \"title\": [f\"заголовок {i}\" for i in range(N)],\n    \"desc\":  [f\"описание {i}\" for i in range(N)],\n})\n\ndef mk_img():\n    return np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n\ndef mk_wav(sr=48000, dur_s=0.2):\n    t = np.linspace(0, dur_s, int(sr*dur_s), endpoint=False, dtype=np.float32)\n    return (0.1*np.sin(2*np.pi*440*t)).astype(np.float32)\n\ndf[\"image_path\"] = [[mk_img(), mk_img()] for _ in range(N)]     # по 2 изображения\ndf[\"audio_path\"] = [[mk_wav(), mk_wav()] for _ in range(N)]     # по 2 аудио\n\n# 2) Инициализация пайплайна: RuCLIP из HF Hub, аудио через CLAP\npipeline = RuCLIPMultiModalClassification(\n    modalities=[\"text\",\"image\",\"audio\"],\n    num_labels=3,\n    target_column_name=\"label\",\n    text_columns=[\"title\",\"desc\"],\n    image_columns=[\"image_path\"],\n    audio_columns=[\"audio_path\"],\n\n    ruclip_model_name=\"ViT-B-32\",\n    ruclip_pretrained=\"hf-hub:ai-forever/ru-clip-vit-base-patch32-224\",  # скачает веса RuCLIP\n    ruclip_context_len=77,\n\n    max_images_per_sample=2,            # по 2 изображения → image_agg применится к 2 признакам\n    image_agg=\"concat\",                 # concat или mean\n\n    audio_cfg={\n        \"type\": \"clap\",\n        \"checkpoint\": \"laion/clap-htsat-unfused\",\n        \"sr\":48000\n    },  # скачает CLAP (~600MB)\n    max_audios_per_sample=2,\n    audio_agg=\"mean\",                   # усредним 2 аудиофичи\n\n    freeze_backbone=True                # фиксируем RuCLIP/CLAP, обучаем только голову\n)\n\n# 3) Обучение\npipeline.fit(\n    train_data=df,\n    epochs=2,\n    test_size=0.25,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    learning_rate=3e-4,\n    metric_name=\"f1\",\n    fp16=True,                          # если CUDA доступна — быстрее\n    logging_steps=5,\n    eval_steps=10,\n    output_dir=\"./result_ex1\",\n    seed=123,\n    hidden=768,                         # размер скрытого слоя головы\n    dropout=0.2,\n    fusion=\"concat\",                    # «безопасный» режим при разных размерностях модальностей\n    early_stopping_patience=2,\n    early_stopping_threshold=0.0\n)\n\n# 4) Предсказания (вернём вероятности классов)\nproba = pipeline.predict(df.iloc[:5], return_proba=True)\nprint(\"proba shape:\", proba.shape)      # (5, 3)\n\n# 5) Эмбеддинги: склеенные и по модальностям\nfused, per = pipeline.get_embeddings(df.iloc[:5], batch_size=2, return_per_modality=True)\nprint(\"fused:\", fused.shape)\nfor k,v in per.items():\n    print(f\"{k}: {v.shape}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nset_seed(7)\n\n# 1) Игрушечные данные: по 2 изображения, без аудио\nN = 10\ndf = pd.DataFrame({\n    \"label\": [\"A\",\"B\"] * (N//2) + ([\"A\"] if N%2 else []),\n    \"title\": [f\"title {i}\" for i in range(N)],\n    \"desc\":  [f\"desc {i}\" for i in range(N)],\n})\ndf[\"image_path\"] = [[np.zeros((224,224,3), dtype=np.uint8), np.ones((224,224,3), dtype=np.uint8)*255] for _ in range(N)]\n\n# 2) Инициализация: ruclip_pretrained=None (не скачиваем веса), макс. картинок=2\npipeline = RuCLIPMultiModalClassification(\n    modalities=[\"text\",\"image\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"title\",\"desc\"],\n    image_columns=[\"image_path\"],\n\n    ruclip_model_name=\"ViT-B-32\",\n    ruclip_pretrained=None,             # офлайн-режим (случайные веса RuCLIP)\n    ruclip_context_len=77,\n\n    max_images_per_sample=2,\n    image_agg=\"mean\",                   # усреднение 2 картинок\n    freeze_backbone=True\n)\n\n# 3) Обучение с «богатыми» параметрами\npipeline.fit(\n    train_data=df,\n    epochs=3,\n    test_size=0.3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-4,\n    metric_name=\"accuracy\",\n    fp16=False,                         # с случайными весами можно и на CPU\n    logging_steps=2,\n    eval_steps=4,\n    output_dir=\"./result_ex2\",\n    seed=7,\n    hidden=256,\n    dropout=0.1,\n    fusion=\"concat\",\n    early_stopping_patience=1\n)\n\n# 4) Предсказания (строковые метки), изменим batch_size на лету\ny_str = pipeline.predict(df.iloc[:6], return_label_str=True, batch_size=3)\nprint(\"pred labels:\", y_str.tolist())\n\n# 5) Только склеенные эмбеддинги (без разбиения по модальностям)\nfused = pipeline.get_embeddings(df.iloc[:6], batch_size=3, return_per_modality=False)\nprint(\"fused emb:\", fused.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 3.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nset_seed(1)\n\n# 1) Минимальные данные: одно изображение/объект, без аудио\nN = 6\ndf = pd.DataFrame({\n    \"label\": [\"кошки\",\"собаки\"] * (N//2) + ([\"кошки\"] if N%2 else []),\n    \"title\": [f\"пример {i}\" for i in range(N)],\n})\n# Можно не давать desc — оставим только 'title'\ndf[\"image_path\"] = [np.random.randint(0,255,(224,224,3),dtype=np.uint8) for _ in range(N)]\n\n# 2) Инициализация с минимальной настройкой\npipeline = RuCLIPMultiModalClassification(\n    modalities=[\"text\",\"image\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"title\"],\n    image_columns=[\"image_path\"],\n    # Если не хотите скачивать RuCLIP — установите None\n    ruclip_model_name=\"ViT-B-32\",\n    ruclip_pretrained=None,\n    freeze_backbone=True\n)\n\n# 3) Короткое обучение и предсказание\npipeline.fit(df, epochs=1, test_size=0.33, per_device_train_batch_size=4, metric_name=\"accuracy\", fp16=False)\npred = pipeline.predict(df.iloc[:3], return_label_str=True)\nprint(\"pred:\", pred.tolist())","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}