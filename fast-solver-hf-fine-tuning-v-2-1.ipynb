{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Дообучение encoder'а для классификации токенов.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"# Установка зависимостей (при необходимости)\n!pip install -q evaluate seqeval transformers datasets\n\nimport gc\nimport math\nfrom typing import List, Dict, Any, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport evaluate\nfrom transformers import (\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForTokenClassification,\n    TrainerCallback,\n    PrinterCallback,\n)\nfrom transformers.modeling_outputs import ModelOutput\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Устанавливает фиксированное зерно для Python, NumPy и PyTorch (включая все доступные CUDA-устройства).\n\n    :param seed: Значение зерна.\n    \"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    \"\"\"\n    Внешний прогресс‑бар и консольный логгер метрик/лоссов для стабильного отображения на больших данных.\n    \"\"\"\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                try:\n                    parts.append(f\"{k.replace('eval_', '')} {float(v):.4f}\")\n                except Exception:\n                    pass\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    try:\n                        extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n                    except Exception:\n                        pass\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\nclass WeightedTokenCETrainer(Trainer):\n    \"\"\"\n    Кастомный Trainer для токен-классификации с взвешенной CrossEntropy.\n    Веса считаются с учетом числа токенов класса (после разметки и чанкинга):\n      weight_i = N / (K * n_i),\n    где K — число классов, n_i — число токенов класса i, N — сумма всех n_i.\n    Отсутствующие классы получают вес 0. Метки -100 игнорируются в потере.\n\n    Поддержка DataParallel: если logits дублируются по числу GPU, метки тайлятся соответствующим образом.\n    \"\"\"\n    def __init__(self, *args, class_weights=None, **kwargs):\n        # Тихо переводим устаревший аргумент tokenizer в processing_class (совместимость)\n        processing = kwargs.pop(\"tokenizer\", None)\n        if processing is not None and \"processing_class\" not in kwargs:\n            kwargs[\"processing_class\"] = processing\n\n        super().__init__(*args, **kwargs)\n\n        self.class_weights = None\n        if class_weights is not None:\n            self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32)\n        self._warned_label_tiling = False\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Считает взвешенный CrossEntropyLoss по токенам, игнорируя -100.\n        Корректно обрабатывает случай DataParallel (дублирование по batch-оси).\n        \"\"\"\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]  # [B, L, C]\n\n        # Приводим размеры в соответствие при DataParallel (если нужно)\n        if logits.size(0) != labels.size(0):\n            ngpu = torch.cuda.device_count()\n            if ngpu > 1 and logits.size(0) == labels.size(0) * ngpu:\n                labels = labels.repeat_interleave(ngpu, dim=0)\n                if not self._warned_label_tiling:\n                    print(f\"[Warning] DataParallel удвоил batch для logits. \"\n                          f\"Повторяем labels x{ngpu}. logits: {tuple(logits.shape)}, labels: {tuple(labels.shape)}\")\n                    self._warned_label_tiling = True\n            else:\n                raise ValueError(f\"Batch size mismatch: logits {tuple(logits.shape)} vs labels {tuple(labels.shape)}\")\n\n        # Взвешенный CE с ignore_index=-100\n        loss_fct = torch.nn.CrossEntropyLoss(\n            weight=(self.class_weights.to(logits.device) if self.class_weights is not None else None),\n            ignore_index=-100,\n        )\n        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n\nclass TokenClassification:\n    \"\"\"\n    Пайплайн токен‑классификации с поддержкой больших данных (чанковое обучение по документам и sliding window).\n\n    Возможности:\n      - Автоматическая разметка (sliding window) с перекрытием stride.\n      - Взвешенная CrossEntropy по токенам (по всему train) с игнорированием -100.\n      - Чанковое обучение: подстановка train_dataset кусками документов (fit_chunk_size_docs).\n      - Внешний tqdm прогресс‑бар (стабильный) + консольный лог метрик.\n\n    Необходимые импорты:\n    import numpy as np\n    import torch\n    import evaluate\n    from transformers import (\n        AutoModelForTokenClassification,\n        AutoTokenizer,\n        Trainer,\n        TrainingArguments,\n        DataCollatorForTokenClassification,\n    )\n    import pandas as pd\n    from datasets import Dataset\n    from sklearn.model_selection import train_test_split\n    from tqdm.auto import tqdm\n    \"\"\"\n    def __init__(\n        self,\n        checkpoint: str,\n        label2id: Dict[str, int],\n        tokens_column_name: str,\n        tags_column_name: str\n    ):\n        \"\"\"\n        Инициализация модели и токенизатора.\n\n        :param checkpoint: Имя/путь чекпоинта (HF Hub).\n        :param label2id: Отображение тегов в id.\n        :param tokens_column_name: Имя колонки с токенами (список строк).\n        :param tags_column_name: Имя колонки с метками (список тегов или id).\n        \"\"\"\n        self.id2label = {v: k for k, v in label2id.items()}\n        self.label2id = label2id\n\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            checkpoint,\n            num_labels=len(self.id2label),\n            id2label=self.id2label,\n            label2id=self.label2id,\n            ignore_mismatched_sizes=True\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.tokens_column_name = tokens_column_name\n        self.tags_column_name = tags_column_name\n\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.data_collator = DataCollatorForTokenClassification(tokenizer=self.tokenizer)\n        self.progress_callback: Optional[TrainerCallback] = None\n\n    @staticmethod\n    def _align_labels_with_word_ids(labels_ids: List[int], word_ids: List[Optional[int]]) -> List[int]:\n        \"\"\"\n        Выравнивает метки по word_ids от токенизатора: на первый токен слова ставится метка,\n        остальные токены слова получают -100. Спец‑токены (None) получают -100.\n\n        :param labels_ids: Список id‑меток длины = числу слов в документе.\n        :param word_ids: Список индексов слов длины = числу токенов в чанке (или None для спец‑токенов).\n        :return: Список меток длины = числу токенов в чанке.\n        \"\"\"\n        new_labels = []\n        prev_word_id = None\n        for wid in word_ids:\n            if wid is None:\n                new_labels.append(-100)\n            else:\n                if wid != prev_word_id:\n                    new_labels.append(labels_ids[wid])\n                else:\n                    new_labels.append(-100)\n            prev_word_id = wid\n        return new_labels\n\n    def _tokenize_and_align_chunk(\n        self,\n        docs_tokens: List[List[str]],\n        docs_labels_ids: List[List[int]],\n        max_length: int,\n        stride: int\n    ) -> Dataset:\n        \"\"\"\n        Токенизирует список документов (списки токенов) с возвратом переполнений (sliding window),\n        выравнивает метки по word_ids, формирует Dataset для обучения/валидации.\n\n        :param docs_tokens: Список документов; каждый документ — список токенов (слов).\n        :param docs_labels_ids: Список документов; каждый — список id‑меток по словам.\n        :param max_length: Максимальная длина последовательности модели.\n        :param stride: Перекрытие при нарезке (число токенов).\n        :return: datasets.Dataset с полями input_ids, attention_mask, labels.\n        \"\"\"\n        enc = self.tokenizer(\n            docs_tokens,\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True\n        )\n        mapping = enc.pop(\"overflow_to_sample_mapping\")  # len = кол-во получившихся чанков\n        num_chunks = len(enc[\"input_ids\"])\n\n        all_labels = []\n        for i in range(num_chunks):\n            doc_idx = int(mapping[i])\n            word_ids = enc.word_ids(batch_index=i)\n            aligned = self._align_labels_with_word_ids(docs_labels_ids[doc_idx], word_ids)\n            all_labels.append(aligned)\n\n        return Dataset.from_dict({\n            \"input_ids\": enc[\"input_ids\"],\n            \"attention_mask\": enc[\"attention_mask\"],\n            \"labels\": all_labels\n        })\n\n    def _count_total_chunks(\n        self,\n        docs_tokens: List[List[str]],\n        max_length: int,\n        stride: int,\n        batch_docs: int = 64\n    ) -> int:\n        \"\"\"\n        Оценивает общее число чанков (последовательностей) после sliding window\n        для списка документов, без хранения признаков.\n\n        :param docs_tokens: Список документов (список токенов-слов).\n        :param max_length: Максимальная длина последовательности модели.\n        :param stride: Перекрытие при нарезке.\n        :param batch_docs: Размер пачки документов для ускорения токенизации.\n        :return: Общее число последовательностей.\n        \"\"\"\n        total = 0\n        for i in range(0, len(docs_tokens), batch_docs):\n            batch = docs_tokens[i:i + batch_docs]\n            enc = self.tokenizer(\n                batch,\n                is_split_into_words=True,\n                return_overflowing_tokens=True,\n                max_length=max_length,\n                stride=stride,\n                truncation=True\n            )\n            total += len(enc[\"input_ids\"])\n        return total\n\n    def _compute_class_weights_over_docs(\n        self,\n        docs_tokens: List[List[str]],\n        docs_labels_ids: List[List[int]],\n        max_length: int,\n        stride: int,\n        batch_docs: int = 32\n    ) -> np.ndarray:\n        \"\"\"\n        Считает веса классов по ВСЕМ токенам тренировки, выравнивая метки и\n        не сохраняя полный датасет в памяти.\n\n        :param docs_tokens: Список документов: список токенов (слов).\n        :param docs_labels_ids: Список документов: список id‑меток по словам.\n        :param max_length: Максимальная длина последовательности.\n        :param stride: Перекрытие при нарезке (в токенах).\n        :param batch_docs: Размер батча документов при токенизации.\n        :return: Вектор весов классов формы [K].\n        \"\"\"\n        num_labels = len(self.id2label)\n        counts = np.zeros(num_labels, dtype=np.int64)\n\n        for i in tqdm(range(0, len(docs_tokens), batch_docs), desc=\"Подсчет частот классов (token-level)\"):\n            toks = docs_tokens[i:i + batch_docs]\n            labs = docs_labels_ids[i:i + batch_docs]\n\n            enc = self.tokenizer(\n                toks,\n                is_split_into_words=True,\n                return_overflowing_tokens=True,\n                max_length=max_length,\n                stride=stride,\n                truncation=True\n            )\n            mapping = enc.pop(\"overflow_to_sample_mapping\")\n            num_chunks = len(enc[\"input_ids\"])\n\n            for j in range(num_chunks):\n                doc_idx = int(mapping[j])\n                word_ids = enc.word_ids(batch_index=j)\n                aligned = self._align_labels_with_word_ids(labs[doc_idx], word_ids)\n                arr = np.asarray(aligned, dtype=np.int64)\n                arr = arr[arr >= 0]  # игнорируем -100\n                if arr.size > 0:\n                    bc = np.bincount(arr, minlength=num_labels)\n                    counts += bc\n\n        N = counts.sum()\n        weights = np.zeros(num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        if N > 0:\n            weights[nonzero] = N / (num_labels * counts[nonzero].astype(np.float32))\n        return weights\n\n    def _setup_compute_metrics(self):\n        \"\"\"\n        Создает функцию подсчета seqeval-метрик (precision/recall/f1/accuracy).\n        \"\"\"\n        metric = evaluate.load(\"seqeval\")\n\n        def compute_seqeval_metrics(p):\n            # Поддержка EvalPrediction и tuple\n            if isinstance(p, (tuple, list)):\n                predictions, labels = p\n            else:\n                predictions, labels = p.predictions, p.label_ids\n\n            predictions = np.argmax(predictions, axis=2)\n\n            true_predictions = [\n                [self.id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n\n            true_labels = [\n                [self.id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n\n            results = metric.compute(predictions=true_predictions, references=true_labels)\n            return {\n                \"precision\": results.get(\"overall_precision\", 0.0),\n                \"recall\": results.get(\"overall_recall\", 0.0),\n                \"f1\": results.get(\"overall_f1\", 0.0),\n                \"accuracy\": results.get(\"overall_accuracy\", 0.0),\n            }\n\n        self.compute_metrics = compute_seqeval_metrics\n\n    def _prepare_dataset_with_sliding_window(self, df: pd.DataFrame, max_length: int, stride: int) -> Dataset:\n        \"\"\"\n        Готовит токенизированный Dataset для списка документов (используется, например, для валидации).\n        Выполняет:\n          - маппинг строковых тегов -> id при необходимости,\n          - токенизацию с переполнениями,\n          - выравнивание меток по word_ids.\n\n        :param df: Датафрейм с колонками токенов и меток.\n        :param max_length: Максимальная длина модели.\n        :param stride: Перекрытие sliding window.\n        :return: datasets.Dataset с полями input_ids, attention_mask, labels.\n        \"\"\"\n        docs_tokens = df[self.tokens_column_name].tolist()\n        docs_labels = df[self.tags_column_name].tolist()\n\n        # Если метки строковые — переводим в id\n        if len(docs_labels) and isinstance(docs_labels[0][0], str):\n            docs_labels = [[self.label2id[tag] for tag in tags] for tags in docs_labels]\n\n        return self._tokenize_and_align_chunk(docs_tokens, docs_labels, max_length, stride)\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        test_size: float = 0.2,\n        learning_rate: float = 2e-5,\n        fp16: bool = True,\n        stride: int = 128,\n        logging_steps: int = 50,\n        eval_steps: int = 100,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        fit_chunk_size_docs: Optional[int] = None\n    ):\n        \"\"\"\n        Обучает модель токен‑классификации с поддержкой больших данных:\n        train_dataset подставляется чанками документов, внутри которых выполняется sliding window.\n\n        :param train_data: Датафрейм: колонки с токенами и метками на уровне слов.\n        :param epochs: Кол-во эпох.\n        :param per_device_train_batch_size: Размер батча на устройство.\n        :param gradient_accumulation_steps: Шаги аккумуляции градиентов.\n        :param test_size: Доля валидации (по документам).\n        :param learning_rate: LR для AdamW.\n        :param fp16: Использовать fp16 (если bf16 не доступен).\n        :param stride: Перекрытие при нарезке (в токенах).\n        :param logging_steps: Частота логирования.\n        :param eval_steps: Частота валидации/сохранения.\n        :param output_dir: Папка для артефактов.\n        :param seed: Зерно.\n        :param fit_chunk_size_docs: Сколько документов подставлять в один тренировочный чанк. Если None — все.\n        :return: self.\n        \"\"\"\n        set_seed(seed)\n        max_length = int(getattr(self.model.config, \"max_position_embeddings\", 512))\n\n        # Маппинг строковых тегов -> id (все данные)\n        df_all = train_data.copy()\n        if len(df_all) and len(df_all[self.tags_column_name]) and isinstance(df_all[self.tags_column_name].iloc[0][0], str):\n            df_all[self.tags_column_name] = df_all[self.tags_column_name].apply(\n                lambda tags: [self.label2id[tag] for tag in tags]\n            )\n\n        # Сплит по документам (потом каждый набор будет чанковаться по sliding window)\n        df_train, df_eval = train_test_split(df_all, test_size=test_size, random_state=seed, shuffle=True)\n\n        # Eval датасет можно подготовить целиком (обычно компактнее train)\n        eval_dataset = self._prepare_dataset_with_sliding_window(df_eval, max_length, stride)\n\n        # Документы тренировки (списки токенов и меток)\n        train_docs_tokens = df_train[self.tokens_column_name].tolist()\n        train_docs_labels = df_train[self.tags_column_name].tolist()\n\n        # Веса классов на всём train (по токенам, без хранения полного tokenized train)\n        class_weights = self._compute_class_weights_over_docs(\n            docs_tokens=train_docs_tokens,\n            docs_labels_ids=train_docs_labels,\n            max_length=max_length,\n            stride=stride,\n            batch_docs=32\n        )\n\n        # Настройка метрик\n        self._setup_compute_metrics()\n\n        # bf16 если доступен, иначе fp16 при флаге\n        bf16_ok = bool(torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"f1\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=bool(fp16 and torch.cuda.is_available() and not bf16_ok),\n            bf16=bool(bf16_ok and not fp16),\n            dataloader_num_workers=0,\n            seed=seed,\n            remove_unused_columns=False,\n            disable_tqdm=True  # используем внешний tqdm\n        )\n\n        data_collator = self.data_collator\n\n        # Вспомогательные: шаги по количеству сэмплов (последовательностей), а не документов\n        def steps_for_size(n_samples: int, bsz: int, accum: int) -> int:\n            \"\"\"\n            Оценивает число оптимизационных шагов на чанке из n_samples последовательностей.\n            \"\"\"\n            return max(0, math.ceil(math.ceil(n_samples / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(n_docs: int, chunk_docs: int):\n            \"\"\"\n            Генератор срезов индексов документов по chunk_docs.\n            \"\"\"\n            for i in range(0, n_docs, chunk_docs):\n                yield slice(i, min(i + chunk_docs, n_docs))\n\n        # Объем чанка по документам (по умолчанию — все документы)\n        n_docs = len(train_docs_tokens)\n        chunk_docs = int(fit_chunk_size_docs) if (fit_chunk_size_docs and fit_chunk_size_docs > 0) else n_docs\n\n        # Предварительный расчет total_steps (по числу последовательностей после токенизации)\n        total_steps = 0\n        rng = np.random.default_rng(seed)\n        doc_indices = np.arange(n_docs)\n\n        for _ in range(epochs):\n            rng.shuffle(doc_indices)\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = doc_indices[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                # считаем, сколько получится последовательностей в этом чанке документов\n                n_samples = self._count_total_chunks(toks_chunk, max_length, stride, batch_docs=64)\n                total_steps += steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n\n        # Инициализируем Trainer с \"минимальным\" train_dataset (пустой/минимальный чанк)\n        # чтобы не держать всю тренировочную выборку\n        if n_docs > 0:\n            init_chunk_ds = self._tokenize_and_align_chunk(\n                [train_docs_tokens[0]], [train_docs_labels[0]], max_length, stride\n            )\n        else:\n            init_chunk_ds = eval_dataset  # fallback\n\n        self.trainer = WeightedTokenCETrainer(\n            model=self.model,\n            args=args,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            train_dataset=init_chunk_ds,\n            eval_dataset=eval_dataset,\n            tokenizer=self.tokenizer,\n            class_weights=class_weights\n        )\n        # Удаляем стандартный принтер логов, чтобы не конфликтовал с внешним tqdm\n        try:\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n\n        # Планировщик под рассчитанное число шагов\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        # Внешний прогресс-бар\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        # Основной цикл обучения по эпохам/чанкам документов\n        steps_done = 0\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            order = np.arange(n_docs)\n            rng.shuffle(order)\n\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = order[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                labs_chunk = [train_docs_labels[i] for i in idx]\n\n                # Готовим датасет последовательностей для этого чанка документов\n                ds_chunk = self._tokenize_and_align_chunk(toks_chunk, labs_chunk, max_length, stride)\n                self.trainer.train_dataset = ds_chunk\n\n                # Шаги на текущем чанке\n                n_samples = len(ds_chunk)  # число последовательностей после sliding window\n                chunk_steps = steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk\n                    continue\n\n                # Дообучаем до steps_done + chunk_steps\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                # Очистка памяти\n                del ds_chunk\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n        return self\n\n    def _predict_single_document(self, tokens: List[str], stride: int) -> List[str]:\n        \"\"\"\n        Предсказывает метки на уровне слов для одного документа с помощью sliding window.\n\n        :param tokens: Список токенов (слов) документа.\n        :param stride: Перекрытие при нарезке.\n        :return: Список предсказанных тегов (строки) длиной = числу слов.\n        \"\"\"\n        max_length = int(getattr(self.model.config, \"max_position_embeddings\", 512))\n\n        tokenized_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n        )\n        tokenized_inputs.pop(\"overflow_to_sample_mapping\", None)\n        chunk_dataset = Dataset.from_dict(tokenized_inputs)\n\n        outputs = self.trainer.predict(chunk_dataset)\n        predictions = np.argmax(outputs.predictions, axis=2)\n\n        num_original_words = len(tokens)\n        final_predictions = np.full(num_original_words, -1, dtype=np.int32)\n\n        for i, chunk_preds in enumerate(predictions):\n            chunk_word_ids = tokenized_inputs.word_ids(batch_index=i)\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if word_id is not None and final_predictions[word_id] == -1:\n                    final_predictions[word_id] = int(chunk_preds[token_pos])\n\n        return [self.id2label.get(pid, 'O') for pid in final_predictions]\n\n    def predict(self, df: pd.DataFrame, stride: int = 128) -> List[List[str]]:\n        \"\"\"\n        Делает предсказания на уровне слов для набора документов (через sliding window).\n\n        :param df: Датафрейм с колонкой токенов (список слов).\n        :param stride: Перекрытие при нарезке.\n        :return: Список предсказанных последовательностей тегов по документам.\n        \"\"\"\n        all_final_labels = []\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Предсказание (sliding window)\"):\n            original_tokens = row[self.tokens_column_name]\n            if not original_tokens:\n                all_final_labels.append([])\n                continue\n            document_labels = self._predict_single_document(original_tokens, stride)\n            all_final_labels.append(document_labels)\n        return all_final_labels\n\n    def _get_embeddings_single_document(self, tokens: List[str], stride: int, device: torch.device) -> np.ndarray:\n        \"\"\"\n        Возвращает усредненные эмбеддинги токенов на уровне слов (после объединения частей\n        от sliding window) для одного документа.\n\n        :param tokens: Список токенов (слов).\n        :param stride: Перекрытие при нарезке.\n        :param device: Целевой девайс модели.\n        :return: Массив [num_words, hidden_size].\n        \"\"\"\n        max_length = int(getattr(self.model.config, \"max_position_embeddings\", 512))\n        num_original_words = len(tokens)\n\n        chunk_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        chunk_inputs.pop(\"overflow_to_sample_mapping\")\n\n        with torch.no_grad():\n            base_model = getattr(self.trainer.model, self.trainer.model.base_model_prefix)\n            outputs = base_model(**chunk_inputs)\n\n        chunk_embeddings = outputs.last_hidden_state  # [num_chunks, seq_len, hidden]\n\n        hidden_size = int(self.model.config.hidden_size)\n        final_word_embeddings = torch.zeros(num_original_words, hidden_size, device=device)\n        word_counts = torch.zeros(num_original_words, device=device)\n\n        for i in range(len(chunk_embeddings)):\n            chunk_embeds = chunk_embeddings[i]\n            chunk_word_ids = chunk_inputs.word_ids(batch_index=i)\n\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if word_id is not None:\n                    final_word_embeddings[word_id] += chunk_embeds[token_pos]\n                    # считаем количество фрагментов для усреднения по слову\n                    if token_pos == 0 or chunk_word_ids[token_pos - 1] != word_id:\n                        word_counts[word_id] += 1\n\n        average_embeddings = final_word_embeddings / (word_counts.unsqueeze(1) + 1e-8)\n        return average_embeddings.detach().cpu().numpy()\n\n    def get_embeddings(self, df: pd.DataFrame, stride: int = 128) -> List[np.ndarray]:\n        \"\"\"\n        Генерирует эмбеддинги на уровне слов для набора документов (через sliding window).\n\n        :param df: Датафрейм с колонкой токенов (список слов).\n        :param stride: Перекрытие при нарезке.\n        :return: Список массивов [num_words, hidden_size] для каждого документа.\n        :raises RuntimeError: Если модель не обучена.\n        \"\"\"\n        if self.trainer is None or self.trainer.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        self.trainer.model.eval()\n        device = self.trainer.model.device\n        all_final_embeddings = []\n\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Генерация эмбеддингов (sliding window)\"):\n            original_tokens = row[self.tokens_column_name]\n            if not original_tokens:\n                all_final_embeddings.append(np.zeros((0, int(self.model.config.hidden_size)), dtype=np.float32))\n                continue\n\n            document_embeddings = self._get_embeddings_single_document(original_tokens, stride, device)\n            all_final_embeddings.append(document_embeddings)\n\n        return all_final_embeddings","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nПример использования.","metadata":{}},{"cell_type":"code","source":"train_data = pd.DataFrame({\n    'tokens': [\n        ['Федор', 'Достоевский', 'родился', 'в', 'Москве', '.'],\n        ['Анна', 'Керн', 'была', 'музой', 'Пушкина', '.'],\n        ['Компания', 'Яндекс', 'представила', 'Алису', '.'],\n        ['Илон', 'Маск', 'основал', 'SpaceX', 'и', 'Tesla', '.']\n    ],\n    'ner_tags': [\n        ['B-PER', 'I-PER', 'O', 'O', 'B-LOC', 'O'],\n        ['B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O'],\n        ['O', 'B-ORG', 'O', 'B-PER', 'O'],\n        ['B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'B-ORG', 'O']\n    ]\n})\n\n# Для предсказания нам нужны только токены\nsubmission_data = pd.DataFrame({\n    'tokens': [\n        ['Лев', 'Толстой', 'написал', 'роман', '\"', 'Война', 'и', 'мир', '\"', '.'],\n        ['Сергей', 'Королев', 'работал', 'в', 'РКК', '\"', 'Энергия', '\"', '.']\n    ]\n})\n\ntrain_data = pd.concat([train_data] * 15, axis=0)\nsubmission_data = pd.concat([submission_data] * 15, axis=0)\n\n# 2. Создание и обучение модели\n# Создаем маппинг из тегов в ID\ntags = train_data['ner_tags'].explode().unique()\nlabel2id = {tag: i for i, tag in enumerate(tags)}\n\nmodel = TokenClassification(\n    checkpoint='DeepPavlov/rubert-base-cased',\n    label2id=label2id,\n    tokens_column_name='tokens',\n    tags_column_name='ner_tags'\n)\n\nmodel.fit(\n    train_data,\n    epochs=3,\n    test_size=0.25,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-5,\n    fp16=False,\n    logging_steps=10,\n    eval_steps=10\n)\n\n# 3. Прогнозирование и получение эмбеддингов\nlabels = model.predict(submission_datap[:5])\nembeddings = model.get_embeddings(submission_data[:5])\n\nprint(labels)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-09-01T09:57:18.543397Z","iopub.status.idle":"2025-09-01T09:57:18.543608Z","shell.execute_reply.started":"2025-09-01T09:57:18.543506Z","shell.execute_reply":"2025-09-01T09:57:18.543516Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install -q wav2clip torchaudio evaluate pillow\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport gc\nimport math\nfrom typing import List, Dict, Any, Optional, Union, Tuple, Callable\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport evaluate\nfrom transformers import Trainer, TrainingArguments, TrainerCallback, PrinterCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom tqdm.auto import tqdm\n\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Фиксирует зерно для воспроизводимости.\n\n    :param seed: Целое число для инициализации генераторов случайных чисел.\n    \"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef to_pil(x: Union[str, np.ndarray, Image.Image]) -> Image.Image:\n    \"\"\"\n    Приводит вход к PIL.Image в формате RGB.\n\n    :param x: Путь к изображению, np.ndarray (H,W[,C]) или PIL.Image.\n    :return: PIL.Image (RGB).\n    :raises ValueError: Если тип входа не поддерживается.\n    \"\"\"\n    if isinstance(x, Image.Image):\n        return x.convert(\"RGB\")\n    if isinstance(x, str):\n        return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray):\n        return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    \"\"\"\n    Загружает аудиофайл и при необходимости ресемплирует до target_sr.\n\n    :param path: Путь к файлу (wav/flac и т.п.).\n    :param target_sr: Целевая частота дискретизации.\n    :return: Одноканальный сигнал формы [T] float32.\n    :raises RuntimeError: Если torchaudio не установлен.\n    \"\"\"\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)  # [C, T]\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr:\n        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\nclass MultiComboDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset для различных комбинаций модальностей (text/image/audio).\n    Поддерживает несколько изображений/аудио per-сэмпл за счёт того, что ячейки могут быть списками.\n\n    Важно: сам датасет хранит лишь ссылки/пути/строки. Реальная загрузка PIL/аудио происходит в collate бэкенда\n    (лениво и батчево), что позволяет работать с большими данными.\n\n    :param df: Источник данных (DataFrame).\n    :param target_col: Имя колонки с таргетом (классовой меткой).\n    :param label2id: Словарь {значение_метки -> id}. Значения меток в df[target_col] должны встречаться в ключах.\n    :param text_columns: Текстовые колонки; их значения передаются в tokenizer_fn.\n    :param image_columns: Колонки с изображениями (значение — путь/PIL/numpy или список таковых).\n    :param audio_columns: Колонки с аудио (значение — путь/массив или список таковых).\n    :param text_tokenizer_fn: Функция для токенизации текста. Принимает dict колонок и special_tokens.\n    :param special_tokens: Специальные токены для токенизатора.\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        label2id: Dict[Any, int],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None\n    ):\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.label2id = label2id\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n\n    def __len__(self) -> int:\n        \"\"\"\n        :return: Количество элементов.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        \"\"\"\n        Возвращает элемент датасета.\n\n        :param idx: Индекс строки.\n        :return: Словарь с ключами:\n                 - 'labels' (int id)\n                 - 'text' (str), если есть текстовые колонки\n                 - 'images' (list), если есть колонки картинок\n                 - 'audios' (list), если есть колонки аудио\n        \"\"\"\n        row = self.df.iloc[idx]\n        item = {\"labels\": int(self.label2id[row[self.target_col]]) if self.target_col in row else 0}\n\n        if self.text_columns:\n            if self.text_tokenizer_fn:\n                text_data = {c: str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns}\n                item[\"text\"] = self.text_tokenizer_fn(text_data, self.special_tokens)\n            else:\n                # Fallback на простую конкатенацию\n                sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n                item[\"text\"] = sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n        def _as_list(v):\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                return []\n            if isinstance(v, (list, tuple)):\n                return list(v)\n            return [v]\n\n        if self.image_columns:\n            imgs = []\n            for c in self.image_columns:\n                if c in row:\n                    imgs.extend(_as_list(row[c]))\n            item[\"images\"] = imgs\n\n        if self.audio_columns:\n            auds = []\n            for c in self.audio_columns:\n                if c in row:\n                    auds.extend(_as_list(row[c]))\n            item[\"audios\"] = auds\n\n        return item\n\n\nclass BaseBackend(nn.Module):\n    \"\"\"\n    Базовый класс мультимодального бэкенда.\n\n    Атрибуты:\n      - name: Название бэкенда (str).\n      - supported: Набор поддерживаемых модальностей, например {'text','image'}.\n      - embed_dim: Базовая размерность эмбеддингов (int).\n      - out_dim_per_modality: Реальные выходные размерности (dict modality->int), учитывая агрегацию (concat/mean).\n      - text_tokenizer_fn: Функция токенизации текста.\n      - special_tokens: Специальные токены для токенизатора.\n    \"\"\"\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n    text_tokenizer_fn: Optional[Callable] = None\n    special_tokens: Dict[str, str] = {}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Собирает батч для Trainer из списка элементов Dataset.\n\n        :param batch: Список элементов (из MultiComboDataset.__getitem__).\n        :return: Словарь с 'labels' (LongTensor) и 'backend_inputs' (dict).\n        \"\"\"\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует модальности и возвращает L2-нормированные эмбеддинги.\n\n        :param backend_inputs: Подготовленные входы (collate).\n        :param device: Девайс для инференса.\n        :return: Словарь {'text':[B,*], 'image':[B,*], 'audio':[B,*]} по доступным модальностям.\n        \"\"\"\n        raise NotImplementedError\n\n    def freeze_all(self):\n        \"\"\"\n        Замораживает параметры бэкенда (requires_grad=False), полезно для linear probing.\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        \"\"\"\n        Возвращает выходную размерность эмбеддинга по модальности с учётом агрегации.\n\n        :param modality: 'text' | 'image' | 'audio'.\n        :return: Размерность вектора.\n        \"\"\"\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n    def set_text_tokenizer(self, tokenizer_fn: Optional[Callable], special_tokens: Optional[Dict[str, str]] = None):\n        \"\"\"\n        Устанавливает функцию токенизации текста.\n\n        :param tokenizer_fn: Функция токенизации.\n        :param special_tokens: Специальные токены.\n        \"\"\"\n        self.text_tokenizer_fn = tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n\n\nclass ClipBackend(BaseBackend):\n    \"\"\"\n    Бэкенд CLIP (HF) для модальностей: text + image.\n    Поддерживает несколько изображений per-сэмпл с агрегацией (concat или mean).\n\n    :param checkpoint: Модель CLIP на HF (например, 'openai/clip-vit-base-patch32').\n    :param max_length: Максимальная длина текстовых токенов.\n    :param freeze: Заморозить ли веса CLIP.\n    :param max_images: Максимум картинок на сэмпл при concat-паде.\n    :param image_agg: 'concat' или 'mean' — как агрегировать несколько изображений.\n    :param text_tokenizer_fn: Функция токенизации текста.\n    :param special_tokens: Специальные токены.\n    \"\"\"\n    name = \"clip\"\n    supported = {\"text\", \"image\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        max_images: int = 1,\n        image_agg: str = \"concat\",\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None\n    ):\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.max_images = int(max_images)\n        self.image_agg = image_agg\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Collate для CLIP: ленивая загрузка изображений, подготовка токенов текста.\n\n        :param batch: Список элементов датасета.\n        :return: {'labels': LongTensor[B], 'backend_inputs': {...}}\n        \"\"\"\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n\n        text_inputs = self.processor(\n            text=texts, padding=True, truncation=True,\n            max_length=self.max_length, return_tensors=\"pt\"\n        )\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенация до max_k эмбеддингов на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги изображений [M, D], где M = сумма counts.\n        :param counts: Количество изображений на сэмпл (длина B).\n        :param max_k: Максимум картинок на сэмпл.\n        :return: [B, D*max_k], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усреднение эмбеддингов изображений по сэмплу.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количество изображений на сэмпл (длина B).\n        :return: [B, D], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует текст/изображения через CLIP и агрегирует изображения.\n\n        :param backend_inputs: Выход collate.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'image':[B, D*max_images] или [B,D]}.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                img_z = self._concat_padded(img_flat, counts, self.max_images)\n            else:\n                img_z = self._mean_pool(img_flat, counts)\n        else:\n            if self.image_agg == \"concat\":\n                img_z = torch.zeros((len(counts), self.embed_dim * self.max_images), device=device)\n            else:\n                img_z = torch.zeros((len(counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"image\": img_z}\n\n\nclass ClapBackend(BaseBackend):\n    \"\"\"\n    Бэкенд CLAP (HF) для модальностей: text + audio.\n    Поддерживает несколько аудио per-сэмпл (concat/mean).\n\n    :param checkpoint: Модель CLAP (например, 'laion/clap-htsat-unfused').\n    :param freeze: Заморозить ли веса CLAP.\n    :param max_audios: Максимум аудио на сэмпл при concat-паде.\n    :param audio_agg: 'concat' или 'mean' — как агрегировать несколько аудио.\n    :param text_tokenizer_fn: Функция токенизации текста.\n    :param special_tokens: Специальные токены.\n    \"\"\"\n    name = \"clap\"\n    supported = {\"text\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"laion/clap-htsat-unfused\",\n        freeze: bool = True,\n        max_audios: int = 1,\n        audio_agg: str = \"concat\",\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None\n    ):\n        super().__init__()\n        from transformers import ClapModel, ClapProcessor\n        self.model = ClapModel.from_pretrained(checkpoint)\n        self.processor = ClapProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(getattr(self.model.config, \"projection_dim\", 512))\n        sr = getattr(self.processor, \"sampling_rate\", None)\n        if sr is None:\n            fe = getattr(self.processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n        self.sr = int(sr)\n        self.max_audios = int(max_audios)\n        self.audio_agg = audio_agg\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        if freeze:\n            self.freeze_all()\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"audio\": aud_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Collate для CLAP: ленивая загрузка и препроцессинг аудио, токены текста.\n\n        :param batch: Список элементов датасета.\n        :return: {'labels': LongTensor[B], 'backend_inputs': {...}}\n        \"\"\"\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"CLAP ожидает путь к аудио или numpy.ndarray\")\n\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_audios):\n            aud_proc = self.processor(audios=flat_audios, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n            audio_inputs = {\"input_features\": aud_proc[\"input_features\"]}\n        else:\n            audio_inputs = {\"input_features\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенация аудио-эмбеддингов (до max_k) с нулевым паддингом.\n\n        :param embs: Плоские эмбеддинги аудио [M, D].\n        :param counts: Кол-во аудио на сэмпл (B).\n        :param max_k: Максимум аудио на сэмпл.\n        :return: [B, D*max_k], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усреднение аудио-эмбеддингов по сэмплу.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Кол-во аудио на сэмпл (B).\n        :return: [B, D], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует текст/аудио через CLAP и агрегирует аудио.\n\n        :param backend_inputs: Выход collate.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'audio':[B, D*max_audios] или [B,D]}.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"audio_counts\"].tolist()\n        af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n        if af is not None:\n            af = af.to(device)\n            aud_flat = self.model.get_audio_features(input_features=af)\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                aud_z = self._concat_padded(aud_flat, counts, self.max_audios)\n            else:\n                aud_z = self._mean_pool(aud_flat, counts)\n        else:\n            if self.audio_agg == \"concat\":\n                aud_z = torch.zeros((len(counts), self.embed_dim * self.max_audios), device=device)\n            else:\n                aud_z = torch.zeros((len(counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"audio\": aud_z}\n\n\nclass FlexibleMultiBackend(BaseBackend):\n    \"\"\"\n    Гибкий бэкенд, позволяющий использовать разные модели для разных модальностей.\n    Поддерживает произвольные комбинации текстовых, визуальных и аудио моделей.\n\n    :param text_model_config: Конфигурация текстовой модели {'checkpoint', 'model_type', 'max_length'}.\n    :param image_model_config: Конфигурация визуальной модели {'checkpoint', 'model_type', 'max_images', 'image_agg'}.\n    :param audio_model_config: Конфигурация аудио модели {'checkpoint', 'model_type', 'max_audios', 'audio_agg', 'sr'}.\n    :param freeze: Заморозить ли веса всех моделей.\n    :param text_tokenizer_fn: Функция токенизации текста.\n    :param special_tokens: Специальные токены.\n    \"\"\"\n    name = \"flexible_multi\"\n    \n    def __init__(\n        self,\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        freeze: bool = True,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None\n    ):\n        super().__init__()\n        self.supported = set()\n        self.out_dim_per_modality = {}\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        \n        # Инициализация текстовой модели\n        self.text_model = None\n        self.text_processor = None\n        self.text_config = text_model_config or {}\n        if text_model_config:\n            self._init_text_model(text_model_config)\n            self.supported.add(\"text\")\n        \n        # Инициализация визуальной модели\n        self.image_model = None\n        self.image_processor = None\n        self.image_config = image_model_config or {}\n        if image_model_config:\n            self._init_image_model(image_model_config)\n            self.supported.add(\"image\")\n        \n        # Инициализация аудио модели\n        self.audio_model = None\n        self.audio_processor = None\n        self.audio_config = audio_model_config or {}\n        if audio_model_config:\n            self._init_audio_model(audio_model_config)\n            self.supported.add(\"audio\")\n        \n        if freeze:\n            self.freeze_all()\n    \n    def _init_text_model(self, config: Dict[str, Any]):\n        \"\"\"\n        Инициализирует текстовую модель согласно конфигурации.\n        \n        :param config: Словарь с 'checkpoint', 'model_type' и опциональными параметрами.\n        \"\"\"\n        from transformers import AutoModel, AutoTokenizer, CLIPTextModel, CLIPTokenizer\n        \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto')\n        \n        if model_type == 'clip':\n            self.text_model = CLIPTextModel.from_pretrained(checkpoint)\n            self.text_processor = CLIPTokenizer.from_pretrained(checkpoint)\n            # Для CLIP текстовой модели используем projection_dim\n            dim = self.text_model.config.projection_dim\n        elif model_type == 'bert' or model_type == 'auto':\n            self.text_model = AutoModel.from_pretrained(checkpoint)\n            self.text_processor = AutoTokenizer.from_pretrained(checkpoint)\n            dim = self.text_model.config.hidden_size\n        else:\n            raise ValueError(f\"Неизвестный model_type для текста: {model_type}\")\n        \n        self.text_config['max_length'] = config.get('max_length', 512)\n        self.text_config['dim'] = dim\n        self.text_config['model_type'] = model_type\n        self.out_dim_per_modality['text'] = dim\n    \n    def _init_image_model(self, config: Dict[str, Any]):\n        \"\"\"\n        Инициализирует визуальную модель согласно конфигурации.\n        \n        :param config: Словарь с 'checkpoint', 'model_type' и опциональными параметрами.\n        \"\"\"\n        from transformers import AutoModel, AutoImageProcessor, CLIPVisionModel, CLIPImageProcessor\n        \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto')\n        \n        if model_type == 'clip':\n            self.image_model = CLIPVisionModel.from_pretrained(checkpoint)\n            self.image_processor = CLIPImageProcessor.from_pretrained(checkpoint)\n            # Для CLIPVisionModel используем hidden_size, так как get_image_features не проецирует\n            dim = self.image_model.config.hidden_size\n        elif model_type in ['dinov2', 'vit', 'auto']:\n            self.image_model = AutoModel.from_pretrained(checkpoint)\n            self.image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n            dim = self.image_model.config.hidden_size\n        else:\n            raise ValueError(f\"Неизвестный model_type для изображений: {model_type}\")\n        \n        self.image_config['max_images'] = config.get('max_images', 1)\n        self.image_config['image_agg'] = config.get('image_agg', 'concat')\n        self.image_config['dim'] = dim\n        self.image_config['model_type'] = model_type\n        \n        if self.image_config['image_agg'] == 'concat':\n            self.out_dim_per_modality['image'] = dim * self.image_config['max_images']\n        else:\n            self.out_dim_per_modality['image'] = dim\n    \n    def _init_audio_model(self, config: Dict[str, Any]):\n        \"\"\"\n        Инициализирует аудио модель согласно конфигурации.\n        \n        :param config: Словарь с 'checkpoint', 'model_type' и опциональными параметрами.\n        \"\"\"\n        from transformers import AutoModel, AutoProcessor, ClapAudioModel, ClapProcessor\n        \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto')\n        \n        if model_type == 'clap':\n            from transformers import ClapModel\n            self.audio_model = ClapModel.from_pretrained(checkpoint)\n            self.audio_processor = ClapProcessor.from_pretrained(checkpoint)\n            dim = getattr(self.audio_model.config, \"projection_dim\", 512)\n            sr = getattr(self.audio_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.audio_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n        elif model_type in ['whisper', 'wav2vec2', 'auto']:\n            self.audio_model = AutoModel.from_pretrained(checkpoint)\n            self.audio_processor = AutoProcessor.from_pretrained(checkpoint)\n            dim = self.audio_model.config.hidden_size\n            sr = self.audio_processor.feature_extractor.sampling_rate\n        else:\n            raise ValueError(f\"Неизвестный model_type для аудио: {model_type}\")\n        \n        self.audio_config['sr'] = config.get('sr', sr)\n        self.audio_config['max_audios'] = config.get('max_audios', 1)\n        self.audio_config['audio_agg'] = config.get('audio_agg', 'concat')\n        self.audio_config['dim'] = dim\n        self.audio_config['model_type'] = model_type\n        \n        if self.audio_config['audio_agg'] == 'concat':\n            self.out_dim_per_modality['audio'] = dim * self.audio_config['max_audios']\n        else:\n            self.out_dim_per_modality['audio'] = dim\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Собирает батч для всех активных модальностей с корректной обработкой пропусков.\n        \"\"\"\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n        backend_inputs = {}\n        batch_size = len(batch)\n        \n        # Обработка текста\n        if self.text_model is not None:\n            texts = []\n            for b in batch:\n                text = b.get(\"text\", \"\")\n                # Если текст пустой или None, используем пробел как заглушку\n                texts.append(text if text else \" \")\n            \n            text_inputs = self.text_processor(\n                texts, padding=True, truncation=True,\n                max_length=self.text_config.get('max_length', 512),\n                return_tensors=\"pt\"\n            )\n            backend_inputs[\"text_inputs\"] = {k: v for k, v in text_inputs.items()}\n        \n        # Обработка изображений\n        if self.image_model is not None:\n            images_lists = [b.get(\"images\", []) for b in batch]\n            flat_images = []\n            img_counts = []\n            batch_indices = []  # Отслеживаем к какому сэмплу относится каждое изображение\n            \n            for idx, lst in enumerate(images_lists):\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                # Фильтруем None и пустые значения\n                lst = [img for img in lst if img is not None]\n                img_counts.append(len(lst))\n                for img in lst:\n                    flat_images.append(to_pil(img))\n                    batch_indices.append(idx)\n            \n            if len(flat_images) > 0:\n                img_proc = self.image_processor(images=flat_images, return_tensors=\"pt\")\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": img_proc[\"pixel_values\"]}\n            else:\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": None}\n            \n            backend_inputs[\"image_counts\"] = torch.tensor(img_counts, dtype=torch.long)\n            backend_inputs[\"image_batch_indices\"] = batch_indices\n        \n        # Обработка аудио\n        if self.audio_model is not None:\n            audios_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios = []\n            aud_counts = []\n            audio_batch_indices = []\n            \n            for idx, lst in enumerate(audios_lists):\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                # Фильтруем None и пустые значения\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_config['sr']))\n                    elif isinstance(a, np.ndarray):\n                        flat_audios.append(a.astype(np.float32))\n                    audio_batch_indices.append(idx)\n            \n            if len(flat_audios) > 0:\n                if self.audio_config.get('model_type') == 'clap':\n                    aud_proc = self.audio_processor(\n                        audios=flat_audios, \n                        sampling_rate=self.audio_config['sr'], \n                        padding=True, \n                        return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_features\": aud_proc[\"input_features\"]}\n                else:\n                    aud_proc = self.audio_processor(\n                        flat_audios, \n                        sampling_rate=self.audio_config['sr'],\n                        padding=True,\n                        return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_values\": aud_proc[\"input_values\"]}\n            else:\n                backend_inputs[\"audio_inputs\"] = {\"input_features\": None, \"input_values\": None}\n            \n            backend_inputs[\"audio_counts\"] = torch.tensor(aud_counts, dtype=torch.long)\n            backend_inputs[\"audio_batch_indices\"] = audio_batch_indices\n        \n        backend_inputs[\"batch_size\"] = batch_size\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _aggregate_embeddings(\n        self, \n        embs: Optional[torch.Tensor], \n        counts: List[int], \n        max_k: int, \n        dim: int, \n        agg_type: str,\n        batch_size: int,\n        device: torch.device\n    ) -> torch.Tensor:\n        \"\"\"\n        Безопасная агрегация эмбеддингов с правильной обработкой пустых сэмплов.\n        \"\"\"\n        # Определяем актуальную размерность\n        actual_dim = embs.size(1) if embs is not None and embs.numel() > 0 else dim\n        \n        # Создаем выходной тензор нужного размера\n        if agg_type == 'concat':\n            out_shape = (batch_size, actual_dim * max_k)\n        else:  # mean\n            out_shape = (batch_size, actual_dim)\n        \n        out = torch.zeros(out_shape, device=device, dtype=torch.float32)\n        \n        # Если нет эмбеддингов, возвращаем нули\n        if embs is None or embs.numel() == 0:\n            return out\n        \n        # Агрегируем эмбеддинги для каждого сэмпла\n        offset = 0\n        for i, count in enumerate(counts):\n            if count > 0:\n                sample_embs = embs[offset:offset + count]\n                \n                if agg_type == 'concat':\n                    # Берем до max_k эмбеддингов\n                    take = sample_embs[:max_k]\n                    # Паддинг если нужно\n                    if take.size(0) < max_k:\n                        pad = torch.zeros((max_k - take.size(0), actual_dim), \n                                        device=device, dtype=embs.dtype)\n                        take = torch.cat([take, pad], dim=0)\n                    out[i] = take.reshape(-1)\n                else:  # mean\n                    out[i] = sample_embs.mean(dim=0)\n                \n                offset += count\n        \n        return F.normalize(out, dim=-1)\n    \n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует все активные модальности через соответствующие модели.\n        \"\"\"\n        results = {}\n        \n        # Определяем актуальный размер батча из labels или первого доступного тензора\n        actual_batch_size = None\n        \n        # Пробуем определить размер батча из различных источников\n        if \"text_inputs\" in backend_inputs:\n            for v in backend_inputs[\"text_inputs\"].values():\n                if torch.is_tensor(v) and v.dim() > 0:\n                    actual_batch_size = v.size(0)\n                    break\n        \n        if actual_batch_size is None and \"image_counts\" in backend_inputs:\n            actual_batch_size = len(backend_inputs[\"image_counts\"])\n        \n        if actual_batch_size is None and \"audio_counts\" in backend_inputs:\n            actual_batch_size = len(backend_inputs[\"audio_counts\"])\n        \n        # Если все еще не определен, используем сохраненный\n        if actual_batch_size is None:\n            actual_batch_size = backend_inputs.get(\"batch_size\", 1)\n        \n        # Кодирование текста\n        if self.text_model is not None and \"text_inputs\" in backend_inputs:\n            text_inputs = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n            \n            # Обрезаем входы если нужно (для DataParallel)\n            if text_inputs.get(\"input_ids\") is not None:\n                current_batch_size = text_inputs[\"input_ids\"].size(0)\n                if current_batch_size != actual_batch_size:\n                    actual_batch_size = min(actual_batch_size, current_batch_size)\n                    text_inputs = {k: v[:actual_batch_size] if torch.is_tensor(v) else v \n                                  for k, v in text_inputs.items()}\n            \n            if self.text_config.get('model_type') == 'clip':\n                text_z = self.text_model.get_text_features(**text_inputs)\n            elif hasattr(self.text_model, 'get_text_features'):\n                text_z = self.text_model.get_text_features(**text_inputs)\n            else:\n                outputs = self.text_model(**text_inputs)\n                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n                    text_z = outputs.pooler_output\n                else:\n                    text_z = outputs.last_hidden_state.mean(dim=1)\n            \n            results[\"text\"] = F.normalize(text_z, dim=-1)\n            # Обновляем actual_batch_size на основе реального выхода\n            actual_batch_size = text_z.size(0)\n        \n        # Кодирование изображений\n        if self.image_model is not None and \"image_inputs\" in backend_inputs:\n            pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n            counts = backend_inputs[\"image_counts\"]\n            \n            # Обрезаем counts до actual_batch_size\n            if len(counts) > actual_batch_size:\n                counts = counts[:actual_batch_size]\n            counts = counts.tolist()\n            \n            # Проверяем, есть ли вообще изображения для обработки\n            total_images_needed = sum(counts)\n            \n            if pi is not None and pi.numel() > 0 and total_images_needed > 0:\n                pi = pi.to(device)\n                \n                # Обрезаем изображения согласно counts\n                if pi.size(0) > total_images_needed:\n                    pi = pi[:total_images_needed]\n                \n                if self.image_config.get('model_type') == 'clip':\n                    outputs = self.image_model(pixel_values=pi)\n                    img_flat = outputs.pooler_output\n                elif hasattr(self.image_model, 'get_image_features'):\n                    img_flat = self.image_model.get_image_features(pixel_values=pi)\n                else:\n                    outputs = self.image_model(pixel_values=pi)\n                    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n                        img_flat = outputs.pooler_output\n                    else:\n                        img_flat = outputs.last_hidden_state[:, 0]\n                \n                img_flat = F.normalize(img_flat, dim=-1)\n                actual_img_dim = img_flat.size(1)\n            else:\n                img_flat = None\n                actual_img_dim = self.image_config.get('dim', 768)\n            \n            # Используем безопасную агрегацию\n            img_z = self._aggregate_embeddings(\n                img_flat, counts,\n                self.image_config['max_images'],\n                actual_img_dim,\n                self.image_config['image_agg'],\n                len(counts),  # Используем длину counts как размер батча\n                device\n            )\n            \n            # Обновляем размерность если изменилась\n            if actual_img_dim != self.image_config.get('dim'):\n                self.image_config['dim'] = actual_img_dim\n                if self.image_config['image_agg'] == 'concat':\n                    self.out_dim_per_modality['image'] = actual_img_dim * self.image_config['max_images']\n                else:\n                    self.out_dim_per_modality['image'] = actual_img_dim\n            \n            results[\"image\"] = img_z\n        \n        # Кодирование аудио\n        if self.audio_model is not None and \"audio_inputs\" in backend_inputs:\n            counts = backend_inputs[\"audio_counts\"]\n            \n            # Обрезаем counts до actual_batch_size\n            if len(counts) > actual_batch_size:\n                counts = counts[:actual_batch_size]\n            counts = counts.tolist()\n            \n            # Проверяем, есть ли вообще аудио для обработки\n            total_audios_needed = sum(counts)\n            \n            # Получаем эмбеддинги аудио\n            aud_flat = None\n            actual_aud_dim = self.audio_config.get('dim', 768)\n            \n            if total_audios_needed > 0:  # Обрабатываем только если есть аудио\n                if self.audio_config.get('model_type') == 'clap':\n                    af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n                    if af is not None and af.numel() > 0:\n                        af = af.to(device)\n                        \n                        # Обрезаем аудио согласно counts\n                        if af.size(0) > total_audios_needed:\n                            af = af[:total_audios_needed]\n                        \n                        # Проверяем, что тензор не пустой после обрезки\n                        if af.numel() > 0:\n                            aud_flat = self.audio_model.get_audio_features(input_features=af)\n                            aud_flat = F.normalize(aud_flat, dim=-1)\n                            actual_aud_dim = aud_flat.size(1)\n                else:\n                    av = backend_inputs[\"audio_inputs\"][\"input_values\"]\n                    if av is not None and av.numel() > 0:\n                        av = av.to(device)\n                        \n                        # Обрезаем аудио согласно counts\n                        if av.size(0) > total_audios_needed:\n                            av = av[:total_audios_needed]\n                        \n                        # Проверяем, что тензор не пустой после обрезки\n                        if av.numel() > 0:\n                            outputs = self.audio_model(input_values=av)\n                            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n                                aud_flat = outputs.pooler_output\n                            else:\n                                aud_flat = outputs.last_hidden_state.mean(dim=1)\n                            aud_flat = F.normalize(aud_flat, dim=-1)\n                            actual_aud_dim = aud_flat.size(1)\n            \n            # Используем безопасную агрегацию (она обработает None для aud_flat)\n            aud_z = self._aggregate_embeddings(\n                aud_flat, counts,\n                self.audio_config['max_audios'],\n                actual_aud_dim,\n                self.audio_config['audio_agg'],\n                len(counts),  # Используем длину counts как размер батча\n                device\n            )\n            \n            # Обновляем размерность если изменилась\n            if aud_flat is not None and actual_aud_dim != self.audio_config.get('dim'):\n                self.audio_config['dim'] = actual_aud_dim\n                if self.audio_config['audio_agg'] == 'concat':\n                    self.out_dim_per_modality['audio'] = actual_aud_dim * self.audio_config['max_audios']\n                else:\n                    self.out_dim_per_modality['audio'] = actual_aud_dim\n            \n            results[\"audio\"] = aud_z\n        \n        # Убеждаемся, что все результаты имеют одинаковый размер батча\n        if results:\n            min_batch_size = min(v.size(0) for v in results.values())\n            if any(v.size(0) != min_batch_size for v in results.values()):\n                results = {k: v[:min_batch_size] for k, v in results.items()}\n        \n        return results\n\n\nclass SingleBackboneClassifier(nn.Module):\n    \"\"\"\n    Классификатор поверх одного мультимодального бэкенда: encode -> fuse -> MLP голова.\n\n    :param backend: Экземпляр бэкенда (CLIP/CLAP/FlexibleMultiBackend).\n    :param modalities: Активные модальности (учёт порядка важен при concat): подмножество ['image','text','audio'].\n    :param num_labels: Количество классов.\n    :param fusion: 'concat' (объединение признаков) или 'mean' (среднее по модальностям).\n    :param hidden: Размер скрытого слоя головы.\n    :param dropout: Дропаут в голове.\n    \"\"\"\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_labels: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать, а у нас: {dict(zip(order, dims))}')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        \"\"\"\n        Находит девайс по первому тензору во входах; иначе выбирает доступный cuda/cpu.\n\n        :param obj: Любая структура с тензорами.\n        :return: torch.device.\n        \"\"\"\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Объединяет эмбеддинги модальностей согласно self.fusion.\n\n        :param z: Словарь эмбеддингов по модальностям.\n        :return: Fused тензор [B, *].\n        \"\"\"\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        batch_size = None\n        \n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() == 3:\n                    t = t.mean(dim=1)\n                elif t.dim() > 3:\n                    t = t.view(t.size(0), -1)\n                feats.append(t)\n                if batch_size is None:\n                    batch_size = t.size(0)\n        \n        # Убеждаемся, что все тензоры имеют одинаковый размер батча\n        if batch_size is not None:\n            feats = [f[:batch_size] for f in feats]\n        \n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            sizes = [f.size(-1) for f in feats]\n            if len(set(sizes)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать, а у нас: {sizes}')\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        \"\"\"\n        Прямой проход модели.\n\n        :param backend_inputs: Входы для бэкенда (из его collate).\n        :param labels: Игнорируется (loss считает Trainer).\n        :return: SequenceClassifierOutput с logits [B, num_labels].\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        \n        # Кодируем модальности\n        z = self.backend.encode(backend_inputs, device=device)\n        \n        # Проверяем, что получили эмбеддинги\n        if not z:\n            raise ValueError(\"Backend не вернул эмбеддинги\")\n        \n        # Объединяем эмбеддинги модальностей\n        fused = self._fuse(z)\n        \n        # Пропускаем через классификационную голову\n        logits = self.head(fused)\n        \n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        \"\"\"\n        Извлекает fused эмбеддинги (и опционально по модальностям).\n\n        :param backend_inputs: Входы для бэкенда.\n        :param return_per_modality: Вернуть также словарь {'text','image','audio'}.\n        :return: fused [B, *] или (fused, per_modality).\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\nclass WeightedCETrainer(Trainer):\n    \"\"\"\n    Trainer с CrossEntropyLoss и поддержкой весов классов.\n    При отсутствии class_weights может вычислять их по частотам train-меток.\n\n    :param num_labels: Количество классов.\n    :param train_labels: Список/массив train-меток (int).\n    :param class_weights: Веса классов (list/np.ndarray/torch.Tensor).\n    \"\"\"\n    def __init__(self, *args, num_labels=None, train_labels=None, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_labels = num_labels\n        if class_weights is not None:\n            w = torch.as_tensor(class_weights, dtype=torch.float32)\n        else:\n            w = None\n            if train_labels is not None and num_labels is not None:\n                train_labels = np.asarray(train_labels).astype(int)\n                counts = np.bincount(train_labels, minlength=num_labels)\n                n = counts.sum()\n                weights = np.zeros(num_labels, dtype=np.float32)\n                nonzero = counts > 0\n                weights[nonzero] = n / (num_labels * counts[nonzero].astype(np.float32))\n                w = torch.tensor(weights, dtype=torch.float32)\n        self.class_weights = w\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        \"\"\"\n        Считает CrossEntropyLoss с опциональными весами классов. \n        Корректно обрабатывает DataParallel и защищается от NaN.\n\n        :param model: Модель.\n        :param inputs: Батч: {'labels': LongTensor[B], 'backend_inputs': {...}}.\n        :param return_outputs: Возвращать ли outputs вместе с loss.\n        :param num_items_in_batch: Совместимость с Trainer API (не используется).\n        :return: loss (и outputs, если return_outputs=True).\n        \"\"\"\n        labels = inputs.pop(\"labels\")\n        \n        # Проверяем, используется ли DataParallel\n        is_parallel = isinstance(model, (nn.DataParallel, nn.parallel.DistributedDataParallel))\n        \n        # Вызываем forward модели\n        outputs = model(**inputs)\n        logits = outputs.logits\n        \n        # Переносим labels на устройство logits\n        labels = labels.to(logits.device)\n        \n        # Обработка несоответствия размеров при DataParallel\n        if logits.size(0) != labels.size(0):\n            # Если logits меньше labels (может быть при разделении батча между GPU)\n            if logits.size(0) < labels.size(0):\n                # Обрезаем labels до размера logits\n                labels = labels[:logits.size(0)]\n            # Если logits больше labels (DataParallel может дублировать)\n            elif is_parallel:\n                # Для DataParallel: повторяем labels для каждой реплики\n                num_replicas = logits.size(0) // labels.size(0)\n                if logits.size(0) == labels.size(0) * num_replicas:\n                    labels = labels.repeat_interleave(num_replicas)\n                else:\n                    # Если размеры не кратны, берем первые logits.size(0) элементов\n                    labels = labels.repeat(num_replicas + 1)[:logits.size(0)]\n            else:\n                # В других случаях просто обрезаем до минимального размера\n                min_size = min(logits.size(0), labels.size(0))\n                logits = logits[:min_size]\n                labels = labels[:min_size]\n        \n        # Проверка на NaN и Inf в logits\n        if torch.isnan(logits).any() or torch.isinf(logits).any():\n            # Заменяем NaN и Inf на нули\n            logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n        \n        # Добавляем небольшую константу для численной стабильности\n        eps = 1e-7\n        logits = logits + eps\n        \n        # Вычисляем loss с проверкой\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        \n        try:\n            loss = nn.CrossEntropyLoss(weight=weight)(logits, labels.long())\n            \n            # Проверка на NaN в loss\n            if torch.isnan(loss) or torch.isinf(loss):\n                # Если loss все еще NaN, используем упрощенный вариант без весов\n                loss = nn.CrossEntropyLoss()(logits, labels.long())\n                \n                # Если все еще NaN, возвращаем малое значение\n                if torch.isnan(loss) or torch.isinf(loss):\n                    loss = torch.tensor(0.01, device=logits.device, requires_grad=True)\n        except Exception as e:\n            # В случае любой ошибки возвращаем малое значение loss\n            print(f\"Warning: Error computing loss: {e}\")\n            loss = torch.tensor(0.01, device=logits.device, requires_grad=True)\n        \n        return (loss, outputs) if return_outputs else loss\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\nclass SingleModelMultiComboClassification:\n    \"\"\"\n    Пайплайн: одна мультимодальная модель (бэкенд) + классификационная голова + HuggingFace Trainer.\n\n    Поддерживаемые комбинации модальностей:\n      - ['text','image']         -> ClipBackend или FlexibleMultiBackend\n      - ['text','audio']         -> ClapBackend или FlexibleMultiBackend\n      - ['image','audio']        -> FlexibleMultiBackend\n      - ['text','image','audio'] -> FlexibleMultiBackend\n\n    Возможности:\n      - Мульти-изображения/аудио per-сэмпл (concat/mean агрегация).\n      - Обучение на больших данных: чанковая подстановка train_dataset, стабильный прогресс‑бар и логи.\n      - Взвешенная CrossEntropy по частотам классов (для дисбаланса).\n      - Гибкая архитектура: возможность использовать разные модели для разных модальностей.\n      - Кастомная токенизация текста через переданную функцию.\n\n    Необходимые импорты:\n    !pip install -q wav2clip torchaudio evaluate pillow\n    import gc\n    import math\n    from typing import List, Dict, Any, Optional, Union, Tuple, Callable\n    import numpy as np\n    import pandas as pd\n    from PIL import Image\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import Dataset, DataLoader\n    import evaluate\n    from transformers import Trainer, TrainingArguments, TrainerCallback, PrinterCallback\n    from transformers.modeling_outputs import SequenceClassifierOutput\n    from tqdm.auto import tqdm\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        num_labels: int,\n        target_column_name: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1\n    ):\n        \"\"\"\n        :param modalities: Список модальностей ('text','image','audio') в любом порядке.\n        :param num_labels: Количество классов.\n        :param target_column_name: Имя столбца таргета в DataFrame.\n        :param text_columns: Имена текстовых колонок.\n        :param image_columns: Имена колонок изображений (значения — пути или списки путей/объектов).\n        :param audio_columns: Имена колонок аудио (значения — пути/массивы или списки).\n        :param text_tokenizer_fn: Функция токенизации текста. Принимает dict колонок и special_tokens.\n        :param special_tokens: Специальные токены для токенизатора.\n        :param backend: 'auto' | 'clip' | 'clap' | 'flexible'.\n        :param clip_checkpoint: Чекпоинт CLIP (для backend='clip').\n        :param clap_checkpoint: Чекпоинт CLAP (для backend='clap').\n        :param text_model_config: Конфиг текстовой модели для FlexibleMultiBackend.\n        :param image_model_config: Конфиг визуальной модели для FlexibleMultiBackend.\n        :param audio_model_config: Конфиг аудио модели для FlexibleMultiBackend.\n        :param fusion: 'concat' или 'mean' — тип фьюжна эмбеддингов.\n        :param freeze_backbone: Заморозить веса бэкенда (linear probing).\n        :param clip_max_length: Максимальная длина токенов в CLIP.\n        :param max_images_per_sample: Максимум картинок при concat-агрегации.\n        :param max_audios_per_sample: Максимум аудио при concat-агрегации.\n        \"\"\"\n        self.modalities = sorted(list(set(modalities)))\n        self.num_labels = num_labels\n        self.target_column_name = target_column_name\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.backend_name = backend\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n\n        self.label2id: Optional[Dict[Any, int]] = None\n        self.id2label: Optional[Dict[int, str]] = None\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[PbarConsoleLogger] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        \"\"\"\n        Инициализирует бэкенд согласно backend='auto'|'clip'|'clap'|'flexible' и проверяет совместимость модальностей.\n\n        :raises ValueError: При неподдерживаемой комбинации модальностей.\n        \"\"\"\n        mods = set(self.modalities)\n        name = self.backend_name\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                name = \"clip\"\n            elif mods == {\"text\", \"audio\"}:\n                name = \"clap\"\n            else:\n                name = \"flexible\"\n\n        if name == \"clip\":\n            self.backend = ClipBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                max_images=self.max_images_per_sample,\n                image_agg=\"concat\",\n                text_tokenizer_fn=self.text_tokenizer_fn,\n                special_tokens=self.special_tokens\n            )\n        elif name == \"clap\":\n            self.backend = ClapBackend(\n                checkpoint=self.clap_checkpoint,\n                freeze=self.freeze_backbone,\n                max_audios=self.max_audios_per_sample,\n                audio_agg=\"concat\",\n                text_tokenizer_fn=self.text_tokenizer_fn,\n                special_tokens=self.special_tokens\n            )\n        elif name == \"flexible\":\n            # Автоматическая конфигурация если не заданы модели\n            if \"text\" in mods and self.text_model_config is None:\n                self.text_model_config = {\n                    'checkpoint': 'bert-base-uncased',\n                    'model_type': 'bert',\n                    'max_length': 512\n                }\n            if \"image\" in mods and self.image_model_config is None:\n                self.image_model_config = {\n                    'checkpoint': 'google/vit-base-patch16-224',\n                    'model_type': 'vit',\n                    'max_images': self.max_images_per_sample,\n                    'image_agg': 'concat'\n                }\n            if \"audio\" in mods and self.audio_model_config is None:\n                self.audio_model_config = {\n                    'checkpoint': self.clap_checkpoint,\n                    'model_type': 'clap',\n                    'max_audios': self.max_audios_per_sample,\n                    'audio_agg': 'concat',\n                    'sr': 48000\n                }\n            \n            self.backend = FlexibleMultiBackend(\n                text_model_config=self.text_model_config if \"text\" in mods else None,\n                image_model_config=self.image_model_config if \"image\" in mods else None,\n                audio_model_config=self.audio_model_config if \"audio\" in mods else None,\n                freeze=self.freeze_backbone,\n                text_tokenizer_fn=self.text_tokenizer_fn,\n                special_tokens=self.special_tokens\n            )\n        else:\n            raise ValueError(f\"Неизвестный backend: {name}\")\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}. \"\n                             f\"Поддерживает: {self.backend.supported}\")\n\n    def _setup_metrics(self, metric_name: str):\n        \"\"\"\n        Создаёт функцию подсчёта метрик для Trainer.\n\n        :param metric_name: 'f1' или 'accuracy'.\n        :raises ValueError: Если метрика не поддерживается.\n        \"\"\"\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            metric = evaluate.load(\"f1\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids, average=\"weighted\")\n        elif metric_name == \"accuracy\":\n            metric = evaluate.load(\"accuracy\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids)\n        else:\n            raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        \"\"\"\n        Перемешивает и делит DataFrame на train/eval.\n\n        :param df: Полный датафрейм.\n        :param test_size: Доля валидации (0..1).\n        :param seed: Зерно.\n        :return: (df_train, df_eval).\n        \"\"\"\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data(self, df: pd.DataFrame):\n        \"\"\"\n        Проверяет соответствие колонок выбранным модальностям.\n\n        :param df: Источник данных.\n        :raises ValueError: При отсутствии необходимых колонок.\n        \"\"\"\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"f1\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1,\n        gradient_checkpointing: bool = False,\n        fit_chunk_size: Optional[int] = None\n    ):\n        \"\"\"\n        Обучает классификационную голову поверх выбранного бэкенда.\n        Поддерживает обучение на больших данных за счёт чанков: train_dataset подставляется кусками.\n\n        :param train_data: Полный датафрейм с данными и таргетом.\n        :param epochs: Количество эпох.\n        :param test_size: Доля валидации.\n        :param per_device_train_batch_size: Размер батча на устройство.\n        :param gradient_accumulation_steps: Шаги аккумуляции градиента.\n        :param learning_rate: Learning rate для оптимизатора.\n        :param metric_name: 'f1' или 'accuracy' — метрика выбора лучшей модели.\n        :param fp16: Использовать fp16 при наличии CUDA (если доступен bf16 — он будет использован вместо fp16).\n        :param logging_steps: Частота логирования шагов.\n        :param eval_steps: Шаги между валидациями/сохранениями.\n        :param output_dir: Каталог для артефактов.\n        :param seed: Зерно.\n        :param hidden: Размер скрытого слоя головы.\n        :param dropout: Дропаут в голове.\n        :param gradient_checkpointing: Делать ли чекпоинты во время обучения для экономии VRAM.\n        :param fit_chunk_size: Размер чанка обучающей выборки. Если None — весь train как один чанк.\n        :return: self.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        classes = sorted(train_data[self.target_column_name].unique().tolist())\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n        df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n\n        # Датасет валидации держим целиком (обычно небольшой).\n        ds_eval = MultiComboDataset(\n            df_eval, self.target_column_name, self.label2id,\n            self.text_columns, self.image_columns, self.audio_columns,\n            self.text_tokenizer_fn, self.special_tokens\n        )\n\n        # Веса классов по всему train (не по чанку).\n        y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n        counts = np.bincount(y_train_all, minlength=self.num_labels)\n        n_all = counts.sum()\n        class_weights = np.zeros(self.num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        class_weights[nonzero] = n_all / (self.num_labels * counts[nonzero].astype(np.float32))\n\n        # Модель и метрики\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        self._setup_metrics(metric_name)\n\n        # Настройки точности\n        bf16_ok = bool(torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=bool(fp16 and torch.cuda.is_available() and not bf16_ok),\n            bf16=bool(bf16_ok and not fp16),\n            dataloader_num_workers=os.cpu_count(),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def data_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            \"\"\"\n            Адаптер collate для Trainer: делегирует бэкенду сборку батча.\n\n            :param batch_list: Список элементов Dataset.\n            :return: Батч для model.forward(): {'labels': LongTensor, 'backend_inputs': {...}}.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        # Вспомогательные функции для чанков\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            \"\"\"\n            Оценивает число шагов оптимизации на чанке размера sz.\n\n            :param sz: Количество примеров в чанке.\n            :param bsz: Размер батча.\n            :param accum: Шаги аккумуляции.\n            :return: Число оптимизационных шагов.\n            \"\"\"\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            \"\"\"\n            Генератор срезов индексов по chunk_size.\n\n            :param index_array: Индексы обучающей выборки.\n            :param chunk_size: Размер чанка.\n            :yield: Срез индексов.\n            \"\"\"\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        # Индексы train\n        n_train = len(df_train)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n\n        # Чанк по умолчанию — весь train\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        # Предварительный рассчёт общего числа шагов (для прогресс‑бара и планировщика)\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        # Инициализация Trainer с «пустым» train датасетом (минимальный чанк), чтобы не держать весь train\n        dummy_idx = np.arange(min(len(df_train), 1))\n        ds_train_init = MultiComboDataset(\n            df_train.iloc[dummy_idx], self.target_column_name, self.label2id,\n            self.text_columns, self.image_columns, self.audio_columns,\n            self.text_tokenizer_fn, self.special_tokens\n        ) if len(dummy_idx) > 0 else ds_eval\n\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            num_labels=self.num_labels,\n            train_labels=y_train_all,\n            class_weights=class_weights\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        # Планировщик на рассчитанное количество шагов\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        # Внешний прогресс‑бар + консольный лог\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        # Основной цикл обучения по эпохам и чанкам\n        steps_done = 0\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n\n            for slc in chunk_slices(shuffled, chunk_size):\n                # Подставляем чанк\n                chunk_df = df_train.iloc[slc]\n                ds_chunk = MultiComboDataset(\n                    chunk_df, self.target_column_name, self.label2id,\n                    self.text_columns, self.image_columns, self.audio_columns,\n                    self.text_tokenizer_fn, self.special_tokens\n                )\n                self.trainer.train_dataset = ds_chunk\n\n                # Считаем шаги на чанке и настраиваем max_steps Trainer\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk, chunk_df\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                # Очистка памяти между чанками\n                del ds_chunk, chunk_df\n                gc.collect()\n                if torch.cuda.is_available():\n                    for i in range(torch.cuda.device_count()):\n                        with torch.cuda.device(i):\n                            torch.cuda.empty_cache()\n                            torch.cuda.ipc_collect()  # Очистка IPC памяти\n                    \n                    # Синхронизация всех GPU (важно при DataParallel)\n                    if torch.cuda.device_count() > 1:\n                        for i in range(torch.cuda.device_count()):\n                            torch.cuda.synchronize(i)\n                    \n                    # Дополнительная очистка (если используется)\n                    if hasattr(torch.cuda, 'reset_peak_memory_stats'):\n                        for i in range(torch.cuda.device_count()):\n                            torch.cuda.reset_peak_memory_stats(i)\n\n        pbar.close()\n        return self\n\n    def predict(self, df: pd.DataFrame, return_label_str: bool = False) -> np.ndarray:\n        \"\"\"\n        Делает предсказания классов на новых данных.\n\n        :param df: Датафрейм с теми же колонками модальностей, что и при обучении.\n        :param return_label_str: Если True — вернуть строковые метки; иначе — id.\n        :return: np.ndarray предсказанных меток.\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n        ds = MultiComboDataset(\n            df_c, self.target_column_name, self.label2id,\n            self.text_columns, self.image_columns, self.audio_columns,\n            self.text_tokenizer_fn, self.special_tokens\n        )\n        preds = self.trainer.predict(test_dataset=ds)\n        y_pred = np.argmax(preds.predictions, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        \"\"\"\n        Извлекает fused эмбеддинги (и опционально по модальностям) для новых данных.\n\n        :param df: Датафрейм с нужными колонками модальностей.\n        :param batch_size: Размер батча при инференсе.\n        :param return_per_modality: Вернуть также словарь эмбеддингов {'text','image','audio'}.\n        :return: np.ndarray fused [N, D_fused] или (fused, per_modality_dict).\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        # Определяем девайс модели\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n\n        ds = MultiComboDataset(\n            df_c, self.target_column_name, self.label2id,\n            self.text_columns, self.image_columns, self.audio_columns,\n            self.text_tokenizer_fn, self.special_tokens\n        )\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            \"\"\"\n            Collate для DataLoader при извлечении эмбеддингов.\n\n            :param batch_list: Список элементов.\n            :return: Батч 'backend_inputs' для модели.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device: torch.device):\n            \"\"\"\n            Рекурсивно переносит тензоры на device.\n\n            :param obj: Тензор/словарь/список/кортеж/прочее.\n            :param device: torch.device.\n            :return: Объект с перенесёнными тензорами.\n            \"\"\"\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        with torch.no_grad():\n            for batch in loader:\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        return fused_arr, per_mod\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Очистка памяти\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'encoders'):\n            for encoder in self.encoders.values():\n                if hasattr(encoder, 'model'):\n                    del encoder.model\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T06:50:22.286169Z","iopub.execute_input":"2025-09-02T06:50:22.286447Z","iopub.status.idle":"2025-09-02T06:52:28.294188Z","shell.execute_reply.started":"2025-09-02T06:50:22.286427Z","shell.execute_reply":"2025-09-02T06:52:28.293582Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-09-02 06:52:04.426743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756795924.781288      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756795924.881084      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Создание фиктивных данных.","metadata":{}},{"cell_type":"code","source":"# --- 1. Подготовка и генерация сложных данных ---\n\n# Установка/проверка библиотек\n!pip install -q scipy transformers evaluate accelerate\n\nimport os\nimport shutil\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom scipy.io.wavfile import write as write_wav\n\ndef create_complex_dummy_data(\n    num_samples: int = 1000,\n    class_weights: List[float] = [0.1, 0.6, 0.3] # Несбалансированные классы\n) -> pd.DataFrame:\n    \"\"\"Создает сложный фиктивный DataFrame для демонстрации.\"\"\"\n    data_dir = \"./complex_test_data\"\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    labels = [\"Documentary\", \"Music Video\", \"Sports Review\"]\n    rng = np.random.default_rng(42)\n\n    # Генерация несбалансированных меток\n    generated_labels = rng.choice(labels, num_samples, p=class_weights)\n\n    data = []\n    for i, label in enumerate(generated_labels):\n        sample = {'id': f'video_{i}'}\n\n        # --- Текстовая модальность ---\n        if rng.random() > 0.1: # 10% пропусков текста\n            sample['title'] = f\"Video Title {i}: A look into {label}\"\n            sample['transcript'] = f\"Welcome to this {label.lower()}. Today we discuss topic {i % 10}.\"\n        else:\n            sample['title'] = None\n            sample['transcript'] = \"\"\n\n        # --- Визуальная модальность ---\n        if rng.random() > 0.05: # 5% пропусков изображений\n            num_images = rng.integers(1, 6) # от 1 до 5 кадров\n            img_paths = []\n            for j in range(num_images):\n                path = os.path.join(data_dir, f\"frame_{i}_{j}.png\")\n                img = Image.fromarray(rng.integers(0, 256, (224, 224, 3), dtype=np.uint8))\n                img.save(path)\n                img_paths.append(path)\n            sample['keyframes'] = img_paths\n        else:\n            sample['keyframes'] = []\n\n        # --- Аудио модальность ---\n        if rng.random() > 0.15: # 15% пропусков аудио\n            num_audios = rng.integers(1, 4) # от 1 до 3 аудиоклипов\n            audio_paths = []\n            for j in range(num_audios):\n                path = os.path.join(data_dir, f\"sfx_{i}_{j}.wav\")\n                sr = 48000 # CLAP требует высокую SR\n                t = np.linspace(0., 1.5, int(sr * 1.5))\n                amplitude = np.iinfo(np.int16).max * 0.3\n                freq = rng.uniform(200, 1200)\n                waveform = (amplitude * np.sin(2. * np.pi * freq * t)).astype(np.int16)\n                write_wav(path, sr, waveform)\n                audio_paths.append(path)\n            sample['audio_effects'] = audio_paths\n        else:\n            sample['audio_effects'] = None\n            \n        sample['genre'] = label\n        data.append(sample)\n\n    df = pd.DataFrame(data)\n    print(f\"Создан DataFrame размером {df.shape}\")\n    print(\"Распределение классов:\")\n    print(df['genre'].value_counts(normalize=True))\n    print(\"\\nПример строки:\")\n    print(df.iloc[rng.integers(0, num_samples)].to_dict())\n    return df\n\n# Генерация данных\nlarge_complex_df = create_complex_dummy_data(num_samples=50) # Уменьшено для быстрого запуска","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T06:52:28.295787Z","iopub.execute_input":"2025-09-02T06:52:28.296328Z","iopub.status.idle":"2025-09-02T06:52:33.095788Z","shell.execute_reply.started":"2025-09-02T06:52:28.296305Z","shell.execute_reply":"2025-09-02T06:52:33.094910Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Создан DataFrame размером (50, 6)\nРаспределение классов:\ngenre\nMusic Video      0.58\nSports Review    0.36\nDocumentary      0.06\nName: proportion, dtype: float64\n\nПример строки:\n{'id': 'video_32', 'title': 'Video Title 32: A look into Music Video', 'transcript': 'Welcome to this music video. Today we discuss topic 2.', 'keyframes': ['./complex_test_data/frame_32_0.png', './complex_test_data/frame_32_1.png', './complex_test_data/frame_32_2.png', './complex_test_data/frame_32_3.png'], 'audio_effects': ['./complex_test_data/sfx_32_0.wav', './complex_test_data/sfx_32_1.wav', './complex_test_data/sfx_32_2.wav'], 'genre': 'Music Video'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"video_classifier = SingleModelMultiComboClassification(\n    modalities=['text', 'image', 'audio'],\n    num_labels=3,\n    target_column_name='genre',\n    text_columns=['title', 'transcript'],\n    image_columns=['keyframes'],\n    audio_columns=['audio_effects'],\n\n    backend='flexible',\n    text_model_config={\n        'checkpoint': 'microsoft/deberta-v3-small',\n        'model_type': 'auto',\n        'max_length': 128\n    },\n    image_model_config={\n        'checkpoint': 'openai/clip-vit-base-patch32',\n        'model_type': 'clip',\n        'max_images': 3,\n        'image_agg': 'mean'\n    },\n    audio_model_config={\n        'checkpoint': 'laion/clap-htsat-unfused',\n        'model_type': 'clap',\n        'max_audios': 2,\n        'audio_agg': 'mean'\n    },\n\n    fusion='concat',\n    freeze_backbone=True,\n)\n\nvideo_classifier.fit(\n    train_data=large_complex_df,\n    epochs=2,\n    test_size=0.2,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=5e-6,\n    metric_name=\"f1\",\n    fp16=True,\n    logging_steps=5,\n    eval_steps=10,\n    output_dir=\"./video_classifier_results\",\n    seed=42,\n    hidden=512,\n    dropout=0.2,\n    fit_chunk_size=10\n)\n\ninference_df = large_complex_df.sample(5, random_state=42)\npredicted_genres = video_classifier.predict(inference_df, return_label_str=True)\nprint(predicted_genres)\n\nfused_embeddings, per_modality_embeddings = video_classifier.get_embeddings(\n    inference_df,\n    return_per_modality=True\n)\nprint(f\"\\nРазмер Fused (объединенных) эмбеддингов: {fused_embeddings.shape}\")\nprint(\"Размеры эмбеддингов по каждой модальности:\")\nfor modality, embs in per_modality_embeddings.items():\n    print(f\"  - {modality.capitalize()}: {embs.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T06:52:33.096930Z","iopub.execute_input":"2025-09-02T06:52:33.097251Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b30b880ea7f64ca9a95f77138e17ec48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d181ce180af941a88091263e8b317313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5236f6913864d318642cf141b71008d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2046a60b7164fd48c3c4997bc652e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c1841a3edd24e6a8c241aa9db3e2952"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4950c745c75543c098b37940c7927297"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31ec879f56d7475bba2d75fef336d1b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba4bec80e49a47b89618fac86436a319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"037def3453b3453e9093a348a0af6014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5429fa4a86d9486ba3ca8260afe3ce32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/615M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614c996bacd44db098e216061d6b6e2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/541 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3d3e720258642588d111f450698fb5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/614M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7800d3268984edd98fbc45476e63713"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0753d1929fb84298ae1cea2ec0d64291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"521c36e712084da8b1e0d12f4b3e7892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a260707bfb8e407bba7230ac8a7b54d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22be3b3bef30414d8b2260767d9bdffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42606df4e9a425baab16c983fef921c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99440ff16554b84b866b292e7d6f71e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Progress:   0%|          | 0/24 [00:00<?, ?step/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d2854aac3d41b89b88bd4eee94ea91"}},"metadata":{}},{"name":"stdout","text":"step: 10, train loss: 0.6063000000, val loss: 1.072237968, val f1: 0.4800000000\nstep: 20, train loss: 0.5039000000, val loss: 1.162315965, val f1: 0.4800000000\n['Music Video' 'Music Video' 'Documentary' 'Music Video' 'Music Video']\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pipeline = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\", \"audio\"],\n    num_labels=4,\n    target_column_name=\"label\",\n\n    # Колонки данных\n    text_columns=[\"text_1\", \"text_2\", \"text_3\"],\n    image_columns=[\"image_path_1\", \"image_path_2\"],\n    audio_columns=[\"audio_path_1\", \"audio_path_2\"],\n\n    # Бэкенд и фьюжн\n    backend=\"auto\",     # для ['text','image','audio'] автоматически выберется clip_wav2clip\n    fusion=\"concat\",\n    freeze_backbone=True,   # linear probing; поставьте False для тонкой донастройки энкодеров\n\n    # Ограничения по количеству мультимодальных входов (для concat-агрегации)\n    max_images_per_sample=2,  # под две картинки\n    max_audios_per_sample=1    # под одно аудио (если две — выставьте 2 и добавьте вторую колонку)\n)\n\n# Обучение (с поддержкой «больших данных» за счёт чанков)\npipeline.fit(\n    df_text_image_audio,\n    epochs=2,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    metric_name=\"f1\",\n    fp16=True,\n    logging_steps=10,\n    eval_steps=10,\n    fit_chunk_size=25,\n    output_dir=\"./result\",\n    seed=42\n)\n\npreds = pipeline.predict(df_text_image_audio[:5], return_label_str=True)\nemb = pipeline.get_embeddings(df_text_image_audio[:5], batch_size=8)\n\nprint(f'preds: {preds}')\nprint(f'embeddings: {emb}')\n\n# При желании — эмбеддинги по модальностям отдельно\nfused, per_mod = pipeline.get_embeddings(\n    df_text_image_audio.head(8),\n    batch_size=8,\n    return_per_modality=True\n)\nprint(\"fused.shape:\", fused.shape)\nfor m, arr in per_mod.items():\n    print(f\"{m} emb shape:\", arr.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-09-01T10:00:32.065283Z","iopub.execute_input":"2025-09-01T10:00:32.065576Z","iopub.status.idle":"2025-09-01T10:13:40.010425Z","shell.execute_reply.started":"2025-09-01T10:00:32.065556Z","shell.execute_reply":"2025-09-01T10:13:40.009754Z"},"collapsed":true},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Progress:   0%|          | 0/52 [00:00<?, ?step/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2147d6c89843fb91c0722cb5bc4a16"}},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://github.com/descriptinc/lyrebird-wav2clip/releases/download/v0.1.0-alpha/Wav2CLIP.pt\" to /root/.cache/torch/hub/checkpoints/Wav2CLIP.pt\nDownloading: \"https://github.com/descriptinc/lyrebird-wav2clip/releases/download/v0.1.0-alpha/Wav2CLIP.pt\" to /root/.cache/torch/hub/checkpoints/Wav2CLIP.pt\n\n  0%|          | 0.00/46.7M [00:00<?, ?B/s]\u001b[A\n\n  0%|          | 0.00/46.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n 16%|█▌        | 7.38M/46.7M [00:00<00:00, 77.1MB/s]\u001b[A\n\n 22%|██▏       | 10.1M/46.7M [00:00<00:00, 100MB/s]\u001b[A\u001b[A\n 53%|█████▎    | 24.8M/46.7M [00:00<00:00, 139MB/s] \u001b[A\n\n100%|██████████| 46.7M/46.7M [00:00<00:00, 165MB/s]\u001b[A\u001b[A\n\n100%|██████████| 46.7M/46.7M [00:00<00:00, 142MB/s]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"step: 10, train loss: 1.4042000000, val loss: 1.425664186, val f1: 0.2635167464\nstep: 20, train loss: 1.2494000000, val loss: 1.705203414, val f1: 0.1702786378\nstep: 30, train loss: 0.6303000000, val loss: 1.981891394, val f1: 0.2939502603\nstep: 40, train loss: 0.6404000000, val loss: 2.015444756, val f1: 0.2605555556\nstep: 50, train loss: 0.9030000000, val loss: 1.774872541, val f1: 0.1573593074\npreds: ['B' 'A' 'A' 'C' 'A']\nembeddings: [[ 0.00718042 -0.0178346  -0.01379171 ...  0.10906745 -0.02433352\n  -0.01562322]\n [-0.00491063 -0.0060696  -0.02768656 ...  0.08318765 -0.00345009\n   0.00758308]\n [-0.00189047 -0.01706851 -0.02053576 ...  0.08181054 -0.0128179\n  -0.00466248]\n [-0.00369233 -0.01330828 -0.01572154 ...  0.10906745 -0.02433352\n  -0.01562322]\n [-0.00369233 -0.01330828 -0.01572154 ...  0.10906745 -0.02433352\n  -0.01562322]]\nfused.shape: (8, 2048)\naudio emb shape: (8, 512)\nimage emb shape: (8, 1024)\ntext emb shape: (8, 512)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Дообучение регрессора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация регрессора.","metadata":{}},{"cell_type":"code","source":"!pip install -q wav2clip torchaudio evaluate pillow\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport gc\nimport math\nfrom typing import List, Dict, Any, Optional, Union, Tuple, Callable\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport evaluate\nfrom transformers import Trainer, TrainingArguments, TrainerCallback, PrinterCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom tqdm.auto import tqdm\n\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Фиксирует зерно для воспроизводимости.\n\n    :param seed: Целое число для инициализации генераторов случайных чисел.\n    \"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef to_pil(x: Union[str, np.ndarray, Image.Image]) -> Image.Image:\n    \"\"\"\n    Приводит вход к PIL.Image в формате RGB.\n\n    :param x: Путь к изображению, np.ndarray (H,W[,C]) или PIL.Image.\n    :return: PIL.Image (RGB).\n    :raises ValueError: Если тип входа не поддерживается.\n    \"\"\"\n    if isinstance(x, Image.Image):\n        return x.convert(\"RGB\")\n    if isinstance(x, str):\n        return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray):\n        return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    \"\"\"\n    Загружает аудиофайл и при необходимости ресемплирует до target_sr.\n\n    :param path: Путь к файлу (wav/flac и т.п.).\n    :param target_sr: Целевая частота дискретизации.\n    :return: Одноканальный сигнал формы [T] float32.\n    :raises RuntimeError: Если torchaudio не установлен.\n    \"\"\"\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)  # [C, T]\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr:\n        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\nclass MultiComboRegressionDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset для различных комбинаций модальностей (text/image/audio) для регрессии.\n    Поддерживает несколько изображений/аудио per-сэмпл за счёт того, что ячейки могут быть списками.\n    Поддерживает множественные целевые переменные.\n\n    Важно: сам датасет хранит лишь ссылки/пути/строки. Реальная загрузка PIL/аудио происходит в collate бэкенда\n    (лениво и батчево), что позволяет работать с большими данными.\n\n    :param df: Источник данных (DataFrame).\n    :param target_columns: Список имён колонок с целевыми значениями.\n    :param text_columns: Текстовые колонки; их значения передаются в tokenizer_fn.\n    :param image_columns: Колонки с изображениями (значение — путь/PIL/numpy или список таковых).\n    :param audio_columns: Колонки с аудио (значение — путь/массив или список таковых).\n    :param text_tokenizer_fn: Функция для токенизации текста. Принимает dict колонок и special_tokens.\n    :param special_tokens: Специальные токены для токенизатора.\n    :param target_normalizer: Опциональная функция нормализации целевых значений.\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_columns: List[str],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        target_normalizer: Optional[Callable] = None\n    ):\n        self.df = df.reset_index(drop=True)\n        self.target_columns = target_columns\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.target_normalizer = target_normalizer\n\n    def __len__(self) -> int:\n        \"\"\"\n        :return: Количество элементов.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        \"\"\"\n        Возвращает элемент датасета.\n\n        :param idx: Индекс строки.\n        :return: Словарь с ключами:\n                 - 'labels' (np.ndarray с целевыми значениями)\n                 - 'text' (str), если есть текстовые колонки\n                 - 'images' (list), если есть колонки картинок\n                 - 'audios' (list), если есть колонки аудио\n        \"\"\"\n        row = self.df.iloc[idx]\n        \n        # Извлекаем целевые значения\n        targets = []\n        for col in self.target_columns:\n            if col in row:\n                val = row[col]\n                # Обработка NaN значений\n                if pd.isna(val):\n                    val = 0.0\n                targets.append(float(val))\n            else:\n                targets.append(0.0)\n        \n        targets = np.array(targets, dtype=np.float32)\n        \n        # Применяем нормализацию если есть\n        if self.target_normalizer is not None:\n            targets = self.target_normalizer(targets)\n        \n        item = {\"labels\": targets}\n\n        if self.text_columns:\n            if self.text_tokenizer_fn:\n                text_data = {c: str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns}\n                item[\"text\"] = self.text_tokenizer_fn(text_data, self.special_tokens)\n            else:\n                # Fallback на простую конкатенацию\n                sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n                item[\"text\"] = sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n        def _as_list(v):\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                return []\n            if isinstance(v, (list, tuple)):\n                return list(v)\n            return [v]\n\n        if self.image_columns:\n            imgs = []\n            for c in self.image_columns:\n                if c in row:\n                    imgs.extend(_as_list(row[c]))\n            item[\"images\"] = imgs\n\n        if self.audio_columns:\n            auds = []\n            for c in self.audio_columns:\n                if c in row:\n                    auds.extend(_as_list(row[c]))\n            item[\"audios\"] = auds\n\n        return item\n\n\nclass BaseBackend(nn.Module):\n    \"\"\"\n    Базовый класс мультимодального бэкенда.\n\n    Атрибуты:\n      - name: Название бэкенда (str).\n      - supported: Набор поддерживаемых модальностей, например {'text','image'}.\n      - embed_dim: Базовая размерность эмбеддингов (int).\n      - out_dim_per_modality: Реальные выходные размерности (dict modality->int), учитывая агрегацию (concat/mean).\n      - text_tokenizer_fn: Функция токенизации текста.\n      - special_tokens: Специальные токены для токенизатора.\n    \"\"\"\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n    text_tokenizer_fn: Optional[Callable] = None\n    special_tokens: Dict[str, str] = {}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Собирает батч для Trainer из списка элементов Dataset.\n\n        :param batch: Список элементов (из MultiComboRegressionDataset.__getitem__).\n        :return: Словарь с 'labels' (FloatTensor) и 'backend_inputs' (dict).\n        \"\"\"\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует модальности и возвращает L2-нормированные эмбеддинги.\n\n        :param backend_inputs: Подготовленные входы (collate).\n        :param device: Девайс для инференса.\n        :return: Словарь {'text':[B,*], 'image':[B,*], 'audio':[B,*]} по доступным модальностям.\n        \"\"\"\n        raise NotImplementedError\n\n    def freeze_all(self):\n        \"\"\"\n        Замораживает параметры бэкенда (requires_grad=False), полезно для linear probing.\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        \"\"\"\n        Возвращает выходную размерность эмбеддинга по модальности с учётом агрегации.\n\n        :param modality: 'text' | 'image' | 'audio'.\n        :return: Размерность вектора.\n        \"\"\"\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n    def set_text_tokenizer(self, tokenizer_fn: Optional[Callable], special_tokens: Optional[Dict[str, str]] = None):\n        \"\"\"\n        Устанавливает функцию токенизации текста.\n\n        :param tokenizer_fn: Функция токенизации.\n        :param special_tokens: Специальные токены.\n        \"\"\"\n        self.text_tokenizer_fn = tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n\n\nclass ClipBackend(BaseBackend):\n    \"\"\"\n    Бэкенд CLIP (HF) для модальностей: text + image.\n    Поддерживает несколько изображений per-сэмпл с агрегацией (concat или mean).\n\n    :param checkpoint: Модель CLIP на HF (например, 'openai/clip-vit-base-patch32').\n    :param max_length: Максимальная длина текстовых токенов.\n    :param freeze: Заморозить ли веса CLIP.\n    :param max_images: Максимум картинок на сэмпл при concat-паде.\n    :param image_agg: 'concat' или 'mean' — как агрегировать несколько изображений.\n    :param text_tokenizer_fn: Функция токенизации текста.\n    :param special_tokens: Специальные токены.\n    \"\"\"\n    name = \"clip\"\n    supported = {\"text\", \"image\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        max_images: int = 1,\n        image_agg: str = \"concat\",\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None\n    ):\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.max_images = int(max_images)\n        self.image_agg = image_agg\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Collate для CLIP: ленивая загрузка изображений, подготовка токенов текста.\n\n        :param batch: Список элементов датасета.\n        :return: {'labels': FloatTensor[B, num_targets], 'backend_inputs': {...}}\n        \"\"\"\n        labels = torch.tensor(np.stack([b.get(\"labels\", np.array([0.0])) for b in batch]), dtype=torch.float32)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n\n        text_inputs = self.processor(\n            text=texts, padding=True, truncation=True,\n            max_length=self.max_length, return_tensors=\"pt\"\n        )\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенация до max_k эмбеддингов на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги изображений [M, D], где M = сумма counts.\n        :param counts: Количество изображений на сэмпл (длина B).\n        :param max_k: Максимум картинок на сэмпл.\n        :return: [B, D*max_k], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усреднение эмбеддингов изображений по сэмплу.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количество изображений на сэмпл (длина B).\n        :return: [B, D], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует текст/изображения через CLIP и агрегирует изображения.\n\n        :param backend_inputs: Выход collate.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'image':[B, D*max_images] или [B,D]}.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                img_z = self._concat_padded(img_flat, counts, self.max_images)\n            else:\n                img_z = self._mean_pool(img_flat, counts)\n        else:\n            if self.image_agg == \"concat\":\n                img_z = torch.zeros((len(counts), self.embed_dim * self.max_images), device=device)\n            else:\n                img_z = torch.zeros((len(counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"image\": img_z}\n\n\nclass ClapBackend(BaseBackend):\n    \"\"\"\n    Бэкенд CLAP (HF) для модальностей: text + audio.\n    Поддерживает несколько аудио per-сэмпл (concat/mean).\n\n    :param checkpoint: Модель CLAP (например, 'laion/clap-htsat-unfused').\n    :param freeze: Заморозить ли веса CLAP.\n    :param max_audios: Максимум аудио на сэмпл при concat-паде.\n    :param audio_agg: 'concat' или 'mean' — как агрегировать несколько аудио.\n    :param text_tokenizer_fn: Функция токенизации текста.\n    :param special_tokens: Специальные токены.\n    \"\"\"\n    name = \"clap\"\n    supported = {\"text\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"laion/clap-htsat-unfused\",\n        freeze: bool = True,\n        max_audios: int = 1,\n        audio_agg: str = \"concat\",\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None\n    ):\n        super().__init__()\n        from transformers import ClapModel, ClapProcessor\n        self.model = ClapModel.from_pretrained(checkpoint)\n        self.processor = ClapProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(getattr(self.model.config, \"projection_dim\", 512))\n        sr = getattr(self.processor, \"sampling_rate\", None)\n        if sr is None:\n            fe = getattr(self.processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n        self.sr = int(sr)\n        self.max_audios = int(max_audios)\n        self.audio_agg = audio_agg\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        if freeze:\n            self.freeze_all()\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"audio\": aud_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Collate для CLAP: ленивая загрузка и препроцессинг аудио, токены текста.\n\n        :param batch: Список элементов датасета.\n        :return: {'labels': FloatTensor[B, num_targets], 'backend_inputs': {...}}\n        \"\"\"\n        labels = torch.tensor(np.stack([b.get(\"labels\", np.array([0.0])) for b in batch]), dtype=torch.float32)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"CLAP ожидает путь к аудио или numpy.ndarray\")\n\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_audios):\n            aud_proc = self.processor(audios=flat_audios, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n            audio_inputs = {\"input_features\": aud_proc[\"input_features\"]}\n        else:\n            audio_inputs = {\"input_features\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенация аудио-эмбеддингов (до max_k) с нулевым паддингом.\n\n        :param embs: Плоские эмбеддинги аудио [M, D].\n        :param counts: Кол-во аудио на сэмпл (B).\n        :param max_k: Максимум аудио на сэмпл.\n        :return: [B, D*max_k], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усреднение аудио-эмбеддингов по сэмплу.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Кол-во аудио на сэмпл (B).\n        :return: [B, D], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует текст/аудио через CLAP и агрегирует аудио.\n\n        :param backend_inputs: Выход collate.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'audio':[B, D*max_audios] или [B,D]}.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"audio_counts\"].tolist()\n        af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n        if af is not None:\n            af = af.to(device)\n            aud_flat = self.model.get_audio_features(input_features=af)\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                aud_z = self._concat_padded(aud_flat, counts, self.max_audios)\n            else:\n                aud_z = self._mean_pool(aud_flat, counts)\n        else:\n            if self.audio_agg == \"concat\":\n                aud_z = torch.zeros((len(counts), self.embed_dim * self.max_audios), device=device)\n            else:\n                aud_z = torch.zeros((len(counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"audio\": aud_z}\n\n\nclass FlexibleMultiBackend(BaseBackend):\n    \"\"\"\n    Гибкий бэкенд, позволяющий использовать разные модели для разных модальностей.\n    Поддерживает произвольные комбинации текстовых, визуальных и аудио моделей.\n\n    :param text_model_config: Конфигурация текстовой модели {'checkpoint', 'model_type', 'max_length'}.\n    :param image_model_config: Конфигурация визуальной модели {'checkpoint', 'model_type', 'max_images', 'image_agg'}.\n    :param audio_model_config: Конфигурация аудио модели {'checkpoint', 'model_type', 'max_audios', 'audio_agg', 'sr'}.\n    :param freeze: Заморозить ли веса всех моделей.\n    :param text_tokenizer_fn: Функция токенизации текста.\n    :param special_tokens: Специальные токены.\n    \"\"\"\n    name = \"flexible_multi\"\n    \n    def __init__(\n        self,\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        freeze: bool = True,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None\n    ):\n        super().__init__()\n        self.supported = set()\n        self.out_dim_per_modality = {}\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        \n        # Инициализация текстовой модели\n        self.text_model = None\n        self.text_processor = None\n        self.text_config = text_model_config or {}\n        if text_model_config:\n            self._init_text_model(text_model_config)\n            self.supported.add(\"text\")\n        \n        # Инициализация визуальной модели\n        self.image_model = None\n        self.image_processor = None\n        self.image_config = image_model_config or {}\n        if image_model_config:\n            self._init_image_model(image_model_config)\n            self.supported.add(\"image\")\n        \n        # Инициализация аудио модели\n        self.audio_model = None\n        self.audio_processor = None\n        self.audio_config = audio_model_config or {}\n        if audio_model_config:\n            self._init_audio_model(audio_model_config)\n            self.supported.add(\"audio\")\n        \n        if freeze:\n            self.freeze_all()\n    \n    def _init_text_model(self, config: Dict[str, Any]):\n        \"\"\"\n        Инициализирует текстовую модель согласно конфигурации.\n        \n        :param config: Словарь с 'checkpoint', 'model_type' и опциональными параметрами.\n        \"\"\"\n        from transformers import AutoModel, AutoTokenizer, CLIPTextModel, CLIPTokenizer\n        \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto')\n        \n        if model_type == 'clip':\n            self.text_model = CLIPTextModel.from_pretrained(checkpoint)\n            self.text_processor = CLIPTokenizer.from_pretrained(checkpoint)\n            # Для CLIP текстовой модели используем projection_dim\n            dim = self.text_model.config.projection_dim\n        elif model_type == 'bert' or model_type == 'auto':\n            self.text_model = AutoModel.from_pretrained(checkpoint)\n            self.text_processor = AutoTokenizer.from_pretrained(checkpoint)\n            dim = self.text_model.config.hidden_size\n        else:\n            raise ValueError(f\"Неизвестный model_type для текста: {model_type}\")\n        \n        self.text_config['max_length'] = config.get('max_length', 512)\n        self.text_config['dim'] = dim\n        self.text_config['model_type'] = model_type\n        self.out_dim_per_modality['text'] = dim\n    \n    def _init_image_model(self, config: Dict[str, Any]):\n        \"\"\"\n        Инициализирует визуальную модель согласно конфигурации.\n        \n        :param config: Словарь с 'checkpoint', 'model_type' и опциональными параметрами.\n        \"\"\"\n        from transformers import AutoModel, AutoImageProcessor, CLIPVisionModel, CLIPImageProcessor\n        \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto')\n        \n        if model_type == 'clip':\n            self.image_model = CLIPVisionModel.from_pretrained(checkpoint)\n            self.image_processor = CLIPImageProcessor.from_pretrained(checkpoint)\n            # Для CLIPVisionModel используем hidden_size, так как get_image_features не проецирует\n            dim = self.image_model.config.hidden_size\n        elif model_type in ['dinov2', 'vit', 'auto']:\n            self.image_model = AutoModel.from_pretrained(checkpoint)\n            self.image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n            dim = self.image_model.config.hidden_size\n        else:\n            raise ValueError(f\"Неизвестный model_type для изображений: {model_type}\")\n        \n        self.image_config['max_images'] = config.get('max_images', 1)\n        self.image_config['image_agg'] = config.get('image_agg', 'concat')\n        self.image_config['dim'] = dim\n        self.image_config['model_type'] = model_type\n        \n        if self.image_config['image_agg'] == 'concat':\n            self.out_dim_per_modality['image'] = dim * self.image_config['max_images']\n        else:\n            self.out_dim_per_modality['image'] = dim\n    \n    def _init_audio_model(self, config: Dict[str, Any]):\n        \"\"\"\n        Инициализирует аудио модель согласно конфигурации.\n        \n        :param config: Словарь с 'checkpoint', 'model_type' и опциональными параметрами.\n        \"\"\"\n        from transformers import AutoModel, AutoProcessor, ClapAudioModel, ClapProcessor\n        \n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto')\n        \n        if model_type == 'clap':\n            from transformers import ClapModel\n            self.audio_model = ClapModel.from_pretrained(checkpoint)\n            self.audio_processor = ClapProcessor.from_pretrained(checkpoint)\n            dim = getattr(self.audio_model.config, \"projection_dim\", 512)\n            sr = getattr(self.audio_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.audio_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n        elif model_type in ['whisper', 'wav2vec2', 'auto']:\n            self.audio_model = AutoModel.from_pretrained(checkpoint)\n            self.audio_processor = AutoProcessor.from_pretrained(checkpoint)\n            dim = self.audio_model.config.hidden_size\n            sr = self.audio_processor.feature_extractor.sampling_rate\n        else:\n            raise ValueError(f\"Неизвестный model_type для аудио: {model_type}\")\n        \n        self.audio_config['sr'] = config.get('sr', sr)\n        self.audio_config['max_audios'] = config.get('max_audios', 1)\n        self.audio_config['audio_agg'] = config.get('audio_agg', 'concat')\n        self.audio_config['dim'] = dim\n        self.audio_config['model_type'] = model_type\n        \n        if self.audio_config['audio_agg'] == 'concat':\n            self.out_dim_per_modality['audio'] = dim * self.audio_config['max_audios']\n        else:\n            self.out_dim_per_modality['audio'] = dim\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Собирает батч для всех активных модальностей с корректной обработкой пропусков.\n        \"\"\"\n        labels = torch.tensor(np.stack([b.get(\"labels\", np.array([0.0])) for b in batch]), dtype=torch.float32)\n        backend_inputs = {}\n        batch_size = len(batch)\n        \n        # Обработка текста\n        if self.text_model is not None:\n            texts = []\n            for b in batch:\n                text = b.get(\"text\", \"\")\n                # Если текст пустой или None, используем пробел как заглушку\n                texts.append(text if text else \" \")\n            \n            text_inputs = self.text_processor(\n                texts, padding=True, truncation=True,\n                max_length=self.text_config.get('max_length', 512),\n                return_tensors=\"pt\"\n            )\n            backend_inputs[\"text_inputs\"] = {k: v for k, v in text_inputs.items()}\n        \n        # Обработка изображений\n        if self.image_model is not None:\n            images_lists = [b.get(\"images\", []) for b in batch]\n            flat_images = []\n            img_counts = []\n            batch_indices = []  # Отслеживаем к какому сэмплу относится каждое изображение\n            \n            for idx, lst in enumerate(images_lists):\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                # Фильтруем None и пустые значения\n                lst = [img for img in lst if img is not None]\n                img_counts.append(len(lst))\n                for img in lst:\n                    flat_images.append(to_pil(img))\n                    batch_indices.append(idx)\n            \n            if len(flat_images) > 0:\n                img_proc = self.image_processor(images=flat_images, return_tensors=\"pt\")\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": img_proc[\"pixel_values\"]}\n            else:\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": None}\n            \n            backend_inputs[\"image_counts\"] = torch.tensor(img_counts, dtype=torch.long)\n            backend_inputs[\"image_batch_indices\"] = batch_indices\n        \n        # Обработка аудио\n        if self.audio_model is not None:\n            audios_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios = []\n            aud_counts = []\n            audio_batch_indices = []\n            \n            for idx, lst in enumerate(audios_lists):\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                # Фильтруем None и пустые значения\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_config['sr']))\n                    elif isinstance(a, np.ndarray):\n                        flat_audios.append(a.astype(np.float32))\n                    audio_batch_indices.append(idx)\n            \n            if len(flat_audios) > 0:\n                if self.audio_config.get('model_type') == 'clap':\n                    aud_proc = self.audio_processor(\n                        audios=flat_audios, \n                        sampling_rate=self.audio_config['sr'], \n                        padding=True, \n                        return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_features\": aud_proc[\"input_features\"]}\n                else:\n                    aud_proc = self.audio_processor(\n                        flat_audios, \n                        sampling_rate=self.audio_config['sr'],\n                        padding=True,\n                        return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_values\": aud_proc[\"input_values\"]}\n            else:\n                backend_inputs[\"audio_inputs\"] = {\"input_features\": None, \"input_values\": None}\n            \n            backend_inputs[\"audio_counts\"] = torch.tensor(aud_counts, dtype=torch.long)\n            backend_inputs[\"audio_batch_indices\"] = audio_batch_indices\n        \n        backend_inputs[\"batch_size\"] = batch_size\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _aggregate_embeddings(\n        self, \n        embs: Optional[torch.Tensor], \n        counts: List[int], \n        max_k: int, \n        dim: int, \n        agg_type: str,\n        batch_size: int,\n        device: torch.device\n    ) -> torch.Tensor:\n        \"\"\"\n        Безопасная агрегация эмбеддингов с правильной обработкой пустых сэмплов.\n        \"\"\"\n        # Определяем актуальную размерность\n        actual_dim = embs.size(1) if embs is not None and embs.numel() > 0 else dim\n        \n        # Создаем выходной тензор нужного размера\n        if agg_type == 'concat':\n            out_shape = (batch_size, actual_dim * max_k)\n        else:  # mean\n            out_shape = (batch_size, actual_dim)\n        \n        out = torch.zeros(out_shape, device=device, dtype=torch.float32)\n        \n        # Если нет эмбеддингов, возвращаем нули\n        if embs is None or embs.numel() == 0:\n            return out\n        \n        # Агрегируем эмбеддинги для каждого сэмпла\n        offset = 0\n        for i, count in enumerate(counts):\n            if count > 0:\n                sample_embs = embs[offset:offset + count]\n                \n                if agg_type == 'concat':\n                    # Берем до max_k эмбеддингов\n                    take = sample_embs[:max_k]\n                    # Паддинг если нужно\n                    if take.size(0) < max_k:\n                        pad = torch.zeros((max_k - take.size(0), actual_dim), \n                                        device=device, dtype=embs.dtype)\n                        take = torch.cat([take, pad], dim=0)\n                    out[i] = take.reshape(-1)\n                else:  # mean\n                    out[i] = sample_embs.mean(dim=0)\n                \n                offset += count\n        \n        return F.normalize(out, dim=-1)\n    \n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует все активные модальности через соответствующие модели.\n        \"\"\"\n        results = {}\n        \n        # Определяем актуальный размер батча из labels или первого доступного тензора\n        actual_batch_size = None\n        \n        # Пробуем определить размер батча из различных источников\n        if \"text_inputs\" in backend_inputs:\n            for v in backend_inputs[\"text_inputs\"].values():\n                if torch.is_tensor(v) and v.dim() > 0:\n                    actual_batch_size = v.size(0)\n                    break\n        \n        if actual_batch_size is None and \"image_counts\" in backend_inputs:\n            actual_batch_size = len(backend_inputs[\"image_counts\"])\n        \n        if actual_batch_size is None and \"audio_counts\" in backend_inputs:\n            actual_batch_size = len(backend_inputs[\"audio_counts\"])\n        \n        # Если все еще не определен, используем сохраненный\n        if actual_batch_size is None:\n            actual_batch_size = backend_inputs.get(\"batch_size\", 1)\n        \n        # Кодирование текста\n        if self.text_model is not None and \"text_inputs\" in backend_inputs:\n            text_inputs = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n            \n            # Обрезаем входы если нужно (для DataParallel)\n            if text_inputs.get(\"input_ids\") is not None:\n                current_batch_size = text_inputs[\"input_ids\"].size(0)\n                if current_batch_size != actual_batch_size:\n                    actual_batch_size = min(actual_batch_size, current_batch_size)\n                    text_inputs = {k: v[:actual_batch_size] if torch.is_tensor(v) else v \n                                  for k, v in text_inputs.items()}\n            \n            if self.text_config.get('model_type') == 'clip':\n                text_z = self.text_model.get_text_features(**text_inputs)\n            elif hasattr(self.text_model, 'get_text_features'):\n                text_z = self.text_model.get_text_features(**text_inputs)\n            else:\n                outputs = self.text_model(**text_inputs)\n                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n                    text_z = outputs.pooler_output\n                else:\n                    text_z = outputs.last_hidden_state.mean(dim=1)\n            \n            results[\"text\"] = F.normalize(text_z, dim=-1)\n            # Обновляем actual_batch_size на основе реального выхода\n            actual_batch_size = text_z.size(0)\n        \n        # Кодирование изображений\n        if self.image_model is not None and \"image_inputs\" in backend_inputs:\n            pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n            counts = backend_inputs[\"image_counts\"]\n            \n            # Обрезаем counts до actual_batch_size\n            if len(counts) > actual_batch_size:\n                counts = counts[:actual_batch_size]\n            counts = counts.tolist()\n            \n            # Проверяем, есть ли вообще изображения для обработки\n            total_images_needed = sum(counts)\n            \n            if pi is not None and pi.numel() > 0 and total_images_needed > 0:\n                pi = pi.to(device)\n                \n                # Обрезаем изображения согласно counts\n                if pi.size(0) > total_images_needed:\n                    pi = pi[:total_images_needed]\n                \n                if self.image_config.get('model_type') == 'clip':\n                    outputs = self.image_model(pixel_values=pi)\n                    img_flat = outputs.pooler_output\n                elif hasattr(self.image_model, 'get_image_features'):\n                    img_flat = self.image_model.get_image_features(pixel_values=pi)\n                else:\n                    outputs = self.image_model(pixel_values=pi)\n                    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n                        img_flat = outputs.pooler_output\n                    else:\n                        img_flat = outputs.last_hidden_state[:, 0]\n                \n                img_flat = F.normalize(img_flat, dim=-1)\n                actual_img_dim = img_flat.size(1)\n            else:\n                img_flat = None\n                actual_img_dim = self.image_config.get('dim', 768)\n            \n            # Используем безопасную агрегацию\n            img_z = self._aggregate_embeddings(\n                img_flat, counts,\n                self.image_config['max_images'],\n                actual_img_dim,\n                self.image_config['image_agg'],\n                len(counts),  # Используем длину counts как размер батча\n                device\n            )\n            \n            # Обновляем размерность если изменилась\n            if actual_img_dim != self.image_config.get('dim'):\n                self.image_config['dim'] = actual_img_dim\n                if self.image_config['image_agg'] == 'concat':\n                    self.out_dim_per_modality['image'] = actual_img_dim * self.image_config['max_images']\n                else:\n                    self.out_dim_per_modality['image'] = actual_img_dim\n            \n            results[\"image\"] = img_z\n        \n        # Кодирование аудио\n        if self.audio_model is not None and \"audio_inputs\" in backend_inputs:\n            counts = backend_inputs[\"audio_counts\"]\n            \n            # Обрезаем counts до actual_batch_size\n            if len(counts) > actual_batch_size:\n                counts = counts[:actual_batch_size]\n            counts = counts.tolist()\n            \n            # Проверяем, есть ли вообще аудио для обработки\n            total_audios_needed = sum(counts)\n            \n            # Получаем эмбеддинги аудио\n            aud_flat = None\n            actual_aud_dim = self.audio_config.get('dim', 768)\n            \n            if total_audios_needed > 0:  # Обрабатываем только если есть аудио\n                if self.audio_config.get('model_type') == 'clap':\n                    af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n                    if af is not None and af.numel() > 0:\n                        af = af.to(device)\n                        \n                        # Обрезаем аудио согласно counts\n                        if af.size(0) > total_audios_needed:\n                            af = af[:total_audios_needed]\n                        \n                        # Проверяем, что тензор не пустой после обрезки\n                        if af.numel() > 0:\n                            aud_flat = self.audio_model.get_audio_features(input_features=af)\n                            aud_flat = F.normalize(aud_flat, dim=-1)\n                            actual_aud_dim = aud_flat.size(1)\n                else:\n                    av = backend_inputs[\"audio_inputs\"][\"input_values\"]\n                    if av is not None and av.numel() > 0:\n                        av = av.to(device)\n                        \n                        # Обрезаем аудио согласно counts\n                        if av.size(0) > total_audios_needed:\n                            av = av[:total_audios_needed]\n                        \n                        # Проверяем, что тензор не пустой после обрезки\n                        if av.numel() > 0:\n                            outputs = self.audio_model(input_values=av)\n                            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n                                aud_flat = outputs.pooler_output\n                            else:\n                                aud_flat = outputs.last_hidden_state.mean(dim=1)\n                            aud_flat = F.normalize(aud_flat, dim=-1)\n                            actual_aud_dim = aud_flat.size(1)\n            \n            # Используем безопасную агрегацию (она обработает None для aud_flat)\n            aud_z = self._aggregate_embeddings(\n                aud_flat, counts,\n                self.audio_config['max_audios'],\n                actual_aud_dim,\n                self.audio_config['audio_agg'],\n                len(counts),  # Используем длину counts как размер батча\n                device\n            )\n            \n            # Обновляем размерность если изменилась\n            if aud_flat is not None and actual_aud_dim != self.audio_config.get('dim'):\n                self.audio_config['dim'] = actual_aud_dim\n                if self.audio_config['audio_agg'] == 'concat':\n                    self.out_dim_per_modality['audio'] = actual_aud_dim * self.audio_config['max_audios']\n                else:\n                    self.out_dim_per_modality['audio'] = actual_aud_dim\n            \n            results[\"audio\"] = aud_z\n        \n        # Убеждаемся, что все результаты имеют одинаковый размер батча\n        if results:\n            min_batch_size = min(v.size(0) for v in results.values())\n            if any(v.size(0) != min_batch_size for v in results.values()):\n                results = {k: v[:min_batch_size] for k, v in results.items()}\n        \n        return results\n\n\nclass SingleBackboneRegressor(nn.Module):\n    \"\"\"\n    Регрессор поверх одного мультимодального бэкенда: encode -> fuse -> MLP голова.\n\n    :param backend: Экземпляр бэкенда (CLIP/CLAP/FlexibleMultiBackend).\n    :param modalities: Активные модальности (учёт порядка важен при concat): подмножество ['image','text','audio'].\n    :param num_targets: Количество целевых переменных для регрессии.\n    :param fusion: 'concat' (объединение признаков) или 'mean' (среднее по модальностям).\n    :param hidden: Размер скрытого слоя головы.\n    :param dropout: Дропаут в голове.\n    \"\"\"\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_targets: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_targets = num_targets\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать, а у нас: {dict(zip(order, dims))}')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_targets)\n        )\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        \"\"\"\n        Находит девайс по первому тензору во входах; иначе выбирает доступный cuda/cpu.\n\n        :param obj: Любая структура с тензорами.\n        :return: torch.device.\n        \"\"\"\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Объединяет эмбеддинги модальностей согласно self.fusion.\n\n        :param z: Словарь эмбеддингов по модальностям.\n        :return: Fused тензор [B, *].\n        \"\"\"\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        batch_size = None\n        \n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() == 3:\n                    t = t.mean(dim=1)\n                elif t.dim() > 3:\n                    t = t.view(t.size(0), -1)\n                feats.append(t)\n                if batch_size is None:\n                    batch_size = t.size(0)\n        \n        # Убеждаемся, что все тензоры имеют одинаковый размер батча\n        if batch_size is not None:\n            feats = [f[:batch_size] for f in feats]\n        \n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            sizes = [f.size(-1) for f in feats]\n            if len(set(sizes)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать, а у нас: {sizes}')\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        \"\"\"\n        Прямой проход модели.\n\n        :param backend_inputs: Входы для бэкенда (из его collate).\n        :param labels: Игнорируется (loss считает Trainer).\n        :return: SequenceClassifierOutput с logits [B, num_targets].\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        \n        # Кодируем модальности\n        z = self.backend.encode(backend_inputs, device=device)\n        \n        # Проверяем, что получили эмбеддинги\n        if not z:\n            raise ValueError(\"Backend не вернул эмбеддинги\")\n        \n        # Объединяем эмбеддинги модальностей\n        fused = self._fuse(z)\n        \n        # Пропускаем через регрессионную голову\n        logits = self.head(fused)\n        \n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        \"\"\"\n        Извлекает fused эмбеддинги (и опционально по модальностям).\n\n        :param backend_inputs: Входы для бэкенда.\n        :param return_per_modality: Вернуть также словарь {'text','image','audio'}.\n        :return: fused [B, *] или (fused, per_modality).\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\nclass RegressionTrainer(Trainer):\n    \"\"\"\n    Trainer с MSELoss для регрессии множественных целевых переменных.\n    Поддерживает опциональные веса для разных целевых переменных.\n\n    :param num_targets: Количество целевых переменных.\n    :param target_weights: Опциональные веса для каждой целевой переменной.\n    :param loss_type: Тип функции потерь ('mse', 'mae', 'huber').\n    \"\"\"\n    def __init__(self, *args, num_targets=None, target_weights=None, loss_type='mse', **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_targets = num_targets\n        self.loss_type = loss_type.lower()\n        \n        if target_weights is not None:\n            self.target_weights = torch.as_tensor(target_weights, dtype=torch.float32)\n        else:\n            self.target_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        \"\"\"\n        Считает регрессионную функцию потерь (MSE/MAE/Huber).\n        Корректно обрабатывает DataParallel и защищается от NaN.\n\n        :param model: Модель.\n        :param inputs: Батч: {'labels': FloatTensor[B, num_targets], 'backend_inputs': {...}}.\n        :param return_outputs: Возвращать ли outputs вместе с loss.\n        :param num_items_in_batch: Совместимость с Trainer API (не используется).\n        :return: loss (и outputs, если return_outputs=True).\n        \"\"\"\n        labels = inputs.pop(\"labels\")\n        \n        # Проверяем, используется ли DataParallel\n        is_parallel = isinstance(model, (nn.DataParallel, nn.parallel.DistributedDataParallel))\n        \n        # Вызываем forward модели\n        outputs = model(**inputs)\n        predictions = outputs.logits\n        \n        # Переносим labels на устройство predictions\n        labels = labels.to(predictions.device)\n        \n        # Обработка несоответствия размеров при DataParallel\n        if predictions.size(0) != labels.size(0):\n            # Если predictions меньше labels (может быть при разделении батча между GPU)\n            if predictions.size(0) < labels.size(0):\n                # Обрезаем labels до размера predictions\n                labels = labels[:predictions.size(0)]\n            # Если predictions больше labels (DataParallel может дублировать)\n            elif is_parallel:\n                # Для DataParallel: повторяем labels для каждой реплики\n                num_replicas = predictions.size(0) // labels.size(0)\n                if predictions.size(0) == labels.size(0) * num_replicas:\n                    labels = labels.repeat_interleave(num_replicas, dim=0)\n                else:\n                    # Если размеры не кратны, берем первые predictions.size(0) элементов\n                    labels = labels.repeat(num_replicas + 1, dim=0)[:predictions.size(0)]\n            else:\n                # В других случаях просто обрезаем до минимального размера\n                min_size = min(predictions.size(0), labels.size(0))\n                predictions = predictions[:min_size]\n                labels = labels[:min_size]\n        \n        # Проверка на NaN и Inf в predictions\n        if torch.isnan(predictions).any() or torch.isinf(predictions).any():\n            # Заменяем NaN и Inf на нули\n            predictions = torch.nan_to_num(predictions, nan=0.0, posinf=1e4, neginf=-1e4)\n        \n        # Вычисляем loss\n        try:\n            if self.loss_type == 'mse':\n                loss = F.mse_loss(predictions, labels, reduction='none')\n            elif self.loss_type == 'mae':\n                loss = F.l1_loss(predictions, labels, reduction='none')\n            elif self.loss_type == 'huber':\n                loss = F.huber_loss(predictions, labels, reduction='none', delta=1.0)\n            else:\n                raise ValueError(f\"Неизвестный тип функции потерь: {self.loss_type}\")\n            \n            # Применяем веса если есть\n            if self.target_weights is not None:\n                weights = self.target_weights.to(loss.device)\n                # Расширяем веса до размера батча\n                if weights.dim() == 1 and loss.dim() == 2:\n                    weights = weights.unsqueeze(0).expand_as(loss)\n                loss = loss * weights\n            \n            # Усредняем по всем измерениям\n            loss = loss.mean()\n            \n            # Проверка на NaN в loss\n            if torch.isnan(loss) or torch.isinf(loss):\n                # Если loss NaN, возвращаем малое значение\n                loss = torch.tensor(0.01, device=predictions.device, requires_grad=True)\n                \n        except Exception as e:\n            # В случае любой ошибки возвращаем малое значение loss\n            print(f\"Warning: Error computing loss: {e}\")\n            loss = torch.tensor(0.01, device=predictions.device, requires_grad=True)\n        \n        return (loss, outputs) if return_outputs else loss\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\nclass SingleModelMultiComboRegression:\n    \"\"\"\n    Пайплайн: одна мультимодальная модель (бэкенд) + регрессионная голова + HuggingFace Trainer.\n    Поддерживает регрессию множественных целевых переменных.\n\n    Поддерживаемые комбинации модальностей:\n      - ['text','image']         -> ClipBackend или FlexibleMultiBackend\n      - ['text','audio']         -> ClapBackend или FlexibleMultiBackend\n      - ['image','audio']        -> FlexibleMultiBackend\n      - ['text','image','audio'] -> FlexibleMultiBackend\n\n    Возможности:\n      - Мульти-изображения/аудио per-сэмпл (concat/mean агрегация).\n      - Обучение на больших данных: чанковая подстановка train_dataset, стабильный прогресс‑бар и логи.\n      - Множественные целевые переменные с опциональными весами.\n      - Гибкая архитектура: возможность использовать разные модели для разных модальностей.\n      - Кастомная токенизация текста через переданную функцию.\n      - Различные функции потерь (MSE, MAE, Huber).\n\n    Необходимые импорты:\n    !pip install -q wav2clip torchaudio evaluate pillow\n    import gc\n    import math\n    from typing import List, Dict, Any, Optional, Union, Tuple, Callable\n    import numpy as np\n    import pandas as pd\n    from PIL import Image\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import Dataset, DataLoader\n    import evaluate\n    from transformers import Trainer, TrainingArguments, TrainerCallback, PrinterCallback\n    from transformers.modeling_outputs import SequenceClassifierOutput\n    from tqdm.auto import tqdm\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        num_targets: int,\n        target_column_names: List[str],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1,\n        target_normalizer: Optional[Callable] = None,\n        target_denormalizer: Optional[Callable] = None\n    ):\n        \"\"\"\n        :param modalities: Список модальностей ('text','image','audio') в любом порядке.\n        :param num_targets: Количество целевых переменных.\n        :param target_column_names: Список имён столбцов целевых переменных в DataFrame.\n        :param text_columns: Имена текстовых колонок.\n        :param image_columns: Имена колонок изображений (значения — пути или списки путей/объектов).\n        :param audio_columns: Имена колонок аудио (значения — пути/массивы или списки).\n        :param text_tokenizer_fn: Функция токенизации текста. Принимает dict колонок и special_tokens.\n        :param special_tokens: Специальные токены для токенизатора.\n        :param backend: 'auto' | 'clip' | 'clap' | 'flexible'.\n        :param clip_checkpoint: Чекпоинт CLIP (для backend='clip').\n        :param clap_checkpoint: Чекпоинт CLAP (для backend='clap').\n        :param text_model_config: Конфиг текстовой модели для FlexibleMultiBackend.\n        :param image_model_config: Конфиг визуальной модели для FlexibleMultiBackend.\n        :param audio_model_config: Конфиг аудио модели для FlexibleMultiBackend.\n        :param fusion: 'concat' или 'mean' — тип фьюжна эмбеддингов.\n        :param freeze_backbone: Заморозить веса бэкенда (linear probing).\n        :param clip_max_length: Максимальная длина токенов в CLIP.\n        :param max_images_per_sample: Максимум картинок при concat-агрегации.\n        :param max_audios_per_sample: Максимум аудио при concat-агрегации.\n        :param target_normalizer: Опциональная функция нормализации целевых значений.\n        :param target_denormalizer: Опциональная функция денормализации предсказаний.\n        \"\"\"\n        self.modalities = sorted(list(set(modalities)))\n        self.num_targets = num_targets\n        self.target_column_names = target_column_names\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.backend_name = backend\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n        self.target_normalizer = target_normalizer\n        self.target_denormalizer = target_denormalizer\n\n        # Проверка соответствия\n        if len(self.target_column_names) != self.num_targets:\n            raise ValueError(f\"Количество target_column_names ({len(self.target_column_names)}) \"\n                           f\"не соответствует num_targets ({self.num_targets})\")\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneRegressor] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[PbarConsoleLogger] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        \"\"\"\n        Инициализирует бэкенд согласно backend='auto'|'clip'|'clap'|'flexible' и проверяет совместимость модальностей.\n\n        :raises ValueError: При неподдерживаемой комбинации модальностей.\n        \"\"\"\n        mods = set(self.modalities)\n        name = self.backend_name\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                name = \"clip\"\n            elif mods == {\"text\", \"audio\"}:\n                name = \"clap\"\n            else:\n                name = \"flexible\"\n\n        if name == \"clip\":\n            self.backend = ClipBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                max_images=self.max_images_per_sample,\n                image_agg=\"concat\",\n                text_tokenizer_fn=self.text_tokenizer_fn,\n                special_tokens=self.special_tokens\n            )\n        elif name == \"clap\":\n            self.backend = ClapBackend(\n                checkpoint=self.clap_checkpoint,\n                freeze=self.freeze_backbone,\n                max_audios=self.max_audios_per_sample,\n                audio_agg=\"concat\",\n                text_tokenizer_fn=self.text_tokenizer_fn,\n                special_tokens=self.special_tokens\n            )\n        elif name == \"flexible\":\n            # Автоматическая конфигурация если не заданы модели\n            if \"text\" in mods and self.text_model_config is None:\n                self.text_model_config = {\n                    'checkpoint': 'bert-base-uncased',\n                    'model_type': 'bert',\n                    'max_length': 512\n                }\n            if \"image\" in mods and self.image_model_config is None:\n                self.image_model_config = {\n                    'checkpoint': 'google/vit-base-patch16-224',\n                    'model_type': 'vit',\n                    'max_images': self.max_images_per_sample,\n                    'image_agg': 'concat'\n                }\n            if \"audio\" in mods and self.audio_model_config is None:\n                self.audio_model_config = {\n                    'checkpoint': self.clap_checkpoint,\n                    'model_type': 'clap',\n                    'max_audios': self.max_audios_per_sample,\n                    'audio_agg': 'concat',\n                    'sr': 48000\n                }\n            \n            self.backend = FlexibleMultiBackend(\n                text_model_config=self.text_model_config if \"text\" in mods else None,\n                image_model_config=self.image_model_config if \"image\" in mods else None,\n                audio_model_config=self.audio_model_config if \"audio\" in mods else None,\n                freeze=self.freeze_backbone,\n                text_tokenizer_fn=self.text_tokenizer_fn,\n                special_tokens=self.special_tokens\n            )\n        else:\n            raise ValueError(f\"Неизвестный backend: {name}\")\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}. \"\n                             f\"Поддерживает: {self.backend.supported}\")\n\n    def _setup_metrics(self, metric_names: Union[str, List[str]]):\n        \"\"\"\n        Создаёт функцию подсчёта метрик для Trainer.\n\n        :param metric_names: 'mse', 'mae', 'r2' или список таких метрик.\n        \"\"\"\n        if isinstance(metric_names, str):\n            metric_names = [metric_names]\n        \n        metric_names = [m.lower() for m in metric_names]\n        \n        def compute(p):\n            preds = p.predictions\n            labels = p.label_ids\n            results = {}\n            \n            for metric_name in metric_names:\n                if metric_name == \"mse\":\n                    mse = np.mean((preds - labels) ** 2)\n                    results[\"mse\"] = float(mse)\n                elif metric_name == \"mae\":\n                    mae = np.mean(np.abs(preds - labels))\n                    results[\"mae\"] = float(mae)\n                elif metric_name == \"r2\":\n                    # R² score для каждой целевой переменной, затем усредняем\n                    from sklearn.metrics import r2_score\n                    if preds.ndim == 1:\n                        r2 = r2_score(labels, preds)\n                    else:\n                        r2_scores = []\n                        for i in range(preds.shape[1]):\n                            r2_scores.append(r2_score(labels[:, i], preds[:, i]))\n                        r2 = np.mean(r2_scores)\n                    results[\"r2\"] = float(r2)\n                else:\n                    raise ValueError(f'Неизвестная метрика: {metric_name}')\n            \n            return results\n        \n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        \"\"\"\n        Перемешивает и делит DataFrame на train/eval.\n\n        :param df: Полный датафрейм.\n        :param test_size: Доля валидации (0..1).\n        :param seed: Зерно.\n        :return: (df_train, df_eval).\n        \"\"\"\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data(self, df: pd.DataFrame):\n        \"\"\"\n        Проверяет соответствие колонок выбранным модальностям и наличие целевых колонок.\n\n        :param df: Источник данных.\n        :raises ValueError: При отсутствии необходимых колонок.\n        \"\"\"\n        # Проверка целевых колонок\n        missing_targets = [c for c in self.target_column_names if c not in df.columns]\n        if missing_targets:\n            raise ValueError(f\"В DataFrame отсутствуют целевые колонки: {missing_targets}\")\n        \n        # Проверка модальностей\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_names: Union[str, List[str]] = \"mse\",\n        loss_type: str = \"mse\",\n        target_weights: Optional[List[float]] = None,\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1,\n        gradient_checkpointing: bool = False,\n        fit_chunk_size: Optional[int] = None\n    ):\n        \"\"\"\n        Обучает регрессионную голову поверх выбранного бэкенда.\n        Поддерживает обучение на больших данных за счёт чанков: train_dataset подставляется кусками.\n\n        :param train_data: Полный датафрейм с данными и таргетами.\n        :param epochs: Количество эпох.\n        :param test_size: Доля валидации.\n        :param per_device_train_batch_size: Размер батча на устройство.\n        :param gradient_accumulation_steps: Шаги аккумуляции градиента.\n        :param learning_rate: Learning rate для оптимизатора.\n        :param metric_names: 'mse', 'mae', 'r2' или список метрик для валидации.\n        :param loss_type: 'mse', 'mae' или 'huber' — функция потерь для обучения.\n        :param target_weights: Опциональные веса для каждой целевой переменной.\n        :param fp16: Использовать fp16 при наличии CUDA (если доступен bf16 — он будет использован вместо fp16).\n        :param logging_steps: Частота логирования шагов.\n        :param eval_steps: Шаги между валидациями/сохранениями.\n        :param output_dir: Каталог для артефактов.\n        :param seed: Зерно.\n        :param hidden: Размер скрытого слоя головы.\n        :param dropout: Дропаут в голове.\n        :param gradient_checkpointing: Делать ли чекпоинты во время обучения для экономии VRAM.\n        :param fit_chunk_size: Размер чанка обучающей выборки. Если None — весь train как один чанк.\n        :return: self.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n\n        # Датасет валидации держим целиком (обычно небольшой).\n        ds_eval = MultiComboRegressionDataset(\n            df_eval, self.target_column_names,\n            self.text_columns, self.image_columns, self.audio_columns,\n            self.text_tokenizer_fn, self.special_tokens,\n            self.target_normalizer\n        )\n\n        # Модель и метрики\n        self.model = SingleBackboneRegressor(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_targets=self.num_targets,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        self._setup_metrics(metric_names)\n\n        # Определяем метрику для выбора лучшей модели\n        if isinstance(metric_names, str):\n            best_metric = f\"eval_{metric_names}\"\n        else:\n            # Используем первую метрику из списка\n            best_metric = f\"eval_{metric_names[0]}\"\n\n        # Настройки точности\n        bf16_ok = bool(torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=best_metric,\n            greater_is_better=best_metric == \"eval_r2\",  # Для R² больше — лучше, для MSE/MAE меньше\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=bool(fp16 and torch.cuda.is_available() and not bf16_ok),\n            bf16=bool(bf16_ok and not fp16),\n            dataloader_num_workers=os.cpu_count(),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def data_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            \"\"\"\n            Адаптер collate для Trainer: делегирует бэкенду сборку батча.\n\n            :param batch_list: Список элементов Dataset.\n            :return: Батч для model.forward(): {'labels': FloatTensor, 'backend_inputs': {...}}.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        # Вспомогательные функции для чанков\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            \"\"\"\n            Оценивает число шагов оптимизации на чанке размера sz.\n\n            :param sz: Количество примеров в чанке.\n            :param bsz: Размер батча.\n            :param accum: Шаги аккумуляции.\n            :return: Число оптимизационных шагов.\n            \"\"\"\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            \"\"\"\n            Генератор срезов индексов по chunk_size.\n\n            :param index_array: Индексы обучающей выборки.\n            :param chunk_size: Размер чанка.\n            :yield: Срез индексов.\n            \"\"\"\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        # Индексы train\n        n_train = len(df_train)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n\n        # Чанк по умолчанию — весь train\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        # Предварительный рассчёт общего числа шагов (для прогресс‑бара и планировщика)\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        # Инициализация Trainer с «пустым» train датасетом (минимальный чанк), чтобы не держать весь train\n        dummy_idx = np.arange(min(len(df_train), 1))\n        ds_train_init = MultiComboRegressionDataset(\n            df_train.iloc[dummy_idx], self.target_column_names,\n            self.text_columns, self.image_columns, self.audio_columns,\n            self.text_tokenizer_fn, self.special_tokens,\n            self.target_normalizer\n        ) if len(dummy_idx) > 0 else ds_eval\n\n        self.trainer = RegressionTrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            num_targets=self.num_targets,\n            target_weights=target_weights,\n            loss_type=loss_type\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        # Планировщик на рассчитанное количество шагов\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        # Внешний прогресс‑бар + консольный лог\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        # Основной цикл обучения по эпохам и чанкам\n        steps_done = 0\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n\n            for slc in chunk_slices(shuffled, chunk_size):\n                # Подставляем чанк\n                chunk_df = df_train.iloc[slc]\n                ds_chunk = MultiComboRegressionDataset(\n                    chunk_df, self.target_column_names,\n                    self.text_columns, self.image_columns, self.audio_columns,\n                    self.text_tokenizer_fn, self.special_tokens,\n                    self.target_normalizer\n                )\n                self.trainer.train_dataset = ds_chunk\n\n                # Считаем шаги на чанке и настраиваем max_steps Trainer\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk, chunk_df\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                # Очистка памяти между чанками\n                del ds_chunk, chunk_df\n                gc.collect()\n                if torch.cuda.is_available():\n                    for i in range(torch.cuda.device_count()):\n                        with torch.cuda.device(i):\n                            torch.cuda.empty_cache()\n                            torch.cuda.ipc_collect()  # Очистка IPC памяти\n                    \n                    # Синхронизация всех GPU (важно при DataParallel)\n                    if torch.cuda.device_count() > 1:\n                        for i in range(torch.cuda.device_count()):\n                            torch.cuda.synchronize(i)\n                    \n                    # Дополнительная очистка (если используется)\n                    if hasattr(torch.cuda, 'reset_peak_memory_stats'):\n                        for i in range(torch.cuda.device_count()):\n                            torch.cuda.reset_peak_memory_stats(i)\n\n        pbar.close()\n        return self\n\n    def predict(self, df: pd.DataFrame, return_denormalized: bool = True) -> np.ndarray:\n        \"\"\"\n        Делает предсказания регрессионных значений на новых данных.\n\n        :param df: Датафрейм с теми же колонками модальностей, что и при обучении.\n        :param return_denormalized: Если True и есть denormalizer — применить денормализацию.\n        :return: np.ndarray предсказанных значений [N, num_targets].\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        \n        df_c = df.copy()\n        # Добавляем фиктивные целевые колонки если их нет\n        for col in self.target_column_names:\n            if col not in df_c.columns:\n                df_c[col] = 0.0\n        \n        ds = MultiComboRegressionDataset(\n            df_c, self.target_column_names,\n            self.text_columns, self.image_columns, self.audio_columns,\n            self.text_tokenizer_fn, self.special_tokens,\n            self.target_normalizer\n        )\n        \n        preds = self.trainer.predict(test_dataset=ds)\n        predictions = preds.predictions\n        \n        # Применяем денормализацию если нужно\n        if return_denormalized and self.target_denormalizer is not None:\n            predictions = np.array([self.target_denormalizer(p) for p in predictions])\n        \n        return predictions\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        \"\"\"\n        Извлекает fused эмбеддинги (и опционально по модальностям) для новых данных.\n\n        :param df: Датафрейм с нужными колонками модальностей.\n        :param batch_size: Размер батча при инференсе.\n        :param return_per_modality: Вернуть также словарь эмбеддингов {'text','image','audio'}.\n        :return: np.ndarray fused [N, D_fused] или (fused, per_modality_dict).\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        # Определяем девайс модели\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = df.copy()\n        # Добавляем фиктивные целевые колонки если их нет\n        for col in self.target_column_names:\n            if col not in df_c.columns:\n                df_c[col] = 0.0\n\n        ds = MultiComboRegressionDataset(\n            df_c, self.target_column_names,\n            self.text_columns, self.image_columns, self.audio_columns,\n            self.text_tokenizer_fn, self.special_tokens,\n            self.target_normalizer\n        )\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            \"\"\"\n            Collate для DataLoader при извлечении эмбеддингов.\n\n            :param batch_list: Список элементов.\n            :return: Батч 'backend_inputs' для модели.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device: torch.device):\n            \"\"\"\n            Рекурсивно переносит тензоры на device.\n\n            :param obj: Тензор/словарь/список/кортеж/прочее.\n            :param device: torch.device.\n            :return: Объект с перенесёнными тензорами.\n            \"\"\"\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        with torch.no_grad():\n            for batch in loader:\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        return fused_arr, per_mod\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Очистка памяти\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'encoders'):\n            for encoder in self.encoders.values():\n                if hasattr(encoder, 'model'):\n                    del encoder.model\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-09-02T10:44:47.214662Z","iopub.execute_input":"2025-09-02T10:44:47.214965Z","iopub.status.idle":"2025-09-02T10:46:54.801871Z","shell.execute_reply.started":"2025-09-02T10:44:47.214930Z","shell.execute_reply":"2025-09-02T10:46:54.800725Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-09-02 10:46:32.435412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756809992.688290      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756809992.762199      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Создание фиктивных данных.","metadata":{}},{"cell_type":"code","source":"# --- 1. Подготовка и генерация сложных данных для регрессии ---\n\n# Установка/проверка библиотек\n!pip install -q scipy transformers evaluate accelerate scikit-learn\n\nimport os\nimport shutil\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom scipy.io.wavfile import write as write_wav\nfrom typing import List, Dict, Any, Optional\n\ndef create_complex_regression_data(\n    num_samples: int = 1000,\n    noise_level: float = 0.1,\n    missing_rate: float = 0.05\n) -> pd.DataFrame:\n    \"\"\"\n    Создает сложный фиктивный DataFrame для демонстрации регрессии.\n    Предсказываем несколько метрик видео:\n    - popularity_score: 0-100 (популярность)\n    - engagement_rate: 0-1 (вовлеченность)\n    - quality_rating: 1-10 (оценка качества)\n    - duration_minutes: 0-60 (длительность в минутах)\n    \"\"\"\n    data_dir = \"./complex_regression_data\"\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    rng = np.random.default_rng(42)\n    \n    # Скрытые факторы, влияющие на целевые переменные\n    content_quality = rng.uniform(0, 1, num_samples)\n    production_value = rng.uniform(0, 1, num_samples)\n    topic_relevance = rng.uniform(0, 1, num_samples)\n\n    data = []\n    for i in range(num_samples):\n        sample = {'id': f'video_{i}'}\n        \n        # Базовые значения для целевых переменных (зависят от скрытых факторов)\n        base_popularity = 30 * content_quality[i] + 40 * topic_relevance[i] + 20 * production_value[i]\n        base_engagement = 0.3 * content_quality[i] + 0.5 * topic_relevance[i] + 0.2 * production_value[i]\n        base_quality = 3 + 4 * production_value[i] + 3 * content_quality[i]\n        base_duration = 5 + 30 * production_value[i] + 15 * content_quality[i]\n\n        # --- Текстовая модальность ---\n        if rng.random() > 0.1:  # 10% пропусков текста\n            # Текст влияет на popularity и engagement\n            text_quality = rng.uniform(0.5, 1.5)\n            sample['title'] = f\"Video Title {i}: {'Amazing' if text_quality > 1 else 'Standard'} Content\"\n            sample['description'] = f\"This video covers topic {i % 20} with {'high' if text_quality > 1 else 'medium'} quality production.\"\n            \n            # Текст влияет на метрики\n            base_popularity += 10 * text_quality\n            base_engagement *= text_quality\n        else:\n            sample['title'] = None\n            sample['description'] = \"\"\n\n        # --- Визуальная модальность ---\n        if rng.random() > 0.05:  # 5% пропусков изображений\n            num_images = rng.integers(1, 6)  # от 1 до 5 кадров\n            visual_quality = rng.uniform(0.7, 1.3)\n            img_paths = []\n            for j in range(num_images):\n                path = os.path.join(data_dir, f\"frame_{i}_{j}.png\")\n                # Создаём изображения с разной яркостью (имитация качества)\n                brightness = int(128 + 50 * visual_quality)\n                img = Image.fromarray(\n                    rng.integers(brightness-50, brightness+50, (224, 224, 3), dtype=np.uint8)\n                )\n                img.save(path)\n                img_paths.append(path)\n            sample['keyframes'] = img_paths\n            \n            # Визуальное качество влияет на quality_rating и popularity\n            base_quality += visual_quality\n            base_popularity += 5 * visual_quality\n        else:\n            sample['keyframes'] = []\n\n        # --- Аудио модальность ---\n        if rng.random() > 0.15:  # 15% пропусков аудио\n            num_audios = rng.integers(1, 4)  # от 1 до 3 аудиоклипов\n            audio_complexity = rng.uniform(0.6, 1.4)\n            audio_paths = []\n            for j in range(num_audios):\n                path = os.path.join(data_dir, f\"audio_{i}_{j}.wav\")\n                sr = 48000\n                duration = 1.5 * audio_complexity\n                t = np.linspace(0., duration, int(sr * duration))\n                amplitude = np.iinfo(np.int16).max * 0.3\n                # Более сложный аудио = несколько частот\n                freq1 = rng.uniform(200, 600)\n                freq2 = rng.uniform(800, 1200) * audio_complexity\n                waveform = amplitude * (\n                    0.6 * np.sin(2. * np.pi * freq1 * t) + \n                    0.4 * np.sin(2. * np.pi * freq2 * t)\n                )\n                waveform = waveform.astype(np.int16)\n                write_wav(path, sr, waveform)\n                audio_paths.append(path)\n            sample['audio_tracks'] = audio_paths\n            \n            # Аудио влияет на engagement и duration\n            base_engagement += 0.1 * audio_complexity\n            base_duration += 5 * audio_complexity\n        else:\n            sample['audio_tracks'] = None\n\n        # Добавляем шум к целевым переменным\n        noise = rng.normal(0, noise_level, 4)\n        \n        # Финальные целевые переменные с ограничениями\n        sample['popularity_score'] = np.clip(base_popularity + noise[0] * 10, 0, 100)\n        sample['engagement_rate'] = np.clip(base_engagement + noise[1] * 0.1, 0, 1)\n        sample['quality_rating'] = np.clip(base_quality + noise[2], 1, 10)\n        sample['duration_minutes'] = np.clip(base_duration + noise[3] * 5, 0, 60)\n        \n        # Иногда добавляем пропущенные значения\n        if rng.random() < missing_rate:\n            target_to_miss = rng.choice(['popularity_score', 'engagement_rate', 'quality_rating', 'duration_minutes'])\n            sample[target_to_miss] = np.nan\n        \n        data.append(sample)\n\n    df = pd.DataFrame(data)\n    \n    # Заполняем пропущенные значения медианами\n    for col in ['popularity_score', 'engagement_rate', 'quality_rating', 'duration_minutes']:\n        df[col].fillna(df[col].median(), inplace=True)\n    \n    print(f\"Создан DataFrame размером {df.shape}\")\n    print(\"\\nСтатистика целевых переменных:\")\n    target_cols = ['popularity_score', 'engagement_rate', 'quality_rating', 'duration_minutes']\n    print(df[target_cols].describe())\n    print(\"\\nКорреляция между целевыми переменными:\")\n    print(df[target_cols].corr().round(2))\n    print(\"\\nПример строки:\")\n    print(df.iloc[rng.integers(0, num_samples)].to_dict())\n    return df\n\n# Генерация данных\nlarge_regression_df = create_complex_regression_data(num_samples=50)  # Уменьшено для быстрого запуска","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-09-02T10:46:54.803623Z","iopub.execute_input":"2025-09-02T10:46:54.804264Z","iopub.status.idle":"2025-09-02T10:47:01.090945Z","shell.execute_reply.started":"2025-09-02T10:46:54.804231Z","shell.execute_reply":"2025-09-02T10:47:01.089786Z"}},"outputs":[{"name":"stdout","text":"Создан DataFrame размером (50, 9)\n\nСтатистика целевых переменных:\n       popularity_score  engagement_rate  quality_rating  duration_minutes\ncount         50.000000        50.000000       50.000000         50.000000\nmean          57.843007         0.547682        7.266436         30.337809\nstd           15.695369         0.212910        1.338546          9.705532\nmin           12.860543         0.111881        4.766225         11.072555\n25%           46.880115         0.406967        6.380709         24.031727\n50%           60.992039         0.525003        7.140203         30.515140\n75%           68.173288         0.659600        8.207817         36.114128\nmax           92.587913         1.000000       10.000000         50.589904\n\nКорреляция между целевыми переменными:\n                  popularity_score  engagement_rate  quality_rating  \\\npopularity_score              1.00             0.81            0.65   \nengagement_rate               0.81             1.00            0.40   \nquality_rating                0.65             0.40            1.00   \nduration_minutes              0.60             0.37            0.96   \n\n                  duration_minutes  \npopularity_score              0.60  \nengagement_rate               0.37  \nquality_rating                0.96  \nduration_minutes              1.00  \n\nПример строки:\n{'id': 'video_17', 'title': None, 'description': '', 'keyframes': ['./complex_regression_data/frame_17_0.png', './complex_regression_data/frame_17_1.png', './complex_regression_data/frame_17_2.png'], 'audio_tracks': None, 'popularity_score': 43.16699883652734, 'engagement_rate': 0.4335764590931506, 'quality_rating': 5.587711450309015, 'duration_minutes': 15.680882324987596}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/1982614490.py:133: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df[col].fillna(df[col].median(), inplace=True)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"# --- 2. Обучение регрессионной модели ---\n\n# Функции нормализации для целевых переменных (опционально)\ndef normalize_targets(targets: np.ndarray) -> np.ndarray:\n    \"\"\"Нормализует целевые переменные в диапазон [0, 1]\"\"\"\n    # targets: [popularity_score, engagement_rate, quality_rating, duration_minutes]\n    normalized = targets.copy()\n    normalized[0] = targets[0] / 100.0  # popularity: 0-100 -> 0-1\n    normalized[1] = targets[1]  # engagement: уже 0-1\n    normalized[2] = (targets[2] - 1) / 9.0  # quality: 1-10 -> 0-1\n    normalized[3] = targets[3] / 60.0  # duration: 0-60 -> 0-1\n    return normalized\n\ndef denormalize_targets(normalized: np.ndarray) -> np.ndarray:\n    \"\"\"Денормализует предсказания обратно в исходные диапазоны\"\"\"\n    targets = normalized.copy()\n    targets[0] = targets[0] * 100.0  # popularity\n    targets[1] = targets[1]  # engagement\n    targets[2] = targets[2] * 9.0 + 1  # quality\n    targets[3] = targets[3] * 60.0  # duration\n    return targets\n\n# Инициализация регрессора\nvideo_regressor = SingleModelMultiComboRegression(\n    modalities=['text', 'image', 'audio'],\n    num_targets=4,  # Предсказываем 4 метрики\n    target_column_names=[\n        'popularity_score', \n        'engagement_rate', \n        'quality_rating', \n        'duration_minutes'\n    ],\n    text_columns=['title', 'description'],\n    image_columns=['keyframes'],\n    audio_columns=['audio_tracks'],\n    \n    # Конфигурация моделей\n    backend='flexible',\n    text_model_config={\n        'checkpoint': 'microsoft/deberta-v3-small',\n        'model_type': 'auto',\n        'max_length': 128\n    },\n    image_model_config={\n        'checkpoint': 'openai/clip-vit-base-patch32',\n        'model_type': 'clip',\n        'max_images': 3,\n        'image_agg': 'mean'  # Усредняем эмбеддинги изображений\n    },\n    audio_model_config={\n        'checkpoint': 'laion/clap-htsat-unfused',\n        'model_type': 'clap',\n        'max_audios': 2,\n        'audio_agg': 'mean'  # Усредняем эмбеддинги аудио\n    },\n    \n    fusion='concat',\n    freeze_backbone=True,  # Замораживаем веса предобученных моделей\n    \n    # Опциональная нормализация\n    target_normalizer=normalize_targets,\n    target_denormalizer=denormalize_targets\n)\n\n# Обучение модели\nvideo_regressor.fit(\n    train_data=large_regression_df,\n    epochs=2,\n    test_size=0.2,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=5e-5,\n    metric_names=['mse', 'mae', 'r2'],  # Множественные метрики\n    loss_type='huber',  # Huber loss более устойчив к выбросам\n    target_weights=[1.0, 2.0, 1.5, 0.5],  # Веса важности для каждой целевой переменной\n    fp16=True,\n    logging_steps=5,\n    eval_steps=10,\n    output_dir=\"./video_regressor_results\",\n    seed=42,\n    hidden=512,\n    dropout=0.2,\n    fit_chunk_size=10  # Обучение по чанкам для экономии памяти\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-02T10:47:01.095391Z","iopub.execute_input":"2025-09-02T10:47:01.095739Z","iopub.status.idle":"2025-09-02T10:55:04.748933Z","shell.execute_reply.started":"2025-09-02T10:47:01.095700Z","shell.execute_reply":"2025-09-02T10:55:04.748014Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b9fa927dfb41798e60d6e895b67c8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329c2805e0e34eeab922ac6dbcc56a7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a551de00fa4c4477b81ad5e57e627703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b34d25f3403344c2a999e84280e4c3e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5e0a1e145e943fc8d1b6f7c93c134ca"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8479a6adff6043d194dcb8ebb535f9da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f71cc7ccdc342f49afa350597ce7a49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81cde916153043198f7d2ee3b0786b12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4fb295c8ee64604b6a4d561e7f3a6a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42f2fe5ed54e4220992c202c59135adc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/615M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c42ad10abc014f86895d368e500d9d54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/614M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85a4c4f0fa5c4108ab8339482e6beac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/541 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a2e39f66c264f8c8c5c109501a2c93a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21993a7863b54555812398e8ed4092bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfe85455a4af4e2aa278a675b792abfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15dcc82d406b4ebe91973a62ce7f67b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744e7feecc2745b0805aa895d925b7ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8af70a52b6d45c48b9df85d0aa10fc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training Progress:   0%|          | 0/24 [00:00<?, ?step/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2575c7666d2542a395e27f5fd3c7397b"}},"metadata":{}},{"name":"stdout","text":"step: 10, train loss: 0.0247000000, val loss: 0.03613502532, val mse: 0.0543262362, val mae: 0.1850181818, val r2: -1.3600142285\nstep: 20, train loss: 0.0286000000, val loss: 0.01989620551, val mse: 0.0322626196, val mae: 0.1529631168, val r2: -0.7065489928\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<__main__.SingleModelMultiComboRegression at 0x7b70cca09350>"},"metadata":{}}],"execution_count":3}]}