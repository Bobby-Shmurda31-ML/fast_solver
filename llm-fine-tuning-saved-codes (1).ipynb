{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ReRanker с подбором весов","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nfrom scipy.optimize import differential_evolution\nfrom scipy.stats import spearmanr\nimport pickle\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom collections import Counter\nfrom rouge_score import rouge_scorer\n\n\nclass TextReRankerWeightsOptimizer:\n    def __init__(\n        self,\n        use_rouge_with_source=True,\n        use_extractive_coverage=True,\n        use_lead3_overlap=False,\n        use_length_simple=True,\n        use_compression_ratio=False,\n        use_novel_ngrams=False,\n        \n        length_params=None,\n        compression_params=None,\n        weights=None,\n        device='cuda',\n        cache_dir='./reranker_cache'\n    ):\n        self.use_rouge_with_source = use_rouge_with_source\n        self.use_extractive_coverage = use_extractive_coverage\n        self.use_lead3_overlap = use_lead3_overlap\n        self.use_length_simple = use_length_simple\n        self.use_compression_ratio = use_compression_ratio\n        self.use_novel_ngrams = use_novel_ngrams\n        \n        self.length_params = length_params or {'target_min': 50, 'target_max': 70}\n        self.compression_params = compression_params or {'optimal_ratio': 0.15, 'sigma': 0.05}\n        \n        self.weights = weights or self._get_default_weights()\n        \n        self.device = device\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n        \n        # Быстрые scorers\n        self._rouge_scorer = None\n        self._rouge2_scorer = None  # для целевой метрики\n    \n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state['_rouge_scorer'] = None\n        state['_rouge2_scorer'] = None\n        return state\n    \n    def __setstate__(self, state):\n        self.__dict__.update(state)\n    \n    def _get_default_weights(self):\n        active_metrics = []\n        \n        if self.use_rouge_with_source:\n            active_metrics.append('rouge_with_source')\n        if self.use_extractive_coverage:\n            active_metrics.append('extractive_coverage')\n        if self.use_lead3_overlap:\n            active_metrics.append('lead3_overlap')\n        if self.use_length_simple:\n            active_metrics.append('length_simple')\n        if self.use_compression_ratio:\n            active_metrics.append('compression_ratio')\n        if self.use_novel_ngrams:\n            active_metrics.append('novel_ngrams')\n        \n        if not active_metrics:\n            return {}\n        \n        weight = 1.0 / len(active_metrics)\n        return {metric: weight for metric in active_metrics}\n    \n    def _load_rouge_scorer(self):\n        if self._rouge_scorer is None:\n            self._rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n        return self._rouge_scorer\n    \n    def _load_rouge2_scorer(self):\n        \"\"\"Для целевой метрики (ROUGE-2)\"\"\"\n        if self._rouge2_scorer is None:\n            self._rouge2_scorer = rouge_scorer.RougeScorer(\n                ['rouge1', 'rouge2', 'rougeL'], \n                use_stemmer=True\n            )\n        return self._rouge2_scorer\n    \n    # ==================== МЕТРИКИ ====================\n    \n    @staticmethod\n    def _get_ngrams_fast(text, n):\n        words = text.lower().split()\n        return Counter([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])\n    \n    def compute_batch(self, candidates, source_text):\n        if not candidates:\n            return []\n        \n        # Предвычисление\n        source_ngrams = None\n        source_bigrams_set = None\n        lead3_text = None\n        source_len = None\n        \n        if self.use_extractive_coverage or self.use_novel_ngrams:\n            source_ngrams = {\n                1: self._get_ngrams_fast(source_text, 1),\n                2: self._get_ngrams_fast(source_text, 2),\n                3: self._get_ngrams_fast(source_text, 3),\n                4: self._get_ngrams_fast(source_text, 4),\n            }\n        \n        if self.use_novel_ngrams:\n            source_bigrams_set = set(source_ngrams[2].keys())\n        \n        if self.use_lead3_overlap:\n            sentences = re.split(r'[.!?]+', source_text)\n            sentences = [s.strip() for s in sentences if s.strip()]\n            lead3_text = ' '.join(sentences[:3]) if len(sentences) >= 3 else source_text\n        \n        if self.use_compression_ratio:\n            source_len = len(source_text.split())\n        \n        # Вычисление\n        all_scores = {}\n        \n        if self.use_rouge_with_source:\n            scorer = self._load_rouge_scorer()\n            scores = []\n            for candidate in candidates:\n                score = scorer.score(source_text, candidate)\n                scores.append(score['rougeL'].fmeasure)\n            all_scores['rouge_with_source'] = scores\n        \n        if self.use_extractive_coverage:\n            scores = []\n            for candidate in candidates:\n                ngram_scores = []\n                for n in [1, 2, 3, 4]:\n                    cand_ngrams = self._get_ngrams_fast(candidate, n)\n                    if not cand_ngrams:\n                        continue\n                    overlap = sum((cand_ngrams & source_ngrams[n]).values())\n                    total = sum(cand_ngrams.values())\n                    ngram_scores.append(overlap / total)\n                scores.append(np.mean(ngram_scores) if ngram_scores else 0.0)\n            all_scores['extractive_coverage'] = scores\n        \n        if self.use_lead3_overlap:\n            scorer = self._load_rouge_scorer()\n            scores = []\n            for candidate in candidates:\n                score = scorer.score(lead3_text, candidate)\n                scores.append(score['rougeL'].fmeasure)\n            all_scores['lead3_overlap'] = scores\n        \n        if self.use_length_simple:\n            target_min = self.length_params['target_min']\n            target_max = self.length_params['target_max']\n            \n            scores = []\n            for candidate in candidates:\n                words = len(candidate.split())\n                if target_min <= words <= target_max:\n                    score = 1.0\n                elif words < target_min:\n                    score = words / target_min\n                else:\n                    score = max(0, 1 - (words - target_max) / (target_max * 0.5))\n                scores.append(score)\n            all_scores['length_simple'] = scores\n        \n        if self.use_compression_ratio:\n            optimal = self.compression_params['optimal_ratio']\n            sigma = self.compression_params['sigma']\n            \n            scores = []\n            for candidate in candidates:\n                cand_len = len(candidate.split())\n                ratio = cand_len / source_len if source_len > 0 else 0\n                score = np.exp(-((ratio - optimal) ** 2) / (2 * sigma ** 2))\n                scores.append(score)\n            all_scores['compression_ratio'] = scores\n        \n        if self.use_novel_ngrams:\n            scores = []\n            for candidate in candidates:\n                cand_bigrams = self._get_ngrams_fast(candidate, 2)\n                if not cand_bigrams:\n                    scores.append(0.0)\n                    continue\n                novel = sum(count for bigram, count in cand_bigrams.items() \n                           if bigram not in source_bigrams_set)\n                total = sum(cand_bigrams.values())\n                scores.append(novel / total if total > 0 else 0.0)\n            all_scores['novel_ngrams'] = scores\n        \n        results = []\n        for i in range(len(candidates)):\n            candidate_scores = {metric: all_scores[metric][i] for metric in all_scores}\n            results.append(candidate_scores)\n        \n        return results\n    \n    def compute(self, text, source_text=None, return_individual=True, return_weighted=True):\n        batch_results = self.compute_batch([text], source_text)\n        scores = batch_results[0]\n        \n        result = {}\n        if return_individual:\n            result['scores'] = scores\n        \n        if return_weighted:\n            weighted_score = sum(scores[m] * self.weights.get(m, 0) for m in scores)\n            result['weighted_score'] = weighted_score\n        \n        return result\n    \n    # ==================== RANKING ====================\n    \n    def rank_candidates(self, candidates, source_text=None):\n        if not candidates:\n            return []\n        \n        batch_scores = self.compute_batch(candidates, source_text)\n        \n        results = []\n        for idx, scores in enumerate(batch_scores):\n            weighted_score = sum(scores[m] * self.weights.get(m, 0) for m in scores)\n            results.append((idx, weighted_score, scores))\n        \n        results.sort(key=lambda x: x[1], reverse=True)\n        \n        return results\n    \n    def get_best_candidate(self, candidates, source_text=None):\n        results = self.rank_candidates(candidates, source_text)\n        best_idx = results[0][0]\n        return candidates[best_idx]\n    \n    # ==================== FIT ====================\n    \n    def fit(\n        self,\n        candidates_list,\n        x_texts,\n        y_texts,\n        metric='rouge2',\n        cache_name=None,\n        use_cache=True,\n        max_iter=50,\n        popsize=15,\n        seed=42,\n        n_workers=1,\n        print_correlations=False\n    ):\n        if len(candidates_list) != len(x_texts) or len(candidates_list) != len(y_texts):\n            raise ValueError(\n                f\"Length mismatch: candidates_list={len(candidates_list)}, \"\n                f\"x_texts={len(x_texts)}, y_texts={len(y_texts)}\"\n            )\n        \n        # Предвычисление метрик\n        precomputed_metrics = self._load_or_compute_metrics(\n            candidates_list, x_texts, cache_name, use_cache\n        )\n        \n        # КЭШИРУЕМ целевые scores для всех кандидатов\n        print(\"Предвычисление целевой метрики для всех кандидатов...\")\n        target_scores_cache = self._precompute_target_scores(\n            candidates_list, y_texts, metric\n        )\n        \n        metric_names = list(precomputed_metrics[0][0].keys())\n        n_metrics = len(metric_names)\n\n        print(f\"\\nМетрики: {metric_names}\")\n        print(f\"Целевая метрика: {metric}\")\n        \n        if print_correlations:            \n            self._print_correlations_fast(\n                precomputed_metrics, target_scores_cache,\n                metric_names, n_samples=30\n            )\n        \n        # Оптимизация с кэшированными target scores\n        best_weights, best_score = self._optimize_weights_fast(\n            candidates_list, precomputed_metrics, target_scores_cache,\n            metric_names, max_iter, popsize, seed, n_workers\n        )\n        \n        self.weights = {\n            metric_names[i]: best_weights[i]\n            for i in range(n_metrics)\n        }\n\n        print(\"\\n\" + \"=\"*60)\n        print(\"РЕЗУЛЬТАТЫ КАЛИБРОВКИ\")\n        print(\"=\"*60)\n        print(f\"Best {metric}: {best_score:.4f}\")\n        print(f\"\\nОптимальные веса:\")\n        for name, weight in self.weights.items():\n            print(f\"  {name:20s}: {weight:.4f}\")\n        print(\"=\"*60)\n\n        return self.weights, best_score\n    \n    def _load_or_compute_metrics(self, candidates_list, x_texts, cache_name, use_cache):\n        cache_path = None\n        if cache_name and use_cache:\n            cache_path = self.cache_dir / f\"{cache_name}_metrics.pkl\"\n        \n        if cache_path and cache_path.exists():\n            print(f\"Загрузка метрик из кэша: {cache_path}\")\n            with open(cache_path, 'rb') as f:\n                return pickle.load(f)\n        \n        print(\"Предвычисление метрик...\")\n        precomputed = []\n        \n        for candidates, x_text in tqdm(list(zip(candidates_list, x_texts))):\n            candidate_metrics = self.compute_batch(candidates, x_text)\n            precomputed.append(candidate_metrics)\n        \n        if cache_path:\n            with open(cache_path, 'wb') as f:\n                pickle.dump(precomputed, f)\n            print(f\"Метрики сохранены в кэш: {cache_path}\")\n        \n        return precomputed\n    \n    def _precompute_target_scores(self, candidates_list, y_texts, metric='rouge2'):\n        \"\"\"\n        КЛЮЧЕВАЯ ОПТИМИЗАЦИЯ: предвычисляем целевую метрику для всех кандидатов.\n        Это делается ОДИН раз, а не на каждом шаге differential_evolution!\n        \"\"\"\n        scorer = self._load_rouge2_scorer()\n        \n        target_scores = []\n        \n        for candidates, y_text in tqdm(zip(candidates_list, y_texts), \n                                       total=len(candidates_list),\n                                       desc=\"Целевая метрика\"):\n            candidate_scores = []\n            for candidate in candidates:\n                score = scorer.score(y_text, candidate)\n                \n                if metric == 'rouge1':\n                    candidate_scores.append(score['rouge1'].fmeasure)\n                elif metric == 'rouge2':\n                    candidate_scores.append(score['rouge2'].fmeasure)\n                elif metric == 'rougeL':\n                    candidate_scores.append(score['rougeL'].fmeasure)\n                else:\n                    raise ValueError(f\"Unknown metric: {metric}\")\n            \n            target_scores.append(candidate_scores)\n        \n        return target_scores\n    \n    def _print_correlations_fast(self, precomputed_metrics, target_scores_cache,\n                                 metric_names, n_samples=30):\n        \"\"\"Быстрая версия с кэшированными target scores\"\"\"\n        print(\"\\nКорреляция с целевой метрикой:\")\n        \n        metric_spearman = {name: [] for name in metric_names}\n        \n        for i in range(min(n_samples, len(precomputed_metrics))):\n            target_values = target_scores_cache[i]\n            \n            for metric_name in metric_names:\n                metric_values = [\n                    precomputed_metrics[i][j][metric_name] \n                    for j in range(len(precomputed_metrics[i]))\n                ]\n                \n                if np.std(metric_values) > 1e-10 and np.std(target_values) > 1e-10:\n                    spearman, _ = spearmanr(metric_values, target_values)\n                    metric_spearman[metric_name].append(spearman)\n        \n        for metric_name in metric_names:\n            if metric_spearman[metric_name]:\n                avg_spearman = np.mean(metric_spearman[metric_name])\n                std_spearman = np.std(metric_spearman[metric_name])\n                \n                if abs(avg_spearman) > 0.4:\n                    status = \"✓✓✓\"\n                elif abs(avg_spearman) > 0.25:\n                    status = \"✓✓\"\n                elif abs(avg_spearman) > 0.15:\n                    status = \"✓\"\n                else:\n                    status = \"⚠️\"\n                \n                print(f\"  {status} {metric_name:25s}: {avg_spearman:+.3f} (±{std_spearman:.3f})\")\n    \n    def _optimize_weights_fast(self, candidates_list, precomputed_metrics, \n                               target_scores_cache, metric_names, \n                               max_iter, popsize, seed, n_workers):\n        \"\"\"Быстрая версия с кэшированными target scores\"\"\"\n        n_metrics = len(metric_names)\n        \n        self._opt_precomputed_metrics = precomputed_metrics\n        self._opt_target_scores_cache = target_scores_cache\n        self._opt_metric_names = metric_names\n        self._opt_n_metrics = n_metrics\n        \n        print(f\"\\nОптимизация весов (workers={n_workers})...\")\n        \n        result = differential_evolution(\n            self._objective_function_fast,\n            bounds=[(0, 1)] * n_metrics,\n            strategy='best1bin',\n            maxiter=max_iter,\n            popsize=popsize,\n            tol=0.001,\n            mutation=(0.5, 1.5),\n            recombination=0.7,\n            seed=seed,\n            workers=n_workers,\n            updating='deferred' if n_workers > 1 else 'immediate',\n            polish=True,\n            disp=True\n        )\n        \n        del self._opt_precomputed_metrics\n        del self._opt_target_scores_cache\n        del self._opt_metric_names\n        del self._opt_n_metrics\n        \n        weights_raw = result.x\n        weights_sum = sum(weights_raw)\n        \n        if weights_sum < 1e-10:\n            print(\"⚠️ WARNING: Все веса близки к нулю, используем равные веса\")\n            weights_normalized = np.ones(n_metrics) / n_metrics\n        else:\n            weights_normalized = weights_raw / weights_sum\n        \n        best_score = -result.fun\n        \n        return weights_normalized, best_score\n    \n    def _objective_function_fast(self, weights):\n        \"\"\"\n        БЫСТРАЯ версия: использует кэшированные target scores.\n        Не нужно вызывать ROUGE на каждом шаге!\n        \"\"\"\n        total_score = 0.0\n        \n        for metrics_list, target_scores in zip(\n            self._opt_precomputed_metrics, \n            self._opt_target_scores_cache\n        ):\n            # Вычисляем weighted scores для всех кандидатов\n            weighted_scores = [\n                sum(weights[i] * metrics_dict.get(self._opt_metric_names[i], 0) \n                    for i in range(self._opt_n_metrics))\n                for metrics_dict in metrics_list\n            ]\n            \n            # Выбираем лучшего по нашим метрикам\n            best_idx = np.argmax(weighted_scores)\n            \n            # Берём его целевой score из кэша (БЕЗ вызова ROUGE!)\n            total_score += target_scores[best_idx]\n        \n        # Усредняем\n        avg_score = total_score / len(self._opt_precomputed_metrics)\n        \n        return -avg_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Метрики","metadata":{}},{"cell_type":"code","source":"!pip -q install bert_score rouge_score\n\nfrom collections import Counter\nimport numpy as np\nfrom rouge_score import rouge_scorer\nfrom bert_score import score as bert_score\nimport torch\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef exact_match(prediction, reference):\n    return prediction.strip().lower() == reference.strip().lower()\n# Higher is better (0-1)\n\ndef token_f1(prediction, reference):\n    pred_tokens = prediction.lower().split()\n    ref_tokens = reference.lower().split()\n    \n    common = Counter(pred_tokens) & Counter(ref_tokens)\n    num_same = sum(common.values())\n    \n    if num_same == 0:\n        return 0.0\n    \n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(ref_tokens)\n    f1 = 2 * precision * recall / (precision + recall)\n    \n    return f1\n# Higher is better (0-1)\n\ndef compute_bleu(prediction, reference):\n    pred_tokens = prediction.lower().split()\n    ref_tokens = reference.lower().split()\n    \n    smoothing = SmoothingFunction().method1\n    return sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n# Higher is better (0-1)\n\ndef compute_rouge(prediction, reference):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = scorer.score(reference, prediction)\n    \n    return {\n        'rouge1': scores['rouge1'].fmeasure,\n        'rouge2': scores['rouge2'].fmeasure,\n        'rougeL': scores['rougeL'].fmeasure\n    }\n# Higher is better (0-1)\n\ndef compute_bertscore(predictions, references):\n    P, R, F1 = bert_score(predictions, references, lang='en', verbose=False)\n    return {\n        'precision': P.mean().item(),\n        'recall': R.mean().item(),\n        'f1': F1.mean().item()\n    }\n# Higher is better (0-1)\n\ndef compute_meteor(prediction, reference):\n    pred_tokens = prediction.lower().split()\n    ref_tokens = reference.lower().split()\n    \n    return meteor_score([ref_tokens], pred_tokens)\n# Higher is better (0-1)\n\ndef perplexity(model, tokenizer, texts):\n    total_loss = 0\n    total_tokens = 0\n    \n    for text in texts:\n        inputs = tokenizer(text, return_tensors='pt').to(model.device)\n        \n        with torch.no_grad():\n            outputs = model(**inputs, labels=inputs.input_ids)\n            loss = outputs.loss\n        \n        total_loss += loss.item() * inputs.input_ids.size(1)\n        total_tokens += inputs.input_ids.size(1)\n    \n    return np.exp(total_loss / total_tokens)\n# Lower is better\n\ndef distinct_n(texts, n=2):\n    all_ngrams = []\n    \n    for text in texts:\n        tokens = text.lower().split()\n        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n        all_ngrams.extend(ngrams)\n    \n    if not all_ngrams:\n        return 0.0\n    \n    return len(set(all_ngrams)) / len(all_ngrams)\n# Higher is better (0-1, measures diversity)\n\ndef self_bleu(texts):\n    if len(texts) < 2:\n        return 0.0\n    \n    scores = []\n    smoothing = SmoothingFunction().method1\n    \n    for i, text in enumerate(texts):\n        others = texts[:i] + texts[i+1:]\n        if not others:\n            continue\n        \n        text_tokens = text.lower().split()\n        if not text_tokens:\n            continue\n        \n        refs_tokens = [other.lower().split() for other in others if other.strip()]\n        refs_tokens = [ref for ref in refs_tokens if ref]\n        \n        if not refs_tokens:\n            continue\n        \n        try:\n            score = sentence_bleu(refs_tokens, text_tokens, smoothing_function=smoothing)\n            scores.append(score)\n        except:\n            continue\n    \n    return np.mean(scores) if scores else 0.0\n# Lower is better (0-1, measures diversity - lower means more diverse)\n\npredictions = [\n    \"The cat sat on the mat\",\n    \"A dog runs in the park\",\n    \"She loves reading books\"\n]\n\nreferences = [\n    \"A cat was sitting on the mat\",\n    \"The dog is running\",\n    \"She loves reading books\"\n]\n\nem_scores = [exact_match(p, r) for p, r in zip(predictions, references)]\nf1_scores = [token_f1(p, r) for p, r in zip(predictions, references)]\nbleu_scores = [compute_bleu(p, r) for p, r in zip(predictions, references)]\n\nrouge_scores = [compute_rouge(p, r) for p, r in zip(predictions, references)]\nrouge_avg = {\n    'rouge1': np.mean([s['rouge1'] for s in rouge_scores]),\n    'rouge2': np.mean([s['rouge2'] for s in rouge_scores]),\n    'rougeL': np.mean([s['rougeL'] for s in rouge_scores])\n}\n\nbertscore = compute_bertscore(predictions, references)\nmeteor_scores = [compute_meteor(p, r) for p, r in zip(predictions, references)]\n\ndistinct = distinct_n(predictions, n=2)\nsbleu = self_bleu(predictions)\n\nprint(f\"EM: {np.mean(em_scores):.3f}\")\nprint(f\"F1: {np.mean(f1_scores):.3f}\")\nprint(f\"BLEU: {np.mean(bleu_scores):.3f}\")\nprint(f\"ROUGE-1: {rouge_avg['rouge1']:.3f}\")\nprint(f\"ROUGE-2: {rouge_avg['rouge2']:.3f}\")\nprint(f\"ROUGE-L: {rouge_avg['rougeL']:.3f}\")\nprint(f\"BERTScore F1: {bertscore['f1']:.3f}\")\nprint(f\"METEOR: {np.mean(meteor_scores):.3f}\")\nprint(f\"Distinct-2: {distinct:.3f}\")\nprint(f\"Self-BLEU: {sbleu:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:31:12.901307Z","iopub.execute_input":"2025-11-15T10:31:12.901606Z","iopub.status.idle":"2025-11-15T10:31:17.308690Z","shell.execute_reply.started":"2025-11-15T10:31:12.901577Z","shell.execute_reply":"2025-11-15T10:31:17.307832Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"EM: 0.333\nF1: 0.672\nBLEU: 0.411\nROUGE-1: 0.738\nROUGE-2: 0.455\nROUGE-L: 0.672\nBERTScore F1: 0.966\nMETEOR: 0.684\nDistinct-2: 1.000\nSelf-BLEU: 0.027\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# работа с A100","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,  # BF16\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    'Qwen/Qwen2.5-7B-Instruct',\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # одна GPU\n    attn_implementation=\"flash_attention_2\"  # Flash Attention, ускорение модели, качество не ухудшается\n)\n\nargs = TrainingArguments(\n    output_dir='./output',\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    fp16=False,\n    bf16=True,  # BF16\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RAG","metadata":{}},{"cell_type":"code","source":"class RAG:\n    def __init__(self, checkpoint='BAAI/bge-base-en-v1.5', device='cuda'):\n        self.model = AutoModel.from_pretrained(checkpoint).to(device)\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.device = device\n        \n        self.x_texts = None\n        self.y_texts = None\n        self.embeddings = None\n\n    def fit(self, x_texts, y_texts, batch_size=32):\n        self.x_texts = x_texts\n        self.y_texts = y_texts\n        all_embeddings = []\n\n        for i in tqdm(range(0, len(x_texts), batch_size), desc='RAG fitting'):\n            batch = x_texts[i:i + batch_size]\n            \n            inputs = self.tokenizer(\n                batch,\n                max_length=512,\n                truncation=True,\n                padding='longest',\n                return_tensors='pt'\n            ).to(self.device)\n            \n            with torch.no_grad():\n                embeddings = self.model(**inputs).last_hidden_state[:, 0]\n            \n            all_embeddings.append(embeddings.cpu())\n\n        self.embeddings = torch.cat(all_embeddings, dim=0).numpy()\n        self.embeddings = self.embeddings / np.linalg.norm(\n            self.embeddings, axis=1, keepdims=True\n        )\n\n    def predict(self, x_texts, k=3, batch_size=32):\n        if isinstance(x_texts, str):\n            x_texts = [x_texts]\n            single = True\n        else:\n            single = False\n\n        all_results = []\n        \n        for i in range(0, len(x_texts), batch_size):\n            batch = x_texts[i:i + batch_size]\n            \n            inputs = self.tokenizer(\n                batch,\n                max_length=512,\n                truncation=True,\n                padding='longest',\n                return_tensors='pt'\n            ).to(self.device)\n            \n            with torch.no_grad():\n                query_embs = self.model(**inputs).last_hidden_state[:, 0]\n            \n            query_embs = query_embs.cpu().numpy()\n            query_embs = query_embs / np.linalg.norm(query_embs, axis=1, keepdims=True)\n            \n            similarities = np.dot(query_embs, self.embeddings.T)\n            \n            for j, sims in enumerate(similarities):\n                top_k = np.argsort(sims)[-k - len(x_texts):][::-1]\n                \n                results = []\n                for idx in top_k:\n                    if self.x_texts[idx] == batch[j]:\n                        continue\n\n                    results.append({\n                        'x': self.x_texts[idx],\n                        'y': self.y_texts[idx],\n                        'similarity': float(sims[idx]),\n                        'index': int(idx)\n                    })\n                \n                all_results.append(results[:k])\n        \n        return all_results[0] if single else all_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'] для LLaMA / Llama-2 / Llama-3 / Mistral / Qwen / Yi","metadata":{}},{"cell_type":"markdown","source":"# Форматы данных для LLM","metadata":{}},{"cell_type":"markdown","source":"1. Instruction-following (самый популярный)","metadata":{}},{"cell_type":"code","source":"# Формат Alpaca\ntemplate = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n{response}\"\"\"\n\n# Формат ChatML (для chat-моделей)\ntemplate = \"\"\"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n{assistant_response}<|im_end|>\"\"\"\n\n# Формат Llama/Mistral Instruct\ntemplate = \"\"\"<s>[INST] {instruction} [/INST] {response}</s>\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Структура датасета","metadata":{}},{"cell_type":"code","source":"# Вариант 1: Простой (для SFTTrainer)\ndataset = [\n    {\"text\": \"### Вопрос: Столица России?\\n### Ответ: Москва\"},\n    {\"text\": \"### Вопрос: 2+2=?\\n### Ответ: 4\"},\n]\n\n# Вариант 2: Разделенный (лучше для контроля)\ndataset = [\n    {\n        \"instruction\": \"Столица России?\",\n        \"response\": \"Москва\"\n    },\n    {\n        \"instruction\": \"2+2=?\",\n        \"response\": \"4\"\n    },\n]\n\n# Вариант 3: С контекстом\ndataset = [\n    {\n        \"instruction\": \"Суммаризируй текст\",\n        \"input\": \"Длинный текст...\",\n        \"output\": \"Краткое содержание\"\n    }\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Маскирование промпта","metadata":{}},{"cell_type":"markdown","source":"Способ 1: Автоматический (SFTTrainer)","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom datasets import Dataset\n\n# 1. Подготовка данных\ndata = [\n    {\"instruction\": \"Столица России?\", \"response\": \"Москва\"},\n    {\"instruction\": \"Автор 'Война и мир'?\", \"response\": \"Лев Толстой\"},\n]\n\ndataset = Dataset.from_list(data)\n\n# 2. Функция форматирования\ndef formatting_func(example):\n    return f\"### Инструкция:\\n{example['instruction']}\\n\\n### Ответ:\\n{example['response']}\"\n\n# 3. Обучение\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,  # эффективный batch = 4*4 = 16\n    learning_rate=2e-4,\n    fp16=True,  # или bf16=True для новых GPU\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    optim=\"paged_adamw_8bit\",  # экономия памяти\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    formatting_func=formatting_func,\n    max_seq_length=512,  # максимальная длина последовательности\n    peft_config=lora_config,  # из прошлого блока\n    args=training_args,\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Способ 2: Продвинутый (кастомный Data Collator)","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List\nfrom transformers import DataCollatorForLanguageModeling\n\n@dataclass\nclass DataCollatorForCompletionOnlyLM:\n    tokenizer: any\n    response_template: str = \"### Ответ:\\n\"\n    mlm: bool = False\n    \n    def __call__(self, examples: List[Dict[str, List[int]]]) -> Dict[str, any]:\n        batch = {\n            \"input_ids\": [],\n            \"attention_mask\": [],\n            \"labels\": []\n        }\n        \n        for example in examples:\n            # Токенизируем полный текст\n            full_text = example[\"text\"]\n            tokenized = self.tokenizer(\n                full_text,\n                truncation=True,\n                max_length=512,\n                padding=False,\n            )\n            \n            input_ids = tokenized[\"input_ids\"]\n            \n            # Находим где начинается ответ\n            response_token_ids = self.tokenizer.encode(\n                self.response_template, \n                add_special_tokens=False\n            )\n            \n            # Ищем шаблон в input_ids\n            labels = [-100] * len(input_ids)\n            \n            for i in range(len(input_ids) - len(response_token_ids)):\n                if input_ids[i:i+len(response_token_ids)] == response_token_ids:\n                    # Нашли начало ответа\n                    response_start = i + len(response_token_ids)\n                    labels[response_start:] = input_ids[response_start:]\n                    break\n            \n            batch[\"input_ids\"].append(input_ids)\n            batch[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n            batch[\"labels\"].append(labels)\n        \n        # Padding\n        from torch.nn.utils.rnn import pad_sequence\n        import torch\n        \n        batch[\"input_ids\"] = pad_sequence(\n            [torch.tensor(x) for x in batch[\"input_ids\"]], \n            batch_first=True, \n            padding_value=self.tokenizer.pad_token_id\n        )\n        batch[\"attention_mask\"] = pad_sequence(\n            [torch.tensor(x) for x in batch[\"attention_mask\"]], \n            batch_first=True, \n            padding_value=0\n        )\n        batch[\"labels\"] = pad_sequence(\n            [torch.tensor(x) for x in batch[\"labels\"]], \n            batch_first=True, \n            padding_value=-100\n        )\n        \n        return batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Способ 3: Готовый инструмент из trl","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM\n\n# Указываем шаблон ответа\nresponse_template = \"### Ответ:\\n\"\n\ncollator = DataCollatorForCompletionOnlyLM(\n    response_template=response_template,\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# Используем в Trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    data_collator=collator,\n    # ...\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TrainingArguments: Что важно","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    # === Основное ===\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    \n    # === Batch size ===\n    per_device_train_batch_size=4,  # на 1 GPU\n    gradient_accumulation_steps=4,   # накапливаем градиенты\n    # Реальный batch = 4 * 4 = 16\n    \n    # === Learning rate ===\n    learning_rate=2e-4,  # для LoRA обычно выше: 1e-4 до 5e-4\n    lr_scheduler_type=\"cosine\",  # или \"linear\"\n    warmup_steps=100,  # или warmup_ratio=0.1\n    \n    # === Оптимизация памяти ===\n    fp16=True,  # для старых GPU (V100, RTX 2080)\n    # bf16=True,  # для новых GPU (A100, RTX 3090+) - лучше чем fp16\n    gradient_checkpointing=True,  # экономия памяти за счет скорости\n    optim=\"paged_adamw_8bit\",  # 8-bit optimizer от bitsandbytes\n    \n    # === Логирование ===\n    logging_steps=10,\n    logging_dir=\"./logs\",\n    report_to=\"tensorboard\",  # или \"wandb\"\n    \n    # === Сохранение ===\n    save_strategy=\"epoch\",  # \"steps\", \"epoch\", \"no\"\n    save_total_limit=2,  # храним только 2 последних чекпоинта\n    \n    # === Eval (если есть val set) ===\n    evaluation_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"loss\",\n    \n    # === Прочее ===\n    remove_unused_columns=False,  # важно для SFTTrainer\n    dataloader_num_workers=4,  # параллельная загрузка данных\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Полный пример: От данных до обученной модели","metadata":{}},{"cell_type":"code","source":"!pip -q install transformers>=4.38.0 trl>=0.8.0 peft>=0.9.0 bitsandbytes\n\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\n\ncheckpoint = \"mistralai/Mistral-7B-v0.1\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_type=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\n    quantization_config=bnb_config,\n    device_map='auto'\n)\n\nprepare_model_for_kbit_training(model)\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout=0.05,\n    bias='none',\n    task_type='CAUSAL_LM'\n)\n\ntrain_data = [\n    {\n        \"question\": \"Переведи на английский: Привет, как дела?\",\n        \"answer\": \"Hello, how are you?\"\n    },\n    {\n        \"question\": \"Реши: 15 * 8\",\n        \"answer\": \"120\"\n    }\n]\ndataset = Dataset.from_list([\n    {\"text\": f\"<s>[INST] {item['question']} [/INST] {item['answer']}</s>\"}\n    for item in train_data\n])\n\nargs = TrainingArguments(\n    optim='paged_adamw_8bit',\n    report_to='none',\n    output_dir='./result',\n    fp16=torch.cuda.is_available()\n)\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    args=args,\n    peft_config=lora_config\n)\ntrainer.train()\ntrainer.save_model(\"./qa_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:05:05.912300Z","iopub.execute_input":"2025-11-09T07:05:05.912944Z","iopub.status.idle":"2025-11-09T07:08:47.322783Z","shell.execute_reply.started":"2025-11-09T07:05:05.912910Z","shell.execute_reply":"2025-11-09T07:08:47.321847Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-09 07:06:42.452891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762672002.680153      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762672002.744361      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10bf71b90a6f476c8c2ec52233e49cce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27834fcd3de406fac4238660e925c70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b26667f5d7504ef2b4db80e80e0f5f87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001b1b79213e43d186be13a9a140e538"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd8eeafb43054d268bf38b5e9349a9ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ad1627cdd734e91b275c18015de233a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1035e9c9ee384c81a9c8fef14ffb1edc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b02ae79b52244e92b00d485e55f8b95e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7016b85b47d24cbb9289c8973649c145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933b96de1aee448da9af6de0383743cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc4b706f42542fe9c40ca3198f692ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8621a1e8d1cd409d9b1fb0f080189e37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72d0ea43281445d395c2ddd3e7beadcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6460fced3bbe40449a68cf84aa128563"}},"metadata":{}},{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from peft import PeftModel\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\nmodel = PeftModel.from_pretrained(model, \"./qa_model\")\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:08:47.323999Z","iopub.execute_input":"2025-11-09T07:08:47.324276Z","iopub.status.idle":"2025-11-09T07:09:12.059106Z","shell.execute_reply.started":"2025-11-09T07:08:47.324256Z","shell.execute_reply":"2025-11-09T07:09:12.058460Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931c9fc61771459592a1d64c8f3e24e3"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): MistralRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Подготовка промпта\nprompt = \"<s>[INST] Привет, как дела? [/INST]\"\n\n# Токенизация\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(trainer.model.device)\n\n# Генерация\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=256,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.2\n)\n\n# Декодирование\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = tokenizer('<s>[INST] Приветики-пистолетики! [/INST]</s>', return_tensors='pt').to(trainer.model.device)\noutputs = model(\n    **inputs,\n    max_new_tokens=256,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    repetitiin_penalty=1.2\n)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Подготовка промпта\nprompt = \"<s>[INST] Привет, как дела? [/INST]\"\n\n# Токенизация\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(trainer.model.device)\n\n# Генерация\noutputs = model.generate(\n    **inputs,\n    \n    # === Длина ===\n    max_new_tokens=256,      # максимум новых токенов\n    min_new_tokens=10,       # минимум (опционально)\n    max_length=512,          # альтернатива: общая длина (prompt + generation)\n    \n    # === Стратегия декодирования ===\n    do_sample=True,          # False = greedy, True = sampling\n    \n    # === Temperature (креативность) ===\n    temperature=0.7,         # 0.1 = консервативно, 1.0 = нормально, 2.0 = креативно\n                             # <0.7: факты, код, переводы\n                             # 0.7-1.0: обычная генерация\n                             # >1.0: креативное письмо\n    \n    # === Top-p (nucleus sampling) ===\n    top_p=0.9,              # рассматриваем топ токенов с суммарной вероятностью 90%\n                            # 0.9-0.95: хороший баланс\n                            # 0.5: более консервативно\n                            # 0.99: почти все токены\n    \n    # === Top-k sampling ===\n    top_k=50,               # рассматриваем только топ-50 токенов\n                            # обычно 40-100\n                            # 0 = выключено\n    \n    # === Repetition penalty ===\n    repetition_penalty=1.2, # штраф за повторения\n                            # 1.0 = нет штрафа\n                            # 1.1-1.5: легкий штраф (обычно хорошо)\n                            # >1.5: сильный штраф\n    \n    # # === Stopping criteria ===\n    # eos_token_id=tokenizer.eos_token_id,\n    # pad_token_id=tokenizer.pad_token_id,\n    \n    # === Другое ===\n    num_return_sequences=1,  # сколько вариантов генерировать\n    num_beams=1,            # beam search (1 = выключен)\n)\n\n# Декодирование\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Если не хватает памяти","metadata":{}},{"cell_type":"code","source":"# Уменьшите:\nper_device_train_batch_size=2  # было 4\nmax_seq_length=256  # было 512\nr=8  # было 16 в LoRA\n\n# Добавьте:\ngradient_checkpointing=True\noptim=\"paged_adamw_8bit\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Блок 3: Inference, генерация и валидация","metadata":{}},{"cell_type":"markdown","source":"Часть 1: Загрузка обученной модели","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\nmodel = PeftModel.from_pretrained(model, \"./qa_model\")\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Часть 2: Генерация текста","metadata":{}},{"cell_type":"code","source":"# Подготовка промпта\nprompt = \"<s>[INST] Hello! [/INST]\"\n\n# Токенизация\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(trainer.model.device)\n\n# Генерация\noutputs = trainer.model.generate(\n    **inputs,\n    max_new_tokens=100,  # сколько токенов сгенерировать\n    do_sample=False,     # greedy decoding\n)\n\n# Декодирование\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:11:39.914349Z","iopub.execute_input":"2025-11-09T07:11:39.914643Z","iopub.status.idle":"2025-11-09T07:11:55.946817Z","shell.execute_reply.started":"2025-11-09T07:11:39.914622Z","shell.execute_reply":"2025-11-09T07:11:55.946118Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nBoth `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"[INST] Hello! [/INST]\n\nHi there, I'm new to this forum. My name is Kieran and I am from the United Kingdom (hence my accent). I have been playing TF2 for 5 years now, and have always loved it. It has never failed to give me fun times with friends or strangers alike.\nI've just recently got back into trading after a long break, so if you want to trade with someone who knows what they are doing then feel free to send me an offer. Also, any tips would be appreciated as I'm not very good at trading myself xD .\nI will also like to join your group, as I think that we could help each other out in trades.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Параметры генерации","metadata":{}},{"cell_type":"code","source":"outputs = model.generate(\n    **inputs,\n    \n    # === Длина ===\n    max_new_tokens=256,      # максимум новых токенов\n    min_new_tokens=10,       # минимум (опционально)\n    max_length=512,          # альтернатива: общая длина (prompt + generation)\n    \n    # === Стратегия декодирования ===\n    do_sample=True,          # False = greedy, True = sampling\n    \n    # === Temperature (креативность) ===\n    temperature=0.7,         # 0.1 = консервативно, 1.0 = нормально, 2.0 = креативно\n                             # <0.7: факты, код, переводы\n                             # 0.7-1.0: обычная генерация\n                             # >1.0: креативное письмо\n    \n    # === Top-p (nucleus sampling) ===\n    top_p=0.9,              # рассматриваем топ токенов с суммарной вероятностью 90%\n                            # 0.9-0.95: хороший баланс\n                            # 0.5: более консервативно\n                            # 0.99: почти все токены\n    \n    # === Top-k sampling ===\n    top_k=50,               # рассматриваем только топ-50 токенов\n                            # обычно 40-100\n                            # 0 = выключено\n    \n    # === Repetition penalty ===\n    repetition_penalty=1.2, # штраф за повторения\n                            # 1.0 = нет штрафа\n                            # 1.1-1.5: легкий штраф (обычно хорошо)\n                            # >1.5: сильный штраф\n    \n    # === Stopping criteria ===\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.pad_token_id,\n    \n    # === Другое ===\n    num_return_sequences=1,  # сколько вариантов генерировать\n    num_beams=1,            # beam search (1 = выключен)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:09:43.686036Z","iopub.execute_input":"2025-11-09T07:09:43.686576Z","iopub.status.idle":"2025-11-09T07:10:11.753967Z","shell.execute_reply.started":"2025-11-09T07:09:43.686551Z","shell.execute_reply":"2025-11-09T07:10:11.753347Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nBoth `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Комбинации параметров для разных задач","metadata":{}},{"cell_type":"code","source":"# 1. Факты, QA, перевод (нужна точность)\ngeneration_config_precise = {\n    \"max_new_tokens\": 100,\n    \"do_sample\": True,\n    \"temperature\": 0.3,\n    \"top_p\": 0.85,\n    \"repetition_penalty\": 1.1,\n}\n\n# 2. Обычная генерация (баланс)\ngeneration_config_balanced = {\n    \"max_new_tokens\": 200,\n    \"do_sample\": True,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"top_k\": 50,\n    \"repetition_penalty\": 1.2,\n}\n\n# 3. Креативное письмо\ngeneration_config_creative = {\n    \"max_new_tokens\": 300,\n    \"do_sample\": True,\n    \"temperature\": 1.0,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.15,\n}\n\n# 4. Детерминированная генерация (для debug)\ngeneration_config_greedy = {\n    \"max_new_tokens\": 100,\n    \"do_sample\": False,  # greedy decoding\n}\n\n# Использование\noutputs = model.generate(**inputs, **generation_config_balanced)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Batch генерация","metadata":{}},{"cell_type":"code","source":"prompts = [\n    \"<s>[INST] Столица России? [/INST]\",\n    \"<s>[INST] 2+2=? [/INST]\",\n    \"<s>[INST] Кто написал 'Евгений Онегин'? [/INST]\"\n]\n\n# Токенизация с padding\ninputs = tokenizer(\n    prompts, \n    return_tensors=\"pt\", \n    padding=True,  # важно!\n    truncation=True,\n    max_length=512\n).to(model.device)\n\n# Генерация\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=50,\n    temperature=0.7,\n    top_p=0.9,\n    pad_token_id=tokenizer.pad_token_id\n)\n\n# Декодирование\nfor i, output in enumerate(outputs):\n    text = tokenizer.decode(output, skip_special_tokens=True)\n    print(f\"Prompt {i}: {text}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_response(model, tokenizer, promts):\n    inputs = tokenizer(\n        promts,\n        return_tensors='pt',\n        padding='longest',\n        truncation=True,\n        max_length=512\n    ).to(model.device)\n    outputs = model.generate(\n        **inputs,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.2,\n        max_new_tokens=256\n    )\n\n    texts = []\n    for output in outputs:\n        output = output[inputs.input_ids.shape[1]:]\n        text = tokenizer.decode(output, skip_special_tokens=True)\n        texts.append(text)\n\n    return texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def template_processing(question, answer=None):\n    if answer is None:\n        return tokenizer.apply_chat_template(\n            [{'role': 'user', 'content': f'здесь задача модели:\\n\\n{question}'}],\n            tokenize=False,\n            add_generation_promt=True\n        )\n    else:\n        return tokenizer.apply_chat_template(\n            [{'role': 'user', 'content': f'здесь задача модели:\\n\\n{question}'},\n             {'role': 'assistant', 'content': answer}],\n            tokenize=False\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Часть 3: Валидация в процессе обучения","metadata":{}},{"cell_type":"markdown","source":"Вариант 1: Простой callback для генерации примеров","metadata":{}},{"cell_type":"code","source":"from transformers import TrainerCallback\n\nclass GenerationCallback(TrainerCallback):\n    def __init__(self, tokenizer, test_prompts, every_n_steps=100):\n        self.tokenizer = tokenizer\n        self.test_prompts = test_prompts\n        self.every_n_steps = every_n_steps\n    \n    def on_step_end(self, args, state, control, model=None, **kwargs):\n        if state.global_step % self.every_n_steps == 0:\n            print(f\"\\n{'='*50}\")\n            print(f\"Generation at step {state.global_step}\")\n            print(f\"{'='*50}\")\n            \n            model.eval()\n            for prompt in self.test_prompts:\n                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n                \n                with torch.no_grad():\n                    outputs = model.generate(\n                        **inputs,\n                        max_new_tokens=50,\n                        temperature=0.7,\n                        top_p=0.9,\n                        do_sample=True\n                    )\n                \n                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n                print(f\"\\nPrompt: {prompt}\")\n                print(f\"Generated: {generated}\")\n            \n            model.train()\n            print(f\"{'='*50}\\n\")\n\n# Использование\ntest_prompts = [\n    \"<s>[INST] Столица России? [/INST]\",\n    \"<s>[INST] Что такое Python? [/INST]\",\n]\n\ntrainer = SFTTrainer(\n    model=model,\n    # ... остальные параметры\n    callbacks=[GenerationCallback(tokenizer, test_prompts, every_n_steps=50)]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Вариант 2: Validation set с метриками","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\n# Разделяем данные\ntrain_data = data[:800]\nval_data = data[800:]\n\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# В TrainingArguments добавляем\ntraining_args = TrainingArguments(\n    # ...\n    evaluation_strategy=\"steps\",  # или \"epoch\"\n    eval_steps=100,               # оценивать каждые 100 шагов\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n)\n\n# Trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,  # добавили validation set\n    # ...\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Вариант 3: Кастомные метрики (ROUGE, BLEU)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom evaluate import load\n\n# Загружаем метрики\nrouge = load('rouge')\nbleu = load('bleu')\n\n# Сам генерируй и считай метрики\ndef evaluate_model(model, val_data):\n    rouge = load('rouge')\n    predictions = []\n    references = []\n    \n    for item in val_data[:50]:\n        prompt = f\"<s>[INST] {item['question']} [/INST]\"\n        \n        # Генерируем\n        generated = generate_response(model, tokenizer, prompt)\n        \n        predictions.append(generated)\n        references.append(item['answer'])\n    \n    scores = rouge.compute(predictions=predictions, references=references)\n    return scores  # чем больше, тем лучше\n\n########################################## ИЛИ\n\nrouge = evaluate.load('rouge')\ndef evaluate_model(model, val_data):\n    predictions = generate_response(model, tokenizer, [f\"<s>[INST] {item['question']} [/INST]\" for item in val_data])\n    references = [item['answer'] for item in val_data]\n\n    return rouge.compute(predictions=predictions, references=references)\n\n########################################## ИЛИ\n\nrouge = evaluate.load('rouge')\ndef evaluate_model(model, tokenizer, val_data, batch_size=8):\n    predictions = []\n    for i in tqdm(range(0, len(val_data), batch_size)):\n        batch = [val_data[j] for j in range(i, min(i+batch_size, len(val_data)))]\n        batch_predictions = generate_response(\n            model, tokenizer,\n            [tokenizer.apply_chat_template(\n                [{'role': 'user', 'content': item['text']}],\n                add_generation_prompt=True, tokenize=False\n            ) for item in batch]\n        )\n        predictions.extend(batch_predictions)\n    references = [item['summary'] for item in val_data]\n    return rouge.compute(predictions=predictions, references=references)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Вариант 4: Полноценная валидация с генерацией (для олимпиады)","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, tokenizer, val_data, num_samples=50):\n    \"\"\"\n    Оценка модели на validation set с реальной генерацией\n    \"\"\"\n    model.eval()\n    results = {\n        \"rouge1\": [],\n        \"rouge2\": [],\n        \"rougeL\": [],\n        \"exact_match\": 0,\n    }\n    \n    rouge_metric = load('rouge')\n    \n    for i, example in enumerate(val_data[:num_samples]):\n        # Формируем промпт\n        prompt = f\"<s>[INST] {example['instruction']} [/INST]\"\n        true_response = example['response']\n        \n        # Генерируем\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=100,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True\n            )\n        \n        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Убираем промпт из сгенерированного текста\n        generated = generated.replace(prompt, \"\").strip()\n        \n        # Считаем метрики\n        rouge_scores = rouge_metric.compute(\n            predictions=[generated],\n            references=[true_response]\n        )\n        \n        results[\"rouge1\"].append(rouge_scores[\"rouge1\"])\n        results[\"rouge2\"].append(rouge_scores[\"rouge2\"])\n        results[\"rougeL\"].append(rouge_scores[\"rougeL\"])\n        \n        # Exact match (для простых задач типа QA)\n        if generated.strip().lower() == true_response.strip().lower():\n            results[\"exact_match\"] += 1\n    \n    # Усредняем\n    final_results = {\n        \"rouge1\": np.mean(results[\"rouge1\"]),\n        \"rouge2\": np.mean(results[\"rouge2\"]),\n        \"rougeL\": np.mean(results[\"rougeL\"]),\n        \"exact_match\": results[\"exact_match\"] / num_samples,\n    }\n    \n    model.train()\n    return final_results\n\n# Использование\nval_results = evaluate_model(model, tokenizer, val_data)\nprint(val_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Часть 4: Debugging и типичные проблемы","metadata":{}},{"cell_type":"markdown","source":"Проблема 1: Модель повторяет промпт","metadata":{}},{"cell_type":"code","source":"# Проблема:\nprompt = \"Вопрос: Столица России?\"\n# Генерация: \"Вопрос: Столица России? Вопрос: Столица России? Вопрос...\"\n\n# Решение 1: Увеличить repetition_penalty\noutputs = model.generate(\n    **inputs,\n    repetition_penalty=1.5,  # было 1.2\n)\n\n# Решение 2: Правильно форматировать промпт (использовать тот же формат что при обучении)\nprompt = \"<s>[INST] Столица России? [/INST]\"  # как в обучении!\n\n# Решение 3: Убрать промпт из вывода\ngenerated = tokenizer.decode(outputs[0], skip_special_tokens=True)\nresponse = generated.replace(prompt, \"\").strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Проблема 2: Модель генерирует бессмыслицу","metadata":{}},{"cell_type":"code","source":"# Причины:\n# 1. Слишком высокий temperature\ntemperature=0.5  # вместо 1.5\n\n# 2. Модель недообучена\n# Проверьте loss, обучите дольше\n\n# 3. Слишком мало данных\n# Нужно минимум 100-500 качественных примеров\n\n# 4. Неправильный формат промпта\n# Используйте ТОТ ЖЕ формат что и при обучении!","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Проблема 3: Модель обрывается на середине","metadata":{}},{"cell_type":"code","source":"# Проблема: генерация заканчивается слишком рано\n\n# Решение 1: Увеличить max_new_tokens\nmax_new_tokens=256  # было 50\n\n# Решение 2: Проверить eos_token\nprint(f\"EOS token: {tokenizer.eos_token}\")\nprint(f\"EOS token ID: {tokenizer.eos_token_id}\")\n\n# Решение 3: Добавить min_new_tokens\nmin_new_tokens=20","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Проблема 4: Медленная генерация","metadata":{}},{"cell_type":"code","source":"# Решение 1: Использовать квантизацию\n# (уже покрыто выше)\n\n# Решение 2: Уменьшить max_new_tokens\nmax_new_tokens=100  # было 512\n\n# Решение 3: Использовать greedy вместо sampling\ndo_sample=False  # быстрее, но менее разнообразно\n\n# Решение 4: Batch inference\n# (покрыто выше)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Часть 6: Быстрый inference для олимпиады","metadata":{}},{"cell_type":"code","source":"class LLMInference:\n    def __init__(self, base_model_name, adapter_path, use_4bit=True):\n        \"\"\"Класс для быстрого inference\"\"\"\n        \n        if use_4bit:\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16\n            )\n            self.model = AutoModelForCausalLM.from_pretrained(\n                base_model_name,\n                quantization_config=bnb_config,\n                device_map=\"auto\"\n            )\n        else:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                base_model_name,\n                torch_dtype=torch.float16,\n                device_map=\"auto\"\n            )\n        \n        self.model = PeftModel.from_pretrained(self.model, adapter_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        self.model.eval()\n    \n    def generate(self, prompt, max_new_tokens=100, temperature=0.7, **kwargs):\n        \"\"\"Генерация одного ответа\"\"\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                top_p=kwargs.get('top_p', 0.9),\n                do_sample=True,\n                repetition_penalty=kwargs.get('repetition_penalty', 1.2),\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        \n        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Убираем промпт\n        response = generated.replace(prompt, \"\").strip()\n        return response\n    \n    def batch_generate(self, prompts, **kwargs):\n        \"\"\"Batch генерация\"\"\"\n        inputs = self.tokenizer(\n            prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=512\n        ).to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=kwargs.get('max_new_tokens', 100),\n                temperature=kwargs.get('temperature', 0.7),\n                top_p=kwargs.get('top_p', 0.9),\n                do_sample=True,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        \n        results = []\n        for i, output in enumerate(outputs):\n            generated = self.tokenizer.decode(output, skip_special_tokens=True)\n            response = generated.replace(prompts[i], \"\").strip()\n            results.append(response)\n        \n        return results\n\n# Использование\ninferencer = LLMInference(\n    base_model_name=\"mistralai/Mistral-7B-v0.1\",\n    adapter_path=\"./final_model\"\n)\n\nresponse = inferencer.generate(\n    \"<s>[INST] Столица России? [/INST]\",\n    max_new_tokens=50,\n    temperature=0.5\n)\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Часть 7: Сохранение и загрузка для submission","metadata":{}},{"cell_type":"code","source":"# === После обучения ===\n\n# Вариант 1: Сохранить только LoRA адаптер (маленький размер)\ntrainer.save_model(\"./lora_adapter\")\n# Размер: ~10-50 MB\n\n# Вариант 2: Объединить и сохранить полную модель\nmodel = model.merge_and_unload()\nmodel.save_pretrained(\"./full_model\")\ntokenizer.save_pretrained(\"./full_model\")\n# Размер: ~13 GB для 7B модели\n\n# === Для загрузки ===\n\n# Вариант 1: Загрузить LoRA адаптер\nbase_model = AutoModelForCausalLM.from_pretrained(...)\nmodel = PeftModel.from_pretrained(base_model, \"./lora_adapter\")\n\n# Вариант 2: Загрузить полную модель\nmodel = AutoModelForCausalLM.from_pretrained(\"./full_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"./full_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}