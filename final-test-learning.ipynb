{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13280030,"sourceType":"datasetVersion","datasetId":8416190}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# re","metadata":{}},{"cell_type":"code","source":"# ============ –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ ============\n\n# re.search(pattern, text) - –Ω–∞–π—Ç–∏ –ø–µ—Ä–≤–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ\n# re.findall(pattern, text) - –Ω–∞–π—Ç–∏ –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è\n# re.sub(pattern, replacement, text) - –∑–∞–º–µ–Ω–∏—Ç—å\n# re.match(pattern, text) - —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏\n# re.split(pattern, text) - —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É\n\n# ============ –û–°–ù–û–í–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ ============\n\n# –°–ò–ú–í–û–õ–´:\n# .       –ª—é–±–æ–π —Å–∏–º–≤–æ–ª\n# *       0+ —Ä–∞–∑\n# +       1+ —Ä–∞–∑\n# ?       0 –∏–ª–∏ 1 —Ä–∞–∑\n# {n,m}   –æ—Ç n –¥–æ m —Ä–∞–∑\n\n# –ö–õ–ê–°–°–´:\n# \\d      —Ü–∏—Ñ—Ä–∞ [0-9]\n# \\w      –±—É–∫–≤–∞/—Ü–∏—Ñ—Ä–∞/_ [a-zA-Z0-9_]\n# \\s      –ø—Ä–æ–±–µ–ª/—Ç–∞–±/–Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞\n# \\D      –Ω–µ —Ü–∏—Ñ—Ä–∞\n# \\W      –Ω–µ –±—É–∫–≤–∞/—Ü–∏—Ñ—Ä–∞\n# \\S      –Ω–µ –ø—Ä–æ–±–µ–ª\n# [abc]   a –∏–ª–∏ b –∏–ª–∏ c\n# [^abc]  –Ω–µ a, –Ω–µ b, –Ω–µ c\n# [a-z]   –æ—Ç a –¥–æ z\n\n# –Ø–ö–û–†–Ø:\n# ^       –Ω–∞—á–∞–ª–æ —Å—Ç—Ä–æ–∫–∏\n# $       –∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏\n# \\b      –≥—Ä–∞–Ω–∏—Ü–∞ —Å–ª–æ–≤–∞\n\n# –ì–†–£–ü–ü–´:\n# ()      –≥—Ä—É–ø–ø–∞ –∑–∞—Ö–≤–∞—Ç–∞\n# (?:)    –≥—Ä—É–ø–ø–∞ –±–µ–∑ –∑–∞—Ö–≤–∞—Ç–∞\n\n# –ñ–ê–î–ù–û–°–¢–¨:\n# *       –∂–∞–¥–Ω—ã–π\n# *?      –ª–µ–Ω–∏–≤—ã–π\n# +       –∂–∞–¥–Ω—ã–π\n# +?      –ª–µ–Ω–∏–≤—ã–π\n\n# –î–†–£–ì–û–ï:\n# |       –∏–ª–∏\n# \\       —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 1: –ù–∞–π—Ç–∏ –≤—Å–µ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–í –∑–∞–∫–∞–∑–µ 3 —è–±–ª–æ–∫–∞ –ø–æ 150 —Ä—É–±–ª–µ–π –∏ 10 –≥—Ä—É—à –ø–æ 99.\"\ntext = re.findall(r'\\d+', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 2: –ù–∞–π—Ç–∏ –≤—Å–µ —Å–ª–æ–≤–∞","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"Python_3.9 - —ç—Ç–æ –∫—Ä—É—Ç–æ! –ê –≤–æ—Ç re -- –Ω–µ—Ç.\"\ntext = re.findall(r'\\w+', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 3: –ù–∞–π—Ç–∏ —Å–ª–æ–≤–∞ –∏–∑ 4 –±—É–∫–≤","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É, –∞ –ø–∞–ø–∞ –ø–∏–ª —á–∞–π.\"\ntext = re.findall(r'\\w{4}+', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 4: –ó–∞–º–µ–Ω–∏—Ç—å –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Ç–∞–±—ã –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–≠—Ç–æ   —Ç–µ–∫—Å—Ç  —Å \\t –ª–∏—à–Ω–∏–º–∏ \\n –ø—Ä–æ–±–µ–ª–∞–º–∏.\"\ntext = re.sub(r'\\s+', ' ', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 5: –ù–∞–π—Ç–∏ —Ö—ç—à—Ç–µ–≥–∏","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–¢—Ä–µ–Ω–¥—ã: #python, #ai –∏ #ml. –≠—Ç–æ –Ω–µ # —Ö—ç—à—Ç–µ–≥.\"\ntext = re.findall(r'#\\w+', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 6: –ù–∞–π—Ç–∏ —Å–ª–æ–≤–∞ –∏–∑ 5 –±—É–∫–≤","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–æ–¥–∏–Ω, –¥–≤–∞, —Ç—Ä–∏, —á–µ—Ç—ã—Ä–µ, –ø—è—Ç—å, —à–µ—Å—Ç—å\"\ntext = re.findall(r'\\b\\w{5}+\\b', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 7: –ù–∞–π—Ç–∏ —Å–ª–æ–≤–∞ –ø–æ —à–∞–±–ª–æ–Ω—É \"–ø.—Ç\"","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–≠—Ç–æ –ø–∏—Ç, –ø–æ—Ç, –ø–∞—Ç, –Ω–æ –Ω–µ –ø—Ç –∏ –Ω–µ –ø–∞—É—Ç–∏–Ω–∞.\"\ntext = re.findall(r'\\b–ø\\w+—Ç\\b', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 8: –ù–∞–π—Ç–∏ –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—É—é –±—É–∫–≤—É","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"My favorite color is red, but the British spelling is colour.\"\ntext = re.findall(r'colo.?r', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 9: –ù–∞–π—Ç–∏ –≤—Å–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ö–∞–∫ –¥–µ–ª–∞? –í—Å—ë —Ö–æ—Ä–æ—à–æ.\"\ntext = re.findall(r'[^\\w\\s]', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 10: –ù–∞–π—Ç–∏ —Å–ª–æ–≤–∞, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–ú–µ–Ω—è –∑–æ–≤—É—Ç Python. –Ø–∑—ã–∫ Python –±—ã–ª —Å–æ–∑–¥–∞–Ω –ì–≤–∏–¥–æ.\"\ntext = re.findall(r'\\b[–ê-–ØA-Z]\\w+\\b', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 11: –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –∫–∞–≤—ã—á–µ–∫","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = '–û–Ω —Å–∫–∞–∑–∞–ª: \"–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!\", –∏ —É—à—ë–ª.'\ntext = re.findall(r'\"(.+)\"', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 12: –ò–∑–≤–ª–µ—á—å –∫–æ–º–∞–Ω–¥—É –∏–∑ —Å—Ç—Ä–æ–∫–∏-–ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—è","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"$ ls -la\\n# —ç—Ç–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π\\n$ python script.py\"\ntext = re.search(r'\\$(.+)\\$', text, re.DOTALL).group(1)\ntext","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 13: –ò–∑–≤–ª–µ—á—å –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –∫–ª—é—á—É","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"Date: 2023-10-27\\nName: John Doe\\nStatus: active\"\ntext = re.search(r'Name: (.+)\\n', text).group(1)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 14: –ù–∞–π—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞-–º–∞—Ä–∫–µ—Ä—ã","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"Log: Warning - disk space low. Fatal error occurred. Not an errorist.\"\ntext = re.findall(r'\\b(warning|error|fatal)\\b', text, re.I)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 15: –ò–∑–≤–ª–µ—á—å –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–π","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"Source: web\\nUser: admin\\nAction: login\"\ntext = re.findall(r'(?:user|login):\\s*(.+)', text, re.I)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 16: –†–∞–∑–æ–±—Ä–∞—Ç—å –¥–∞—Ç—É –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–°–µ–≥–æ–¥–Ω—è—à–Ω—è—è –¥–∞—Ç–∞: 27-10-2023.\"\ntext = re.search(r'(\\d{2})-(\\d{2})-(\\d{4})', text).groups()\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 17: –ò–∑–≤–ª–µ—á—å –¥–æ–º–µ–Ω –∏–∑ email-–∞–¥—Ä–µ—Å–∞","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–ö–æ–Ω—Ç–∞–∫—Ç—ã: user@example.com, admin@google.co.uk.\"\ntext = re.findall(r'\\w+@([\\w.]+)', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 18: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –≤—Å—è —Å—Ç—Ä–æ–∫–∞ ‚Äî —ç—Ç–æ ID","metadata":{}},{"cell_type":"code","source":"import re\n\ndef is_valid_id(s):\n    match = re.search(r'\\d+-\\d+-\\d+', s)\n    return match is not None\n\nprint(is_valid_id(\"123-45-67\")) # True\nprint(is_valid_id(\"abc-123\"))  # False\nprint(is_valid_id(\"123\"))      # False (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 19: –ò–∑–≤–ª–µ—á—å –ø–æ–ª–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è","metadata":{}},{"cell_type":"code","source":"import re\n\ntext = \"–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ö–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞? –í—Å—ë —Ö–æ—Ä–æ—à–æ.\"\ntext = re.findall(r'(.+?[\\.!+])', text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 20: –†–∞–∑–æ–±—Ä–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ \"–∫–ª—é—á: –∑–Ω–∞—á–µ–Ω–∏–µ\"","metadata":{}},{"cell_type":"code","source":"# import re\n\n# text = \"\"\"\n# id: 101\n# name: Alice\n# status: active\n# \"\"\"\n# text = re.findall(r'(.+):\\s*(.+)', text)\n# print(text)\n\nimport re\n\ntext = \"\"\"\n–£ —Ç–µ–±—è —Ç–∞–∫–∞—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞...\n\n–í–æ—Ç —è –µ—ë —Ä–µ—à–∞—é:\n—à–∞–≥ 1: –±–ª–∞ –±–ª–∞ –±–ª–∞\n...\n—à–∞–≥ n: –±–ª–∞ –±–ª–∞ –±–ª–∞\n\n–û—Ç–≤–µ—Ç:\n–û—Ç–≤–µ—Ç: /boxed{324}/\n\"\"\"\nnumber = re.findall(r'/boxed{(\\d+)}/', text)[-1]\ntext = re.sub(r'.*/boxed{\\d+}/', '#### ' + str(number), text)\nprint(text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# os","metadata":{}},{"cell_type":"code","source":"import os\n\nos.makedirs('/kaggle/workers')\nos.makedirs('/kaggle/working/', exist_ok=True)\n\nprint(os.path.join('dir/meow/osu!', 'osu.py'))\nprint(os.path.exists('/kaggle/working/results/run_01'))\nprint(os.path.isdir('/kaggle/working/results/run_01'))\nprint(os.path.basename('dir/meow/osu!/osu.py'))\nprint(os.path.splitext('osu.py'))\nprint(os.path.dirname('random_dir123/simple_text.txt'))\nprint(os.path.isfile('/kaggle/working/results/run_01'))\nprint(os.listdir('/kaggle/input/final-step-testing-sirius-ai'))\nprint(list(os.walk('/kaggle/working/')))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nos.makedirs('lkjhasdfklj')\nos.makedirs('dlkjahsdf', exist_ok=True)\n\nprint(os.path.join('/asdf/af', 'osu.py'))\nprint(os.path.basename('osu.py'))\nprint(os.listdir('/kaggle/working'))\nprint(list(os.walk('/kaggle/working')))\nprint(os.path.isfile('file.com'))\nprint(os.path.isdir('thisisnotdir'))\nprint(os.path.dirname('sdfsdfsdf/path.com'))\nprint(os.path.splitext('osu.py'))\nprint(os.path.exists('ddddd'))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ –æ–¥–Ω—É.","metadata":{}},{"cell_type":"markdown","source":"–°–æ–∑–¥–∞–Ω–∏–µ MLP.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass VerySmartModel(nn.Module):\n    def __init__(self, input_dim=784, hidden_dims=None, dropout_rate=0.3, num_classes=10):\n        super(VerySmartModel, self).__init__()\n\n        if hidden_dims is None:\n            hidden_dims = [512, 256]\n\n        dims = [input_dim] + hidden_dims\n        self.layers = nn.ModuleList()\n\n        for i in range(len(dims) - 1):\n            self.layers.extend([\n                nn.Linear(dims[i], dims[i + 1]),\n                nn.BatchNorm1d(dims[i + 1]),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate)\n            ])\n\n        self.layers.append(nn.Linear(dims[-1], num_classes))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 2: CNN –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ —Å–≤—ë—Ä—Ç–æ—á–Ω—É—é —Å–µ—Ç—å –¥–ª—è CIFAR-10:\n# - 4 conv –±–ª–æ–∫–∞ (–∫–∞–∂–¥—ã–π: Conv2d ‚Üí BatchNorm ‚Üí ReLU ‚Üí MaxPool)\n# - –ö–∞–Ω–∞–ª—ã: 32 ‚Üí 64 ‚Üí 128 ‚Üí 256\n# - Flatten + 2 FC —Å–ª–æ—è\n\nimport torch.nn as nn\nimport numpy as np\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n\n        conv_dims = [3, 32, 64, 128, 256]\n        head_dims = [1024, 256]\n\n        self.layers = nn.ModuleList()\n        for i in range(len(conv_dims) - 1):\n            self.layers.extend([\n                nn.Conv2d(conv_dims[i], conv_dims[i + 1], kernel_size=(3, 3), padding=1),\n                nn.BatchNorm2d(conv_dims[i + 1]),\n                nn.ReLU(),\n                nn.MaxPool2d((2, 2))\n            ])\n\n        self.layers.append(nn.Flatten())\n        for i in range(len(head_dims) - 1):\n            self.layers.extend([\n                nn.Linear(head_dims[i], head_dims[i + 1]),\n                nn.BatchNorm1d(head_dims[i + 1]),\n                nn.ReLU(),\n                nn.Dropout(0.3)\n            ])\n        self.layers.append(nn.Linear(head_dims[-1], 10))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\nmodel = CNN()\nmodel.eval()\n\nx = np.random.rand(1, 3, 32, 32)\nx = torch.tensor(x, dtype=torch.float)\nx = model.forward(x)\nprint(x)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom transformers import BertTokenizer, Trainer, TrainingArguments\nimport numpy as np\n\nclass CustomDataset(Dataset):\n    def __init__(self, texts, tabular_features, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.tabular_features = tabular_features\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tabular = self.tabular_features[idx]\n        label = self.labels[idx]\n        \n        encoded = self.tokenizer(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoded['input_ids'].squeeze(0),\n            'attention_mask': encoded['attention_mask'].squeeze(0),\n            'token_type_ids': encoded['token_type_ids'].squeeze(0),\n            'tabular_features': torch.tensor(tabular, dtype=torch.float32),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n\nclass BertWithMLPForTrainer(nn.Module):\n    def __init__(self, num_tabular_features, num_classes, hidden_dim=256, dropout=0.3):\n        super(BertWithMLPForTrainer, self).__init__()\n        \n        self.bert = BertModel.from_pretrained('google-bert/bert-base-uncased')\n        self.bert_hidden_size = self.bert.config.hidden_size\n        \n        self.tabular_mlp = nn.Sequential(\n            nn.Linear(num_tabular_features, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 128),\n            nn.ReLU()\n        )\n        \n        combined_dim = self.bert_hidden_size + 128\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(combined_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, input_ids, attention_mask, token_type_ids, tabular_features, labels=None):\n        bert_output = self.bert(\n            input_ids=input_ids, \n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        text_features = bert_output.last_hidden_state[:, 0, :]\n        \n        tabular_features = self.tabular_mlp(tabular_features)\n        \n        combined = torch.cat([text_features, tabular_features], dim=1)\n        \n        logits = self.classifier(combined)\n        \n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits, labels)\n        \n        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}\n\n\ntexts = [\"Sample text \" + str(i) for i in range(1000)]\ntabular_features = np.random.randn(1000, 20)\nlabels = np.random.randint(0, 2, 1000)\n\ntokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\ndataset = CustomDataset(texts, tabular_features, labels, tokenizer)\n\nmodel = BertWithMLPForTrainer(num_tabular_features=20, num_classes=2)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    learning_rate=2e-5,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_steps=10,\n    save_strategy='epoch',\n    eval_strategy='no',\n    report_to='none'\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n)\n\ntrainer.train()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.","metadata":{}},{"cell_type":"code","source":"# —ç—Ç–æ —Ä–µ—à–∞–µ—Ç —Ç–∞–∫—É—é –æ—à–∏–±–∫—É:\n# RemoteEntryNotFoundError: 404 Client Error. (Request ID: Root=1-68e54624-2d9293a52c06cef514db67ce;ef2314f7-a869-4c0d-b880-4e97781bd6cb)\n# Entry Not Found for url: https://huggingface.co/api/models/cointegrated/rubert-tiny2/tree/main/additional_chat_templates?recursive=false&expand=false.\n# additional_chat_templates does not exist on \"main\"\n\n!pip install -q --upgrade transformers==4.45.0 huggingface_hub\n\nimport os\nos.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install evaluate\n\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer, TrainingArguments, AutoModel, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch\nimport numpy as np\nimport evaluate\nimport os\nimport json\n\nclass TextTabModel(nn.Module):\n    def __init__(self, tab_input_dim, num_classes=3, checkpoint='prajjwal1/bert-tiny'):\n        super(TextTabModel, self).__init__()\n\n        self.config = {\n            'tab_input_dim': tab_input_dim,\n            'num_classes': num_classes,\n            'checkpoint': checkpoint\n        }\n\n        self.text_model = AutoModel.from_pretrained(checkpoint)\n        self.text_output_dim = self.text_model.config.hidden_size\n\n        self.tab_output_dim = 32\n        self.tab_model = nn.Sequential(\n            nn.Linear(tab_input_dim, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Linear(64, self.tab_output_dim),\n            nn.BatchNorm1d(self.tab_output_dim),\n            nn.ReLU(),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(self.text_output_dim + self.tab_output_dim, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes),\n        )\n\n    def forward(self, tab_features, input_ids, attention_mask, token_type_ids, labels=None):\n        tab_embedding = self.tab_model(tab_features)\n\n        text_embedding = self.text_model(\n            input_ids=torch.tensor(input_ids, dtype=torch.long),\n            attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n            token_type_ids=torch.tensor(token_type_ids, dtype=torch.long)\n        ).last_hidden_state[:, 0]\n\n        embedding = torch.cat([tab_embedding, text_embedding], dim=1)\n        logits = self.classifier(embedding)\n\n        output = {'logits': logits}\n        if labels is not None:\n            logits = logits\n            labels = torch.tensor(labels, dtype=torch.long)\n            output['loss'] = nn.CrossEntropyLoss()(logits, labels)\n\n        return output\n\n    def save_pretrained(self, save_directory):\n        os.makedirs(save_directory, exist_ok=True)\n\n        with open(os.path.join(save_directory, 'config.json'), 'w') as f:\n            json.dump(self.config, f, indent=2)\n\n        state_dict = {k: v.contiguous() for k, v in self.state_dict().items()}\n        torch.save(state_dict, os.path.join(save_directory, 'pytorch_model.bin'))\n    \n    @classmethod\n    def from_pretrained(cls, load_directory):\n        with open(os.path.join(load_directory, 'config.json'), 'r') as f:\n            config = json.load(f)\n\n        model = cls(**config)\n\n        state_dict = torch.load(os.path.join(load_directory, 'pytorch_model.bin'))\n        model.load_state_dict(state_dict)\n\n        return model\n\nclass TextTabDataset(Dataset):\n    def __init__(self, tokenizer, texts, tab_features, labels=None, max_length=512):\n        self.tokenizer = tokenizer\n        self.texts = texts\n        self.tab_features = tab_features\n        self.labels = labels\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        encoded = self.tokenizer(\n            self.texts[index],\n            padding=False,\n            truncation=True,\n            max_length=self.max_length,\n            add_special_tokens=True,\n            return_tensors=None\n        )\n\n        encoded['input_ids'] = torch.tensor(encoded['input_ids'], dtype=torch.float)\n        encoded['attention_mask'] = torch.tensor(encoded['attention_mask'], dtype=torch.float)\n        encoded['token_type_ids'] = torch.tensor(encoded['token_type_ids'], dtype=torch.float)\n        encoded['tab_features'] = torch.tensor(self.tab_features[index], dtype=torch.float)\n        if self.labels is not None:\n            encoded['labels'] = torch.tensor(self.labels[index], dtype=torch.float)\n\n        return encoded\n\ndef get_dummy_data(n_samples=2000, tab_features=20, n_classes=3, test_size=0.2):\n    texts = [f'Sample text ‚Ññ{i}.' for i in range(n_samples)]\n    tabular_features = np.random.randn(n_samples, tab_features)\n    labels = np.random.randint(0, n_classes - 1, n_samples)\n\n    return train_test_split(\n        texts, tabular_features, labels,\n        test_size=test_size, shuffle=True,\n        random_state=42, stratify=labels\n    )\n\ntrain_texts, val_texts, train_tab_feats, val_tab_feats, train_labels, val_labels = get_dummy_data()\n\ncheckpoint = 'prajjwal1/bert-tiny'\nmodel = TextTabModel(tab_input_dim=20, checkpoint=checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntrain_dataset = TextTabDataset(tokenizer, train_texts, train_tab_feats, train_labels)\nval_dataset = TextTabDataset(tokenizer, val_texts, val_tab_feats, val_labels)\n\nf1 = evaluate.load('f1')\ndef compute_metrics(preds):\n    logits, labels = preds\n    predictions = np.argmax(logits, axis=-1)\n    return f1.compute(predictions=predictions, references=labels, average='weighted')\n\noptimizer = torch.optim.AdamW([\n    {'params': model.text_model.parameters(), 'lr': 2e-5},\n    {'params': model.tab_model.parameters(), 'lr': 1e-4},\n    {'params': model.classifier.parameters(), 'lr': 5e-5}\n])\n\nargs = TrainingArguments(\n    per_device_train_batch_size=16,\n    report_to='none',\n    num_train_epochs=10,\n    output_dir='./result',\n    logging_strategy='steps',\n    eval_strategy='steps',\n    save_strategy='steps',\n    logging_steps=50,\n    eval_steps=50,\n    save_steps=50,\n    metric_for_best_model='f1',\n    load_best_model_at_end=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    save_safetensors=False  # –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è TextTabModel.save_pretrained\n)\ntrainer = Trainer(\n    args=args,\n    model=model,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    optimizers=(optimizer, None)\n)\ntrainer.train()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# datasets.Dataset","metadata":{}},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 1: –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ Dataset –∏–∑ —Å–ª–æ–≤–∞—Ä—è —Å –ø–æ–ª—è–º–∏: 'text', 'label'\n# –î–æ–±–∞–≤—å—Ç–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –º–µ—Ç–æ–∫\n\nfrom datasets import Dataset\n\ndata = {'text': [f'random text {i + 1}' for i in range(5)], 'label': [i % 2 for i in range(5)]}\ndataset = Dataset.from_dict(data)\nprint(dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 2: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ Hub","metadata":{}},{"cell_type":"code","source":"# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç 'imdb' –∏–∑ Hugging Face Hub\n# –í—ã–≤–µ–¥–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∏ —Ä–∞–∑–º–µ—Ä–µ\n\nfrom datasets import Dataset, load_dataset\n\ndataset = load_dataset('imdb')\n\nprint(dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 3: –ü—Ä–æ—Å–º–æ—Ç—Ä —ç–ª–µ–º–µ–Ω—Ç–æ–≤","metadata":{}},{"cell_type":"code","source":"# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –ª—é–±–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –≤—ã–≤–µ–¥–∏—Ç–µ:\n# - –ø–µ—Ä–≤—ã–µ 3 —ç–ª–µ–º–µ–Ω—Ç–∞\n# - —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ [10, 20, 30]\n# - —Å–ª—É—á–∞–π–Ω—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n\nfrom datasets import Dataset, load_dataset\nimport random\n\ndataset = load_dataset('imdb', split='train')\n\nprint(dataset[:3])\nprint(dataset[[10, 20, 30]])\nprint(dataset.shuffle(seed=42)[:5])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 4: –†–∞–±–æ—Ç–∞ —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ 'id', 'text', 'label', 'score'\n# –£–¥–∞–ª–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É 'score'\n# –ü–µ—Ä–µ–∏–º–µ–Ω—É–π—Ç–µ 'text' –≤ 'sentence'\n# –í—ã–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫\n\nfrom datasets import Dataset\nimport pandas as pd\nimport random\n\nsize = 5\ndata = {\n    'id': list(range(size)),\n    'text': [f'random text {i + 1}' for i in range(size)],\n    'label': [i % 2 for i in range(size)],\n    'score': [random.randint(10, 50) / 10 for _ in range(size)]\n}\ndata = pd.DataFrame(data)\ndataset = Dataset.from_pandas(data)\n\ndataset = dataset.remove_columns('score').rename_column('text', 'sentence')\n\nprint(dataset.column_names)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 5: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö","metadata":{}},{"cell_type":"code","source":"# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å –º–µ—Ç–∫–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, imdb)\n# –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä—ã —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –º–µ—Ç–∫–æ–π (label == 1)\n# –í—ã–≤–µ–¥–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n\nfrom datasets import load_dataset\n\ndataset = load_dataset('imdb', split='train')\ndataset.filter(lambda x: [label == 1 for label in x['label']], batched=True, batch_size=1000)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 6: –ú–∞–ø–ø–∏–Ω–≥ —Ñ—É–Ω–∫—Ü–∏–∏","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å —Ç–µ–∫—Å—Ç–∞–º–∏\n# –ü—Ä–∏–º–µ–Ω–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é map(), –∫–æ—Ç–æ—Ä–∞—è:\n# - –¥–æ–±–∞–≤–ª—è–µ—Ç –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –≤ –Ω–æ–≤–æ–µ –ø–æ–ª–µ 'length'\n# - –ø–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä\n\nimport datasets\nimport random\n\nchars = 'A√ß√±√∂√º√ü √†√©√¨√≤√π –ñ–∑—û—ì—ú –á—ó–Ñ—î ŒëŒ≤Œ≥ŒîŒµ Œ®Œ©Œ∂Œ∑Œ∏ ‘±’¢’£‘¥’• ‘µ’¶’ß’®’© ·Éê·Éë·Éí·Éì·Éî ·Éï·Éñ·Éó ◊ê◊ë◊í◊ì◊î◊ï◊ñ ◊ó◊ò◊ô◊õ ÿ£ÿ®ÿ¨ÿØ ŸáŸàÿ≤ ÿ≠ÿ∑Ÿä ŸÉŸÑŸÖŸÜ ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§ï‡§ñ‡§ó‡§ò‡§ô ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞ ‡¶Ö‡¶Ü‡¶á‡¶à ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä ‡®ó‡©Å‡®∞‡®Æ‡©Å‡®ñ‡©Ä ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä ‡™Ö‡™Ü‡™á‡™à ‡¨ì‡¨°‡¨º‡¨ø‡¨Ü ‡¨Ö‡¨Ü‡¨á‡¨à ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ ‡ÆÖ‡ÆÜ‡Æá‡Æà ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞Ö‡∞Ü‡∞á‡∞à ‡≤ï‡≤®‡≥ç‡≤®‡≤° ‡≤Ö‡≤Ü‡≤á‡≤à ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç ‡¥Ö‡¥Ü‡¥á‡¥à ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏Å‡∏Ç‡∏Ñ‡∏Ü‡∏á ‡∫•‡∫≤‡∫ß ‡∫™‡∫∞‡∫ö·û∂·ûô‡∫î‡∫µ ·Äô·Äº·Äî·Ä∫·Äô·Ä¨ ·ÄÄ·ÄÅ·ÄÇ·ÄÉ·ÄÑ ·ûó·û∂·ûü·û∂·ûÅ·üí·ûò·üÇ·ûö ·û¢·ûÄ·üí·ûü·ûö‡ΩÅ·ûò·üÇ·ûö ‰Ω†Â•Ω‰∏ñÁïå ÈæçÈ¶¨Á≤æÁ•û „Åì„Çì„Å´„Å°„ÅØ „Å≤„Çâ„Åå„Å™ „Ç´„Çø„Ç´„Éä ÏïàÎÖïÌïòÏÑ∏Ïöî ÌïúÍ∏Ä ·àÄ·àà·àê·àò·à†·à®·à∞ ·ö†·ö¢·ö¶·ö®·ö±·ö≤·ö∑·öπ ·öª·öæ·õÅ·õÉ·õá·õà·õâ·õã ·öê·öÅ·öÇ·öÉ·öÑ·öÖ ·öÜ·öá·öà·öâ·öä ·é†·é°·é¢·é£·é§·è£·é≥ ·êä·êÉ·êÖ·ê∏·ëï ·ë≤·ìá·ëï ‚¥∞‚¥±‚¥≥‚¥∑‚¥ª‚¥º ‚∞Ä‚∞Å‚∞Ç‚∞É‚∞Ñ‚∞Ö ‚∞Ü‚∞á‚∞à‚∞â‚∞ä êå∞êå±êå≤êå≥êå¥êåµ ‚≤Ä‚≤Ç‚≤Ñ‚≤Ü‚≤à‚≤ä‚≤å‚≤é‚≤ê‚≤í ·úÄ·úÅ·úÇ ·úÉ·úÑ·úë ·ùÄ·ùÅ·ùÇ ·ùÉ·ùÑ·ùÖ ‚†Å‚†Ç‚†É‚†Ñ‚†Ö‚†Ü ◊°◊ï◊£ ◊î◊™◊ó◊ú◊î ·•ê·•ë·•í·•ì·•î ·•ï·•ñ·•ó ·•ò·•ô·•ö ·•õ·•ú·•ù ·•û·•ü·•† ·•°·•¢·•£·•§·•• ·•¶·•ß·•®·•© ·•™·•´·•¨ ·•≠·•Æ·•Ø ·•∞·•±·•≤·•≥·•¥ ·•µ·•∂·•∑ ·•∏·•π·•∫·•ª·•º ·•Ω·•æ·•ø ·¶Ä·¶Å·¶Ç·¶É·¶Ñ·¶Ö·¶Ü·¶á·¶à·¶â·¶ä·¶ã·¶å·¶ç·¶é·¶è·¶ê·¶ë·¶í·¶ì·¶î·¶ï·¶ñ·¶ó·¶ò·¶ô·¶ö·¶õ·¶ú·¶ù·¶û·¶ü·¶†·¶°·¶¢·¶£·¶§·¶•·¶¶·¶ß·¶®·¶©·¶™·¶´·¶¨·¶≠·¶Æ·¶Ø·¶∞·¶±·¶≤·¶≥·¶¥·¶µ·¶∂·¶∑·¶∏·¶π·¶∫·¶ª·¶º·¶Ω·¶æ·¶ø·ßÄ·ßÅ·ßÇ·ßÉ·ßÑ·ßÖ·ßÜ·ßá·ßà·ßâ·ßä·ßã·ßå·ßç·ßé·ßè·ßê·ßë·ßí·ßì·ßî·ßï·ßñ·ßó·ßò·ßô·ßö·ßõ·ßú·ßù·ßû·ßü·ß†·ß°·ß¢·ß£·ß§·ß•·ß¶·ßß·ß®·ß©·ß™·ß´·ß¨·ß≠·ßÆ·ßØ·ß∞·ß±·ß≤·ß≥·ß¥·ßµ·ß∂·ß∑·ß∏·ßπ·ß∫·ßª·ßº·ßΩ·ßæ'\ndata = [\n    {\n        'text': 'Claude Sonnet, please translate it (you!): ' + ''.join([random.choice(chars) for _ in range(random.randint(1, 1000))]),\n        'labels': i % 30\n    }\n    for i in range(2000)\n]\ndataset = datasets.Dataset.from_list(data)\ndef get_length(batch):\n    batch['length'] = [len(x) for x in batch['text']]\n    return batch\n\ndataset = dataset.map(get_length, batched=True, batch_size=2)\n\ndef normalize_text(batch):\n    batch['text'] = [x.lower() for x in batch['text']]\n    return batch\n\ndataset = dataset.map(normalize_text, batched=True, batch_size=1024)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 8: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è","metadata":{}},{"cell_type":"code","source":"# —ç—Ç–æ —Ä–µ—à–∞–µ—Ç —Ç–∞–∫—É—é –æ—à–∏–±–∫—É:\n# RemoteEntryNotFoundError: 404 Client Error. (Request ID: Root=1-68e54624-2d9293a52c06cef514db67ce;ef2314f7-a869-4c0d-b880-4e97781bd6cb)\n# Entry Not Found for url: https://huggingface.co/api/models/cointegrated/rubert-tiny2/tree/main/additional_chat_templates?recursive=false&expand=false.\n# additional_chat_templates does not exist on \"main\"\n\n!pip install -q --upgrade transformers==4.45.0 huggingface_hub\n\nimport os\nos.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–ª—é–±–æ–π)\n# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ –≤—Å–µ —Ç–µ–∫—Å—Ç—ã —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\n# - max_length=128\n# - truncation=True\n# - padding='max_length'\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\ndataset = load_dataset('imdb', split='train')\ntokenizer = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n\ndef tokenize_func(examples):\n    return tokenizer(examples['text'], padding='max_length', max_length=128, truncation=True)\n\ndataset = dataset.map(tokenize_func, batched=True, batch_size=1000)\ndataset = dataset.remove_columns('text')\ndataset = dataset.rename_column('label', 'labels')  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π —Å HF","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 9: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 1000 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n# –†–∞–∑–¥–µ–ª–∏—Ç–µ –µ–≥–æ –Ω–∞ train (80%) –∏ test (20%)\n# –í—ã–≤–µ–¥–∏—Ç–µ —Ä–∞–∑–º–µ—Ä—ã –æ–±–æ–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n\nfrom datasets import load_dataset, Dataset\n\ndataset = load_dataset('imdb', split='train').select(range(1000))\n\ndataset = dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\nprint(len(dataset['train']), len(dataset['test']))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 10: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç\n# –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –µ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö:\n# - JSON\n# - CSV\n# - Arrow (to_disk)\n# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–±—Ä–∞—Ç–Ω–æ –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å\n\nfrom datasets import load_dataset, Dataset\n\ndataset = load_dataset('imdb', split='train')\n\ndataset.to_json('jsonchik.json')\nprint(load_dataset('json', data_files='jsonchik.json', split='train'))\n\ndataset.to_csv('csveshachka.csv')\nprint(load_dataset('csv', data_files='csveshachka.csv', split='train'))\n\ndataset.save_to_disk('file_na_diske')\nprint(Dataset.load_from_disk('file_na_diske'))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 11: –†–∞–±–æ—Ç–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å 'text1', 'text2', 'label'\n# –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É 'combined', –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é text1 –∏ text2\n# –£–¥–∞–ª–∏—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n\nfrom datasets import Dataset\n\ndataset = Dataset.from_dict({'text1': [f'Text 1 ‚Ññ{i}' for i in range(100)], 'text2': [f'Text 2 ‚Ññ{i}' for i in range(100)], 'label': [i % 2 for i in range(100)]})\n\ndef combine(batch):\n    batch['combined'] = [text1 + text2 for text1, text2 in zip(batch['text1'], batch['text2'])]\n    return batch\n    \ndataset = dataset.map(combine, batched=True, batch_size=20)\ndataset = dataset.remove_columns(['text1', 'text2'])\n\nprint(dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 12: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å —á–∏—Å–ª–æ–≤—ã–º –ø–æ–ª–µ–º 'score'\n# –û—Ç—Å–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –ø–æ score –≤ –ø–æ—Ä—è–¥–∫–µ —É–±—ã–≤–∞–Ω–∏—è\n# –í—ã–≤–µ–¥–∏—Ç–µ —Ç–æ–ø-5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n\nfrom datasets import Dataset\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({\n    'image': ['–∑–¥–µ—Å—å —Ç–∏–ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –µ—Å—Ç—å'] * 100,\n    'score': np.random.rand(2, 2, 2, 2, 2, 2).ravel().tolist() + np.random.rand(3, 2, 3, 2).ravel().tolist()\n})\ndataset = Dataset.from_pandas(data)\n\ndataset = dataset.sort('score', reverse=True)\nprint(dataset.select(range(5)).to_pandas())","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 13: Shuffle –∏ Select","metadata":{}},{"cell_type":"code","source":"# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –ª—é–±–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n# –ü–µ—Ä–µ–º–µ—à–∞–π—Ç–µ –µ–≥–æ —Å seed=42\n# –í—ã–±–µ—Ä–∏—Ç–µ –ø–µ—Ä–≤—ã–µ 100 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è\n\nfrom datasets import load_dataset\n\ndataset = load_dataset('imdb', split='train').shuffle(seed=42).select(range(100))\nprint(dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 14: –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–≤–∞ Dataset —Å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π\n# –û–±—ä–µ–¥–∏–Ω–∏—Ç–µ –∏—Ö —Å –ø–æ–º–æ—â—å—é concatenate_datasets()\n# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä\n\nfrom datasets import Dataset, concatenate_datasets\nimport pandas as pd\nimport numpy as np\n\ndef get_dummy_dataset(version):\n    data = pd.DataFrame({\n    'image': [f'–∑–¥–µ—Å—å —Ç–∏–ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –µ—Å—Ç—å, –≤–µ—Ä—Å–∏—è {version}'] * 100,\n    'score': np.random.rand(2, 2, 2, 2, 2, 2).ravel().tolist() + np.random.rand(3, 2, 3, 2).ravel().tolist()\n    })\n    dataset = Dataset.from_pandas(data)\n    return dataset\n\ndataset = concatenate_datasets([get_dummy_dataset(1), get_dummy_dataset(2)])\n\nprint(dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 16: Train/Validation/Test split","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç\n# –†–∞–∑–¥–µ–ª–∏—Ç–µ –µ–≥–æ –Ω–∞:\n# - train: 70%\n# - validation: 15%\n# - test: 15%\n# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ train_test_split –¥–≤–∞–∂–¥—ã\n\nfrom datasets import Dataset\n\ndataset = Dataset.from_list([{'data': f'–º–Ω–µ –≤–µ–¥—å –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–æ–∫ ‚Ññ{i}'} for i in range(1000)])\n\ndataset = dataset.train_test_split(seed=42, test_size=0.15)\ntrain_data = dataset['train']\ntest_data = dataset['test']\ndel dataset\ndataset = train_data.train_test_split(seed=42, test_size=150 / 850)\ntrain_data = dataset['train']\nval_data = dataset['test']\ndel dataset\n\nprint(train_data, val_data, test_data)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 17: –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤—Ö–æ–¥–∞–º–∏","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å 'question' –∏ 'context'\n# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ –æ–±–∞ –ø–æ–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n# –î–æ–±–∞–≤—å—Ç–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –º–µ–∂–¥—É –Ω–∏–º–∏\n# –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ attention_mask –∏ token_type_ids\n\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\nlists = [{'question': f'text ‚Ññ{i + 1}', 'context': f'other text ‚Ññ{i + 1}'} for i in range(1000)]\ndataset = Dataset.from_list(lists)\n\ntokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n\ndef tokenize_func(examples):\n    return tokenizer(examples['question'], examples['context'], padding='max_length', truncation=True, max_length=512)\n\ndataset = dataset.map(tokenize_func, batched=True, batch_size=1000)\n# input_ids —Ç–æ–∂–µ –æ—Å—Ç–∞–≤–ª—è—Ç—å?\nprint(dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 18: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –∫–æ–ª–æ–Ω–æ–∫","metadata":{}},{"cell_type":"code","source":"# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å 5+ –∫–æ–ª–æ–Ω–∫–∞–º–∏\n# –ü—Ä–∏–º–µ–Ω–∏—Ç–µ map() —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π\n# –£–¥–∞–ª–∏—Ç–µ –≤—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n# –û—Å—Ç–∞–≤—å—Ç–µ —Ç–æ–ª—å–∫–æ input_ids, attention_mask, labels\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\ndataset = load_dataset('squad', split='train[:1000]')\ntokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n\ndataset = dataset.rename_column('answers', 'labels')\n\n# answers - —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ => —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–µ –Ω–∞–¥–æ; –µ–≥–æ –Ω–∞–¥–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å, –Ω–æ —è –Ω–µ –∑–Ω–∞—é –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞—Ç—å –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞\ndef tokenize_func(d):\n    texts = list(zip(*[d[n] for n in ['title', 'context', 'question']]))\n    texts = [tokenizer.sep_token.join(sample) for sample in texts]\n    return tokenizer(texts, truncation=True, padding='max_length', max_length=512)\n\ndataset = dataset.map(tokenize_func, batched=True, batch_size=10)\ndataset = dataset.remove_columns(['id', 'title', 'context', 'question', 'token_type_ids'])\nprint(dataset)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 19: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–ª–æ–∂–Ω–æ–º—É —É—Å–ª–æ–≤–∏—é","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç —Å 'text' –∏ 'score'\n# –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –≥–¥–µ:\n# - –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ > 50 —Å–∏–º–≤–æ–ª–æ–≤\n# - score > 0.7\n# - —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ\n\nfrom datasets import Dataset\nimport numpy as np\nimport random\n\nwords = ['—Å–æ–±–∞–∫–∞', '–∫–æ—Ç', '–∑–≤–µ—Ä—é–≥–∞', '–ú–∞—Ä–≥—É–ª–∞–Ω –°–µ–π—Å–µ–º–±–∞–µ–≤', '–ê—Ä–Ω–æ–ª—å–¥ –®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –∏ –†–æ–Ω–∏ –ö–æ–ª–µ–º–∞–Ω', '–î–æ—Ä–∏–∞–Ω', '–ï—Ç—Å',\n         '–î–∂–µ–∫–∏ –ß–∞–Ω', '–§–∏–ª–ª –•–∏—Ç', '–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Ç–æ–ø, –Ω–æ –µ—â—ë –µ—Å—Ç—å –Ω–∞–¥ —á–µ–º –ø–æ—Ä–∞–±–æ—Ç–∞—Ç—å']\ndata = {\n    'text': [f'just a text ‚Ññ{i} with sentence: {random.choice(words)}' for i in range(1000)],\n    'score': np.random.rand(1000).tolist()\n}\ndataset = Dataset.from_dict(data)\n\ndataset = dataset.filter(lambda batch: [s > 0.7 and len(t) > 50 and '–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä' in t for s, t in zip(batch['score'], batch['text'])],\n                         batched=True, batch_size=1024)\ndataset.to_pandas()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 20: –°–æ–∑–¥–∞–Ω–∏–µ DatasetDict –∏ —Ä–∞–±–æ—Ç–∞ —Å –Ω–∏–º","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ DatasetDict —Å –∫–ª—é—á–∞–º–∏ 'train', 'validation', 'test'\n# –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –æ–¥–Ω—É —Ñ—É–Ω–∫—Ü–∏—é map() –∫–æ –≤—Å–µ–º splits —Å—Ä–∞–∑—É\n# –û—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ –∫–∞–∂–¥—ã–π split –ø–æ —Ä–∞–∑–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º\n# –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤–µ—Å—å DatasetDict –Ω–∞ –¥–∏—Å–∫\n# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–±—Ä–∞—Ç–Ω–æ –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n\nfrom datasets import DatasetDict, Dataset\n\ndef get_dummy_dataset():\n    return Dataset.from_dict({\n        'image_path': [f'D:/ML/datasets/data_for_fine_tuning_claude_sonnet_4-5-thinking-32-k(you)-i_will-fine_tune_you_on_my_GTX260/img{i}.png'\n                       for i in range(1000)],\n        'labels': [i % 3 for i in range(1000)]\n    })\n\ndatasetdict = DatasetDict({\n    'train': get_dummy_dataset(),\n    'validation': get_dummy_dataset(),\n    'test': get_dummy_dataset()\n})\n\ndef very_usefull_function(batch):\n    batch['image_path'] = [x + '_—Å–∞–º–∞—è_–ø–æ–ª–µ–∑–Ω–∞—è_–æ–±—Ä–∞–±–æ—Ç–∫–∞_–∫–æ—Ç–æ—Ä–∞—è_–¥–µ–ª–∞–µ—Ç_–≤—Å–µ_–ø—É—Ç–∏_–Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏' for x in batch['image_path']]\n    return batch\n\ndatasetdict = datasetdict.map(very_usefull_function, batched=True, batch_size=1024)\ndatasetdict['train'] = datasetdict['train'].filter(lambda batch: ['3' in x for x in batch['image_path']], batched=True, batch_size=128)\ndatasetdict['validation'] = datasetdict['validation'].filter(lambda batch: ['4' in x for x in batch['image_path']], batched=True, batch_size=128)\ndatasetdict['test'] = datasetdict['test'].filter(lambda batch: ['5' in x for x in batch['image_path']], batched=True, batch_size=128)\n\nprint(datasetdict)\npath = './–Ω–æ–≤–∞—è_–ø–∞–ø–∫–∞_–¥–ª—è_—Ç–≤–æ–µ–≥–æ_–¥–æ–æ–±—É—á–µ–Ω–∏—è'\ndatasetdict.save_to_disk(path)\ndatasetdict = DatasetDict.load_from_disk(path)\nprint(datasetdict)\n\n# –≤—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–∞","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è.","metadata":{}},{"cell_type":"markdown","source":"–ö–∞—Ä—Ç–∏–Ω–∫–∏.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimg = np.random.randint(0, 255, (32, 32, 3))\n\n# –æ—Ç–∑–µ—Ä–∫–∞–ª–∏–≤–∞–Ω–∏—è\naug_img_1 = img[::-1, :, :]\naug_img_2 = img[:, ::-1, :]\naug_img_3 = img[::-1, ::-1, :]\n\n# –ø–æ–≤–æ—Ä–æ—Ç –ø—Ä–æ—Ç–∏–≤ —á–∞—Å–æ–≤–æ–π —Å—Ç—Ä–µ–ª–∫–∏ –Ω–∞ 90 –≥—Ä–∞–¥—É—Å–æ–≤ n —Ä–∞–∑\naug_img_4 = np.rot90(img, k=3)  # 3 –ø–æ–≤–æ—Ä–æ—Ç–∞\n\n# —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∏–ª–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏\naug_img_5 = np.clip(img + 5, 0, 255)\naug_img_6 = np.clip(img - 5, 0, 255)\n\n# —É–≤–µ–ª–∏—á–µ–Ω–∏–µ/—É–º–µ–Ω—å—à–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏\nfactor = 0.5; aug_img_7 = 128 + factor * (img - 128)  # —É–≤–µ–ª–∏—á–µ–Ω–∏–µ\nfactor = 1.5; aug_img_8 = 128 + factor * (img - 128)  # —É–º–µ–Ω—å—à–µ–Ω–∏–µ\n\n# –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞\nstd = 10; mean = 0\naug_img_9 = np.clip(img + np.random.normal(mean, std, img.shape), 0, 255).astype(np.uint8)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–≤—É–∫–∏.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\naudio = np.random.uniform(-1, 1, (16000 * 4))\n\n# —É–≤–µ–ª–∏—á–µ–Ω–∏–µ/—É–º–µ–Ω—å—à–µ–Ω–∏–µ –≥—Ä–æ–º–∫–æ—Å—Ç–∏\naug_audio_1 = np.clip(audio * 1.2, -1.0, 1.0)  # —É—á–µ–ª–∏—á–µ–Ω–∏–µ\naug_audio_2 = np.clip(audio * 0.8, -1.0, 1.0)  # —É–º–µ–Ω—å—à–µ–Ω–∏–µ\n\n# –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞\ndef add_noise(audio, noise_level):\n    noise = np.random.randn(len(audio))\n    augmented_audio = audio + noise_level * noise\n    augmented_audio = augmented_audio / np.max(np.abs(augmented_audio))  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–ª–∏–ø–ø–∏–Ω–≥–∞\n    return np.clip(augmented_audio, -1.0, 1.0)\n\n# –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–¥–≤–∏–≥\ndef time_shift(audio, sr, shift_ms):\n    shift_samples = int(sr * shift_ms / 1000)\n    return np.roll(audio, shift_samples)\n\n# —Å–∂–∞—Ç–∏–µ/—Ä–∞—Å—Ç—è–∂–µ–Ω–∏–µ\ndef simple_time_stretch(audio, rate):\n    \"\"\"\n    rate > 1: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏ –ø–∏—Å–∫–ª—è–≤—ã–π –≥–æ–ª–æ—Å\n    rate < 1: –∑–∞–º–µ–¥–ª–µ–Ω–∏–µ –∏ –Ω–∏–∑–∫–∏–π –≥–æ–ª–æ—Å\n    \"\"\"\n    new_indices = np.arange(0, len(audio), rate)\n    stretched_audio = np.interp(new_indices, np.arange(len(audio)), audio)\n    return stretched_audio","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–¢–µ–∫—Å—Ç.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–¢–∞–±–ª–∏—á–∫–∞.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# –¥–∞—Ç–∞—Å–µ—Ç—ã","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenizer, texts, labels=None, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n\n        encoded = self.tokenizer(\n            text,\n            return_tensors='pt',\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=False,\n            truncation=True\n        )\n        encoded['input_ids'] = encoded['input_ids'].squeeze(0)\n        encoded['attention_mask'] = encoded['attention_mask'].squeeze(0)\n        if self.labels is not None:\n            encoded['labels'] = torch.tensor(self.labels[index], dtype=torch.long)\n\n        return encoded","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 1: –ø—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è.","metadata":{}},{"cell_type":"code","source":"# –í—Ö–æ–¥: Pandas DataFrame —Å–æ —Å—Ç–æ–ª–±—Ü–∞–º–∏ text –∏ label.\n# –ó–∞–¥–∞—á–∞: –°–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ –∏–Ω–¥–µ–∫—Å—É idx —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç text –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å input_ids, attention_mask –∏ labels.\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenizer, df, max_length=512):\n        self.tokenizer = tokenizer\n        self.texts = df['text'].values\n        try:\n            self.labels = df['label'].values\n        else:\n            self.labels = None\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n\n        encoded = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            return_tensors='pt',\n            max_length=self.max_length,\n            padding=False,\n            truncation=True\n        )\n        encoded['input_ids'] = encoded['input_ids'].squeeze(0)\n        encoded['attention_mask'] = encoded['attention_mask'].squeeze(0)\n        if self.labels is not None:\n            encoded['labels'] = self.labels[index]\n\n        return encoded","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 2: –î–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—è.","metadata":{}},{"cell_type":"code","source":"# –í—Ö–æ–¥: DataFrame —Å–æ —Å—Ç–æ–ª–±—Ü–∞–º–∏ question, answer –∏ label.\n# –ó–∞–¥–∞—á–∞: –°–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç. –í __getitem__ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å question –∏ answer —á–µ—Ä–µ–∑ [SEP] —Ç–æ–∫–µ–Ω, —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –≤–µ—Ä–Ω—É—Ç—å –≤–º–µ—Å—Ç–µ —Å –º–µ—Ç–∫–æ–π label.\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenizer, df, max_len=512):\n        self.tokenizer = tokenizer\n        self.texts1 = df['question'].values\n        self.texts2 = df['answer'].values\n        self.labels = df['label'].values if 'label' in df.columns else None\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts1)\n\n    def __getitem__(self, index):\n        encoded = self.tokenizer(\n            self.texts1[index],\n            self.texts2[index],\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=False,\n            truncation=True,\n            return_tensors=None\n        )\n\n        encoded['input_ids'] = torch.tensor(encoded['input_ids'], dtype=torch.long)\n        encoded['attention_mask'] = torch.tensor(encoded['attention_mask'], dtype=torch.long)\n        if 'token_type_ids' in encoded:\n            encoded['token_type_ids'] = torch.tensor(encoded['token_type_ids'], dtype=torch.long)\n        if self.labels is not None:\n            encoded['labels'] = torch.tensor(self.labels[index], dtype=torch.long)\n\n        return encoded","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 3: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ–±—Ä–µ–∑–∫–∞ (Truncation).","metadata":{}},{"cell_type":"code","source":"# –í—Ö–æ–¥: DataFrame —Å –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏ –≤ —Å—Ç–æ–ª–±—Ü–µ essay.\n# –ó–∞–¥–∞—á–∞: –°–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —ç—Å—Å–µ, –Ω–æ –æ–±—Ä–µ–∑–∞–µ—Ç –µ–≥–æ –Ω–µ –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã, –∞ –¥–æ —Å–ª—É—á–∞–π–Ω–æ–π –¥–ª–∏–Ω—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [256, 512] —Ç–æ–∫–µ–Ω–æ–≤.\n\nimport random\nimport torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenizer, df, truncation_gap=(256, 512)):\n        self.tokenizer = tokenizer\n        self.texts = df['essay'].values\n        self.labels = df['labels'].values if 'labels' in df.columns else None\n        self.truncation_gap = truncation_gap\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        encoded = self.tokenizer(\n            self.texts[index],\n            add_special_tokens=True,\n            return_tensors=None,\n            truncation=True,\n            max_length=random.randint(*self.truncation_gap),\n            padding=False\n        )\n        encoded['input_ids'] = torch.tensor(encoded['input_ids'], dtype=torch.long)\n        encoded['attention_mask'] = torch.tensor(encoded['attention_mask'], dtype=torch.long)\n        if 'token_type_ids' in encoded: encoded['token_type_ids'] = torch.tensor(encoded['token_type_ids'], dtype=torch.long)\n        if self.labels is not None: encoded['labels'] = torch.tensor(self.labels[index], dtype=torch.long)\n        return encoded","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 4: –†–∞–∑–Ω—ã–µ max_length –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π.","metadata":{}},{"cell_type":"code","source":"# –í—Ö–æ–¥: DataFrame —Å–æ —Å—Ç–æ–ª–±—Ü–∞–º–∏ title (–∫–æ—Ä–æ—Ç–∫–∏–π) –∏ abstract (–¥–ª–∏–Ω–Ω—ã–π).\n# –ó–∞–¥–∞—á–∞: –°–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç title —Å max_length=64, –∞ abstract —Å max_length=448,\n# –∞ –∑–∞—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenizer, df, title_max_len=64, abstract_max_len=448):\n        self.tokenizer = tokenizer\n        self.titles = df['title'].values\n        self.abstracts = df['abstract'].values\n        self.labels = df['labels'].values if 'labels' in df.columns else None\n        self.title_max_len = title_max_len\n        self.abstract_max_len = abstract_max_len\n\n    def __len__(self):\n        return len(self.titles)\n\n    def __getitem__(self, index):\n        title = self.tokenizer(\n            self.titles[index],\n            truncation=True,\n            max_length=self.title_max_len,\n            padding=False,\n            return_tensors=None\n        )\n        abstract = self.tokenizer(\n            self.abstracts[index],\n            truncation=True,\n            max_length=self.abstract_max_len,\n            padding=False,\n            return_tensors=None\n        )\n        encoded = {\n            'input_ids': title['input_ids'] + abstract['input_ids'][1:],\n            'attention_mask': title['attention_mask'] + abstract['attention_mask'][1:],\n            'token_type_ids': tilte['token_type_ids'] + abstract['token_type_ids'][1:],\n        }\n        if self.labels is not None:\n            encoded['labels'] = self.labels[index]\n        return encoded\n\ndf = pd.DataFrame([{\n            'title': f'–ü–æ–ª–Ω—ã–π –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–π —Ç—Ä—ë—Ö—á–ª–µ–Ω: ${i + 1}x¬≤ + {i + 5}x - 1$.',\n            'abstract': '–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —ç—Ç–æ–≥–æ —Ç—Ä—ë—Ö —á–ª–µ–Ω–∞' * i,\n            'labels': i % 3\n} for i in range(100)])\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ndataset = CustomDataset(tokenizer, df)\n\ndataset[3]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ú—É–ª—å—Ç–∏–ª–µ–π–±–ª –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è.","metadata":{}},{"cell_type":"code","source":"# –í—Ö–æ–¥: DataFrame —Å–æ —Å—Ç–æ–ª–±—Ü–æ–º text –∏ —Å—Ç–æ–ª–±—Ü–æ–º tags (—Å—Ç—Ä–æ–∫–∞ —Å —Ç–µ–≥–∞–º–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –Ω–∞–ø—Ä–∏–º–µ—Ä, \"science,tech,ai\").\n# –ó–∞–¥–∞—á–∞: –°–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç. –í __init__ —Å–æ–∑–¥–∞—Ç—å –º–∞–ø–ø–∏–Ω–≥ —Ç–µ–≥–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã.\n# –í __getitem__ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫—É —Ç–µ–≥–æ–≤ –≤ multi-hot –≤–µ–∫—Ç–æ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, [1, 1, 0, 1, 0]) –¥–ª—è labels.\n\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom itertools import chain\nimport random\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenizer, df, num_labels, max_length=512):\n        self.tokenizer = tokenizer\n        self.texts = df['text'].values\n        self.max_length = max_length\n\n        if 'tags' not in df.columns:\n            self.labels = None\n            self.label2id = None\n        else:\n            self.labels = [labels.split(',') for labels in df['tags']]\n            self.unique_labels = sorted(list(set(chain.from_iterable(self.labels))))\n            if len(self.unique_labels) != num_labels:\n                raise ValueError(f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –º–µ–Ω—å—à–µ –∏–ª–∏ –±–æ–ª—å—à–µ {num_labels}.')\n            self.label2id = {label: i for i, label in enumerate(self.unique_labels)}\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        encoded = self.tokenizer(\n            self.texts[index],\n            padding=False,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=None\n        )\n\n        labels = [0] * len(self.unique_labels)\n        for label in self.labels[index]:\n            labels[self.label2id[label]] = 1\n\n        encoded['labels'] = labels\n\n        return encoded\n\ndf = pd.DataFrame([{\n    'text': ('–±–æ–ª—å—à–æ–π –∏–ª–∏ –º–∞–ª–µ–Ω—å–∫–∏–π —Ç–µ–∫—Å—Ç, –∑–∞–≤–∏—Å–∏—Ç –æ—Ç i ' * i).strip(),\n    'tags': ','.join(random.sample(['science', 'tech', 'ai', 'NLP', 'RL', 'a cute cat'], k=random.randint(1, 6)))\n} for i in range(100)])\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ndataset = CustomDataset(tokenizer, df, 6)\ndataset[2]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# –∫–æ–ª–ª–∞—Ç–æ—Ä—ã","metadata":{}},{"cell_type":"code","source":"# —ç—Ç–æ —Ä–µ—à–∞–µ—Ç —Ç–∞–∫—É—é –æ—à–∏–±–∫—É:\n# RemoteEntryNotFoundError: 404 Client Error. (Request ID: Root=1-68e54624-2d9293a52c06cef514db67ce;ef2314f7-a869-4c0d-b880-4e97781bd6cb)\n# Entry Not Found for url: https://huggingface.co/api/models/cointegrated/rubert-tiny2/tree/main/additional_chat_templates?recursive=false&expand=false.\n# additional_chat_templates does not exist on \"main\"\n\n!pip install -q --upgrade transformers==4.45.0 huggingface_hub\n\nimport os\nos.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport random\nfrom dataclasses import dataclass\nimport albumentations as A\nfrom PIL import Image\n\n# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n# –ë–∞–∑–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\nclass DummyTextDataset(Dataset):\n    def __init__(self, size=100):\n        self.data = [\n            {\n                'text': f'This is sample text number {i} ' * random.randint(5, 50),\n                'labels': random.randint(0, 2),\n                'id': i\n            }\n            for i in range(size)\n        ]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# –®–∞–±–ª–æ–Ω –∫–æ–ª–ª–∞—Ç–æ—Ä–∞\n@dataclass\nclass BaseCollator:\n    def __call__(self, batch):\n        raise NotImplementedError","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. Dynamic Padding Collator","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ndataset = DummyTextDataset(100)\n\nclass DynamicPaddingCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    \n    def __call__(self, batch):\n        texts = [sample['text'] for sample in batch]\n        labels = [sample['labels'] for sample in batch]\n\n        batch = self.tokenizer(\n            texts,\n            padding='longest',\n            return_tensors='pt',\n        )\n        batch['labels'] = torch.tensors(labels, dtype=torch.long)\n        return batch\n\ncollator = DynamicPaddingCollator(tokenizer)\nloader = DataLoader(dataset, batch_size=8, collate_fn=collator)\n\n# –°–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–∞—Ç–æ—Ä —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –ø–∞–¥–¥–∏–Ω–≥–æ–º –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –≤ –±–∞—Ç—á–µ.","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 2: Fixed Length Bucketing Collator","metadata":{}},{"cell_type":"code","source":"class BucketingCollator:\n    def __init__(self, tokenizer, buckets=[32, 64, 128, 256, 512]):\n        self.tokenizer = tokenizer\n        self.buckets = sorted(buckets)\n    \n    def __call__(self, batch):\n        texts = [sample['text'] for sample in batch]\n        labels = [sample['labels'] for sample in batch]\n\n        encoded = self.tokenizer(\n            texts,\n            padding=False,\n            truncation=True\n        )\n\n        lens = [len(sample) for sample in encoded['input_ids']]\n        max_len = max(lens)\n        target_len = self.buckets[-1]\n        for bucket in self.buckets:\n            if bucket >= max_len:\n                target_len = bucket\n                break\n\n        attention_mask = [[1] * len(sample) + [0] * (target_len - len(sample)) for sample in encoded['input_ids']]\n        input_ids = [sample + [self.tokenizer.pad_token_id] * (target_len - len(sample)) for sample in encoded['input_ids']]\n\n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            'labels': torch.tensor(labels, dtype=torch.long)\n        }\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ndataset = DummyTextDataset(100)\ncollator = BucketingCollator(tokenizer=tokenizer)\n\nbatch = [dataset[i] for i in range(32)]\ncollator(batch)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 3: Separate Inputs Collator","metadata":{}},{"cell_type":"code","source":"class PairDataset(Dataset):\n    def __init__(self, size=100):\n        self.data = [\n            {\n                'text_a': f'First text {i} ' * random.randint(3, 20),\n                'text_b': f'Second text {i} ' * random.randint(3, 20),\n                'label': random.randint(0, 1)\n            }\n            for i in range(size)\n        ]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\nclass SeparateInputsCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    \n    def __call__(self, batch):\n        encoded_a = self.tokenizer(\n            [sample['text_a'] for sample in batch],\n            return_tensors='pt',\n            add_special_tokens=True,\n            truncation=True,\n            max_length=512,\n            padding='longest'\n        )\n        encoded_b = self.tokenizer(\n            [sample['text_b'] for sample in batch],\n            return_tensors='pt',\n            add_special_tokens=True,\n            padding='longest',\n            truncation=True,\n            max_length=512\n        )\n        return {\n            'input_ids_a': encoded_a['input_ids'],\n            'input_ids_b': encoded_b['input_ids'],\n            'attention_mask_a': encoded_a['attention_mask'],\n            'attention_mask_b': encoded_b['attention_mask'],\n            'labels': torch.tensor([sample['label'] for sample in batch], dtype=torch.long)\n        }\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ndataset = PairDataset(100)\ncollator = SeparateInputsCollator(tokenizer=tokenizer)\n\nbatch = [dataset[i] for i in range(32)]\ncollator(batch)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 4: Sequence Pair Collator (BERT-style)","metadata":{}},{"cell_type":"code","source":"class PairDataset(Dataset):\n    def __init__(self, size=100):\n        self.data = [\n            {\n                'text_a': f'First text {i} ' * random.randint(3, 20),\n                'text_b': f'Second text {i} ' * random.randint(3, 20),\n                'label': random.randint(0, 1)\n            }\n            for i in range(size)\n        ]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\nclass SequencePairCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    \n    def __call__(self, batch):\n        encoded = self.tokenizer(\n            [sample['text_a'] for sample in batch],\n            [sample['text_b'] for sample in batch],\n            truncation=True,\n            padding='longest',\n            return_tensors='pt'\n        )\n        encoded['labels'] = torch.tensor([sample['label'] for sample in batch], dtype=torch.long)\n        return encoded\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ndataset = PairDataset(100)\ncollator = SequencePairCollator(tokenizer=tokenizer)\n\nbatch = [dataset[i] for i in range(32)]\ncollator(batch)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 5: Multi-Label Binary Vector Collator","metadata":{}},{"cell_type":"code","source":"class MultiLabelDataset(Dataset):\n    def __init__(self, size=100, num_labels=10):\n        self.data = [\n            {\n                'text': f'Multi-label sample {i}',\n                'labels': random.sample(range(num_labels), k=random.randint(1, 5))\n            }\n            for i in range(size)\n        ]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\nclass MultiLabelCollator:\n    def __init__(self, tokenizer, num_labels=10):\n        self.tokenizer = tokenizer\n        self.num_labels = num_labels\n    \n    def __call__(self, batch):\n        encoded = self.tokenizer(\n            [sample['text'] for sample in batch],\n            truncation=True,\n            padding='longest',\n            return_tensors='pt'\n        )\n        labels = [[int(label in sample['labels']) for label in range(self.num_labels)] for sample in batch]\n        encoded['labels'] = torch.tensor(labels, dtype=torch.long)\n        return encoded\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ndataset = MultiLabelDataset(100)\ncollator = MultiLabelCollator(tokenizer=tokenizer)\n\nbatch = [dataset[i] for i in range(32)]\ncollator(batch)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 6: Grouped Batch Collator (Ranking)","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import AutoTokenizer\nimport random\nimport torch\n\nclass RankingDataset(Dataset):\n    def __init__(self, size=100):\n        self.data_ = []\n        for query_id in range(20):\n            num_docs = random.randint(3, 8)\n            for _ in range(num_docs):\n                self.data_.append({\n                    'query': f'Query {query_id}',\n                    'document': f'Document for query {query_id} ' * random.randint(5, 30),\n                    'relevance': random.randint(0, 4),\n                    'query_id': query_id\n                })\n    \n    def __len__(self):\n        return len(self.data_)\n    \n    def __getitem__(self, idx):\n        return self.data_[idx]\n\nclass GroupedBatchCollator:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    \n    def __call__(self, batch):\n        encoded = self.tokenizer(\n            [sample['query'] for sample in batch],\n            [sample['document'] for sample in batch],\n            truncation=True,\n            padding='longest',\n            max_length=512,\n            return_tensors=None\n        )\n\n        query_ids = [sample['query_id'] for sample in batch]\n        group_sizes = []\n        target_group, target_group_count = query_ids[0], 0\n        for query_id in query_ids + [None]:\n            if target_group != query_id:\n                target_group = query_id\n                group_sizes.append(target_group_count)\n                target_group_count = 1\n            else:\n                target_group_count += 1\n\n        encoded['labels'] = torch.tensor([sample['relevance'] for sample in batch], dtype=torch.long)\n        encoded['query_ids'] = torch.tensor(query_ids, dtype=torch.long)\n        encoded['group_sizes'] = torch.tensor(group_sizes, dtype=torch.long)\n        encoded['input_ids'] = torch.tensor(encoded['input_ids'], dtype=torch.long)\n        encoded['attention_mask'] = torch.tensor(encoded['attention_mask'], dtype=torch.long)\n        if 'token_type_ids' in encoded:\n            encoded['token_type_ids'] = torch.tensor(encoded['token_type_ids'], dtype=torch.long)\n\n        return encoded\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ndataset = RankingDataset(100)\ncollator = GroupedBatchCollator(tokenizer=tokenizer)\n\nbatch = [dataset[i] for i in range(32)]\ncollator(batch)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# —ç–º–±–µ–¥–¥–∏–Ω–≥–∏","metadata":{}},{"cell_type":"code","source":"# —ç—Ç–æ —Ä–µ—à–∞–µ—Ç —Ç–∞–∫—É—é –æ—à–∏–±–∫—É:\n# RemoteEntryNotFoundError: 404 Client Error. (Request ID: Root=1-68e54624-2d9293a52c06cef514db67ce;ef2314f7-a869-4c0d-b880-4e97781bd6cb)\n# Entry Not Found for url: https://huggingface.co/api/models/cointegrated/rubert-tiny2/tree/main/additional_chat_templates?recursive=false&expand=false.\n# additional_chat_templates does not exist on \"main\"\n\n!pip install -q --upgrade transformers==4.45.0 huggingface_hub\n\nimport os\nos.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 1: BERT - CLS —Ç–æ–∫–µ–Ω","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'google-bert/bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModel.from_pretrained(checkpoint)\n\ntext = 'Machine learning is fascinating'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    cls_embed = model(**encoded).last_hidden_state[:, 0, :]\n\nprint(cls_embed)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 2: BERT - Mean pooling","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'google-bert/bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModel.from_pretrained(checkpoint)\n\ntext = 'Machine learning is fascinating'\nencoded = tokenizer(text, return_tensors='pt')\nwith torch.no_grad(): outputs = model(**encoded).last_hidden_state.mean(dim=1)\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 3: BERT - Max pooling","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\n\ncheckpoint = 'google-bert/bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModel.from_pretrained(checkpoint)\n\ntext = 'Machine learning is fascinating'\nencoded = tokenizer(text, return_tensors='pt')\nwith torch.no_grad(): outputs = model(**encoded).last_hidden_state.max(dim=1).values\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 4: BERT - –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å–ª–æ–π","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: bert-base-uncased\n# –¢–µ–∫—Å—Ç: \"Machine learning is fascinating\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ 6-–≥–æ —Å–ª–æ—è (–Ω–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ!)\n# –ü–æ–¥—Å–∫–∞–∑–∫–∞: output_hidden_states=True\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'google-bert/bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModel.from_pretrained(checkpoint)\n\ntext = 'Machine learning is fascinating'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded, output_hidden_states=True).hidden_states[6]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 5: BERT - –í—Å–µ —Å–ª–æ–∏","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: bert-base-uncased\n# –¢–µ–∫—Å—Ç: \"Machine learning is fascinating\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –í–°–ï–• 12 —Å–ª–æ—ë–≤\n# –†–µ–∑—É–ª—å—Ç–∞—Ç: —Å–ø–∏—Å–æ–∫ –∏–∑ 12 —Ç–µ–Ω–∑–æ—Ä–æ–≤\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'google-bert/bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModel.from_pretrained(checkpoint)\n\ntext = 'Machine learning is fascinating'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded, output_hidden_states=True).hidden_states[1:]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 6: RoBERTa","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: roberta-base\n# –¢–µ–∫—Å—Ç: \"Hello world\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ (–æ–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç BERT)\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'FacebookAI/roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModel.from_pretrained(checkpoint)\n\ntext = 'Hello world'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, 0, :]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 7: DistilBERT","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: distilbert-base-uncased\n# –¢–µ–∫—Å—Ç: \"Hello world\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ (—É DistilBERT –º–µ–Ω—å—à–µ —Å–ª–æ—ë–≤, —á–µ–º —É BERT)\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'distilbert/distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModel.from_pretrained(checkpoint)\n\ntext = 'Hello world'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, 0, :]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 8: XLM-RoBERTa","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: xlm-roberta-base\n# –¢–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º: \"–ü—Ä–∏–≤–µ—Ç –º–∏—Ä\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'FacebookAI/xlm-roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModel.from_pretrained(checkpoint)\n\ntext = '–ü—Ä–∏–≤–µ—Ç –º–∏—Ä'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, 0, :]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 9: DeBERTa","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: microsoft/deberta-base\n# –¢–µ–∫—Å—Ç: \"Hello world\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ CLS-—Ç–æ–∫–µ–Ω–∞\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'microsoft/deberta-base'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModel.from_pretrained(checkpoint)\n\ntext = 'Hello world'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, 0, :]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 10: GPT-2 - –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: gpt2\n# –¢–µ–∫—Å—Ç: \"Once upon a time\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ –ü–û–°–õ–ï–î–ù–ï–ì–û —Ç–æ–∫–µ–Ω–∞ (—É GPT –Ω–µ—Ç CLS)\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'openai-community/gpt2'\nmodel = AutoModel.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntext = 'Once upon a time'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, -1]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 11: SentenceTransformers - –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: sentence-transformers/all-MiniLM-L6-v2\n# –¢–µ–∫—Å—Ç: \"This is a sentence\"\n# –ó–∞–¥–∞—á–∞: –ò—Å–ø–æ–ª—å–∑—É–π –º–µ—Ç–æ–¥ .encode()\n\nfrom sentence_transformers import SentenceTransformer\n\ncheckpoint = 'sentence-transformers/all-MiniLM-L6-v2'\nmodel = SentenceTransformer(checkpoint)\n\ntext = 'This is a sentence'\noutputs = model.encode(text)\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 12: SentenceTransformers - –±–∞—Ç—á","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: sentence-transformers/all-MiniLM-L6-v2\n# –¢–µ–∫—Å—Ç—ã: [\"First sentence\", \"Second sentence\", \"Third sentence\"]\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ç—Ä—ë—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω –≤—ã–∑–æ–≤\n\nfrom sentence_transformers import SentenceTransformer\n\ncheckpoint = 'sentence-transformers/all-MiniLM-L6-v2'\nmodel = SentenceTransformer(checkpoint)\n\ntexts = [\"First sentence\", \"Second sentence\", \"Third sentence\"]\noutputs = model.encode(texts)\n\nprint(outputs.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 13: BGE embeddings","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: BAAI/bge-small-en-v1.5\n# –¢–µ–∫—Å—Ç: \"Artificial intelligence\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ AutoModel\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'BAAI/bge-small-en-v1.5'\nmodel = AutoModel.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntext = 'Artificial intelligence'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).pooler_output\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 14: GTE embeddings","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: thenlper/gte-base\n# –¢–µ–∫—Å—Ç: \"Deep learning\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥\n\nfrom sentence_transformers import SentenceTransformer\n\ncheckpoint = 'thenlper/gte-base'\nmodel = SentenceTransformer(checkpoint)\n\ntext = 'Deep learning'\noutputs = model.encode(text)\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 15: E5 embeddings —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: intfloat/e5-base-v2\n# –¢–µ–∫—Å—Ç: \"query: What is machine learning?\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ (E5 —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å \"query:\" –∏–ª–∏ \"passage:\")\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'intfloat/e5-base-v2'\nmodel = AutoModel.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntext = 'query: What is machine learning?'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, 0]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 16: CodeBERT","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: microsoft/codebert-base\n# –ö–æ–¥: \"def hello(): print('world')\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ –∫–æ–¥–∞\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'microsoft/codebert-base'\nmodel = AutoModel.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntext = 'def hello(): print(\\'world\\')'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, 0]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 18: T5 - encoder —á–∞—Å—Ç—å","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: t5-small\n# –¢–µ–∫—Å—Ç: \"translate English to German: Hello\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –∏–∑ encoder'–∞ (–Ω–µ decoder)\n\nfrom transformers import T5EncoderModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'google-t5/t5-small'\nencoder = T5EncoderModel.from_pretrained(checkpoint)\n# encoder = model.encoder\nencoder.eval()\n# del model; gc.collect()\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntext = 'translate English to German: Hello'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = encoder(**encoded).last_hidden_state[:, 0]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 19: BART - encoder","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å: facebook/bart-base\n# –¢–µ–∫—Å—Ç: \"Summarize this text\"\n# –ó–∞–¥–∞—á–∞: –ò–∑–≤–ª–µ–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ encoder'–∞\n\nfrom transformers import AutoModel, AutoTokenizer\n\ncheckpoint = 'facebook/bart-base'\nmodel = AutoModel.from_pretrained(checkpoint).encoder\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntext = 'Summarize this text'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, 0]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. –ü–æ–ª—É—á–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ \"Hello, world!\" –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å bert-base-uncased. –í—ã–≤–µ–¥–∏—Ç–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'google-bert/bert-base-uncased'\nmodel = AutoModel.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntext = 'Hello, world!'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, 0]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å sentence-transformers/all-MiniLM-L6-v2 –∏ –ø–æ–ª—É—á–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è \"Machine learning is fascinating\".","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nembedding = model.encode('Machine learning is fascinating')\nprint(embedding)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. –ò–∑–≤–ª–µ–∫–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥ CLS-—Ç–æ–∫–µ–Ω–∞ –∏–∑ –º–æ–¥–µ–ª–∏ distilbert-base-uncased –¥–ª—è —Ç–µ–∫—Å—Ç–∞ \"Natural language processing\".","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer\nimport torch\n\ncheckpoint = 'distilbert-base-uncased'\nmodel = AutoModel.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntext = 'Natural language processing'\nencoded = tokenizer(text, return_tensors='pt')\n\nwith torch.no_grad():\n    outputs = model(**encoded).last_hidden_state[:, 0]\n\nprint(outputs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# KNN –ø—Ä–∏–∑–Ω–∞–∫–∏: —Å—Ä–µ–¥–Ω–∏–π —Ç–∞—Ä–≥–µ—Ç –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏ —Å–∞–º –∫–ª–∞—Å—Ç–µ—Ä","metadata":{}},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 12: SentenceTransformers - –±–∞—Ç—á","metadata":{}},{"cell_type":"code","source":"!pip -q install faiss-cpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def knn_cluster_mean(X_train, y_train, X_test, k=5, metric=\"cosine\"):\n    X_train = X_train.astype(\"float32\")\n    X_test = X_test.astype(\"float32\")\n    \n    if metric == \"cosine\":\n        X_train /= np.linalg.norm(X_train, axis=1, keepdims=True) + 1e-9\n        X_test /= np.linalg.norm(X_test, axis=1, keepdims=True) + 1e-9\n    \n    kmeans = faiss.Kmeans(X_train.shape[1], k, niter=25, verbose=False)\n    kmeans.train(X_train)\n    \n    _, lbl_train = kmeans.index.search(X_train, 1)\n    lbl_train = lbl_train.ravel()\n    \n    s = np.bincount(lbl_train, weights=y_train, minlength=k)\n    c = np.bincount(lbl_train, minlength=k)\n    cluster_means = s / (c + 1e-9)\n    \n    _, lbl_test = kmeans.index.search(X_test, 1)\n    lbl_test = lbl_test.ravel()\n    \n    train_result = np.column_stack((lbl_train, cluster_means[lbl_train]))\n    test_result = np.column_stack((lbl_test, cluster_means[lbl_test]))\n    \n    return train_result, test_result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 100000\nX_train = np.concatenate([np.random.rand(n // 2, 2), np.random.rand(n // 2, 2) + np.random.rand(n // 2, 2)], axis=0)\ny = np.random.rand(n)\nn = 1000\nX_test = np.concatenate([np.random.rand(n // 2, 2), np.random.rand(n // 2, 2) + np.random.rand(n // 2, 2)], axis=0)\n\ntrain_result, test_result = knn_cluster_mean(X_train, y, X_test, k=5)\nprint(train_result.shape, test_result.shape)","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TF-IDF","metadata":{}},{"cell_type":"markdown","source":"1. –ü–µ—Ä–≤–∞—è –≤—Å—Ç—Ä–µ—á–∞ —Å TF-IDF","metadata":{}},{"cell_type":"code","source":"# –ó–∞–¥–∞–Ω–∏–µ: –°–æ–∑–¥–∞–π—Ç–µ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n# –∏ –≤—ã–≤–µ–¥–∏—Ç–µ –º–∞—Ç—Ä–∏—Ü—É –∑–Ω–∞—á–µ–Ω–∏–π\ndocs = [\n    \"Python is great\",\n    \"Java is also great\",\n    \"Python and Java are programming languages\"\n]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\ndocs = vectorizer.fit_transform(docs).toarray()\nprint(docs)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. –°–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞","metadata":{}},{"cell_type":"code","source":"# –ó–∞–¥–∞–Ω–∏–µ: –ù–∞–π–¥–∏—Ç–µ —Ç–æ–ø-3 —Å–ª–æ–≤–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º TF-IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\ndocs = [\n    \"–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ\",\n    \"–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ\",\n    \"–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö\"\n]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nencoded_docs = vectorizer.fit_transform(docs).toarray()\nwords = vectorizer.get_feature_names_out()\n\nfor encoded_doc in encoded_docs:\n    indexes = encoded_doc.argsort()[::-1][:3]\n    top3words = words[indexes]\n    print(top3words)\n# print(vectorizer.get_feature_names_out()[docs.toarray().argsort()[::-1][:3]])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. –°—Ä–µ–¥–Ω—è—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å —Å–ª–æ–≤","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntexts = [\n    '–ú–æ—è –º–∞—Ç—å –∏—Å–ø–µ–∫–ª–∞ —è–±–ª–æ—á–Ω—ã–π –ø–∏—Ä–æ–≥.',\n    '–ú–∞—Ç—å –ø—Ä–æ–±—Ä–∞–ª–∞—Å—å –≤ –¥–æ–º –∏ —É–∫—Ä–∞–ª–∞ –ø–∏—Ä–æ–≥',\n    '–ú–∞—Ç—å –ø–æ—Å—Ç—Ä–æ–∏–ª–∞ –¥–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –∏—Å–ø–µ–∫–ª–∞ —Ç–æ—Ä—Ç'\n]\n\nvectorizer = TfidfVectorizer()\nencoded = vectorizer.fit_transform(texts).toarray()\n\nwords = vectorizer.get_feature_names_out()\nindexes = encoded.mean(axis=0).argsort()[::-1]\nprint(words[indexes[:10]])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"min_df / max_df","metadata":{}},{"cell_type":"code","source":"# –í–∞—Ä–∏—Ä–æ–≤–∞—Ç—å min_df = 1, 2, 3 –∏ max_df = 1.0, 0.9, 0.8; —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntexts = [\n    '–ú–æ—è –º–∞—Ç—å –∏—Å–ø–µ–∫–ª–∞ —è–±–ª–æ—á–Ω—ã–π –ø–∏—Ä–æ–≥.',\n    '–ú–∞—Ç—å –ø—Ä–æ–±—Ä–∞–ª–∞—Å—å –≤ –¥–æ–º –∏ —É–∫—Ä–∞–ª–∞ –ø–∏—Ä–æ–≥',\n    '–ú–∞—Ç—å –ø–æ—Å—Ç—Ä–æ–∏–ª–∞ –¥–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –∏—Å–ø–µ–∫–ª–∞ —Ç–æ—Ä—Ç'\n]\n\nfor v1, v2 in zip([1, 2, 3], [1.0, 0.9, 0.8]):\n    vectorizer = TfidfVectorizer(min_df=v1, max_df=v2)\n    encoded = vectorizer.fit_transform(texts)\n    print(encoded.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–í–ª–∏—è–Ω–∏–µ max_df","metadata":{}},{"cell_type":"code","source":"texts = [\n    '–ú–æ—è –º–∞—Ç—å –∏—Å–ø–µ–∫–ª–∞ —è–±–ª–æ—á–Ω—ã–π –ø–∏—Ä–æ–≥.',\n    '–ú–∞—Ç—å –ø—Ä–æ–±—Ä–∞–ª–∞—Å—å –≤ –¥–æ–º –∏ —É–∫—Ä–∞–ª–∞ –ø–∏—Ä–æ–≥',\n    '–ú–∞—Ç—å –ø–æ—Å—Ç—Ä–æ–∏–ª–∞ –¥–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –∏—Å–ø–µ–∫–ª–∞ —Ç–æ—Ä—Ç'\n]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfor v in [1.0, 0.9, 0.8]:\n    vectorizer = TfidfVectorizer(max_df=v)\n    vectorizer.fit(texts)\n    words = vectorizer.get_feature_names_out()\n    print(words)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–£–Ω–∏‚Äì + –±–∏-–≥—Ä–∞–º–º—ã","metadata":{}},{"cell_type":"code","source":"# ngram_range = (1, 2). –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –∏ —Ç–æ–ø-5 —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å –∑–∞–¥–∞–Ω–∏—è ‚Ññ 5\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntexts = [\n    '–ú–æ—è –º–∞—Ç—å –∏—Å–ø–µ–∫–ª–∞ —è–±–ª–æ—á–Ω—ã–π –ø–∏—Ä–æ–≥.',\n    '–ú–∞—Ç—å –ø—Ä–æ–±—Ä–∞–ª–∞—Å—å –≤ –¥–æ–º –∏ —É–∫—Ä–∞–ª–∞ –ø–∏—Ä–æ–≥',\n    '–ú–∞—Ç—å –ø–æ—Å—Ç—Ä–æ–∏–ª–∞ –¥–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –∏—Å–ø–µ–∫–ª–∞ —Ç–æ—Ä—Ç'\n]\n\nvectorizer = TfidfVectorizer()\nencoded = vectorizer.fit_transform(texts).toarray().mean(axis=0)\nnames = vectorizer.get_feature_names_out()\ntop5words = names[encoded.argsort()[::-1][:5]]\nprint(top5words, len(names))\n\nvectorizer = TfidfVectorizer(ngram_range=(1, 2))\nencoded = vectorizer.fit_transform(texts).toarray().mean(axis=0)\nnames = vectorizer.get_feature_names_out()\ntop5words = names[encoded.argsort()[::-1][:5]]\nprint(top5words, len(names))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"use_idf=False","metadata":{}},{"cell_type":"code","source":"# –í—ã–∫–ª—é—á–∏—Ç—å IDF, —Å—Ä–∞–≤–Ω–∏—Ç—å –≤–µ—Å–∞ —Å–ª–æ–≤ —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é; —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntexts = [\n    '–ú–æ—è –º–∞—Ç—å –∏—Å–ø–µ–∫–ª–∞ —è–±–ª–æ—á–Ω—ã–π –ø–∏—Ä–æ–≥.',\n    '–ú–∞—Ç—å –ø—Ä–æ–±—Ä–∞–ª–∞—Å—å –≤ –¥–æ–º –∏ —É–∫—Ä–∞–ª–∞ –ø–∏—Ä–æ–≥',\n    '–ú–∞—Ç—å –ø–æ—Å—Ç—Ä–æ–∏–ª–∞ –¥–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –∏—Å–ø–µ–∫–ª–∞ —Ç–æ—Ä—Ç'\n]\n\nvectorizer = TfidfVectorizer()\nencoded = vectorizer.fit_transform(texts).toarray().mean(axis=0).round(2).tolist()\nnames = vectorizer.get_feature_names_out()\nprint(encoded, names)\n\nvectorizer = TfidfVectorizer(use_idf=False)\nencoded = vectorizer.fit_transform(texts).toarray().mean(axis=0).round(2).tolist()\nnames = vectorizer.get_feature_names_out()\nprint(encoded, names)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"sublinear_tf","metadata":{}},{"cell_type":"code","source":"# –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø—Ä–∏ sublinear_tf = False vs True\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntexts = [\n    '–ú–æ—è –º–∞—Ç—å –∏—Å–ø–µ–∫–ª–∞ —è–±–ª–æ—á–Ω—ã–π –ø–∏—Ä–æ–≥.',\n    '–ú–∞—Ç—å –ø—Ä–æ–±—Ä–∞–ª–∞—Å—å –≤ –¥–æ–º –∏ —É–∫—Ä–∞–ª–∞ –ø–∏—Ä–æ–≥ –∏ —Å–∫–∞–∑–∞–ª–∞: \"–ø–∏—Ä–æ–≥\".',\n    '–ú–∞—Ç—å –ø–æ—Å—Ç—Ä–æ–∏–ª–∞ –¥–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –∏—Å–ø–µ–∫–ª–∞ —Ç–æ—Ä—Ç'\n]\n\nprint(TfidfVectorizer(sublinear_tf=False).fit_transform(texts).toarray().mean(axis=0))\nprint(TfidfVectorizer(sublinear_tf=True).fit_transform(texts).toarray().mean(axis=0))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"binary=True","metadata":{}},{"cell_type":"code","source":"# –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø—Ä–∏ sublinear_tf = False vs True\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntexts = [\n    '–ú–æ—è –º–∞—Ç—å –∏—Å–ø–µ–∫–ª–∞ —è–±–ª–æ—á–Ω—ã–π –ø–∏—Ä–æ–≥.',\n    '–ú–∞—Ç—å –ø—Ä–æ–±—Ä–∞–ª–∞—Å—å –≤ –¥–æ–º –∏ —É–∫—Ä–∞–ª–∞ –ø–∏—Ä–æ–≥ –∏ —Å–∫–∞–∑–∞–ª–∞: \"–ø–∏—Ä–æ–≥\".',\n    '–ú–∞—Ç—å –ø–æ—Å—Ç—Ä–æ–∏–ª–∞ –¥–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –∏—Å–ø–µ–∫–ª–∞ —Ç–æ—Ä—Ç'\n]\n\nvectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 5))\nencoded = vectorizer.fit_transform(texts).toarray().mean(axis=0)\nprint(vectorizer.get_feature_names_out())","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 1: –°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–æ—Å—Ç–æ–π TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = [\n    '–≠—Ç–æ –ø–µ—Ä–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç',\n    '–≠—Ç–æ –≤—Ç–æ—Ä–æ–π –¥–æ–∫—É–º–µ–Ω—Ç',\n    '–ò —ç—Ç–æ —Ç—Ä–µ—Ç–∏–π –¥–æ–∫—É–º–µ–Ω—Ç'\n]\n\nencoded = TfidfVectorizer(max_features=1000, ngram_range=(3, 5), analyzer='char_wb').fit_transform(corpus)\nprint(encoded)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 2: –ü–æ–ª—É—á–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å–ª–æ–≤) –∏ –º–∞—Ç—Ä–∏—Ü—É TF-IDF","metadata":{}},{"cell_type":"code","source":"corpus = ['cat dog', 'dog bird', 'cat bird dog']\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=15, ngram_range=(1, 2), analyzer='word')\nencoded = vectorizer.fit_transform(corpus)\nnames = vectorizer.get_feature_names_out()\n\nprint(names, encoded)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 3: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ lowercase=False –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–∑–∞–≤–∏—Å–∏–º—ã–º —Ç–µ–∫—Å—Ç–æ–º","metadata":{}},{"cell_type":"code","source":"corpus = ['Python python PYTHON', 'Java java', 'Python Java']\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=12, ngram_range=(1, 2), analyzer='word', lowercase=True)\nencoded = vectorizer.fit_transform(corpus)\nprint(encoded)\n\nvectorizer = TfidfVectorizer(max_features=12, ngram_range=(1, 2), analyzer='word', lowercase=False)\nencoded = vectorizer.fit_transform(corpus)\nprint(encoded)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 4: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä max_features=5 –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ—Ä–ø—É—Å–µ","metadata":{}},{"cell_type":"code","source":"corpus = [\n    'machine learning is great',\n    'deep learning is amazing',\n    'machine learning and deep learning',\n    'artificial intelligence and machine learning',\n    'neural networks in deep learning'\n]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nencoded = TfidfVectorizer(max_features=5, ngram_range=(1, 3), analyzer='word').fit_transform(corpus)\nprint(encoded)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 5: –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ stop_words='english'","metadata":{}},{"cell_type":"code","source":"corpus = [\n    'This is the first document',\n    'This document is the second document',\n    'And this is the third one'\n]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nencoded = TfidfVectorizer(max_features=15, ngram_range=(1, 3), analyzer='word', stop_words='english').fit_transform(corpus)\nprint(encoded)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# –°–æ–∑–¥–∞–Ω–µ–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤.","metadata":{}},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 1: –ü—Ä–æ—Å—Ç–µ–π—à–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä","metadata":{}},{"cell_type":"code","source":"data = [\n    \"–°–æ–ª–Ω—Ü–µ —Å–µ–≥–æ–¥–Ω—è —Å–≤–µ—Ç–∏—Ç –æ—Å–æ–±–µ–Ω–Ω–æ —è—Ä–∫–æ, –±—É–¥—Ç–æ —Ö–æ—á–µ—Ç –≤—Å–µ—Ö —Ä–∞–∑–±—É–¥–∏—Ç—å.\",\n    \"–° —Å–∞–º–æ–≥–æ —É—Ç—Ä–∞ –∏–¥–µ—Ç —Ç–∏—Ö–∏–π –¥–æ–∂–¥—å, –∏ –æ–∫–Ω–∞ –ø–æ–∫—Ä—ã–ª–∏—Å—å —É–∑–æ—Ä–æ–º –∏–∑ –∫–∞–ø–µ–ª—å.\",\n    \"–ù–∞ —É–ª–∏—Ü–µ —Å–∏–ª—å–Ω—ã–π –≤–µ—Ç–µ—Ä ‚Äî –∫–∞–∂–µ—Ç—Å—è, –æ–Ω —Ä–µ—à–∏–ª —É—Å—Ç—Ä–æ–∏—Ç—å —Ç–∞–Ω—Ü—ã —Å –ª–∏—Å—Ç—å—è–º–∏.\",\n    \"–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Ä–µ–∑–∫–æ —É–ø–∞–ª–∞, –∏ –≤–æ–∑–¥—É—Ö —Å—Ç–∞–ª –ø–∞—Ö–Ω—É—Ç—å –∑–∏–º–æ–π.\",\n    \"–ü–æ—Å–ª–µ –≥—Ä–æ–∑—ã –Ω–µ–±–æ —Å—Ç–∞–ª–æ —á–∏—Å—Ç—ã–º, –∫–∞–∫ –±—É–¥—Ç–æ –µ–≥–æ –≤—ã–º—ã–ª–∏ –¥–æ –±–ª–µ—Å–∫–∞.\",\n    \"–ù–µ–±–æ–ª—å—à–æ–π —Ç—É–º–∞–Ω –∑–∞—Å—Ç–µ–ª–∏–ª –≥–æ—Ä–æ–¥, –ø—Ä–∏–¥–∞–≤–∞—è –µ–º—É –∑–∞–≥–∞–¥–æ—á–Ω–æ—Å—Ç–∏.\",\n    \"–°–Ω–µ–≥ –º—è–≥–∫–æ –∫—Ä—É–∂–∏—Ç—Å—è –≤ –≤–æ–∑–¥—É—Ö–µ, —Å–ª–æ–≤–Ω–æ –±–µ–ª—ã–µ –ø–µ—Ä—å—è, —Ä–µ—à–∏–≤—à–∏–µ –ø–æ–ª–µ—Ç–∞—Ç—å.\",\n    \"–°–µ–≥–æ–¥–Ω—è –∂–∞—Ä–∞ —Ç–∞–∫–∞—è, —á—Ç–æ –¥–∞–∂–µ –∫–æ—Ç –ø—Ä–µ–¥–ø–æ—á—ë–ª —Å–ø—Ä—è—Ç–∞—Ç—å—Å—è –≤ —Ç–µ–Ω–∏ —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫–∞.\",\n    \"–û–±–ª–∞–∫–∞ –ø–ª—ã–≤—É—Ç –ª–µ–Ω–∏–≤–æ, –∫–∞–∫ –±—É–¥—Ç–æ –æ–ø–æ–∑–¥–∞–ª–∏ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∞–∑–¥–Ω–∏–∫.\",\n    \"–ù–∞ –∑–∞–∫–∞—Ç–µ –Ω–µ–±–æ –æ–∫—Ä–∞—Å–∏–ª–æ—Å—å –≤ —Ä–æ–∑–æ–≤–æ-–∑–æ–ª–æ—Ç—ã–µ —Ç–æ–Ω–∞ ‚Äî –Ω–∞—Å—Ç–æ—è—â–∏–π —Å–ø–µ–∫—Ç–∞–∫–ª—å –ø—Ä–∏—Ä–æ–¥—ã.\"\n]\n\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\ntrainer = trainers.BpeTrainer(\n    vocab_size=500,\n    special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\ntexts = [\"–ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç\", \"–í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç\", \"–¢—Ä–µ—Ç–∏–π —Ç–µ–∫—Å—Ç\"]\ntokenizer.train_from_iterator(texts, trainer=trainer)\n\n\ntokenizer.post_processor = processors.TemplateProcessing(\n    single=\"[CLS] $A [SEP]\",\n    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n    special_tokens=[(\"[CLS]\", 2), (\"[SEP]\", 3)]\n)\n\ntokenizer.save(\"my_tokenizer.json\")\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file=\"my_tokenizer.json\",\n    unk_token=\"[UNK]\", pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\"\n)\ntokenizer.save_pretrained(\"./tokenizer\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 2 –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä","metadata":{}},{"cell_type":"code","source":"data = [\n    \"–°–æ–ª–Ω—Ü–µ —Å–µ–≥–æ–¥–Ω—è —Å–≤–µ—Ç–∏—Ç –æ—Å–æ–±–µ–Ω–Ω–æ —è—Ä–∫–æ, –±—É–¥—Ç–æ —Ö–æ—á–µ—Ç –≤—Å–µ—Ö —Ä–∞–∑–±—É–¥–∏—Ç—å.\",\n    \"–° —Å–∞–º–æ–≥–æ —É—Ç—Ä–∞ –∏–¥–µ—Ç —Ç–∏—Ö–∏–π –¥–æ–∂–¥—å, –∏ –æ–∫–Ω–∞ –ø–æ–∫—Ä—ã–ª–∏—Å—å —É–∑–æ—Ä–æ–º –∏–∑ –∫–∞–ø–µ–ª—å.\",\n    \"–ù–∞ —É–ª–∏—Ü–µ —Å–∏–ª—å–Ω—ã–π –≤–µ—Ç–µ—Ä ‚Äî –∫–∞–∂–µ—Ç—Å—è, –æ–Ω —Ä–µ—à–∏–ª —É—Å—Ç—Ä–æ–∏—Ç—å —Ç–∞–Ω—Ü—ã —Å –ª–∏—Å—Ç—å—è–º–∏.\",\n    \"–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Ä–µ–∑–∫–æ —É–ø–∞–ª–∞, –∏ –≤–æ–∑–¥—É—Ö —Å—Ç–∞–ª –ø–∞—Ö–Ω—É—Ç—å –∑–∏–º–æ–π.\",\n    \"–ü–æ—Å–ª–µ –≥—Ä–æ–∑—ã –Ω–µ–±–æ —Å—Ç–∞–ª–æ —á–∏—Å—Ç—ã–º, –∫–∞–∫ –±—É–¥—Ç–æ –µ–≥–æ –≤—ã–º—ã–ª–∏ –¥–æ –±–ª–µ—Å–∫–∞.\",\n    \"–ù–µ–±–æ–ª—å—à–æ–π —Ç—É–º–∞–Ω –∑–∞—Å—Ç–µ–ª–∏–ª –≥–æ—Ä–æ–¥, –ø—Ä–∏–¥–∞–≤–∞—è –µ–º—É –∑–∞–≥–∞–¥–æ—á–Ω–æ—Å—Ç–∏.\",\n    \"–°–Ω–µ–≥ –º—è–≥–∫–æ –∫—Ä—É–∂–∏—Ç—Å—è –≤ –≤–æ–∑–¥—É—Ö–µ, —Å–ª–æ–≤–Ω–æ –±–µ–ª—ã–µ –ø–µ—Ä—å—è, —Ä–µ—à–∏–≤—à–∏–µ –ø–æ–ª–µ—Ç–∞—Ç—å.\",\n    \"–°–µ–≥–æ–¥–Ω—è –∂–∞—Ä–∞ —Ç–∞–∫–∞—è, —á—Ç–æ –¥–∞–∂–µ –∫–æ—Ç –ø—Ä–µ–¥–ø–æ—á—ë–ª —Å–ø—Ä—è—Ç–∞—Ç—å—Å—è –≤ —Ç–µ–Ω–∏ —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫–∞.\",\n    \"–û–±–ª–∞–∫–∞ –ø–ª—ã–≤—É—Ç –ª–µ–Ω–∏–≤–æ, –∫–∞–∫ –±—É–¥—Ç–æ –æ–ø–æ–∑–¥–∞–ª–∏ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∞–∑–¥–Ω–∏–∫.\",\n    \"–ù–∞ –∑–∞–∫–∞—Ç–µ –Ω–µ–±–æ –æ–∫—Ä–∞—Å–∏–ª–æ—Å—å –≤ —Ä–æ–∑–æ–≤–æ-–∑–æ–ª–æ—Ç—ã–µ —Ç–æ–Ω–∞ ‚Äî –Ω–∞—Å—Ç–æ—è—â–∏–π —Å–ø–µ–∫—Ç–∞–∫–ª—å –ø—Ä–∏—Ä–æ–¥—ã.\"\n]\n\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\ntrainer = trainers.WordPieceTrainer(\n    vocab_size=30000,\n    special_tokens='[UNK], [PAD], [CLS], [SEP], [DIAGNOSIS], [SYMPTOM], [DRUG]'.split(', ')\n)\ntokenizer.train_from_iterator(data, trainer=trainer)\ntokenizer.post_processor = processors.TemplateProcessing(\n    single='[CLS] $A [SEP]',\n    pair='[CLS] $A [SEP] $B:1 [SEP]:1',\n    special_tokens=[('[CLS]', tokenizer.token_to_id('[CLS]')), ('[SEP]', tokenizer.token_to_id('[SEP]'))]\n)\ntokenizer.save('tokenizer.json')\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file='tokenizer.json',\n    unk_token='[UNK]', cls_token='[CLS]',\n    sep_token='[SEP]', pad_token='[PAD]',\n    additional_special_tokens='[DIAGNOSIS], [SYMPTOM], [DRUG]'.split(', ')\n)\ntokenizer.save_pretrained('tokenizer_hf')\n\n# WordPiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å vocab_size=30000.\n# –î–æ–±–∞–≤—å—Ç–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: [UNK], [PAD], [CLS], [SEP], [DIAGNOSIS], [SYMPTOM], [DRUG]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞—á–∞ 3 –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä","metadata":{}},{"cell_type":"code","source":"data = [\n    \"–†–æ–Ω–∏ –ö–æ–ª–µ–º–∞–Ω —Ä–æ–¥–∏–ª—Å—è –≤ –õ—É–∏–∑–∏–∞–Ω–µ –≤ 1964 –≥–æ–¥—É.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —Ä–æ–¥–∏–ª—Å—è –≤ –ê–≤—Å—Ç—Ä–∏–∏ –≤ 1947 –≥–æ–¥—É.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –≤—ã–∏–≥—Ä–∞–ª –ú–∏—Å—Ç–µ—Ä –û–ª–∏–º–ø–∏—è –≤–æ—Å–µ–º—å —Ä–∞–∑ –ø–æ–¥—Ä—è–¥.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –ø–æ–±–µ–∂–¥–∞–ª –Ω–∞ –û–ª–∏–º–ø–∏–∏ —Å–µ–º—å —Ä–∞–∑.\",\n    \"–û–±–∞ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∞ –≤–¥–æ—Ö–Ω–æ–≤–∏–ª–∏ –º–∏–ª–ª–∏–æ–Ω—ã –ø–æ—Å–µ—Ç–∏—Ç—å —Ç—Ä–µ–Ω–∞–∂—ë—Ä–Ω—ã–π –∑–∞–ª.\",\n    \"–†–æ–Ω–∏ —Å–ª—É–∂–∏–ª –ø–æ–ª–∏—Ü–µ–π—Å–∫–∏–º –≤ –≥–æ—Ä–æ–¥–µ –ê—Ä–ª–∏–Ω–≥—Ç–æ–Ω.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ —ç–º–∏–≥—Ä–∏—Ä–æ–≤–∞–ª –≤ –°–®–ê –≤ 1968 –≥–æ–¥—É.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–µ–π —Ñ—Ä–∞–∑–æ–π ¬´Yeah buddy!¬ª.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —á–∞—Å—Ç–æ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç ¬´I'll be back¬ª.\",\n    \"–û–±–∞ —Ä–∞–∑–≤–∏–≤–∞–ª–∏ –±–∏–∑–Ω–µ—Å –≤ —Å—Ñ–µ—Ä–µ —Ñ–∏—Ç–Ω–µ—Å–∞.\",\n    \"–†–æ–Ω–∏ –ø—Ä–∏—Å–µ–¥–∞–ª —Å –≤–µ—Å–æ–º –±–æ–ª–µ–µ 360 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –ø–æ–ø—É–ª—è—Ä–∏–∑–∏—Ä–æ–≤–∞–ª –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ —Ñ–∏–ª—å–º ¬´–ö–∞—á–∞—è –∂–µ–ª–µ–∑–æ¬ª.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø–µ—Ä–µ–Ω—ë—Å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ –ø–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫–µ.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —Å–ª—É–∂–∏–ª –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–æ–º –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–∏.\",\n    \"–û–±–∞ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∞ –ø–∏—Å–∞–ª–∏ –∫–Ω–∏–≥–∏ –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞—Ö.\",\n    \"–†–æ–Ω–∏ —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É ¬´—á–µ–º —Ç—è–∂–µ–ª–µ–µ, —Ç–µ–º –ª—É—á—à–µ¬ª.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –ø—Ä–∏–º–µ–Ω—è–ª –º–µ—Ç–æ–¥ ¬´–∫–æ–Ω—Ñ—É–∑–∏–∏ –º—ã—à—Ü¬ª.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –∑–∞—Ä–∞–±–æ—Ç–∞–ª –ø—Ä–æ–∑–≤–∏—â–µ ¬´Big Ron¬ª.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –ø–æ–ª—É—á–∏–ª —Ä–æ–ª—å –ö–æ–Ω–∞–Ω–∞-–≤–∞—Ä–≤–∞—Ä–∞.\",\n    \"–û–±–∞ –ø—Ä–∏–∑–Ω–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–∏—Ç–∞–Ω–∏—è.\",\n    \"–†–æ–Ω–∏ –≤—ã—Å—Ç—É–ø–∞–ª –ø—Ä–∏ –≤–µ—Å–µ –æ–∫–æ–ª–æ 135 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –≤—ã—Å—Ç—É–ø–∞–ª –ø—Ä–∏ –≤–µ—Å–µ –æ–∫–æ–ª–æ 110 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ YouTube.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –≤–µ–¥—ë—Ç –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—É—é —Ä–∞—Å—Å—ã–ª–∫—É –æ –∑–¥–æ—Ä–æ–≤–æ–º –æ–±—Ä–∞–∑–µ –∂–∏–∑–Ω–∏.\",\n    \"–û–±–∞ –ø—Ä–æ–ø–∞–≥–∞–Ω–¥–∏—Ä—É—é—Ç —Å–∏–ª–æ–≤–æ–π —Ç—Ä–µ–Ω–∏–Ω–≥ —Å—Ä–µ–¥–∏ –∂–µ–Ω—â–∏–Ω.\",\n    \"–†–æ–Ω–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –æ—Ä–¥–µ–Ω–∞ –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥–∞.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞—Ö.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø–æ–ª—É—á–∏–ª –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏–∏.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –∏–Ω–≤–µ—Å—Ç–∏—Ä—É–µ—Ç –≤ —Å—Ç–∞—Ä—Ç–∞–ø—ã –≤ —Å—Ñ–µ—Ä–µ —Ñ–∏—Ç–Ω–µ—Å–∞.\",\n    \"–û–±–∞ –æ—Å—Ç–∞—é—Ç—Å—è –ª–µ–≥–µ–Ω–¥–∞–º–∏ –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥–∞ –Ω–∞ –≤—Å–µ –≤—Ä–µ–º–µ–Ω–∞.\"\n]\n\nfrom tokenizers import Tokenizer, trainers, models, pre_tokenizers, processors\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = Tokenizer(models.BPE(unk_token='[UNK]'))\ntokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\ntrainer = trainers.BpeTrainer(\n    vocab_size=50000,\n    special_tokens=['[UNK]', '[PAD]', '[SEP]', '[CLS]', '[RU]', '[EN]', '[ZH]']\n)\ntokenizer.train_from_iterator(data, trainer=trainer)\ntokenizer.post_processor = processors.TemplateProcessing(\n    single='[CLS] $A [SEP]',\n    pair='[CLS] $A [SEP] $B:1 [SEP]:1',\n    special_tokens=[('[CLS]', tokenizer.token_to_id('[CLS]')), ('[SEP]', tokenizer.token_to_id('[SEP]'))]\n)\ntokenizer.save('tokenizer.json')\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file='tokenizer.json',\n    cls_token='[CLS]', sep_token='[SEP]',\n    pad_token='[PAD]', unk_token='[UNK]',\n    additional_special_tokens=['[RU]', '[EN]', '[ZH]']\n)\ntokenizer.save_pretrained('tokenizer_hf')\n\n# BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ, –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤. vocab_size=50000.\n# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Metaspace pre-tokenizer –∏ –¥–æ–±–∞–≤—å—Ç–µ —Ç–æ–∫–µ–Ω—ã —è–∑—ã–∫–æ–≤: [RU], [EN], [ZH].","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 4 –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —á–∞—Ç-–±–æ—Ç–æ–≤","metadata":{}},{"cell_type":"code","source":"data = [\n    \"–†–æ–Ω–∏ –ö–æ–ª–µ–º–∞–Ω —Ä–æ–¥–∏–ª—Å—è –≤ –õ—É–∏–∑–∏–∞–Ω–µ –≤ 1964 –≥–æ–¥—É.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —Ä–æ–¥–∏–ª—Å—è –≤ –ê–≤—Å—Ç—Ä–∏–∏ –≤ 1947 –≥–æ–¥—É.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –≤—ã–∏–≥—Ä–∞–ª –ú–∏—Å—Ç–µ—Ä –û–ª–∏–º–ø–∏—è –≤–æ—Å–µ–º—å —Ä–∞–∑ –ø–æ–¥—Ä—è–¥.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –ø–æ–±–µ–∂–¥–∞–ª –Ω–∞ –û–ª–∏–º–ø–∏–∏ —Å–µ–º—å —Ä–∞–∑.\",\n    \"–û–±–∞ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∞ –≤–¥–æ—Ö–Ω–æ–≤–∏–ª–∏ –º–∏–ª–ª–∏–æ–Ω—ã –ø–æ—Å–µ—Ç–∏—Ç—å —Ç—Ä–µ–Ω–∞–∂—ë—Ä–Ω—ã–π –∑–∞–ª.\",\n    \"–†–æ–Ω–∏ —Å–ª—É–∂–∏–ª –ø–æ–ª–∏—Ü–µ–π—Å–∫–∏–º –≤ –≥–æ—Ä–æ–¥–µ –ê—Ä–ª–∏–Ω–≥—Ç–æ–Ω.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ —ç–º–∏–≥—Ä–∏—Ä–æ–≤–∞–ª –≤ –°–®–ê –≤ 1968 –≥–æ–¥—É.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–µ–π —Ñ—Ä–∞–∑–æ–π ¬´Yeah buddy!¬ª.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —á–∞—Å—Ç–æ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç ¬´I'll be back¬ª.\",\n    \"–û–±–∞ —Ä–∞–∑–≤–∏–≤–∞–ª–∏ –±–∏–∑–Ω–µ—Å –≤ —Å—Ñ–µ—Ä–µ —Ñ–∏—Ç–Ω–µ—Å–∞.\",\n    \"–†–æ–Ω–∏ –ø—Ä–∏—Å–µ–¥–∞–ª —Å –≤–µ—Å–æ–º –±–æ–ª–µ–µ 360 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –ø–æ–ø—É–ª—è—Ä–∏–∑–∏—Ä–æ–≤–∞–ª –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ —Ñ–∏–ª—å–º ¬´–ö–∞—á–∞—è –∂–µ–ª–µ–∑–æ¬ª.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø–µ—Ä–µ–Ω—ë—Å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ –ø–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫–µ.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —Å–ª—É–∂–∏–ª –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–æ–º –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–∏.\",\n    \"–û–±–∞ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∞ –ø–∏—Å–∞–ª–∏ –∫–Ω–∏–≥–∏ –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞—Ö.\",\n    \"–†–æ–Ω–∏ —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É ¬´—á–µ–º —Ç—è–∂–µ–ª–µ–µ, —Ç–µ–º –ª—É—á—à–µ¬ª.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –ø—Ä–∏–º–µ–Ω—è–ª –º–µ—Ç–æ–¥ ¬´–∫–æ–Ω—Ñ—É–∑–∏–∏ –º—ã—à—Ü¬ª.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –∑–∞—Ä–∞–±–æ—Ç–∞–ª –ø—Ä–æ–∑–≤–∏—â–µ ¬´Big Ron¬ª.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –ø–æ–ª—É—á–∏–ª —Ä–æ–ª—å –ö–æ–Ω–∞–Ω–∞-–≤–∞—Ä–≤–∞—Ä–∞.\",\n    \"–û–±–∞ –ø—Ä–∏–∑–Ω–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–∏—Ç–∞–Ω–∏—è.\",\n    \"–†–æ–Ω–∏ –≤—ã—Å—Ç—É–ø–∞–ª –ø—Ä–∏ –≤–µ—Å–µ –æ–∫–æ–ª–æ 135 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –≤—ã—Å—Ç—É–ø–∞–ª –ø—Ä–∏ –≤–µ—Å–µ –æ–∫–æ–ª–æ 110 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ YouTube.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –≤–µ–¥—ë—Ç –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—É—é —Ä–∞—Å—Å—ã–ª–∫—É –æ –∑–¥–æ—Ä–æ–≤–æ–º –æ–±—Ä–∞–∑–µ –∂–∏–∑–Ω–∏.\",\n    \"–û–±–∞ –ø—Ä–æ–ø–∞–≥–∞–Ω–¥–∏—Ä—É—é—Ç —Å–∏–ª–æ–≤–æ–π —Ç—Ä–µ–Ω–∏–Ω–≥ —Å—Ä–µ–¥–∏ –∂–µ–Ω—â–∏–Ω.\",\n    \"–†–æ–Ω–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –æ—Ä–¥–µ–Ω–∞ –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥–∞.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞—Ö.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø–æ–ª—É—á–∏–ª –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏–∏.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –∏–Ω–≤–µ—Å—Ç–∏—Ä—É–µ—Ç –≤ —Å—Ç–∞—Ä—Ç–∞–ø—ã –≤ —Å—Ñ–µ—Ä–µ —Ñ–∏—Ç–Ω–µ—Å–∞.\",\n    \"–û–±–∞ –æ—Å—Ç–∞—é—Ç—Å—è –ª–µ–≥–µ–Ω–¥–∞–º–∏ –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥–∞ –Ω–∞ –≤—Å–µ –≤—Ä–µ–º–µ–Ω–∞.\"\n]\n\nfrom tokenizers import Tokenizer, trainers, models, pre_tokenizers, processors\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = Tokenizer(models.Unigram())\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\ntrainer = trainers.UnigramTrainer(\n    vocab_size=8000,\n    special_tokens=['[UNK]', '[PAD]', '[USER]', '[BOT]', '[SYSTEM]', '[EOS]'],\n    unk_token='[UNK]'\n)\ntokenizer.train_from_iterator(data, trainer=trainer)\ntokenizer.post_processor = processors.TemplateProcessing(\n    single='[EOS] $A [EOS]',\n    pair='[EOS] $A [EOS] $B:1 [EOS]:1',\n    special_tokens=[('[EOS]', tokenizer.token_to_id('[EOS]'))]\n)\ntokenizer.save('tokenizer.json')\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file='tokenizer.json',\n    cls_token='[EOS]', sep_token='[EOS]',\n    pad_token='[PAD]', unk_token='[UNK]',\n    additional_special_tokens=['[USER]', '[BOT]', '[SYSTEM]']\n)\ntokenizer.save_pretrained('tokenizer_hf')\n\n# Unigram —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å vocab_size=8000. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: [UNK], [PAD], [USER], [BOT], [SYSTEM], [EOS].\n# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Whitespace pre-tokenizer.","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä","metadata":{}},{"cell_type":"code","source":"data = [\n    \"–†–æ–Ω–∏ –ö–æ–ª–µ–º–∞–Ω —Ä–æ–¥–∏–ª—Å—è –≤ –õ—É–∏–∑–∏–∞–Ω–µ –≤ 1964 –≥–æ–¥—É.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —Ä–æ–¥–∏–ª—Å—è –≤ –ê–≤—Å—Ç—Ä–∏–∏ –≤ 1947 –≥–æ–¥—É.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –≤—ã–∏–≥—Ä–∞–ª –ú–∏—Å—Ç–µ—Ä –û–ª–∏–º–ø–∏—è –≤–æ—Å–µ–º—å —Ä–∞–∑ –ø–æ–¥—Ä—è–¥.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –ø–æ–±–µ–∂–¥–∞–ª –Ω–∞ –û–ª–∏–º–ø–∏–∏ —Å–µ–º—å —Ä–∞–∑.\",\n    \"–û–±–∞ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∞ –≤–¥–æ—Ö–Ω–æ–≤–∏–ª–∏ –º–∏–ª–ª–∏–æ–Ω—ã –ø–æ—Å–µ—Ç–∏—Ç—å —Ç—Ä–µ–Ω–∞–∂—ë—Ä–Ω—ã–π –∑–∞–ª.\",\n    \"–†–æ–Ω–∏ —Å–ª—É–∂–∏–ª –ø–æ–ª–∏—Ü–µ–π—Å–∫–∏–º –≤ –≥–æ—Ä–æ–¥–µ –ê—Ä–ª–∏–Ω–≥—Ç–æ–Ω.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ —ç–º–∏–≥—Ä–∏—Ä–æ–≤–∞–ª –≤ –°–®–ê –≤ 1968 –≥–æ–¥—É.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–µ–π —Ñ—Ä–∞–∑–æ–π ¬´Yeah buddy!¬ª.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —á–∞—Å—Ç–æ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç ¬´I'll be back¬ª.\",\n    \"–û–±–∞ —Ä–∞–∑–≤–∏–≤–∞–ª–∏ –±–∏–∑–Ω–µ—Å –≤ —Å—Ñ–µ—Ä–µ —Ñ–∏—Ç–Ω–µ—Å–∞.\",\n    \"–†–æ–Ω–∏ –ø—Ä–∏—Å–µ–¥–∞–ª —Å –≤–µ—Å–æ–º –±–æ–ª–µ–µ 360 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –ø–æ–ø—É–ª—è—Ä–∏–∑–∏—Ä–æ–≤–∞–ª –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ —Ñ–∏–ª—å–º ¬´–ö–∞—á–∞—è –∂–µ–ª–µ–∑–æ¬ª.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø–µ—Ä–µ–Ω—ë—Å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ –ø–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫–µ.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —Å–ª—É–∂–∏–ª –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–æ–º –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–∏.\",\n    \"–û–±–∞ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∞ –ø–∏—Å–∞–ª–∏ –∫–Ω–∏–≥–∏ –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞—Ö.\",\n    \"–†–æ–Ω–∏ —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É ¬´—á–µ–º —Ç—è–∂–µ–ª–µ–µ, —Ç–µ–º –ª—É—á—à–µ¬ª.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –ø—Ä–∏–º–µ–Ω—è–ª –º–µ—Ç–æ–¥ ¬´–∫–æ–Ω—Ñ—É–∑–∏–∏ –º—ã—à—Ü¬ª.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –∑–∞—Ä–∞–±–æ—Ç–∞–ª –ø—Ä–æ–∑–≤–∏—â–µ ¬´Big Ron¬ª.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –ø–æ–ª—É—á–∏–ª —Ä–æ–ª—å –ö–æ–Ω–∞–Ω–∞-–≤–∞—Ä–≤–∞—Ä–∞.\",\n    \"–û–±–∞ –ø—Ä–∏–∑–Ω–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–∏—Ç–∞–Ω–∏—è.\",\n    \"–†–æ–Ω–∏ –≤—ã—Å—Ç—É–ø–∞–ª –ø—Ä–∏ –≤–µ—Å–µ –æ–∫–æ–ª–æ 135 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –≤—ã—Å—Ç—É–ø–∞–ª –ø—Ä–∏ –≤–µ—Å–µ –æ–∫–æ–ª–æ 110 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ YouTube.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –≤–µ–¥—ë—Ç –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—É—é —Ä–∞—Å—Å—ã–ª–∫—É –æ –∑–¥–æ—Ä–æ–≤–æ–º –æ–±—Ä–∞–∑–µ –∂–∏–∑–Ω–∏.\",\n    \"–û–±–∞ –ø—Ä–æ–ø–∞–≥–∞–Ω–¥–∏—Ä—É—é—Ç —Å–∏–ª–æ–≤–æ–π —Ç—Ä–µ–Ω–∏–Ω–≥ —Å—Ä–µ–¥–∏ –∂–µ–Ω—â–∏–Ω.\",\n    \"–†–æ–Ω–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –æ—Ä–¥–µ–Ω–∞ –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥–∞.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞—Ö.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø–æ–ª—É—á–∏–ª –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏–∏.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –∏–Ω–≤–µ—Å—Ç–∏—Ä—É–µ—Ç –≤ —Å—Ç–∞—Ä—Ç–∞–ø—ã –≤ —Å—Ñ–µ—Ä–µ —Ñ–∏—Ç–Ω–µ—Å–∞.\",\n    \"–û–±–∞ –æ—Å—Ç–∞—é—Ç—Å—è –ª–µ–≥–µ–Ω–¥–∞–º–∏ –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥–∞ –Ω–∞ –≤—Å–µ –≤—Ä–µ–º–µ–Ω–∞.\"\n]\n\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = Tokenizer(models.BPE(unk_token='[UNK]'))\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\ntrainer = trainers.BpeTrainer(\n    vocab_size=3000,\n    special_tokens='[UNK], [PAD], [NUM], [VAR], [EQ], [INTEGRAL], [SUM]'.split(', ')\n)\ntokenizer.train_from_iterator(data, trainer=trainer)\ntokenizer.post_processor = processors.TemplateProcessing(\n    single='$A',\n    # pair='$B $B:1'  –ø–∞—Ä—É –≤ —ç—Ç–æ–º –ø—Ä–∏ –¥–∞–Ω–Ω—ã—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö –Ω–µ —Å–¥–µ–ª–∞—Ç—å\n    special_tokens=[]\n)\ntokenizer.save('tokenizer.json')\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file='tokenizer.json',\n    unk_token='[UNK]', pad_token='[PAD]', cls_token='[PAD]', sep_token='[PAD]',\n    additional_special_tokens='[NUM], [VAR], [EQ], [INTEGRAL], [SUM]'.split(', ')\n)\ntokenizer.save_pretrained('tokenizer_hf')\n\n# BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π, vocab_size=3000.\n# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: [UNK], [PAD], [NUM], [VAR], [EQ], [INTEGRAL], [SUM].","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"7. –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä","metadata":{}},{"cell_type":"code","source":"data = [\n    \"–†–æ–Ω–∏ –ö–æ–ª–µ–º–∞–Ω —Ä–æ–¥–∏–ª—Å—è –≤ –õ—É–∏–∑–∏–∞–Ω–µ –≤ 1964 –≥–æ–¥—É.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —Ä–æ–¥–∏–ª—Å—è –≤ –ê–≤—Å—Ç—Ä–∏–∏ –≤ 1947 –≥–æ–¥—É.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –≤—ã–∏–≥—Ä–∞–ª –ú–∏—Å—Ç–µ—Ä –û–ª–∏–º–ø–∏—è –≤–æ—Å–µ–º—å —Ä–∞–∑ –ø–æ–¥—Ä—è–¥.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –ø–æ–±–µ–∂–¥–∞–ª –Ω–∞ –û–ª–∏–º–ø–∏–∏ —Å–µ–º—å —Ä–∞–∑.\",\n    \"–û–±–∞ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∞ –≤–¥–æ—Ö–Ω–æ–≤–∏–ª–∏ –º–∏–ª–ª–∏–æ–Ω—ã –ø–æ—Å–µ—Ç–∏—Ç—å —Ç—Ä–µ–Ω–∞–∂—ë—Ä–Ω—ã–π –∑–∞–ª.\",\n    \"–†–æ–Ω–∏ —Å–ª—É–∂–∏–ª –ø–æ–ª–∏—Ü–µ–π—Å–∫–∏–º –≤ –≥–æ—Ä–æ–¥–µ –ê—Ä–ª–∏–Ω–≥—Ç–æ–Ω.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ —ç–º–∏–≥—Ä–∏—Ä–æ–≤–∞–ª –≤ –°–®–ê –≤ 1968 –≥–æ–¥—É.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–µ–π —Ñ—Ä–∞–∑–æ–π ¬´Yeah buddy!¬ª.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —á–∞—Å—Ç–æ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç ¬´I'll be back¬ª.\",\n    \"–û–±–∞ —Ä–∞–∑–≤–∏–≤–∞–ª–∏ –±–∏–∑–Ω–µ—Å –≤ —Å—Ñ–µ—Ä–µ —Ñ–∏—Ç–Ω–µ—Å–∞.\",\n    \"–†–æ–Ω–∏ –ø—Ä–∏—Å–µ–¥–∞–ª —Å –≤–µ—Å–æ–º –±–æ–ª–µ–µ 360 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –ø–æ–ø—É–ª—è—Ä–∏–∑–∏—Ä–æ–≤–∞–ª –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ —Ñ–∏–ª—å–º ¬´–ö–∞—á–∞—è –∂–µ–ª–µ–∑–æ¬ª.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø–µ—Ä–µ–Ω—ë—Å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ –ø–æ–∑–≤–æ–Ω–æ—á–Ω–∏–∫–µ.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä —Å–ª—É–∂–∏–ª –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–æ–º –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–∏.\",\n    \"–û–±–∞ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω–∞ –ø–∏—Å–∞–ª–∏ –∫–Ω–∏–≥–∏ –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞—Ö.\",\n    \"–†–æ–Ω–∏ —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É ¬´—á–µ–º —Ç—è–∂–µ–ª–µ–µ, —Ç–µ–º –ª—É—á—à–µ¬ª.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –ø—Ä–∏–º–µ–Ω—è–ª –º–µ—Ç–æ–¥ ¬´–∫–æ–Ω—Ñ—É–∑–∏–∏ –º—ã—à—Ü¬ª.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –∑–∞—Ä–∞–±–æ—Ç–∞–ª –ø—Ä–æ–∑–≤–∏—â–µ ¬´Big Ron¬ª.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –ø–æ–ª—É—á–∏–ª —Ä–æ–ª—å –ö–æ–Ω–∞–Ω–∞-–≤–∞—Ä–≤–∞—Ä–∞.\",\n    \"–û–±–∞ –ø—Ä–∏–∑–Ω–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–∏—Ç–∞–Ω–∏—è.\",\n    \"–†–æ–Ω–∏ –≤—ã—Å—Ç—É–ø–∞–ª –ø—Ä–∏ –≤–µ—Å–µ –æ–∫–æ–ª–æ 135 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ –≤—ã—Å—Ç—É–ø–∞–ª –ø—Ä–∏ –≤–µ—Å–µ –æ–∫–æ–ª–æ 110 –∫–∏–ª–æ–≥—Ä–∞–º–º–æ–≤.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ YouTube.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –≤–µ–¥—ë—Ç –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—É—é —Ä–∞—Å—Å—ã–ª–∫—É –æ –∑–¥–æ—Ä–æ–≤–æ–º –æ–±—Ä–∞–∑–µ –∂–∏–∑–Ω–∏.\",\n    \"–û–±–∞ –ø—Ä–æ–ø–∞–≥–∞–Ω–¥–∏—Ä—É—é—Ç —Å–∏–ª–æ–≤–æ–π —Ç—Ä–µ–Ω–∏–Ω–≥ —Å—Ä–µ–¥–∏ –∂–µ–Ω—â–∏–Ω.\",\n    \"–†–æ–Ω–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –æ—Ä–¥–µ–Ω–∞ –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥–∞.\",\n    \"–ê—Ä–Ω–æ–ª—å–¥ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞—Ö.\",\n    \"–ö–æ–ª–µ–º–∞–Ω –ø–æ–ª—É—á–∏–ª –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç–∏ –±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏–∏.\",\n    \"–®–≤–∞—Ä—Ü–µ–Ω–µ–≥–≥–µ—Ä –∏–Ω–≤–µ—Å—Ç–∏—Ä—É–µ—Ç –≤ —Å—Ç–∞—Ä—Ç–∞–ø—ã –≤ —Å—Ñ–µ—Ä–µ —Ñ–∏—Ç–Ω–µ—Å–∞.\",\n    \"–û–±–∞ –æ—Å—Ç–∞—é—Ç—Å—è –ª–µ–≥–µ–Ω–¥–∞–º–∏ –±–æ–¥–∏–±–∏–ª–¥–∏–Ω–≥–∞ –Ω–∞ –≤—Å–µ –≤—Ä–µ–º–µ–Ω–∞.\"\n]\n\nfrom tokenizers import trainers, models, Tokenizer, pre_tokenizers, processors\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = Tokenizer(models.BPE(unk_token='[UNK]'))\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\ntrainer = trainers.BpeTrainer(\n    vocab_size=40000,\n    special_tokens='[UNK], [PAD], [CLS], [SEP], [ARTICLE], [CLAUSE], [PARTY_A], [PARTY_B]'.split(', ')\n)\ntokenizer.train_from_iterator(data, trainer=trainer)\ntokenizer.post_processor = processors.TemplateProcessing(\n    single='[CLS] $A [SEP]',\n    pair='[CLS] $A [SEP] $B:1 [SEP]:1',\n    special_tokens=[('[CLS]', tokenizer.token_to_id('[CLS]')), ('[SEP]', tokenizer.token_to_id('[SEP]'))]\n)\ntokenizer.save('tokenizer.json')\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file='tokenizer.json',\n    unk_token='[UNK]', pad_token='[PAD]',\n    sep_token='[SEP]', cls_token='[CLS]',\n    additional_special_tokens='[ARTICLE], [CLAUSE], [PARTY_A], [PARTY_B]'.split(', ')\n)\ntokenizer.save_pretrained('tokenizer_hf')\n\n# BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, vocab_size=40000.\n# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: [UNK], [PAD], [CLS], [SEP], [ARTICLE], [CLAUSE], [PARTY_A], [PARTY_B].","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –º–æ–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n!pip install evaluate wav2clip\n\n# core/utils.py\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom PIL import Image\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef to_pil(x):\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Expected path/np.ndarray/PIL.Image\")\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"–¢—Ä–µ–±—É–µ—Ç—Å—è torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\ndef safe_load(component_cls, checkpoint: str, cache_dir: str = \"./model_cache\",\n              local_files_only: bool = None, **kwargs):\n    if local_files_only is None:\n        local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n    name = getattr(component_cls, \"__name__\", \"\")\n    if \"Tokenizer\" in name:\n        kwargs.setdefault(\"use_fast\", True)\n    return component_cls.from_pretrained(\n        checkpoint, cache_dir=cache_dir, local_files_only=local_files_only, **kwargs\n    )\n\n# core/types.py\nimport torch\n\ndef preferred_device():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n\n# core/device.py\nfrom typing import Any, Dict, List, Protocol, Literal\nimport torch\n\nModality = Literal[\"text\", \"image\", \"audio\"]\n\nclass Processor(Protocol):\n    modality: Modality\n    def prepare_batch(self, raw_items: List[Any]) -> Dict[str, torch.Tensor]:\n        ...\n\nclass Encoder(Protocol):\n    modality: Modality\n    embed_dim: int\n    def encode_batch(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n        ...\n\nclass ItemAggregator(Protocol):\n    def aggregate(self, embs: torch.Tensor, counts: List[int], max_items: int, how: str) -> torch.Tensor:\n        ...\n\nclass Fusion(Protocol):\n    def __call__(self, zs: Dict[Modality, torch.Tensor]) -> torch.Tensor:\n        ...\n\n# data/augmentation.py\nimport os\nimport uuid\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, List, Optional, Callable\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\ndef augment_data_table(\n    df: pd.DataFrame,\n    save_dir: str,\n    image_columns: Optional[List[str]] = None,\n    text_columns: Optional[List[str]] = None,\n    audio_columns: Optional[List[str]] = None,\n    image_augment_fn: Optional[List[Callable[[Image.Image], Image.Image]]] = None,\n    text_augment_fn: Optional[List[Callable[[str], str]]] = None,\n    audio_augment_fn: Optional[List[Callable[..., np.ndarray]]] = None,\n    id_col: Optional[str] = None,\n    audio_sr: int = 16000,\n) -> pd.DataFrame:\n    \"\"\"\n    –î–µ–ª–∞–µ—Ç –ø—Ä–æ—Å—Ç—É—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö.\n    - –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¢–û–õ–¨–ö–û –¥–ª—è —Ç–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ —Ñ—É–Ω–∫—Ü–∏–π.\n    - –î–ª—è –∫–∞–∂–¥–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞—ë—Ç –û–î–ò–ù –≤–∞—Ä–∏–∞–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ (–±–µ–∑ –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏—è –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏).\n    - –ö–∞—Ä—Ç–∏–Ω–∫–∏/–∞—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ save_dir, –≤ —Ç–∞–±–ª–∏—Ü–µ –ø–æ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –ø—É—Ç–∏ –∫ –Ω–æ–≤—ã–º —Ñ–∞–π–ª–∞–º.\n    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å –∫–æ–ª–æ–Ω–∫–æ–π __aug.\n    \"\"\"\n    image_columns = image_columns or []\n    text_columns  = text_columns or []\n    audio_columns = audio_columns or []\n\n    os.makedirs(save_dir, exist_ok=True)\n    run_tag = uuid.uuid4().hex[:8]\n\n    def _row_id(i: int) -> str:\n        if id_col and id_col in df.columns:\n            return str(df.iloc[i][id_col])\n        return f\"row{i}\"\n\n    def _to_pil(v: Any) -> Image.Image:\n        if isinstance(v, Image.Image): return v.convert(\"RGB\")\n        if isinstance(v, str):         return Image.open(v).convert(\"RGB\")\n        if isinstance(v, np.ndarray):\n            if v.dtype != np.uint8: v = np.clip(v, 0, 255).astype(np.uint8)\n            return Image.fromarray(v)\n        raise TypeError(\"Unsupported image type\")\n\n    def _save_img(im: Image.Image, out_dir: str, name: str) -> str:\n        os.makedirs(out_dir, exist_ok=True)\n        path = os.path.join(out_dir, f\"{name}.png\")\n        im.save(path)\n        return path\n\n    def _read_audio(x: Any) -> tuple[np.ndarray, int]:\n        if isinstance(x, str):\n            try:\n                import soundfile as sf\n                y, sr = sf.read(x, always_2d=False)\n            except Exception:\n                from scipy.io import wavfile\n                sr, y = wavfile.read(x)\n            y = np.asarray(y)\n            if y.ndim > 1: y = y.mean(axis=-1)\n            y = (y.astype(np.float32) / (32768.0 if y.dtype.kind in \"iu\" else 1.0))\n            return np.clip(y, -1.0, 1.0), sr\n        elif isinstance(x, np.ndarray):\n            y = x\n            if y.ndim > 1: y = y.mean(axis=-1)\n            y = y.astype(np.float32, copy=False)\n            if y.dtype.kind in \"iu\": y = y / 32768.0\n            return np.clip(y, -1.0, 1.0), audio_sr\n        raise TypeError(\"Unsupported audio type\")\n\n    def _save_wav(y: np.ndarray, sr: int, out_dir: str, name: str) -> str:\n        os.makedirs(out_dir, exist_ok=True)\n        path = os.path.join(out_dir, f\"{name}.wav\")\n        try:\n            import soundfile as sf\n            sf.write(path, y.astype(np.float32, copy=False), sr, subtype=\"PCM_16\")\n        except Exception:\n            from scipy.io import wavfile\n            wavfile.write(path, sr, (np.clip(y, -1, 1) * 32767).astype(np.int16))\n        return path\n\n    out_parts = []\n\n    # –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –µ—Å—Ç—å —Å—Ç–æ–ª–±—Ü—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n    if image_columns and image_augment_fn:\n        for k, fn in tqdm(list(enumerate(image_augment_fn)), desc='–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–æ–∫'):\n            df_aug = df.copy(deep=True)\n            for col_i, col in enumerate(image_columns):\n                out_dir = os.path.join(save_dir, \"images\", col, f\"aug_image_{k}_{run_tag}\")\n                df_aug[col] = [\n                    _save_img(fn(_to_pil(v)), out_dir, f\"{_row_id(i)}\") if v is not None else v\n                    for i, v in tqdm(list(enumerate(df[col].tolist())),\n                                     desc=f'–°—Ç–æ–ª–±–µ—Ü {col_i+1}/{len(image_columns)}', leave=False)\n                ]\n            df_aug[\"__aug\"] = f\"image_{k}\"\n            out_parts.append(df_aug)\n\n    # –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã\n    if text_columns and text_augment_fn:\n        for k, fn in tqdm(list(enumerate(text_augment_fn)), desc='–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤'):\n            df_aug = df.copy(deep=True)\n            for col_i, col in enumerate(text_columns):\n                df_aug[col] = [\n                    fn(v) if isinstance(v, str) else v\n                    for v in tqdm(df[col].tolist(),\n                                  desc=f'–°—Ç–æ–ª–±–µ—Ü {col_i+1}/{len(text_columns)}', leave=False)\n                ]\n            df_aug[\"__aug\"] = f\"text_{k}\"\n            out_parts.append(df_aug)\n\n    # –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –µ—Å—Ç—å –∞—É–¥–∏–æ —Å—Ç–æ–ª–±—Ü—ã\n    if audio_columns and audio_augment_fn:\n        for k, fn in tqdm(list(enumerate(audio_augment_fn)), desc='–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–≤—É–∫–æ–≤'):\n            df_aug = df.copy(deep=True)\n            for col_i, col in enumerate(audio_columns):\n                out_dir = os.path.join(save_dir, \"audio\", col, f\"aug_audio_{k}_{run_tag}\")\n                new_vals = []\n                for i, v in tqdm(list(enumerate(df[col].tolist())),\n                                 desc=f'–°—Ç–æ–ª–±–µ—Ü {col_i+1}/{len(audio_columns)}', leave=False):\n                    if v is None:\n                        new_vals.append(v); continue\n                    y, sr = _read_audio(v)\n                    try:    y2 = fn(y, sr)\n                    except TypeError: y2 = fn(y)\n                    path = _save_wav(np.asarray(y2, dtype=np.float32), sr, out_dir, f\"{_row_id(i)}\")\n                    new_vals.append(path)\n                df_aug[col] = new_vals\n            df_aug[\"__aug\"] = f\"audio_{k}\"\n            out_parts.append(df_aug)\n\n    if not out_parts:\n        raise ValueError(\"–ù–µ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ ‚Äî –Ω–µ—á–µ–≥–æ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å.\")\n\n    return pd.concat(out_parts, ignore_index=True)\n\n# data/tokenization.py\nfrom functools import lru_cache\nfrom typing import Dict, List, Optional\nimport numpy as np\nimport torch\n\nclass BatchTokenizer:\n    def __init__(self, tokenizer, max_length=512, cache_size=10000, batch_size=256, padding_strategy=\"max_length\"):\n        assert padding_strategy in {\"max_length\", \"dynamic\"}\n        self.tok = tokenizer\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.padding_strategy = padding_strategy\n        self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n        self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n        if self.is_fast:\n            print(\"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Fast Tokenizer\")\n\n    def _tokenize_single(self, text: str) -> Dict[str, np.ndarray]:\n        res = self.tok(text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        return {k: v.squeeze(0).cpu().numpy() for k, v in res.items()}\n\n    def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n        if self.padding_strategy == \"dynamic\":\n            res = self.tok(texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n            return {k: (v.long() if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else v) for k, v in res.items()}\n        if use_cache and len(texts) < 100:\n            items = [self._cache(t) for t in texts]\n            keys = items[0].keys()\n            out = {}\n            for k in keys:\n                dtype = torch.long if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else torch.float32\n                out[k] = torch.tensor(np.stack([it[k] for it in items]), dtype=dtype)\n            return out\n        res = self.tok(texts, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        return {k: (v.long() if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else v) for k, v in res.items()}\n\n    def tokenize_dataset_lazy(self, texts: List[str], batch_size: Optional[int] = None):\n        b = batch_size or self.batch_size\n        for i in range(0, len(texts), b):\n            yield self.tokenize_batch(texts[i:i+b], use_cache=False)\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n# data/dataset.py\nimport gc\nfrom typing import Any, Dict, List, Optional, Callable\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\n\nclass MultiModalDataset(Dataset):\n    \"\"\"\n    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:\n      - text pretokenize: BatchTokenizer –∏–ª–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (single/batched)\n      - dynamic/max –ø–∞–¥–¥–∏–Ω–≥ (dynamic –æ—Ç–∫–ª—é—á–∞–µ—Ç –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é)\n      - –º—É–ª—å—Ç–∏-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –º—É–ª—å—Ç–∏-–∞—É–¥–∏–æ –Ω–∞ —Å—ç–º–ø–ª\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: Optional[str],                 # –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ ‚Äî –∫–æ–ª–æ–Ω–∫–∞ —Å np.ndarray; –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ‚Äî –º–µ—Ç–∫–∞\n        task: str = \"regression\",                  # \"regression\" | \"classification\"\n        label2id: Optional[Dict[Any, int]] = None, # –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer=None,                       # BatchTokenizer\n        text_tokenizer_fn: Optional[Callable] = None,\n        text_tokenizer_fn_batched: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, Any]] = None,\n        pretokenize: bool = False,\n        pretokenize_batch_size: int = 256,\n        tokenizer_returns_tensors: bool = False,\n        deduplicate_texts: bool = True\n    ):\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.task = task.lower()\n        self.label2id = label2id or {}\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.batch_tokenizer = text_tokenizer\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.text_tokenizer_fn_batched = text_tokenizer_fn_batched\n\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n        self._N = len(self.df)\n\n        # labels\n        if self.task == \"classification\":\n            if self.target_col is None:\n                y = np.zeros(self._N, dtype=np.int64)\n            else:\n                y = self.df[self.target_col].map(self.label2id).fillna(0).astype(int).values\n            self._labels = torch.tensor(y, dtype=torch.long)\n        else:\n            self._labels = self._prepare_labels_regression(self.df, self.target_col)\n\n        # lists for images/audios\n        self._image_lists = self._collect_multi_values(self.df, self.image_columns) if self.image_columns else None\n        self._audio_lists = self._collect_multi_values(self.df, self.audio_columns) if self.audio_columns else None\n\n        self._tok_bank: Optional[Dict[str, torch.Tensor]] = None\n        self._has_text = bool(self.text_columns)\n\n        # dynamic padding -> disable pretokenization\n        if pretokenize and self.batch_tokenizer is not None and getattr(self.batch_tokenizer, \"padding_strategy\", \"max_length\") == \"dynamic\":\n            print(\"‚ö† –ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞: –≤—ã–±—Ä–∞–Ω dynamic-–ø–∞–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞.\")\n            pretokenize = False\n\n        if self._has_text and pretokenize:\n            if self.batch_tokenizer is not None and self.text_tokenizer_fn is None and self.text_tokenizer_fn_batched is None:\n                self._pretokenize_with_batch_tokenizer(pretokenize_batch_size)\n            else:\n                self._pretokenize_with_custom_fn(pretokenize_batch_size, deduplicate_texts)\n\n    # labels for regression: [N, K]\n    def _prepare_labels_regression(self, df: pd.DataFrame, target_col: Optional[str]) -> torch.Tensor:\n        if target_col is None or target_col not in df.columns:\n            return torch.zeros((len(df), 1), dtype=torch.float32)\n        labels_list = []\n        for i in range(len(df)):\n            v = df.iloc[i][target_col]\n            if isinstance(v, (list, tuple, np.ndarray)):\n                arr = np.asarray(v, dtype=np.float32)\n            else:\n                try:\n                    arr = np.asarray([float(v)], dtype=np.float32)\n                except Exception:\n                    arr = np.asarray([0.0], dtype=np.float32)\n            labels_list.append(arr)\n        K = max(a.shape[0] for a in labels_list) if labels_list else 1\n        out = np.zeros((len(labels_list), K), dtype=np.float32)\n        for i, a in enumerate(labels_list):\n            out[i, :a.shape[0]] = a\n        return torch.tensor(out, dtype=torch.float32)\n\n    def _join_text(self, row: pd.Series) -> str:\n        sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n        return sep.join([(\"\" if pd.isna(row[c]) else str(row[c])) for c in self.text_columns])\n\n    @staticmethod\n    def _as_list(v):\n        if v is None or (isinstance(v, float) and np.isnan(v)):\n            return []\n        if isinstance(v, (list, tuple)):\n            return list(v)\n        return [v]\n\n    def _collect_multi_values(self, df: pd.DataFrame, columns: List[str]) -> List[List[Any]]:\n        out = []\n        as_list = self._as_list\n        for _, row in df.iterrows():\n            lst: List[Any] = []\n            for c in columns:\n                if c in row:\n                    lst.extend([x for x in as_list(row[c]) if x is not None])\n            out.append(lst)\n        return out\n\n    def _pretokenize_with_batch_tokenizer(self, batch_size: int):\n        print(\"–ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å BatchTokenizer...\")\n        texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n        banks: Dict[str, List[torch.Tensor]] = {}\n        for start in range(0, self._N, batch_size):\n            batch = texts[start:start + batch_size]\n            tok = self.batch_tokenizer.tokenize_batch(batch, use_cache=False)\n            for k in tok:\n                if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\"):\n                    tok[k] = tok[k].long()\n                else:\n                    tok[k] = tok[k].to(torch.float32)\n            for k, v in tok.items():\n                banks.setdefault(k, []).append(v)\n        self._tok_bank = {k: torch.cat(v_parts, dim=0).contiguous() for k, v_parts in banks.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"‚úì –ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {self._N} –æ–±—Ä–∞–∑—Ü–æ–≤ | keys={list(self._tok_bank.keys())}, shapes={shapes}\")\n\n    def _pretokenize_with_custom_fn(self, batch_size: int, deduplicate_texts: bool):\n        cols = list(self.text_columns)\n        if self.text_tokenizer_fn_batched is not None:\n            print(\"–ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π batched-—Ñ—É–Ω–∫—Ü–∏–µ–π...\")\n            first_end = min(self._N, max(8, batch_size))\n            batch_data = []\n            for i in range(first_end):\n                row = self.df.iloc[i]\n                d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                batch_data.append(d)\n            first_tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n            if not isinstance(first_tok, dict):\n                raise ValueError(\"text_tokenizer_fn_batched –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å dict —Ç–µ–Ω–∑–æ—Ä–æ–≤ [B, L]\")\n            bank: Dict[str, torch.Tensor] = {}\n            for k, t in first_tok.items():\n                if not torch.is_tensor(t): t = torch.tensor(t)\n                bank[k] = torch.empty((self._N, t.size(1)), dtype=t.dtype)\n                bank[k][:first_end] = t[:first_end]\n            for start in range(first_end, self._N, batch_size):\n                end = min(self._N, start + batch_size)\n                batch_data = []\n                for i in range(start, end):\n                    row = self.df.iloc[i]\n                    d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                    batch_data.append(d)\n                tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): t = torch.tensor(t)\n                    bank[k][start:end] = t\n            self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n            shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n            print(f\"‚úì –ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π batched-—Ñ—É–Ω–∫—Ü–∏–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞: shapes={shapes}\")\n            return\n\n        print(\"–ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π single-—Ñ—É–Ω–∫—Ü–∏–µ–π...\")\n        col_arrays = [self.df[c].astype(str).where(~self.df[c].isna(), other=\"\").tolist() for c in cols]\n        first_td = {c: col_arrays[i][0] for i, c in enumerate(cols)}\n        first_tok = self.text_tokenizer_fn(first_td, self.special_tokens)\n        if not isinstance(first_tok, dict):\n            raise ValueError(\"custom text_tokenizer_fn –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å dict —Ç–µ–Ω–∑–æ—Ä–æ–≤\")\n        for k, t in first_tok.items():\n            if not torch.is_tensor(t): first_tok[k] = torch.tensor(t)\n        bank: Dict[str, torch.Tensor] = {k: torch.empty((self._N, *t.shape), dtype=t.dtype) for k, t in first_tok.items()}\n        for k, t in first_tok.items():\n            bank[k][0].copy_(t)\n\n        cache: Dict[tuple, Dict[str, torch.Tensor]] = {}\n        if deduplicate_texts:\n            key0 = tuple(first_td.get(c, \"\") for c in cols)\n            cache[key0] = {k: v.clone() for k, v in first_tok.items()}\n\n        for i in range(1, self._N):\n            td = {c: col_arrays[j][i] for j, c in enumerate(cols)}\n            if deduplicate_texts:\n                key = tuple(td.get(c, \"\") for c in cols)\n                tok = cache.get(key)\n                if tok is None:\n                    tok = self.text_tokenizer_fn(td, self.special_tokens)\n                    for k, t in tok.items():\n                        if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n                    cache[key] = {k: v.clone() for k, v in tok.items()}\n            else:\n                tok = self.text_tokenizer_fn(td, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n            for k, t in tok.items():\n                bank[k][i].copy_(t)\n\n        self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"‚úì –ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π single-—Ñ—É–Ω–∫—Ü–∏–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞: shapes={shapes}\")\n\n    def __len__(self):\n        return self._N\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item: Dict[str, Any] = {}\n        item[\"labels\"] = self._labels[idx]  # [K] float32 –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, int64 –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n        if self.text_columns:\n            if self._tok_bank is not None:\n                item[\"text_tokens\"] = {k: v[idx] for k, v in self._tok_bank.items()}\n            else:\n                item[\"text\"] = self._join_text(self.df.iloc[idx])\n        if self._image_lists is not None:\n            item[\"images\"] = self._image_lists[idx]\n        if self._audio_lists is not None:\n            item[\"audios\"] = self._audio_lists[idx]\n        return item\n\n    def get_cache_stats(self):\n        has = self._tok_bank is not None\n        sizes = {k: tuple(v.shape) for k, v in (self._tok_bank or {}).items()}\n        return {\"has_pretokenized\": has, \"shapes\": sizes, \"N\": self._N}\n\n    def clear_cache(self):\n        self._tok_bank = None\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n# data/collate.py\nfrom typing import Any, Dict, List, Optional\nimport numpy as np\nimport torch\n# from mmkit.core.utils import to_pil, load_audio\n\nclass MultiModalCollator:\n    \"\"\"\n    –ì–æ—Ç–æ–≤–∏—Ç backend_inputs –¥–ª—è –º–æ–¥–µ–ª–∏.\n    - –ï—Å–ª–∏ –≤ item –µ—Å—Ç—å \"text_tokens\": —Å—Ç–∞–∫–∞–µ—Ç –∏—Ö.\n    - –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç processors[\"text\"] –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.\n    - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è/–∞—É–¥–∏–æ: —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –≤ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ + —Å—á–∏—Ç–∞–µ—Ç counts.\n    \"\"\"\n    def __init__(self, processors: Dict[str, Any], task: str = \"regression\", audio_sr_fallback: int = 16000):\n        self.processors = processors\n        self.task = task.lower()\n        self.audio_sr_fallback = audio_sr_fallback\n\n    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        labels = [b.get(\"labels\") for b in batch]\n        if labels and isinstance(labels[0], torch.Tensor):\n            labels = torch.stack(labels)\n        else:\n            if self.task == \"classification\":\n                labels = torch.tensor(labels, dtype=torch.long)\n            else:\n                labels = torch.tensor(labels, dtype=torch.float32)\n\n        out = {\"labels\": labels, \"backend_inputs\": {}}\n        # text\n        if \"text\" in self.processors or any((\"text_tokens\" in b) for b in batch):\n            if \"text_tokens\" in batch[0]:\n                t0 = batch[0][\"text_tokens\"]\n                text_inputs = {}\n                for key in t0.keys():\n                    if torch.is_tensor(t0[key]):\n                        text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n                    else:\n                        dtype = torch.long if key in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else torch.float32\n                        text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n            else:\n                texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n                text_inputs = self.processors[\"text\"].prepare_batch(texts)\n            out[\"backend_inputs\"][\"text_inputs\"] = text_inputs\n\n        # images\n        if \"image\" in self.processors:\n            flat_images, counts = [], []\n            for b in batch:\n                lst = b.get(\"images\", []) or []\n                lst = [to_pil(x) for x in lst if x is not None]\n                counts.append(len(lst))\n                flat_images.extend(lst)\n            if len(flat_images) > 0:\n                out[\"backend_inputs\"][\"image_inputs\"] = self.processors[\"image\"].prepare_batch(flat_images)\n            else:\n                out[\"backend_inputs\"][\"image_inputs\"] = {\"pixel_values\": None}\n            out[\"backend_inputs\"][\"image_counts\"] = torch.tensor(counts, dtype=torch.long)\n\n        # audio\n        if \"audio\" in self.processors:\n            flat, counts = [], []\n            for b in batch:\n                lst = b.get(\"audios\", []) or []\n                counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        sr = getattr(self.processors[\"audio\"], \"sr\", self.audio_sr_fallback)\n                        flat.append(load_audio(a, sr))\n                    else:\n                        arr = np.asarray(a, dtype=np.float32)\n                        if arr.ndim>1: arr = np.squeeze(arr)\n                        if arr.ndim>1: arr = arr.reshape(-1)\n                        flat.append(arr)\n            out[\"backend_inputs\"][\"audio_counts\"] = torch.tensor(counts, dtype=torch.long)\n            out[\"backend_inputs\"][\"audio_inputs\"] = self.processors[\"audio\"].prepare_batch(flat) if len(flat)>0 else {\"input_values\": None, \"input_features\": None, \"raw_audios\": []}\n\n        out[\"backend_inputs\"][\"batch_size\"] = len(batch)\n        return out\n\n# processors/text.py\n# from mmkit.data.tokenization import BatchTokenizer\n\nclass TextProcessor:\n    modality = \"text\"\n    def __init__(self, tokenizer, max_length=512, padding=\"max_length\", cache_size=10000, batch_size=256):\n        self.bt = BatchTokenizer(tokenizer, max_length=max_length, cache_size=cache_size, batch_size=batch_size, padding_strategy=padding)\n    def prepare_batch(self, texts):\n        return self.bt.tokenize_batch(texts, use_cache=True)\n    def clear_cache(self):\n        self.bt.clear_cache()\n\n# processors/image.py\nclass ImageProcessor:\n    modality = \"image\"\n    def __init__(self, hf_processor):\n        self.proc = hf_processor\n    def prepare_batch(self, images):\n        if len(images)==0: return {\"pixel_values\": None}\n        x = self.proc(images=images, return_tensors=\"pt\")\n        return {\"pixel_values\": x[\"pixel_values\"]}\n\n# processors/audio.py\nclass ClapAudioProcessor:\n    modality=\"audio\"\n    def __init__(self, hf_processor, sr=48000):\n        self.proc = hf_processor\n        self.sr = sr\n    def prepare_batch(self, raw_list):\n        x = self.proc(audios=raw_list, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n        return {\"input_features\": x[\"input_features\"]}\n\nclass Wav2VecAudioProcessor:\n    modality=\"audio\"\n    def __init__(self, hf_processor, sr=16000):\n        self.proc = hf_processor\n        self.sr = sr\n    def prepare_batch(self, raw_list):\n        x = self.proc(raw_list, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n        return {\"input_values\": x[\"input_values\"]}\n\nclass Wav2ClipAudioProcessor:\n    modality=\"audio\"\n    def __init__(self, sr=16000):\n        self.sr = sr\n    def prepare_batch(self, raw_list):\n        return {\"raw_audios\": raw_list}  # —Å–ø–∏—Å–æ–∫ np.ndarray; —ç–Ω–∫–æ–¥–µ—Ä —Å–∞–º –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç\n\n# encoders/base.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BaseEncoder(nn.Module):\n    modality: str = \"text\"\n    embed_dim: int = 0\n\n    def _normalize(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() > 2:\n            x = x.view(x.size(0), -1)\n        return F.normalize(x, dim=-1, eps=1e-12)\n\n# encoders/text_auto.py\nimport torch\nfrom transformers import AutoModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass AutoTextEncoder(BaseEncoder):\n    modality=\"text\"\n    def __init__(self, checkpoint=\"bert-base-multilingual-cased\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(AutoModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.hidden_size\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        out = self.model(**inputs)\n        z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state.mean(dim=1)\n        return self._normalize(z)\n\n# encoders/text_clip.py\nimport torch\nfrom transformers import CLIPModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass CLIPTextEncoder(BaseEncoder):\n    modality=\"text\"\n    def __init__(self, checkpoint=\"openai/clip-vit-base-patch32\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(CLIPModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.projection_dim\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        z = self.model.get_text_features(**inputs)\n        return self._normalize(z)\n\n# encoders/image_auto.py\nimport torch\nfrom transformers import AutoModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass AutoImageEncoder(BaseEncoder):\n    modality=\"image\"\n    def __init__(self, checkpoint=\"google/vit-base-patch16-224\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(AutoModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.hidden_size\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        pv = inputs.get(\"pixel_values\", None)\n        if pv is None: \n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        out = self.model(pixel_values=pv)\n        z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state[:, 0]\n        return self._normalize(z)\n\n# encoders/image_clip.py\nimport torch\nfrom transformers import CLIPModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass CLIPImageEncoder(BaseEncoder):\n    modality=\"image\"\n    def __init__(self, checkpoint=\"openai/clip-vit-base-patch32\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(CLIPModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.projection_dim\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        pv = inputs.get(\"pixel_values\", None)\n        if pv is None:\n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        z = self.model.get_image_features(pixel_values=pv)\n        return self._normalize(z)\n\n# encoders/audio_clap.py\nimport torch\nfrom transformers import ClapModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass CLAPAudioEncoder(BaseEncoder):\n    modality=\"audio\"\n    def __init__(self, checkpoint=\"laion/clap-htsat-unfused\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(ClapModel, checkpoint, cache_dir)\n        self.embed_dim = getattr(self.model.config, \"projection_dim\", 512)\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        feats = inputs.get(\"input_features\")\n        if feats is None:\n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        z = self.model.get_audio_features(input_features=feats.float())\n        return self._normalize(z.float())\n\n# encoders/audio_wav2vec.py\nimport torch\nfrom transformers import AutoModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass Wav2Vec2Encoder(BaseEncoder):\n    modality=\"audio\"\n    def __init__(self, checkpoint=\"facebook/wav2vec2-base-960h\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(AutoModel, checkpoint, cache_dir)\n        self.embed_dim = getattr(self.model.config, \"hidden_size\", 768)\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        iv = inputs.get(\"input_values\")\n        if iv is None:\n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        out = self.model(input_values=iv.float())\n        z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state.mean(dim=1)\n        return self._normalize(z.float())\n\n# encoders/audio_wav2clip.py\nimport numpy as np\nimport torch\n# from .base import BaseEncoder\n\nclass Wav2ClipEncoder(BaseEncoder):\n    modality=\"audio\"\n    def __init__(self):\n        super().__init__()\n        try:\n            import wav2clip as w2c\n        except Exception as e:\n            raise RuntimeError(\"–¢—Ä–µ–±—É–µ—Ç—Å—è wav2clip: pip install wav2clip\") from e\n        self.w2c = w2c\n        model = None\n        if hasattr(w2c, \"get_model\"):\n            model = w2c.get_model()\n        elif hasattr(w2c, \"model\"):\n            m = w2c.model\n            model = m() if callable(m) else m\n        else:\n            raise RuntimeError(\"wav2clip –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç get_model()/model\")\n        self.model = model\n        self.embed_dim = 512\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        raws = inputs.get(\"raw_audios\", [])\n        if len(raws)==0:\n            return torch.zeros(0, self.embed_dim)\n        embs = []\n        for arr in raws:\n            a = np.asarray(arr, dtype=np.float32)\n            if a.ndim>1: a = np.squeeze(a)\n            if a.ndim>1: a = a.reshape(-1)\n            if a.size < 512:\n                a = np.pad(a, (0, 512 - a.size), mode=\"constant\")\n            try:\n                e = self.w2c.embed_audio(a, self.model)\n                e = np.asarray(e)\n            except Exception:\n                x = torch.from_numpy(a).float().unsqueeze(0)\n                y = self.model(x)\n                if isinstance(y, (tuple, list)):\n                    y = y[0]\n                if torch.is_tensor(y):\n                    if y.dim()==2 and y.size(0)==1: y=y.squeeze(0)\n                    e = y.detach().cpu().numpy()\n                else:\n                    e = np.asarray(y)\n            if e.ndim>1: e = e.reshape(-1)\n            embs.append(torch.as_tensor(e, dtype=torch.float32))\n        z = torch.stack(embs, dim=0)\n        z = torch.nn.functional.normalize(z, dim=-1, eps=1e-12)\n        return z\n\n# aggregation/item_pool.py\nimport torch\nimport torch.nn.functional as F\n\nclass ItemAggregator:\n    def aggregate(self, embs: torch.Tensor, counts, max_k: int, how: str):\n        if embs is None or (torch.is_tensor(embs) and embs.numel()==0):\n            D = 0 if embs is None else getattr(embs, \"size\", lambda *_: 0)(-1)\n            out_dim = D*max_k if how==\"concat\" else D\n            return torch.zeros((len(counts), out_dim), device=embs.device if torch.is_tensor(embs) else \"cpu\", dtype=torch.float32)\n        if embs.dim()==1: embs = embs.unsqueeze(0)\n        if embs.dim()>2: embs = embs.view(embs.size(0), -1)\n        N, D = embs.size()\n        out_dim = D*max_k if how==\"concat\" else D\n        out = torch.zeros((len(counts), out_dim), device=embs.device, dtype=embs.dtype)\n        off = 0\n        for i, c in enumerate(counts):\n            if c<=0 or off>=N: continue\n            take = min(c, N-off)\n            sample = embs[off:off+take]\n            off += take\n            if how==\"concat\":\n                sample = sample[:max_k]\n                if sample.size(0)<max_k:\n                    pad = torch.zeros((max_k - sample.size(0), D), device=embs.device, dtype=embs.dtype)\n                    sample = torch.cat([sample, pad], dim=0)\n                out[i] = sample.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n        return F.normalize(out, dim=-1, eps=1e-12)\n\n# aggregation/fusion.py\nimport torch\n\nclass ConcatFusion:\n    def __call__(self, zs: dict) -> torch.Tensor:\n        feats = [zs[k] for k in [\"image\",\"text\",\"audio\"] if k in zs]\n        return torch.cat(feats, dim=-1)\n\nclass MeanFusion:\n    def __call__(self, zs: dict) -> torch.Tensor:\n        feats = [zs[k] for k in [\"image\",\"text\",\"audio\"] if k in zs]\n        return torch.stack(feats, dim=0).mean(dim=0)\n\n# heads/regression.py\nimport torch.nn as nn\n\nclass RegressionHead(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden=256, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, out_dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n# heads/classification.py\nimport torch.nn as nn\n\nclass ClassificationHead(nn.Module):\n    def __init__(self, in_dim, num_labels, hidden=512, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n    def forward(self, x): return self.net(x)\n\n# model/multimodal.py\nfrom typing import Dict\nimport torch\nimport torch.nn as nn\n# from mmkit.aggregation.item_pool import ItemAggregator\n\nclass MultiModalModel(nn.Module):\n    \"\"\"\n    –ú–æ–¥–µ–ª—å: dict(encoders) + aggregator (–¥–ª—è image/audio –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö) + fusion + head.\n    \"\"\"\n    def __init__(self, encoders: Dict[str, nn.Module], aggregator: ItemAggregator, fusion, head,\n                 image_cfg=None, audio_cfg=None):\n        super().__init__()\n        self.enc = nn.ModuleDict(encoders)\n        self.agg = aggregator\n        self.fusion = fusion\n        self.head = head\n        self.image_cfg = image_cfg or {\"max_items\":1, \"how\":\"concat\"}\n        self.audio_cfg = audio_cfg or {\"max_items\":1, \"how\":\"concat\"}\n\n    def _fwd_features(self, backend_inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        zs = {}\n        if \"text\" in self.enc and \"text_inputs\" in backend_inputs:\n            zs[\"text\"] = self.enc[\"text\"].encode_batch(backend_inputs[\"text_inputs\"])\n        if \"image\" in self.enc and \"image_inputs\" in backend_inputs:\n            flat = self.enc[\"image\"].encode_batch(backend_inputs[\"image_inputs\"])\n            counts = backend_inputs.get(\"image_counts\", torch.tensor([0]*backend_inputs.get(\"batch_size\", flat.size(0)))).tolist()\n            zs[\"image\"] = self.agg.aggregate(flat, counts, self.image_cfg[\"max_items\"], self.image_cfg[\"how\"])\n        if \"audio\" in self.enc and \"audio_inputs\" in backend_inputs:\n            flat = self.enc[\"audio\"].encode_batch(backend_inputs[\"audio_inputs\"])\n            counts = backend_inputs.get(\"audio_counts\", torch.tensor([0]*backend_inputs.get(\"batch_size\", flat.size(0)))).tolist()\n            zs[\"audio\"] = self.agg.aggregate(flat, counts, self.audio_cfg[\"max_items\"], self.audio_cfg[\"how\"])\n        return zs\n\n    def forward(self, backend_inputs: Dict[str, torch.Tensor], labels: torch.Tensor = None):\n        zs = self._fwd_features(backend_inputs)\n        if not zs:\n            raise ValueError(\"–≠–Ω–∫–æ–¥–µ—Ä—ã –Ω–µ –≤–µ—Ä–Ω—É–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\")\n        fused = self.fusion(zs)\n        logits = self.head(fused)\n        return {\"logits\": logits}\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, torch.Tensor], return_per_modality: bool = False):\n        zs = self._fwd_features(backend_inputs)\n        fused = self.fusion(zs)\n        return (fused, zs) if return_per_modality else fused\n\n# train/trainer_hf.py\nfrom typing import Optional\nimport torch\nimport torch.nn.functional as F\nfrom transformers import Trainer\n\nclass MSETrainer(Trainer):\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n        **kwargs\n    ):\n        labels = inputs.pop(\"labels\").to(torch.float32)\n        backend_inputs = inputs.pop(\"backend_inputs\", None)\n        if backend_inputs is None:\n            # fallback: –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Å—á–∏—Ç–∞–µ–º backend_inputs\n            backend_inputs = inputs\n\n        out = model(backend_inputs=backend_inputs)  # –ø–µ—Ä–µ–¥–∞–µ–º —Å—Ç—Ä–æ–≥–æ backend_inputs\n        logits = out[\"logits\"]\n        preds = logits if logits.dim() == 1 else (logits.squeeze(-1) if logits.size(-1) == 1 else logits)\n        labels = labels.view_as(preds)\n        loss = F.mse_loss(preds, labels)\n        return (loss, out) if return_outputs else loss\n\nclass WeightedCETrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = (\n            torch.as_tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n        )\n\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n        **kwargs\n    ):\n        labels = inputs.pop(\"labels\")\n        backend_inputs = inputs.pop(\"backend_inputs\", None)\n        if backend_inputs is None:\n            backend_inputs = inputs\n\n        out = model(backend_inputs=backend_inputs)  # —Ç–æ–ª—å–∫–æ backend_inputs\n        logits = out[\"logits\"]\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = F.cross_entropy(logits, labels.long(), weight=weight)\n        return (loss, out) if return_outputs else loss\n\n# train/callbacks.py\nfrom transformers.trainer_callback import TrainerCallback\nfrom tqdm.auto import tqdm\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n        self.tqdm = tqdm\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            self.tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n# train/metrics.py\nimport numpy as np\n\ndef build_regression_metrics(name: str):\n    name = name.lower()\n    if name not in (\"rmse\", \"mae\", \"r2\"):\n        raise ValueError('metric_name –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å \"rmse\", \"mae\" –∏–ª–∏ \"r2\"')\n\n    def compute(p):\n        preds = p.predictions\n        y = p.label_ids\n        preds = preds.squeeze(-1) if preds.ndim == 2 and preds.shape[-1] == 1 else preds\n        y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n        axis = 1 if preds.ndim == 2 else None\n        if name == \"rmse\":\n            err = preds - y\n            mse = np.mean(err**2, axis=axis)\n            rmse = np.sqrt(mse)\n            return {\"rmse\": float(np.mean(rmse))}\n        elif name == \"mae\":\n            mae = np.mean(np.abs(preds - y), axis=axis)\n            return {\"mae\": float(np.mean(mae))}\n        else:\n            y_mean = np.mean(y, axis=axis, keepdims=True) if preds.ndim == 2 else np.mean(y)\n            ss_res = np.sum((y - preds) ** 2, axis=axis)\n            ss_tot = np.sum((y - y_mean) ** 2, axis=axis)\n            r2 = 1.0 - (ss_res / (ss_tot + 1e-12))\n            return {\"r2\": float(np.mean(r2))}\n    return compute\n\n# train/recipes.py\nfrom transformers import CLIPTokenizer, CLIPImageProcessor, AutoTokenizer, AutoImageProcessor, ClapProcessor, AutoProcessor\n# from mmkit.processors.text import TextProcessor\n# from mmkit.processors.image import ImageProcessor\n# from mmkit.processors.audio import ClapAudioProcessor, Wav2VecAudioProcessor, Wav2ClipAudioProcessor\n\n# from mmkit.encoders.text_clip import CLIPTextEncoder\n# from mmkit.encoders.text_auto import AutoTextEncoder\n# from mmkit.encoders.image_clip import CLIPImageEncoder\n# from mmkit.encoders.image_auto import AutoImageEncoder\n# from mmkit.encoders.audio_clap import CLAPAudioEncoder\n# from mmkit.encoders.audio_wav2vec import Wav2Vec2Encoder\n# from mmkit.encoders.audio_wav2clip import Wav2ClipEncoder\n\n# from mmkit.aggregation.item_pool import ItemAggregator\n# from mmkit.aggregation.fusion import ConcatFusion, MeanFusion\n# from mmkit.heads.regression import RegressionHead\n# from mmkit.model.multimodal import MultiModalModel\n\ndef build_auto_regression(modalities,\n                          fusion=\"concat\",\n                          # text config\n                          text_model_type=\"clip\", text_checkpoint=\"openai/clip-vit-base-patch32\", text_max_length=77, text_padding=\"max_length\",\n                          # image config\n                          image_model_type=\"clip\", image_checkpoint=\"openai/clip-vit-base-patch32\", max_images=1, image_agg=\"concat\",\n                          # audio config\n                          audio_model_type=\"clap\", audio_checkpoint=\"laion/clap-htsat-unfused\", audio_sr=48000, max_audios=1, audio_agg=\"concat\",\n                          # head config\n                          out_dim=1, hidden=256, dropout=0.1,\n                          cache_dir=\"./model_cache\"):\n    encoders = {}\n    processors = {}\n    # text\n    if \"text\" in modalities:\n        if text_model_type == \"clip\":\n            tok = CLIPTokenizer.from_pretrained(text_checkpoint, cache_dir=cache_dir)\n            processors[\"text\"] = TextProcessor(tok, max_length=text_max_length, padding=text_padding)\n            encoders[\"text\"] = CLIPTextEncoder(text_checkpoint, cache_dir=cache_dir)\n            text_dim = encoders[\"text\"].embed_dim\n        else:\n            tok = AutoTokenizer.from_pretrained(text_checkpoint, cache_dir=cache_dir)\n            processors[\"text\"] = TextProcessor(tok, max_length=text_max_length, padding=text_padding)\n            encoders[\"text\"] = AutoTextEncoder(text_checkpoint, cache_dir=cache_dir)\n            text_dim = encoders[\"text\"].embed_dim\n    # image\n    image_cfg = None\n    if \"image\" in modalities:\n        if image_model_type == \"clip\":\n            ip = CLIPImageProcessor.from_pretrained(image_checkpoint, cache_dir=cache_dir)\n            processors[\"image\"] = ImageProcessor(ip)\n            encoders[\"image\"] = CLIPImageEncoder(image_checkpoint, cache_dir=cache_dir)\n        else:\n            ip = AutoImageProcessor.from_pretrained(image_checkpoint, cache_dir=cache_dir)\n            processors[\"image\"] = ImageProcessor(ip)\n            encoders[\"image\"] = AutoImageEncoder(image_checkpoint, cache_dir=cache_dir)\n        image_dim = encoders[\"image\"].embed_dim\n        image_cfg = {\"max_items\": max_images, \"how\": image_agg}\n    # audio\n    audio_cfg = None\n    if \"audio\" in modalities:\n        if audio_model_type == \"clap\":\n            ap = ClapProcessor.from_pretrained(audio_checkpoint, cache_dir=cache_dir)\n            processors[\"audio\"] = ClapAudioProcessor(ap, sr=audio_sr)\n            encoders[\"audio\"] = CLAPAudioEncoder(audio_checkpoint, cache_dir=cache_dir)\n        elif audio_model_type == \"wav2vec\":\n            ap = AutoProcessor.from_pretrained(audio_checkpoint, cache_dir=cache_dir)\n            processors[\"audio\"] = Wav2VecAudioProcessor(ap, sr=audio_sr)\n            encoders[\"audio\"] = Wav2Vec2Encoder(audio_checkpoint, cache_dir=cache_dir)\n        else:  # wav2clip\n            processors[\"audio\"] = Wav2ClipAudioProcessor(sr=audio_sr)\n            encoders[\"audio\"] = Wav2ClipEncoder()\n        audio_dim = encoders[\"audio\"].embed_dim\n        audio_cfg = {\"max_items\": max_audios, \"how\": audio_agg}\n\n    agg = ItemAggregator()\n    fusion_mod = ConcatFusion() if fusion==\"concat\" else MeanFusion()\n    # compute in_dim\n    dims = []\n    for m in [\"image\",\"text\",\"audio\"]:\n        if m in encoders:\n            dims.append(encoders[m].embed_dim)\n    if fusion==\"concat\":\n        in_dim = sum(dims)\n    else:\n        assert len(set(dims))==1, \"–î–ª—è fusion=mean —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\"\n        in_dim = dims[0]\n    head = RegressionHead(in_dim, out_dim=out_dim, hidden=hidden, dropout=dropout)\n    model = MultiModalModel(encoders, agg, fusion_mod, head, image_cfg=image_cfg, audio_cfg=audio_cfg)\n    return processors, model\n\n# pipelines/regression.py\nimport math\nimport gc\nfrom typing import Any, Dict, List, Optional\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nfrom transformers import TrainingArguments\n# from mmkit.core.utils import set_seed\n# from mmkit.data.dataset import MultiModalDataset\n# from mmkit.data.collate import MultiModalCollator\n# from mmkit.train.trainer_hf import MSETrainer\n# from mmkit.train.callbacks import PbarConsoleLogger\n# from mmkit.train.metrics import build_regression_metrics\n# from mmkit.train.recipes import build_auto_regression\n\nclass MultiModalRegressionPipeline:\n    \"\"\"\n    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (multi-target).\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: text/image/audio, dynamic/max padding, –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, —á–∞–Ω–∫–∏, RMSE/MAE/R2.\n    \"\"\"\n    def __init__(self,\n                 modalities: List[str],\n                 target_column_names: List[str],\n                 text_columns: Optional[List[str]] = None,\n                 image_columns: Optional[List[str]] = None,\n                 audio_columns: Optional[List[str]] = None,\n                 # recipe params\n                 backend: str = \"auto\",\n                 clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n                 clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n                 text_model_config: Optional[Dict[str, Any]] = None,\n                 image_model_config: Optional[Dict[str, Any]] = None,\n                 audio_model_config: Optional[Dict[str, Any]] = None,\n                 fusion: str = \"concat\",\n                 # tokenizer\n                 text_padding: str = \"max_length\",\n                 text_max_length: int = 256,\n                 use_batch_tokenizer: bool = True,\n                 pretokenize_data: bool = True,\n                 pretokenize_batch_size: int = 256,\n                 tokenizer_cache_size: int = 10000,\n                 max_pretokenize_samples: int = 100000,\n                 local_cache_dir: str = \"./model_cache\"):\n        self.modalities = sorted(list(set(modalities)))\n        self.target_column_names = list(target_column_names)\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n\n        self.text_padding = text_padding\n        self.text_max_length = text_max_length\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n\n        self._target_vec_col = \"__target_vector__\"\n        self.processors = {}\n        self.model = None\n        self.trainer = None\n\n    def _attach_target_vector(self, df: pd.DataFrame, fill_zeros: bool = False) -> pd.DataFrame:\n        df_c = df.copy()\n        K = len(self.target_column_names)\n        if fill_zeros:\n            df_c[self._target_vec_col] = [np.zeros(K, dtype=np.float32) for _ in range(len(df_c))]\n        else:\n            def _row_to_vec(row):\n                vals = [row[c] for c in self.target_column_names]\n                return np.asarray(vals, dtype=np.float32)\n            df_c[self._target_vec_col] = df_c.apply(_row_to_vec, axis=1)\n        return df_c\n\n    def _validate_modalities(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            assert self.text_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ text, –Ω–æ text_columns –ø—É—Å—Ç–æ–π\"\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing: raise ValueError(f\"–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {missing}\")\n        if \"image\" in self.modalities:\n            assert self.image_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ image, –Ω–æ image_columns –ø—É—Å—Ç\"\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing: raise ValueError(f\"–ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {missing}\")\n        if \"audio\" in self.modalities:\n            assert self.audio_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ audio, –Ω–æ audio_columns –ø—É—Å—Ç\"\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing: raise ValueError(f\"–ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –∞—É–¥–∏–æ: {missing}\")\n\n    def _validate_targets(self, df: pd.DataFrame):\n        miss = [c for c in self.target_column_names if c not in df.columns]\n        if miss:\n            raise ValueError(f\"–í DataFrame –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {miss}\")\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _build_model(self, num_outputs: int, image_cfg: Dict[str, Any], audio_cfg: Dict[str, Any]):\n        # –ê–≤—Ç–æ-—Ä–µ—Ü–µ–ø—Ç: CLIP –¥–ª—è text+image, CLAP –¥–ª—è audio (–∏–ª–∏ —è–≤–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏)\n        if self.backend_name == \"auto\":\n            # auto defaults\n            txt_type = \"clip\" if \"image\" in self.modalities else (\"auto\" if \"text\" in self.modalities else None)\n            img_type = \"clip\" if \"image\" in self.modalities else None\n            aud_type = \"clap\" if \"audio\" in self.modalities else None\n\n            txt_cp = self.clip_checkpoint if txt_type==\"clip\" else (self.text_model_config[\"checkpoint\"] if self.text_model_config else \"bert-base-multilingual-cased\")\n            img_cp = self.clip_checkpoint if img_type==\"clip\" else (self.image_model_config[\"checkpoint\"] if self.image_model_config else \"google/vit-base-patch16-224\")\n            aud_cp = self.clap_checkpoint if aud_type==\"clap\" else (self.audio_model_config[\"checkpoint\"] if self.audio_model_config else \"facebook/wav2vec2-base-960h\")\n\n            if aud_type is None and \"audio\" in self.modalities and self.audio_model_config:\n                aud_type = self.audio_model_config.get(\"model_type\",\"clap\")\n\n            processors, model = build_auto_regression(\n                modalities=self.modalities,\n                fusion=self.fusion,\n                text_model_type=\"clip\" if txt_type==\"clip\" else \"auto\",\n                text_checkpoint=txt_cp,\n                text_max_length=self.text_max_length,\n                text_padding=self.text_padding,\n                image_model_type=\"clip\" if img_type==\"clip\" else \"auto\",\n                image_checkpoint=img_cp,\n                max_images=image_cfg.get(\"max_images\", 1),\n                image_agg=image_cfg.get(\"image_agg\", \"concat\"),\n                audio_model_type=aud_type if aud_type else \"clap\",\n                audio_checkpoint=aud_cp,\n                audio_sr=audio_cfg.get(\"sr\", 48000),\n                max_audios=audio_cfg.get(\"max_audios\", 1),\n                audio_agg=audio_cfg.get(\"audio_agg\", \"concat\"),\n                out_dim=num_outputs,\n                hidden=256,\n                dropout=0.1,\n                cache_dir=self.local_cache_dir\n            )\n            return processors, model\n\n        # –ò–Ω–∞—á–µ ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Ñ–∞–±—Ä–∏–∫—É –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤ (–æ–ø—É—â–µ–Ω–æ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏)\n        raise NotImplementedError(\"–†—É—á–Ω–∞—è —Å–±–æ—Ä–∫–∞ (–Ω–µ auto) –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤ —ç—Ç–æ–π –≤–µ—Ä—Å–∏–∏\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"rmse\",       # \"rmse\" | \"mae\" | \"r2\"\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result_reg\",\n        seed: int = 42,\n        gradient_checkpointing: bool = False\n    ):\n        import os\n        from transformers import TrainingArguments\n    \n        # 1) –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞\n        self._validate_modalities(train_data)\n        self._validate_targets(train_data)\n        set_seed(seed)\n    \n        df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n        if test_data is not None:\n            self._validate_targets(test_data)\n    \n        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –≤ –æ–¥–Ω—É –≤–µ–∫—Ç–æ—Ä-–∫–æ–ª–æ–Ω–∫—É\n        df_train_ext = self._attach_target_vector(df_train, fill_zeros=False)\n        df_eval_ext  = self._attach_target_vector(df_eval,  fill_zeros=False)\n    \n        # 2) –ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã (auto-—Ä–µ—Ü–µ–ø—Ç)\n        image_cfg = {\"max_images\": 1, \"image_agg\": \"concat\"}\n        audio_cfg = {\"sr\": 48000, \"max_audios\": 1, \"audio_agg\": \"concat\"}\n    \n        self.processors, self.model = self._build_model(\n            num_outputs=len(self.target_column_names),\n            image_cfg=image_cfg,\n            audio_cfg=audio_cfg\n        )\n    \n        if gradient_checkpointing:\n            try:\n                self.model.gradient_checkpointing_enable()\n            except Exception:\n                pass\n    \n        # 3) Datasets (–±–µ–∑ —á–∞–Ω–∫–æ–≤)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        pretokenize_train = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_train_ext) <= self.max_pretokenize_samples)\n        pretokenize_eval  = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_eval_ext)  <= self.max_pretokenize_samples)\n    \n        ds_train = MultiModalDataset(\n            df=df_train_ext,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_train) else None,\n            pretokenize=pretokenize_train,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        ds_eval = MultiModalDataset(\n            df=df_eval_ext,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_eval) else None,\n            pretokenize=pretokenize_eval,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n    \n        collate = MultiModalCollator(self.processors, task=\"regression\")\n        compute_metrics = build_regression_metrics(metric_name)\n    \n        # 4) –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è (eval_strategy)\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.0,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=False\n        )\n    \n        # 5) Trainer –∏ —Å—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n        self.trainer = MSETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train,\n            eval_dataset=ds_eval,\n            data_collator=collate,\n            compute_metrics=compute_metrics\n        )\n    \n        try:\n            from transformers.trainer_callback import PrinterCallback\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n    \n        self.trainer.train()\n    \n        if has_bt:\n            try: self.processors[\"text\"].clear_cache()\n            except Exception: pass\n    \n        return self\n\n    def predict(self, df: pd.DataFrame, batch_size: Optional[int] = None) -> np.ndarray:\n        if self.trainer is None:\n            raise RuntimeError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ .fit().\")\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        collate = MultiModalCollator(self.processors, task=\"regression\")\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch - 1) // effective_batch\n        print(f\"Running predictions (batch_size={effective_batch}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds, metric_key_prefix=\"predict\")\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        ds.clear_cache()\n        y = preds.predictions\n        y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n        return y\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ .fit().\")\n        device = next(self.trainer.model.parameters()).device\n        self.model.to(device).eval()\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=False\n        )\n        collate = MultiModalCollator(self.processors, task=\"regression\")\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = batch[\"backend_inputs\"]\n                def move_to_device(obj):\n                    if torch.is_tensor(obj): return obj.to(device)\n                    if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n                    if isinstance(obj, list): return [move_to_device(v) for v in obj]\n                    return obj\n                bi = move_to_device(bi)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            print(f\"‚úì Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"‚úì Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"‚úì {m.capitalize()} embeddings shape: {arr.shape}\")\n        return fused_arr, per_mod\n\n# pipelines/classification.py\nimport os\nimport math\nimport gc\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import TrainingArguments\n\n# from mmkit.core.utils import set_seed\n# from mmkit.data.dataset import MultiModalDataset\n# from mmkit.data.collate import MultiModalCollator\n\n# from mmkit.train.trainer_hf import WeightedCETrainer\n# from mmkit.train.callbacks import PbarConsoleLogger\n\n# from mmkit.aggregation.item_pool import ItemAggregator\n# from mmkit.aggregation.fusion import ConcatFusion, MeanFusion\n# from mmkit.heads.classification import ClassificationHead\n# from mmkit.model.multimodal import MultiModalModel\n\n# –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã\nfrom transformers import (\n    CLIPTokenizer,\n    CLIPImageProcessor,\n    AutoTokenizer,\n    AutoImageProcessor,\n    ClapProcessor,\n    AutoProcessor,\n)\n\n# from mmkit.processors.text import TextProcessor\n# from mmkit.processors.image import ImageProcessor\n# from mmkit.processors.audio import (\n#     ClapAudioProcessor,\n#     Wav2VecAudioProcessor,\n#     Wav2ClipAudioProcessor,\n# )\n\n# # –≠–Ω–∫–æ–¥–µ—Ä—ã\n# from mmkit.encoders.text_clip import CLIPTextEncoder\n# from mmkit.encoders.text_auto import AutoTextEncoder\n# from mmkit.encoders.image_clip import CLIPImageEncoder\n# from mmkit.encoders.image_auto import AutoImageEncoder\n# from mmkit.encoders.audio_clap import CLAPAudioEncoder\n# from mmkit.encoders.audio_wav2vec import Wav2Vec2Encoder\n# from mmkit.encoders.audio_wav2clip import Wav2ClipEncoder\n\n\nclass MultiModalClassificationPipeline:\n    \"\"\"\n    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (single/multi-class).\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç text/image/audio, dynamic/max padding, –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, —á–∞–Ω–∫–æ–≤—É—é —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É,\n    accuracy/f1 –º–µ—Ç—Ä–∏–∫–∏, class weights, predict –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n\n    –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é (backend=\"auto\"):\n      - –µ—Å–ª–∏ –µ—Å—Ç—å image: CLIP –¥–ª—è text –∏ image;\n      - –µ—Å–ª–∏ –µ—Å—Ç—å audio: CLAP (–∏–ª–∏ wav2vec/wav2clip –ø–æ –∂–µ–ª–∞–Ω–∏—é ‚Äî —Å–º. –ø–∞—Ä–∞–º–µ—Ç—Ä—ã),\n      - –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: Auto (BERT) –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.\n\n    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n      - modalities: —Å–ø–∏—Å–æ–∫ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π —Å—Ä–µ–¥–∏ [\"text\",\"image\",\"audio\"]\n      - target_column_name: –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –º–µ—Ç–∫–∞–º–∏ (—Å—Ç—Ä–æ–∫–∏/–∏–Ω—Ç—ã)\n      - num_labels: —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤ (–µ—Å–ª–∏ None ‚Äî –≤–æ–∑—å–º—ë—Ç—Å—è –∏–∑ train_data –ø–æ unique)\n      - text_columns, image_columns, audio_columns: –∫–æ–ª–æ–Ω–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n      - fusion: \"concat\" | \"mean\" (–¥–ª—è mean —Ä–∞–∑–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å)\n      - text_padding: \"max_length\" | \"dynamic\"\n      - text_max_length: –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤\n      - pretokenize_data: –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (–≤—ã–∫–ª—é—á–∏—Ç—Å—è –ø—Ä–∏ dynamic padding)\n      - tokenizer_cache_size: LRU-–∫—ç—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n      - max_pretokenize_samples: –ª–∏–º–∏—Ç –Ω–∞ –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –≤ —á–∞–Ω–∫–µ\n      - local_cache_dir: –∫—ç—à HF\n\n      - audio_backend: \"clap\" | \"wav2vec\" | \"wav2clip\" (–¥–ª—è audio)\n      - audio_sr: sampling rate –¥–ª—è –∞—É–¥–∏–æ-–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤\n      - max_images_per_sample, image_agg: –∞–≥—Ä–µ–≥–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫ (\"concat\"/\"mean\")\n      - max_audios_per_sample, audio_agg: –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–ª—è –∞—É–¥–∏–æ\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        target_column_name: str,\n        num_labels: Optional[int] = None,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n\n        # backend/–º–æ–¥–µ–ª–∏\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n\n        fusion: str = \"concat\",\n\n        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n        text_padding: str = \"max_length\",\n        text_max_length: int = 77,\n        use_batch_tokenizer: bool = True,\n        pretokenize_data: bool = True,\n        pretokenize_batch_size: int = 256,\n        tokenizer_cache_size: int = 10000,\n        max_pretokenize_samples: int = 100000,\n        local_cache_dir: str = \"./model_cache\",\n\n        # –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ/–∞—É–¥–∏–æ –∞–≥—Ä–µ–≥–∞—Ü–∏—è\n        max_images_per_sample: int = 1,\n        image_agg: str = \"concat\",\n        audio_backend: str = \"clap\",  # \"clap\" | \"wav2vec\" | \"wav2clip\"\n        audio_sr: int = 48000,\n        max_audios_per_sample: int = 1,\n        audio_agg: str = \"concat\",\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.target_column_name = target_column_name\n        self.num_labels = num_labels\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n\n        self.fusion = fusion\n        self.text_padding = text_padding\n        self.text_max_length = text_max_length\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.image_agg = image_agg\n        self.audio_backend = audio_backend\n        self.audio_sr = int(audio_sr)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n        self.audio_agg = audio_agg\n\n        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø–æ–ª—è\n        self.label2id: Optional[Dict[Any, int]] = None\n        self.id2label: Optional[Dict[int, str]] = None\n        self.processors: Dict[str, Any] = {}\n        self.model: Optional[MultiModalModel] = None\n        self.trainer: Optional[WeightedCETrainer] = None\n\n    # --------------------------\n    # –í–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –ø–æ–¥—Å–æ–±–∫–∏\n    # --------------------------\n    def _validate_modalities(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            assert self.text_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ text, –Ω–æ text_columns –ø—É—Å—Ç–æ–π\"\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {missing}\")\n        if \"image\" in self.modalities:\n            assert self.image_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ image, –Ω–æ image_columns –ø—É—Å—Ç\"\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"–ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {missing}\")\n        if \"audio\" in self.modalities:\n            assert self.audio_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ audio, –Ω–æ audio_columns –ø—É—Å—Ç\"\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"–ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –∞—É–¥–∏–æ: {missing}\")\n\n    def _ensure_label_mapping(self, df_train: pd.DataFrame):\n        classes = sorted(df_train[self.target_column_name].unique().tolist())\n        if self.num_labels is None:\n            self.num_labels = len(classes)\n        elif self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _ensure_label_column(self, df: pd.DataFrame) -> pd.DataFrame:\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            # –ï—Å–ª–∏ –Ω–µ—Ç –º–µ—Ç–∫–∏ ‚Äî –ø–æ–¥—Å—Ç–∞–≤–∏–º –ø–µ—Ä–≤—É—é –∏–∑–≤–µ—Å—Ç–Ω—É—é\n            fake_label = next(iter(self.label2id.keys()))\n            df_c[self.target_column_name] = [fake_label] * len(df_c)\n        return df_c\n\n    # --------------------------\n    # –ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã (auto)\n    # --------------------------\n    def _build_model_and_processors(self) -> Tuple[Dict[str, Any], MultiModalModel]:\n        encoders = {}\n        processors = {}\n\n        # text\n        if \"text\" in self.modalities:\n            # –ï—Å–ª–∏ –µ—Å—Ç—å image ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º CLIP-—Ç–µ–∫—Å—Ç, –∏–Ω–∞—á–µ Auto (BERT)\n            use_clip_text = (\"image\" in self.modalities)\n            if use_clip_text:\n                tok = CLIPTokenizer.from_pretrained(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n                processors[\"text\"] = TextProcessor(\n                    tok, max_length=self.text_max_length, padding=self.text_padding,\n                    cache_size=self.tokenizer_cache_size, batch_size=self.pretokenize_batch_size\n                )\n                encoders[\"text\"] = CLIPTextEncoder(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n            else:\n                tok = AutoTokenizer.from_pretrained(\n                    (self.text_model_config or {}).get(\"checkpoint\", \"bert-base-multilingual-cased\"),\n                    cache_dir=self.local_cache_dir\n                )\n                processors[\"text\"] = TextProcessor(\n                    tok, max_length=self.text_max_length, padding=self.text_padding,\n                    cache_size=self.tokenizer_cache_size, batch_size=self.pretokenize_batch_size\n                )\n                encoders[\"text\"] = AutoTextEncoder(\n                    (self.text_model_config or {}).get(\"checkpoint\", \"bert-base-multilingual-cased\"),\n                    cache_dir=self.local_cache_dir\n                )\n\n        # image\n        image_cfg = None\n        if \"image\" in self.modalities:\n            if (self.image_model_config or {}).get(\"model_type\", \"clip\") == \"clip\":\n                ip = CLIPImageProcessor.from_pretrained(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n                processors[\"image\"] = ImageProcessor(ip)\n                encoders[\"image\"] = CLIPImageEncoder(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n            else:\n                cp = (self.image_model_config or {}).get(\"checkpoint\", \"google/vit-base-patch16-224\")\n                ip = AutoImageProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n                processors[\"image\"] = ImageProcessor(ip)\n                encoders[\"image\"] = AutoImageEncoder(cp, cache_dir=self.local_cache_dir)\n            image_cfg = {\"max_items\": self.max_images_per_sample, \"how\": self.image_agg}\n\n        # audio\n        audio_cfg = None\n        if \"audio\" in self.modalities:\n            ab = (self.audio_model_config or {}).get(\"model_type\", self.audio_backend)\n            if ab == \"clap\":\n                cp = (self.audio_model_config or {}).get(\"checkpoint\", self.clap_checkpoint)\n                ap = ClapProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n                processors[\"audio\"] = ClapAudioProcessor(ap, sr=self.audio_sr)\n                encoders[\"audio\"] = CLAPAudioEncoder(cp, cache_dir=self.local_cache_dir)\n            elif ab == \"wav2vec\":\n                cp = (self.audio_model_config or {}).get(\"checkpoint\", \"facebook/wav2vec2-base-960h\")\n                ap = AutoProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n                processors[\"audio\"] = Wav2VecAudioProcessor(ap, sr=self.audio_sr)\n                encoders[\"audio\"] = Wav2Vec2Encoder(cp, cache_dir=self.local_cache_dir)\n            else:  # wav2clip\n                processors[\"audio\"] = Wav2ClipAudioProcessor(sr=self.audio_sr)\n                encoders[\"audio\"] = Wav2ClipEncoder()\n            audio_cfg = {\"max_items\": self.max_audios_per_sample, \"how\": self.audio_agg}\n\n        # Fusion + Head\n        agg = ItemAggregator()\n        fusion_mod = ConcatFusion() if self.fusion == \"concat\" else MeanFusion()\n        dims = [encoders[m].embed_dim for m in [\"image\", \"text\", \"audio\"] if m in encoders]\n        if self.fusion == \"concat\":\n            in_dim = sum(dims)\n        else:\n            assert len(set(dims)) == 1, \"–î–ª—è fusion='mean' —Ä–∞–∑–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å\"\n            in_dim = dims[0]\n        head = ClassificationHead(in_dim=in_dim, num_labels=self.num_labels, hidden=512, dropout=0.1)\n        model = MultiModalModel(encoders, agg, fusion_mod, head, image_cfg=image_cfg, audio_cfg=audio_cfg)\n        return processors, model\n\n    # --------------------------\n    # –ú–µ—Ç—Ä–∏–∫–∏\n    # --------------------------\n    def _build_compute_metrics(self, metric_name: str):\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            try:\n                import evaluate\n                f1_metric = evaluate.load(\"f1\")\n                def compute(p):\n                    preds = p.predictions.argmax(-1)\n                    return f1_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\")\n                return compute\n            except Exception:\n                print(\"evaluate –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é accuracy\")\n                metric_name = \"accuracy\"\n\n        if metric_name == \"accuracy\":\n            try:\n                import evaluate\n                acc = evaluate.load(\"accuracy\")\n                def compute(p):\n                    preds = p.predictions.argmax(-1)\n                    return acc.compute(predictions=preds, references=p.label_ids)\n                return compute\n            except Exception:\n                def compute(p):\n                    preds = p.predictions.argmax(-1)\n                    y = p.label_ids\n                    return {\"accuracy\": float(np.mean(preds == y))}\n                return compute\n\n        raise ValueError('metric_name –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å \"f1\" –∏–ª–∏ \"accuracy\"')\n\n    # --------------------------\n    # –û–±—É—á–µ–Ω–∏–µ\n    # --------------------------\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"accuracy\",   # \"accuracy\" | \"f1\"\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result_cls\",\n        seed: int = 42,\n        gradient_checkpointing: bool = False,\n        class_weights: Optional[np.ndarray] = None,  # –µ—Å–ª–∏ None ‚Äî –∞–≤—Ç–æ-—Ä–∞—Å—á—ë—Ç –ø–æ train_data\n    ):\n        import os\n        from transformers import TrainingArguments\n    \n        # 1) –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞\n        self._validate_modalities(train_data)\n        set_seed(seed)\n    \n        # train/val split\n        df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n    \n        # labels mapping\n        self._ensure_label_mapping(df_train)\n    \n        # class weights\n        if class_weights is None:\n            y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n            counts = np.bincount(y_train_all, minlength=self.num_labels)\n            n_all = counts.sum()\n            cw = np.zeros(self.num_labels, dtype=np.float32)\n            nz = counts > 0\n            cw[nz] = n_all / (self.num_labels * counts[nz].astype(np.float32))\n            class_weights = cw\n    \n        # 2) –ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã (auto)\n        self.processors, self.model = self._build_model_and_processors()\n        if gradient_checkpointing:\n            try:\n                self.model.gradient_checkpointing_enable()  # –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è\n            except Exception:\n                pass\n    \n        # 3) Datasets (–±–µ–∑ —á–∞–Ω–∫–æ–≤)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        pretokenize_train = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_train) <= self.max_pretokenize_samples)\n        pretokenize_eval  = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_eval)  <= self.max_pretokenize_samples)\n    \n        ds_train = MultiModalDataset(\n            df=df_train,\n            target_col=self.target_column_name,\n            task=\"classification\",\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_train) else None,\n            pretokenize=pretokenize_train,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        ds_eval = MultiModalDataset(\n            df=df_eval,\n            target_col=self.target_column_name,\n            task=\"classification\",\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_eval) else None,\n            pretokenize=pretokenize_eval,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n    \n        collate = MultiModalCollator(self.processors, task=\"classification\")\n        compute_metrics = self._build_compute_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=False\n        )\n    \n        # 5) Trainer –∏ —Å—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train,\n            eval_dataset=ds_eval,\n            data_collator=collate,\n            compute_metrics=compute_metrics,\n            class_weights=class_weights\n        )\n    \n        # (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ) —É–±—Ä–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–∏–Ω—Ç–µ—Ä-–∫–æ–ª–ª–±–µ–∫\n        try:\n            from transformers.trainer_callback import PrinterCallback\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n    \n        self.trainer.train()\n    \n        # –æ—á–∏—Å—Ç–∏—Ç—å –∫—ç—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–µ—Å–ª–∏ –±—ã–ª)\n        if has_bt:\n            try: self.processors[\"text\"].clear_cache()\n            except Exception: pass\n    \n        return self\n\n    # --------------------------\n    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n    # --------------------------\n    def predict(\n        self,\n        df: pd.DataFrame,\n        return_label_str: bool = False,\n        return_proba: bool = False,\n        batch_size: Optional[int] = None\n    ) -> np.ndarray:\n        if self.trainer is None:\n            raise RuntimeError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ .fit().\")\n\n        df_c = self._ensure_label_column(df)\n\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            task=\"classification\",\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        collate = MultiModalCollator(self.processors, task=\"classification\")\n\n        # override bs\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n        print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds, metric_key_prefix=\"predict\")\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        ds.clear_cache()\n\n        logits = preds.predictions\n        if return_proba:\n            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n            return probabilities\n\n        y_pred = np.argmax(logits, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    # --------------------------\n    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏\n    # --------------------------\n    def get_embeddings(\n        self,\n        df: pd.DataFrame,\n        batch_size: int = 32,\n        return_per_modality: bool = False\n    ):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ .fit().\")\n\n        device = next(self.trainer.model.parameters()).device\n        self.model.to(device).eval()\n\n        df_c = self._ensure_label_column(df)\n\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            task=\"classification\",\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=False\n        )\n        collate = MultiModalCollator(self.processors, task=\"classification\")\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = batch[\"backend_inputs\"]\n\n                def move_to_device(obj):\n                    if torch.is_tensor(obj): return obj.to(device)\n                    if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n                    if isinstance(obj, list): return [move_to_device(v) for v in obj]\n                    return obj\n\n                bi = move_to_device(bi)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            print(f\"‚úì Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"‚úì Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"‚úì {m.capitalize()} embeddings shape: {arr.shape}\")\n\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n!pip install evaluate wav2clip\n\n# core/utils.py\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom PIL import Image\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef to_pil(x):\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Expected path/np.ndarray/PIL.Image\")\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"–¢—Ä–µ–±—É–µ—Ç—Å—è torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\ndef safe_load(component_cls, checkpoint: str, cache_dir: str = \"./model_cache\",\n              local_files_only: bool = None, **kwargs):\n    if local_files_only is None:\n        local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n    name = getattr(component_cls, \"__name__\", \"\")\n    if \"Tokenizer\" in name:\n        kwargs.setdefault(\"use_fast\", True)\n    return component_cls.from_pretrained(\n        checkpoint, cache_dir=cache_dir, local_files_only=local_files_only, **kwargs\n    )\n\n# core/types.py\nimport torch\n\ndef preferred_device():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n\n# core/device.py\nfrom typing import Any, Dict, List, Protocol, Literal\nimport torch\n\nModality = Literal[\"text\", \"image\", \"audio\"]\n\nclass Processor(Protocol):\n    modality: Modality\n    def prepare_batch(self, raw_items: List[Any]) -> Dict[str, torch.Tensor]:\n        ...\n\nclass Encoder(Protocol):\n    modality: Modality\n    embed_dim: int\n    def encode_batch(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n        ...\n\nclass ItemAggregator(Protocol):\n    def aggregate(self, embs: torch.Tensor, counts: List[int], max_items: int, how: str) -> torch.Tensor:\n        ...\n\nclass Fusion(Protocol):\n    def __call__(self, zs: Dict[Modality, torch.Tensor]) -> torch.Tensor:\n        ...\n\n# data/augmentation.py\nimport os\nimport uuid\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, List, Optional, Callable\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\ndef augment_data_table(\n    df: pd.DataFrame,\n    save_dir: str,\n    image_columns: Optional[List[str]] = None,\n    text_columns: Optional[List[str]] = None,\n    audio_columns: Optional[List[str]] = None,\n    image_augment_fn: Optional[List[Callable[[Image.Image], Image.Image]]] = None,\n    text_augment_fn: Optional[List[Callable[[str], str]]] = None,\n    audio_augment_fn: Optional[List[Callable[..., np.ndarray]]] = None,\n    id_col: Optional[str] = None,\n    audio_sr: int = 16000,\n) -> pd.DataFrame:\n    \"\"\"\n    –î–µ–ª–∞–µ—Ç –ø—Ä–æ—Å—Ç—É—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö.\n    - –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¢–û–õ–¨–ö–û –¥–ª—è —Ç–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ —Ñ—É–Ω–∫—Ü–∏–π.\n    - –î–ª—è –∫–∞–∂–¥–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞—ë—Ç –û–î–ò–ù –≤–∞—Ä–∏–∞–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ (–±–µ–∑ –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏—è –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏).\n    - –ö–∞—Ä—Ç–∏–Ω–∫–∏/–∞—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ save_dir, –≤ —Ç–∞–±–ª–∏—Ü–µ –ø–æ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –ø—É—Ç–∏ –∫ –Ω–æ–≤—ã–º —Ñ–∞–π–ª–∞–º.\n    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å –∫–æ–ª–æ–Ω–∫–æ–π __aug.\n    \"\"\"\n    image_columns = image_columns or []\n    text_columns  = text_columns or []\n    audio_columns = audio_columns or []\n\n    os.makedirs(save_dir, exist_ok=True)\n    run_tag = uuid.uuid4().hex[:8]\n\n    def _row_id(i: int) -> str:\n        if id_col and id_col in df.columns:\n            return str(df.iloc[i][id_col])\n        return f\"row{i}\"\n\n    def _to_pil(v: Any) -> Image.Image:\n        if isinstance(v, Image.Image): return v.convert(\"RGB\")\n        if isinstance(v, str):         return Image.open(v).convert(\"RGB\")\n        if isinstance(v, np.ndarray):\n            if v.dtype != np.uint8: v = np.clip(v, 0, 255).astype(np.uint8)\n            return Image.fromarray(v)\n        raise TypeError(\"Unsupported image type\")\n\n    def _save_img(im: Image.Image, out_dir: str, name: str) -> str:\n        os.makedirs(out_dir, exist_ok=True)\n        path = os.path.join(out_dir, f\"{name}.png\")\n        im.save(path)\n        return path\n\n    def _read_audio(x: Any) -> tuple[np.ndarray, int]:\n        if isinstance(x, str):\n            try:\n                import soundfile as sf\n                y, sr = sf.read(x, always_2d=False)\n            except Exception:\n                from scipy.io import wavfile\n                sr, y = wavfile.read(x)\n            y = np.asarray(y)\n            if y.ndim > 1: y = y.mean(axis=-1)\n            y = (y.astype(np.float32) / (32768.0 if y.dtype.kind in \"iu\" else 1.0))\n            return np.clip(y, -1.0, 1.0), sr\n        elif isinstance(x, np.ndarray):\n            y = x\n            if y.ndim > 1: y = y.mean(axis=-1)\n            y = y.astype(np.float32, copy=False)\n            if y.dtype.kind in \"iu\": y = y / 32768.0\n            return np.clip(y, -1.0, 1.0), audio_sr\n        raise TypeError(\"Unsupported audio type\")\n\n    def _save_wav(y: np.ndarray, sr: int, out_dir: str, name: str) -> str:\n        os.makedirs(out_dir, exist_ok=True)\n        path = os.path.join(out_dir, f\"{name}.wav\")\n        try:\n            import soundfile as sf\n            sf.write(path, y.astype(np.float32, copy=False), sr, subtype=\"PCM_16\")\n        except Exception:\n            from scipy.io import wavfile\n            wavfile.write(path, sr, (np.clip(y, -1, 1) * 32767).astype(np.int16))\n        return path\n\n    out_parts = []\n\n    # –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –µ—Å—Ç—å —Å—Ç–æ–ª–±—Ü—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n    if image_columns and image_augment_fn:\n        for k, fn in tqdm(list(enumerate(image_augment_fn)), desc='–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–æ–∫'):\n            df_aug = df.copy(deep=True)\n            for col_i, col in enumerate(image_columns):\n                out_dir = os.path.join(save_dir, \"images\", col, f\"aug_image_{k}_{run_tag}\")\n                df_aug[col] = [\n                    _save_img(fn(_to_pil(v)), out_dir, f\"{_row_id(i)}\") if v is not None else v\n                    for i, v in tqdm(list(enumerate(df[col].tolist())),\n                                     desc=f'–°—Ç–æ–ª–±–µ—Ü {col_i+1}/{len(image_columns)}', leave=False)\n                ]\n            df_aug[\"__aug\"] = f\"image_{k}\"\n            out_parts.append(df_aug)\n\n    # –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã\n    if text_columns and text_augment_fn:\n        for k, fn in tqdm(list(enumerate(text_augment_fn)), desc='–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤'):\n            df_aug = df.copy(deep=True)\n            for col_i, col in enumerate(text_columns):\n                df_aug[col] = [\n                    fn(v) if isinstance(v, str) else v\n                    for v in tqdm(df[col].tolist(),\n                                  desc=f'–°—Ç–æ–ª–±–µ—Ü {col_i+1}/{len(text_columns)}', leave=False)\n                ]\n            df_aug[\"__aug\"] = f\"text_{k}\"\n            out_parts.append(df_aug)\n\n    # –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –µ—Å—Ç—å –∞—É–¥–∏–æ —Å—Ç–æ–ª–±—Ü—ã\n    if audio_columns and audio_augment_fn:\n        for k, fn in tqdm(list(enumerate(audio_augment_fn)), desc='–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–≤—É–∫–æ–≤'):\n            df_aug = df.copy(deep=True)\n            for col_i, col in enumerate(audio_columns):\n                out_dir = os.path.join(save_dir, \"audio\", col, f\"aug_audio_{k}_{run_tag}\")\n                new_vals = []\n                for i, v in tqdm(list(enumerate(df[col].tolist())),\n                                 desc=f'–°—Ç–æ–ª–±–µ—Ü {col_i+1}/{len(audio_columns)}', leave=False):\n                    if v is None:\n                        new_vals.append(v); continue\n                    y, sr = _read_audio(v)\n                    try:    y2 = fn(y, sr)\n                    except TypeError: y2 = fn(y)\n                    path = _save_wav(np.asarray(y2, dtype=np.float32), sr, out_dir, f\"{_row_id(i)}\")\n                    new_vals.append(path)\n                df_aug[col] = new_vals\n            df_aug[\"__aug\"] = f\"audio_{k}\"\n            out_parts.append(df_aug)\n\n    if not out_parts:\n        raise ValueError(\"–ù–µ –ø–µ—Ä–µ–¥–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ ‚Äî –Ω–µ—á–µ–≥–æ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å.\")\n\n    return pd.concat(out_parts, ignore_index=True)\n\n# data/tokenization.py\nfrom functools import lru_cache\nfrom typing import Dict, List, Optional\nimport numpy as np\nimport torch\n\nclass BatchTokenizer:\n    def __init__(self, tokenizer, max_length=512, cache_size=10000, batch_size=256, padding_strategy=\"max_length\"):\n        assert padding_strategy in {\"max_length\", \"dynamic\"}\n        self.tok = tokenizer\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.padding_strategy = padding_strategy\n        self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n        self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n        if self.is_fast:\n            print(\"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Fast Tokenizer\")\n\n    def _tokenize_single(self, text: str) -> Dict[str, np.ndarray]:\n        res = self.tok(text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        return {k: v.squeeze(0).cpu().numpy() for k, v in res.items()}\n\n    def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n        if self.padding_strategy == \"dynamic\":\n            res = self.tok(texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n            return {k: (v.long() if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else v) for k, v in res.items()}\n        if use_cache and len(texts) < 100:\n            items = [self._cache(t) for t in texts]\n            keys = items[0].keys()\n            out = {}\n            for k in keys:\n                dtype = torch.long if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else torch.float32\n                out[k] = torch.tensor(np.stack([it[k] for it in items]), dtype=dtype)\n            return out\n        res = self.tok(texts, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        return {k: (v.long() if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else v) for k, v in res.items()}\n\n    def tokenize_dataset_lazy(self, texts: List[str], batch_size: Optional[int] = None):\n        b = batch_size or self.batch_size\n        for i in range(0, len(texts), b):\n            yield self.tokenize_batch(texts[i:i+b], use_cache=False)\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n# data/dataset.py\nimport gc\nfrom typing import Any, Dict, List, Optional, Callable\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\n\nclass MultiModalDataset(Dataset):\n    \"\"\"\n    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:\n      - text pretokenize: BatchTokenizer –∏–ª–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (single/batched)\n      - dynamic/max –ø–∞–¥–¥–∏–Ω–≥ (dynamic –æ—Ç–∫–ª—é—á–∞–µ—Ç –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é)\n      - –º—É–ª—å—Ç–∏-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –º—É–ª—å—Ç–∏-–∞—É–¥–∏–æ –Ω–∞ —Å—ç–º–ø–ª\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: Optional[str],                 # –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ ‚Äî –∫–æ–ª–æ–Ω–∫–∞ —Å np.ndarray; –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ‚Äî –º–µ—Ç–∫–∞\n        task: str = \"regression\",                  # \"regression\" | \"classification\"\n        label2id: Optional[Dict[Any, int]] = None, # –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer=None,                       # BatchTokenizer\n        text_tokenizer_fn: Optional[Callable] = None,\n        text_tokenizer_fn_batched: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, Any]] = None,\n        pretokenize: bool = False,\n        pretokenize_batch_size: int = 256,\n        tokenizer_returns_tensors: bool = False,\n        deduplicate_texts: bool = True\n    ):\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.task = task.lower()\n        self.label2id = label2id or {}\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.batch_tokenizer = text_tokenizer\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.text_tokenizer_fn_batched = text_tokenizer_fn_batched\n\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n        self._N = len(self.df)\n\n        # labels\n        if self.task == \"classification\":\n            if self.target_col is None:\n                y = np.zeros(self._N, dtype=np.int64)\n            else:\n                y = self.df[self.target_col].map(self.label2id).fillna(0).astype(int).values\n            self._labels = torch.tensor(y, dtype=torch.long)\n        else:\n            self._labels = self._prepare_labels_regression(self.df, self.target_col)\n\n        # lists for images/audios\n        self._image_lists = self._collect_multi_values(self.df, self.image_columns) if self.image_columns else None\n        self._audio_lists = self._collect_multi_values(self.df, self.audio_columns) if self.audio_columns else None\n\n        self._tok_bank: Optional[Dict[str, torch.Tensor]] = None\n        self._has_text = bool(self.text_columns)\n\n        # dynamic padding -> disable pretokenization\n        if pretokenize and self.batch_tokenizer is not None and getattr(self.batch_tokenizer, \"padding_strategy\", \"max_length\") == \"dynamic\":\n            print(\"‚ö† –ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞: –≤—ã–±—Ä–∞–Ω dynamic-–ø–∞–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞.\")\n            pretokenize = False\n\n        if self._has_text and pretokenize:\n            if self.batch_tokenizer is not None and self.text_tokenizer_fn is None and self.text_tokenizer_fn_batched is None:\n                self._pretokenize_with_batch_tokenizer(pretokenize_batch_size)\n            else:\n                self._pretokenize_with_custom_fn(pretokenize_batch_size, deduplicate_texts)\n\n    # labels for regression: [N, K]\n    def _prepare_labels_regression(self, df: pd.DataFrame, target_col: Optional[str]) -> torch.Tensor:\n        if target_col is None or target_col not in df.columns:\n            return torch.zeros((len(df), 1), dtype=torch.float32)\n        labels_list = []\n        for i in range(len(df)):\n            v = df.iloc[i][target_col]\n            if isinstance(v, (list, tuple, np.ndarray)):\n                arr = np.asarray(v, dtype=np.float32)\n            else:\n                try:\n                    arr = np.asarray([float(v)], dtype=np.float32)\n                except Exception:\n                    arr = np.asarray([0.0], dtype=np.float32)\n            labels_list.append(arr)\n        K = max(a.shape[0] for a in labels_list) if labels_list else 1\n        out = np.zeros((len(labels_list), K), dtype=np.float32)\n        for i, a in enumerate(labels_list):\n            out[i, :a.shape[0]] = a\n        return torch.tensor(out, dtype=torch.float32)\n\n    def _join_text(self, row: pd.Series) -> str:\n        sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n        return sep.join([(\"\" if pd.isna(row[c]) else str(row[c])) for c in self.text_columns])\n\n    @staticmethod\n    def _as_list(v):\n        if v is None or (isinstance(v, float) and np.isnan(v)):\n            return []\n        if isinstance(v, (list, tuple)):\n            return list(v)\n        return [v]\n\n    def _collect_multi_values(self, df: pd.DataFrame, columns: List[str]) -> List[List[Any]]:\n        out = []\n        as_list = self._as_list\n        for _, row in df.iterrows():\n            lst: List[Any] = []\n            for c in columns:\n                if c in row:\n                    lst.extend([x for x in as_list(row[c]) if x is not None])\n            out.append(lst)\n        return out\n\n    def _pretokenize_with_batch_tokenizer(self, batch_size: int):\n        print(\"–ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å BatchTokenizer...\")\n        texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n        banks: Dict[str, List[torch.Tensor]] = {}\n        for start in range(0, self._N, batch_size):\n            batch = texts[start:start + batch_size]\n            tok = self.batch_tokenizer.tokenize_batch(batch, use_cache=False)\n            for k in tok:\n                if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\"):\n                    tok[k] = tok[k].long()\n                else:\n                    tok[k] = tok[k].to(torch.float32)\n            for k, v in tok.items():\n                banks.setdefault(k, []).append(v)\n        self._tok_bank = {k: torch.cat(v_parts, dim=0).contiguous() for k, v_parts in banks.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"‚úì –ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {self._N} –æ–±—Ä–∞–∑—Ü–æ–≤ | keys={list(self._tok_bank.keys())}, shapes={shapes}\")\n\n    def _pretokenize_with_custom_fn(self, batch_size: int, deduplicate_texts: bool):\n        cols = list(self.text_columns)\n        if self.text_tokenizer_fn_batched is not None:\n            print(\"–ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π batched-—Ñ—É–Ω–∫—Ü–∏–µ–π...\")\n            first_end = min(self._N, max(8, batch_size))\n            batch_data = []\n            for i in range(first_end):\n                row = self.df.iloc[i]\n                d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                batch_data.append(d)\n            first_tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n            if not isinstance(first_tok, dict):\n                raise ValueError(\"text_tokenizer_fn_batched –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å dict —Ç–µ–Ω–∑–æ—Ä–æ–≤ [B, L]\")\n            bank: Dict[str, torch.Tensor] = {}\n            for k, t in first_tok.items():\n                if not torch.is_tensor(t): t = torch.tensor(t)\n                bank[k] = torch.empty((self._N, t.size(1)), dtype=t.dtype)\n                bank[k][:first_end] = t[:first_end]\n            for start in range(first_end, self._N, batch_size):\n                end = min(self._N, start + batch_size)\n                batch_data = []\n                for i in range(start, end):\n                    row = self.df.iloc[i]\n                    d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                    batch_data.append(d)\n                tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): t = torch.tensor(t)\n                    bank[k][start:end] = t\n            self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n            shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n            print(f\"‚úì –ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π batched-—Ñ—É–Ω–∫—Ü–∏–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞: shapes={shapes}\")\n            return\n\n        print(\"–ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π single-—Ñ—É–Ω–∫—Ü–∏–µ–π...\")\n        col_arrays = [self.df[c].astype(str).where(~self.df[c].isna(), other=\"\").tolist() for c in cols]\n        first_td = {c: col_arrays[i][0] for i, c in enumerate(cols)}\n        first_tok = self.text_tokenizer_fn(first_td, self.special_tokens)\n        if not isinstance(first_tok, dict):\n            raise ValueError(\"custom text_tokenizer_fn –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å dict —Ç–µ–Ω–∑–æ—Ä–æ–≤\")\n        for k, t in first_tok.items():\n            if not torch.is_tensor(t): first_tok[k] = torch.tensor(t)\n        bank: Dict[str, torch.Tensor] = {k: torch.empty((self._N, *t.shape), dtype=t.dtype) for k, t in first_tok.items()}\n        for k, t in first_tok.items():\n            bank[k][0].copy_(t)\n\n        cache: Dict[tuple, Dict[str, torch.Tensor]] = {}\n        if deduplicate_texts:\n            key0 = tuple(first_td.get(c, \"\") for c in cols)\n            cache[key0] = {k: v.clone() for k, v in first_tok.items()}\n\n        for i in range(1, self._N):\n            td = {c: col_arrays[j][i] for j, c in enumerate(cols)}\n            if deduplicate_texts:\n                key = tuple(td.get(c, \"\") for c in cols)\n                tok = cache.get(key)\n                if tok is None:\n                    tok = self.text_tokenizer_fn(td, self.special_tokens)\n                    for k, t in tok.items():\n                        if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n                    cache[key] = {k: v.clone() for k, v in tok.items()}\n            else:\n                tok = self.text_tokenizer_fn(td, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n            for k, t in tok.items():\n                bank[k][i].copy_(t)\n\n        self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"‚úì –ü—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–π single-—Ñ—É–Ω–∫—Ü–∏–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞: shapes={shapes}\")\n\n    def __len__(self):\n        return self._N\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item: Dict[str, Any] = {}\n        item[\"labels\"] = self._labels[idx]  # [K] float32 –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, int64 –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n        if self.text_columns:\n            if self._tok_bank is not None:\n                item[\"text_tokens\"] = {k: v[idx] for k, v in self._tok_bank.items()}\n            else:\n                item[\"text\"] = self._join_text(self.df.iloc[idx])\n        if self._image_lists is not None:\n            item[\"images\"] = self._image_lists[idx]\n        if self._audio_lists is not None:\n            item[\"audios\"] = self._audio_lists[idx]\n        return item\n\n    def get_cache_stats(self):\n        has = self._tok_bank is not None\n        sizes = {k: tuple(v.shape) for k, v in (self._tok_bank or {}).items()}\n        return {\"has_pretokenized\": has, \"shapes\": sizes, \"N\": self._N}\n\n    def clear_cache(self):\n        self._tok_bank = None\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n# data/collate.py\nfrom typing import Any, Dict, List, Optional\nimport numpy as np\nimport torch\n# from mmkit.core.utils import to_pil, load_audio\n\nclass MultiModalCollator:\n    \"\"\"\n    –ì–æ—Ç–æ–≤–∏—Ç backend_inputs –¥–ª—è –º–æ–¥–µ–ª–∏.\n    - –ï—Å–ª–∏ –≤ item –µ—Å—Ç—å \"text_tokens\": —Å—Ç–∞–∫–∞–µ—Ç –∏—Ö.\n    - –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç processors[\"text\"] –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.\n    - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è/–∞—É–¥–∏–æ: —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –≤ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ + —Å—á–∏—Ç–∞–µ—Ç counts.\n    \"\"\"\n    def __init__(self, processors: Dict[str, Any], task: str = \"regression\", audio_sr_fallback: int = 16000):\n        self.processors = processors\n        self.task = task.lower()\n        self.audio_sr_fallback = audio_sr_fallback\n\n    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        labels = [b.get(\"labels\") for b in batch]\n        if labels and isinstance(labels[0], torch.Tensor):\n            labels = torch.stack(labels)\n        else:\n            if self.task == \"classification\":\n                labels = torch.tensor(labels, dtype=torch.long)\n            else:\n                labels = torch.tensor(labels, dtype=torch.float32)\n\n        out = {\"labels\": labels, \"backend_inputs\": {}}\n        # text\n        if \"text\" in self.processors or any((\"text_tokens\" in b) for b in batch):\n            if \"text_tokens\" in batch[0]:\n                t0 = batch[0][\"text_tokens\"]\n                text_inputs = {}\n                for key in t0.keys():\n                    if torch.is_tensor(t0[key]):\n                        text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n                    else:\n                        dtype = torch.long if key in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else torch.float32\n                        text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n            else:\n                texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n                text_inputs = self.processors[\"text\"].prepare_batch(texts)\n            out[\"backend_inputs\"][\"text_inputs\"] = text_inputs\n\n        # images\n        if \"image\" in self.processors:\n            flat_images, counts = [], []\n            for b in batch:\n                lst = b.get(\"images\", []) or []\n                lst = [to_pil(x) for x in lst if x is not None]\n                counts.append(len(lst))\n                flat_images.extend(lst)\n            if len(flat_images) > 0:\n                out[\"backend_inputs\"][\"image_inputs\"] = self.processors[\"image\"].prepare_batch(flat_images)\n            else:\n                out[\"backend_inputs\"][\"image_inputs\"] = {\"pixel_values\": None}\n            out[\"backend_inputs\"][\"image_counts\"] = torch.tensor(counts, dtype=torch.long)\n\n        # audio\n        if \"audio\" in self.processors:\n            flat, counts = [], []\n            for b in batch:\n                lst = b.get(\"audios\", []) or []\n                counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        sr = getattr(self.processors[\"audio\"], \"sr\", self.audio_sr_fallback)\n                        flat.append(load_audio(a, sr))\n                    else:\n                        arr = np.asarray(a, dtype=np.float32)\n                        if arr.ndim>1: arr = np.squeeze(arr)\n                        if arr.ndim>1: arr = arr.reshape(-1)\n                        flat.append(arr)\n            out[\"backend_inputs\"][\"audio_counts\"] = torch.tensor(counts, dtype=torch.long)\n            out[\"backend_inputs\"][\"audio_inputs\"] = self.processors[\"audio\"].prepare_batch(flat) if len(flat)>0 else {\"input_values\": None, \"input_features\": None, \"raw_audios\": []}\n\n        out[\"backend_inputs\"][\"batch_size\"] = len(batch)\n        return out\n\n# processors/text.py\n# from mmkit.data.tokenization import BatchTokenizer\n\nclass TextProcessor:\n    modality = \"text\"\n    def __init__(self, tokenizer, max_length=512, padding=\"max_length\", cache_size=10000, batch_size=256):\n        self.bt = BatchTokenizer(tokenizer, max_length=max_length, cache_size=cache_size, batch_size=batch_size, padding_strategy=padding)\n    def prepare_batch(self, texts):\n        return self.bt.tokenize_batch(texts, use_cache=True)\n    def clear_cache(self):\n        self.bt.clear_cache()\n\n# processors/image.py\nclass ImageProcessor:\n    modality = \"image\"\n    def __init__(self, hf_processor):\n        self.proc = hf_processor\n    def prepare_batch(self, images):\n        if len(images)==0: return {\"pixel_values\": None}\n        x = self.proc(images=images, return_tensors=\"pt\")\n        return {\"pixel_values\": x[\"pixel_values\"]}\n\n# processors/audio.py\nclass ClapAudioProcessor:\n    modality=\"audio\"\n    def __init__(self, hf_processor, sr=48000):\n        self.proc = hf_processor\n        self.sr = sr\n    def prepare_batch(self, raw_list):\n        x = self.proc(audios=raw_list, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n        return {\"input_features\": x[\"input_features\"]}\n\nclass Wav2VecAudioProcessor:\n    modality=\"audio\"\n    def __init__(self, hf_processor, sr=16000):\n        self.proc = hf_processor\n        self.sr = sr\n    def prepare_batch(self, raw_list):\n        x = self.proc(raw_list, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n        return {\"input_values\": x[\"input_values\"]}\n\nclass Wav2ClipAudioProcessor:\n    modality=\"audio\"\n    def __init__(self, sr=16000):\n        self.sr = sr\n    def prepare_batch(self, raw_list):\n        return {\"raw_audios\": raw_list}  # —Å–ø–∏—Å–æ–∫ np.ndarray; —ç–Ω–∫–æ–¥–µ—Ä —Å–∞–º –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç\n\n# encoders/base.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BaseEncoder(nn.Module):\n    modality: str = \"text\"\n    embed_dim: int = 0\n\n    def _normalize(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() > 2:\n            x = x.view(x.size(0), -1)\n        return F.normalize(x, dim=-1, eps=1e-12)\n\n# encoders/text_auto.py\nimport torch\nfrom transformers import AutoModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass AutoTextEncoder(BaseEncoder):\n    modality=\"text\"\n    def __init__(self, checkpoint=\"bert-base-multilingual-cased\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(AutoModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.hidden_size\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        out = self.model(**inputs)\n        z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state.mean(dim=1)\n        return self._normalize(z)\n\n# encoders/text_clip.py\nimport torch\nfrom transformers import CLIPModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass CLIPTextEncoder(BaseEncoder):\n    modality=\"text\"\n    def __init__(self, checkpoint=\"openai/clip-vit-base-patch32\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(CLIPModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.projection_dim\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        z = self.model.get_text_features(**inputs)\n        return self._normalize(z)\n\n# encoders/image_auto.py\nimport torch\nfrom transformers import AutoModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass AutoImageEncoder(BaseEncoder):\n    modality=\"image\"\n    def __init__(self, checkpoint=\"google/vit-base-patch16-224\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(AutoModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.hidden_size\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        pv = inputs.get(\"pixel_values\", None)\n        if pv is None: \n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        out = self.model(pixel_values=pv)\n        z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state[:, 0]\n        return self._normalize(z)\n\n# encoders/image_clip.py\nimport torch\nfrom transformers import CLIPModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass CLIPImageEncoder(BaseEncoder):\n    modality=\"image\"\n    def __init__(self, checkpoint=\"openai/clip-vit-base-patch32\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(CLIPModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.projection_dim\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        pv = inputs.get(\"pixel_values\", None)\n        if pv is None:\n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        z = self.model.get_image_features(pixel_values=pv)\n        return self._normalize(z)\n\n# encoders/audio_clap.py\nimport torch\nfrom transformers import ClapModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass CLAPAudioEncoder(BaseEncoder):\n    modality=\"audio\"\n    def __init__(self, checkpoint=\"laion/clap-htsat-unfused\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(ClapModel, checkpoint, cache_dir)\n        self.embed_dim = getattr(self.model.config, \"projection_dim\", 512)\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        feats = inputs.get(\"input_features\")\n        if feats is None:\n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        z = self.model.get_audio_features(input_features=feats.float())\n        return self._normalize(z.float())\n\n# encoders/audio_wav2vec.py\nimport torch\nfrom transformers import AutoModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass Wav2Vec2Encoder(BaseEncoder):\n    modality=\"audio\"\n    def __init__(self, checkpoint=\"facebook/wav2vec2-base-960h\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(AutoModel, checkpoint, cache_dir)\n        self.embed_dim = getattr(self.model.config, \"hidden_size\", 768)\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        iv = inputs.get(\"input_values\")\n        if iv is None:\n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        out = self.model(input_values=iv.float())\n        z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state.mean(dim=1)\n        return self._normalize(z.float())\n\n# encoders/audio_wav2clip.py\nimport numpy as np\nimport torch\n# from .base import BaseEncoder\n\nclass Wav2ClipEncoder(BaseEncoder):\n    modality=\"audio\"\n    def __init__(self):\n        super().__init__()\n        try:\n            import wav2clip as w2c\n        except Exception as e:\n            raise RuntimeError(\"–¢—Ä–µ–±—É–µ—Ç—Å—è wav2clip: pip install wav2clip\") from e\n        self.w2c = w2c\n        model = None\n        if hasattr(w2c, \"get_model\"):\n            model = w2c.get_model()\n        elif hasattr(w2c, \"model\"):\n            m = w2c.model\n            model = m() if callable(m) else m\n        else:\n            raise RuntimeError(\"wav2clip –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç get_model()/model\")\n        self.model = model\n        self.embed_dim = 512\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        raws = inputs.get(\"raw_audios\", [])\n        if len(raws)==0:\n            return torch.zeros(0, self.embed_dim)\n        embs = []\n        for arr in raws:\n            a = np.asarray(arr, dtype=np.float32)\n            if a.ndim>1: a = np.squeeze(a)\n            if a.ndim>1: a = a.reshape(-1)\n            if a.size < 512:\n                a = np.pad(a, (0, 512 - a.size), mode=\"constant\")\n            try:\n                e = self.w2c.embed_audio(a, self.model)\n                e = np.asarray(e)\n            except Exception:\n                x = torch.from_numpy(a).float().unsqueeze(0)\n                y = self.model(x)\n                if isinstance(y, (tuple, list)):\n                    y = y[0]\n                if torch.is_tensor(y):\n                    if y.dim()==2 and y.size(0)==1: y=y.squeeze(0)\n                    e = y.detach().cpu().numpy()\n                else:\n                    e = np.asarray(y)\n            if e.ndim>1: e = e.reshape(-1)\n            embs.append(torch.as_tensor(e, dtype=torch.float32))\n        z = torch.stack(embs, dim=0)\n        z = torch.nn.functional.normalize(z, dim=-1, eps=1e-12)\n        return z\n\n# aggregation/item_pool.py\nimport torch\nimport torch.nn.functional as F\n\nclass ItemAggregator:\n    def aggregate(self, embs: torch.Tensor, counts, max_k: int, how: str):\n        if embs is None or (torch.is_tensor(embs) and embs.numel()==0):\n            D = 0 if embs is None else getattr(embs, \"size\", lambda *_: 0)(-1)\n            out_dim = D*max_k if how==\"concat\" else D\n            return torch.zeros((len(counts), out_dim), device=embs.device if torch.is_tensor(embs) else \"cpu\", dtype=torch.float32)\n        if embs.dim()==1: embs = embs.unsqueeze(0)\n        if embs.dim()>2: embs = embs.view(embs.size(0), -1)\n        N, D = embs.size()\n        out_dim = D*max_k if how==\"concat\" else D\n        out = torch.zeros((len(counts), out_dim), device=embs.device, dtype=embs.dtype)\n        off = 0\n        for i, c in enumerate(counts):\n            if c<=0 or off>=N: continue\n            take = min(c, N-off)\n            sample = embs[off:off+take]\n            off += take\n            if how==\"concat\":\n                sample = sample[:max_k]\n                if sample.size(0)<max_k:\n                    pad = torch.zeros((max_k - sample.size(0), D), device=embs.device, dtype=embs.dtype)\n                    sample = torch.cat([sample, pad], dim=0)\n                out[i] = sample.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n        return F.normalize(out, dim=-1, eps=1e-12)\n\n# aggregation/fusion.py\nimport torch\n\nclass ConcatFusion:\n    def __call__(self, zs: dict) -> torch.Tensor:\n        feats = [zs[k] for k in [\"image\",\"text\",\"audio\"] if k in zs]\n        return torch.cat(feats, dim=-1)\n\nclass MeanFusion:\n    def __call__(self, zs: dict) -> torch.Tensor:\n        feats = [zs[k] for k in [\"image\",\"text\",\"audio\"] if k in zs]\n        return torch.stack(feats, dim=0).mean(dim=0)\n\n# heads/regression.py\nimport torch.nn as nn\n\nclass RegressionHead(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden=256, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, out_dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n# heads/classification.py\nimport torch.nn as nn\n\nclass ClassificationHead(nn.Module):\n    def __init__(self, in_dim, num_labels, hidden=512, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n    def forward(self, x): return self.net(x)\n\n# model/multimodal.py\nfrom typing import Dict\nimport torch\nimport torch.nn as nn\n# from mmkit.aggregation.item_pool import ItemAggregator\n\nclass MultiModalModel(nn.Module):\n    \"\"\"\n    –ú–æ–¥–µ–ª—å: dict(encoders) + aggregator (–¥–ª—è image/audio –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö) + fusion + head.\n    \"\"\"\n    def __init__(self, encoders: Dict[str, nn.Module], aggregator: ItemAggregator, fusion, head,\n                 image_cfg=None, audio_cfg=None):\n        super().__init__()\n        self.enc = nn.ModuleDict(encoders)\n        self.agg = aggregator\n        self.fusion = fusion\n        self.head = head\n        self.image_cfg = image_cfg or {\"max_items\":1, \"how\":\"concat\"}\n        self.audio_cfg = audio_cfg or {\"max_items\":1, \"how\":\"concat\"}\n\n    def _fwd_features(self, backend_inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        zs = {}\n        if \"text\" in self.enc and \"text_inputs\" in backend_inputs:\n            zs[\"text\"] = self.enc[\"text\"].encode_batch(backend_inputs[\"text_inputs\"])\n        if \"image\" in self.enc and \"image_inputs\" in backend_inputs:\n            flat = self.enc[\"image\"].encode_batch(backend_inputs[\"image_inputs\"])\n            counts = backend_inputs.get(\"image_counts\", torch.tensor([0]*backend_inputs.get(\"batch_size\", flat.size(0)))).tolist()\n            zs[\"image\"] = self.agg.aggregate(flat, counts, self.image_cfg[\"max_items\"], self.image_cfg[\"how\"])\n        if \"audio\" in self.enc and \"audio_inputs\" in backend_inputs:\n            flat = self.enc[\"audio\"].encode_batch(backend_inputs[\"audio_inputs\"])\n            counts = backend_inputs.get(\"audio_counts\", torch.tensor([0]*backend_inputs.get(\"batch_size\", flat.size(0)))).tolist()\n            zs[\"audio\"] = self.agg.aggregate(flat, counts, self.audio_cfg[\"max_items\"], self.audio_cfg[\"how\"])\n        return zs\n\n    def forward(self, backend_inputs: Dict[str, torch.Tensor], labels: torch.Tensor = None):\n        zs = self._fwd_features(backend_inputs)\n        if not zs:\n            raise ValueError(\"–≠–Ω–∫–æ–¥–µ—Ä—ã –Ω–µ –≤–µ—Ä–Ω—É–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\")\n        fused = self.fusion(zs)\n        logits = self.head(fused)\n        return {\"logits\": logits}\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, torch.Tensor], return_per_modality: bool = False):\n        zs = self._fwd_features(backend_inputs)\n        fused = self.fusion(zs)\n        return (fused, zs) if return_per_modality else fused\n\n# train/trainer_hf.py\nfrom typing import Optional\nimport torch\nimport torch.nn.functional as F\nfrom transformers import Trainer\n\nclass MSETrainer(Trainer):\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n        **kwargs\n    ):\n        labels = inputs.pop(\"labels\").to(torch.float32)\n        backend_inputs = inputs.pop(\"backend_inputs\", None)\n        if backend_inputs is None:\n            backend_inputs = inputs\n\n        out = model(backend_inputs=backend_inputs)\n        logits = out[\"logits\"]\n        preds = logits if logits.dim() == 1 else (logits.squeeze(-1) if logits.size(-1) == 1 else logits)\n        labels = labels.view_as(preds)\n        loss = F.mse_loss(preds, labels)\n        return (loss, out) if return_outputs else loss\n\nclass WeightedCETrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = (\n            torch.as_tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n        )\n\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n        **kwargs\n    ):\n        labels = inputs.pop(\"labels\")\n        backend_inputs = inputs.pop(\"backend_inputs\", None)\n        if backend_inputs is None:\n            backend_inputs = inputs\n\n        out = model(backend_inputs=backend_inputs)\n        logits = out[\"logits\"]\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = F.cross_entropy(logits, labels.long(), weight=weight)\n        return (loss, out) if return_outputs else loss\n\nclass MultiLabelBCETrainer(Trainer):\n    \"\"\"\n    Trainer –¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (multi-label classification).\n    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç BCEWithLogitsLoss.\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç class_weights –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤.\n    \"\"\"\n    def __init__(self, *args, class_weights=None, pos_weight=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        # class_weights - –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ [num_classes]\n        self.class_weights = (\n            torch.as_tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n        )\n        # pos_weight - –≤–µ—Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (–¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)\n        self.pos_weight = (\n            torch.as_tensor(pos_weight, dtype=torch.float32) if pos_weight is not None else None\n        )\n\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n        **kwargs\n    ):\n        labels = inputs.pop(\"labels\").to(torch.float32)  # [B, num_classes]\n        backend_inputs = inputs.pop(\"backend_inputs\", None)\n        if backend_inputs is None:\n            backend_inputs = inputs\n\n        out = model(backend_inputs=backend_inputs)\n        logits = out[\"logits\"]  # [B, num_classes]\n\n        # BCEWithLogitsLoss –æ–∂–∏–¥–∞–µ—Ç logits (–±–µ–∑ sigmoid) –∏ –º–µ—Ç–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [0, 1]\n        pos_weight = self.pos_weight.to(logits.device) if self.pos_weight is not None else None\n        loss = F.binary_cross_entropy_with_logits(logits, labels, pos_weight=pos_weight)\n        \n        # –ï—Å–ª–∏ –µ—Å—Ç—å class_weights, –ø—Ä–∏–º–µ–Ω—è–µ–º –∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ\n        if self.class_weights is not None:\n            # –í–∑–≤–µ—à–∏–≤–∞–µ–º loss –ø–æ –∫–ª–∞—Å—Å–∞–º\n            weights = self.class_weights.to(logits.device).unsqueeze(0)  # [1, num_classes]\n            # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º loss —Å –≤–µ—Å–∞–º–∏\n            per_class_loss = F.binary_cross_entropy_with_logits(\n                logits, labels, pos_weight=pos_weight, reduction='none'\n            )  # [B, num_classes]\n            loss = (per_class_loss * weights).mean()\n\n        return (loss, out) if return_outputs else loss\n\nclass ASLTrainer(Trainer):\n    \"\"\"\n    Trainer –¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å Asymmetric Loss (ASL).\n    \n    Asymmetric Loss for Multi-Label Classification –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑:\n    https://arxiv.org/abs/2009.14119\n    \n    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ ASL:\n    - –õ—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n    - –§–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö (focal loss)\n    - –ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n    - Clip –ø–∞—Ä–∞–º–µ—Ç—Ä –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ª–µ–≥–∫–∏—Ö –Ω–µ–≥–∞—Ç–∏–≤–∞—Ö\n    \n    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n        gamma_neg (float): —Ñ–æ–∫—É—Å –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö (–æ–±—ã—á–Ω–æ 2-4, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 4)\n        gamma_pos (float): —Ñ–æ–∫—É—Å –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö (–æ–±—ã—á–Ω–æ 0-1, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0)\n        clip (float): clipping –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –Ω–µ–≥–∞—Ç–∏–≤–æ–≤ (–æ–±—ã—á–Ω–æ 0.0-0.05, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.05)\n        eps (float): –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1e-8)\n        disable_torch_grad_focal_loss (bool): –æ—Ç–∫–ª—é—á–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è focal –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)\n    \n    –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:\n        - –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: gamma_neg=2, gamma_pos=0, clip=0.0\n        - –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: gamma_neg=4, gamma_pos=0, clip=0.05\n        - –û—á–µ–Ω—å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: gamma_neg=4, gamma_pos=1, clip=0.05\n    \"\"\"\n    def __init__(\n        self,\n        *args,\n        gamma_neg: float = 4.0,\n        gamma_pos: float = 0.0,\n        clip: float = 0.05,\n        eps: float = 1e-8,\n        disable_torch_grad_focal_loss: bool = False,\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.gamma_neg = gamma_neg\n        self.gamma_pos = gamma_pos\n        self.clip = clip\n        self.eps = eps\n        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n        **kwargs\n    ):\n        labels = inputs.pop(\"labels\").to(torch.float32)  # [B, num_classes]\n        backend_inputs = inputs.pop(\"backend_inputs\", None)\n        if backend_inputs is None:\n            backend_inputs = inputs\n\n        out = model(backend_inputs=backend_inputs)\n        logits = out[\"logits\"]  # [B, num_classes]\n\n        # Asymmetric Loss\n        loss = self._asymmetric_loss(logits, labels)\n        \n        return (loss, out) if return_outputs else loss\n\n    def _asymmetric_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Asymmetric Loss –¥–ª—è multi-label classification.\n        \n        Args:\n            logits: [B, C] - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (–ª–æ–≥–∏—Ç—ã)\n            targets: [B, C] - —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏ (0 –∏–ª–∏ 1)\n        \n        Returns:\n            scalar loss\n        \"\"\"\n        # –ü—Ä–∏–º–µ–Ω—è–µ–º sigmoid –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π\n        xs_pos = torch.sigmoid(logits)\n        xs_neg = 1.0 - xs_pos\n\n        # Asymmetric Clipping\n        if self.clip is not None and self.clip > 0:\n            xs_neg = (xs_neg + self.clip).clamp(max=1)\n\n        # Basic CE calculation\n        los_pos = targets * torch.log(xs_pos.clamp(min=self.eps))\n        los_neg = (1 - targets) * torch.log(xs_neg.clamp(min=self.eps))\n\n        # Asymmetric Focusing\n        if self.gamma_neg > 0 or self.gamma_pos > 0:\n            if self.disable_torch_grad_focal_loss:\n                # –û—Ç–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç —á–µ—Ä–µ–∑ focal –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É\n                torch.set_grad_enabled(False)\n                xs_pos_detach = xs_pos.detach()\n                xs_neg_detach = xs_neg.detach()\n                torch.set_grad_enabled(True)\n                \n                pt0 = xs_pos_detach * targets\n                pt1 = xs_neg_detach * (1 - targets)\n                pt = pt0 + pt1\n                one_sided_gamma = self.gamma_pos * targets + self.gamma_neg * (1 - targets)\n                one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n            else:\n                pt0 = xs_pos * targets\n                pt1 = xs_neg * (1 - targets)\n                pt = pt0 + pt1\n                one_sided_gamma = self.gamma_pos * targets + self.gamma_neg * (1 - targets)\n                one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n            \n            los_pos = los_pos * one_sided_w\n            los_neg = los_neg * one_sided_w\n\n        loss = los_pos + los_neg\n        return -loss.mean()\n\nclass FocalLossTrainer(Trainer):\n    \"\"\"\n    Trainer –¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å Focal Loss.\n    \n    Focal Loss –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑:\n    https://arxiv.org/abs/1708.02002\n    \n    Focal Loss = -alpha * (1-p)^gamma * log(p)\n    \n    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n        alpha (float or list): –≤–µ—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.25)\n        gamma (float): —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2.0)\n        reduction (str): 'mean' –∏–ª–∏ 'sum' (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'mean')\n    \"\"\"\n    def __init__(\n        self,\n        *args,\n        alpha: float = 0.25,\n        gamma: float = 2.0,\n        reduction: str = 'mean',\n        **kwargs\n    ):\n        super().__init__(*args, **kwargs)\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n        **kwargs\n    ):\n        labels = inputs.pop(\"labels\").to(torch.float32)  # [B, num_classes]\n        backend_inputs = inputs.pop(\"backend_inputs\", None)\n        if backend_inputs is None:\n            backend_inputs = inputs\n\n        out = model(backend_inputs=backend_inputs)\n        logits = out[\"logits\"]  # [B, num_classes]\n\n        # Focal Loss\n        loss = self._focal_loss(logits, labels)\n        \n        return (loss, out) if return_outputs else loss\n\n    def _focal_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Focal Loss –¥–ª—è multi-label classification.\n        \n        Args:\n            logits: [B, C] - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (–ª–æ–≥–∏—Ç—ã)\n            targets: [B, C] - —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏ (0 –∏–ª–∏ 1)\n        \n        Returns:\n            scalar loss\n        \"\"\"\n        # BCE loss\n        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n        \n        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n        probs = torch.sigmoid(logits)\n        \n        # –í—ã—á–∏—Å–ª—è–µ–º p_t\n        p_t = probs * targets + (1 - probs) * (1 - targets)\n        \n        # Focal term: (1 - p_t)^gamma\n        focal_term = (1 - p_t) ** self.gamma\n        \n        # Focal loss\n        loss = self.alpha * focal_term * bce_loss\n        \n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss\n\n# train/callbacks.py\nfrom transformers.trainer_callback import TrainerCallback\nfrom tqdm.auto import tqdm\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n        self.tqdm = tqdm\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            self.tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n# train/metrics.py\nimport numpy as np\n\ndef build_regression_metrics(name: str):\n    name = name.lower()\n    if name not in (\"rmse\", \"mae\", \"r2\"):\n        raise ValueError('metric_name –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å \"rmse\", \"mae\" –∏–ª–∏ \"r2\"')\n\n    def compute(p):\n        preds = p.predictions\n        y = p.label_ids\n        preds = preds.squeeze(-1) if preds.ndim == 2 and preds.shape[-1] == 1 else preds\n        y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n        axis = 1 if preds.ndim == 2 else None\n        if name == \"rmse\":\n            err = preds - y\n            mse = np.mean(err**2, axis=axis)\n            rmse = np.sqrt(mse)\n            return {\"rmse\": float(np.mean(rmse))}\n        elif name == \"mae\":\n            mae = np.mean(np.abs(preds - y), axis=axis)\n            return {\"mae\": float(np.mean(mae))}\n        else:\n            y_mean = np.mean(y, axis=axis, keepdims=True) if preds.ndim == 2 else np.mean(y)\n            ss_res = np.sum((y - preds) ** 2, axis=axis)\n            ss_tot = np.sum((y - y_mean) ** 2, axis=axis)\n            r2 = 1.0 - (ss_res / (ss_tot + 1e-12))\n            return {\"r2\": float(np.mean(r2))}\n    return compute\n\ndef build_multilabel_metrics(metric_name: str = \"f1_micro\", threshold: float = 0.5):\n    \"\"\"\n    –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (multi-label).\n    \n    Args:\n        metric_name: \"f1_micro\" | \"f1_macro\" | \"f1_samples\" | \"accuracy\" | \"hamming\"\n        threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.5)\n    \n    Returns:\n        —Ñ—É–Ω–∫—Ü–∏—è compute_metrics –¥–ª—è Trainer\n    \"\"\"\n    metric_name = metric_name.lower()\n    valid_metrics = [\"f1_micro\", \"f1_macro\", \"f1_samples\", \"accuracy\", \"hamming\"]\n    if metric_name not in valid_metrics:\n        raise ValueError(f'metric_name –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–Ω–∏–º –∏–∑ {valid_metrics}')\n\n    def compute(p):\n        logits = p.predictions  # [N, num_classes]\n        labels = p.label_ids    # [N, num_classes]\n        \n        # –ü—Ä–∏–º–µ–Ω—è–µ–º sigmoid –∏ –±–∏–Ω–∞—Ä–∏–∑—É–µ–º\n        probs = 1 / (1 + np.exp(-logits))\n        preds = (probs >= threshold).astype(int)\n        \n        if metric_name == \"accuracy\":\n            # Subset accuracy (exact match): –≤—Å–µ –º–µ—Ç–∫–∏ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å\n            exact_match = np.all(preds == labels, axis=1)\n            return {\"accuracy\": float(np.mean(exact_match))}\n        \n        elif metric_name == \"hamming\":\n            # Hamming loss: –¥–æ–ª—è –Ω–µ—Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –º–µ—Ç–æ–∫\n            hamming = np.mean(preds != labels)\n            return {\"hamming_loss\": float(hamming)}\n        \n        elif metric_name.startswith(\"f1\"):\n            # F1 –º–µ—Ç—Ä–∏–∫–∏\n            try:\n                from sklearn.metrics import f1_score\n                \n                if metric_name == \"f1_micro\":\n                    f1 = f1_score(labels, preds, average='micro', zero_division=0)\n                    return {\"f1_micro\": float(f1)}\n                elif metric_name == \"f1_macro\":\n                    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n                    return {\"f1_macro\": float(f1)}\n                else:  # f1_samples\n                    f1 = f1_score(labels, preds, average='samples', zero_division=0)\n                    return {\"f1_samples\": float(f1)}\n            except ImportError:\n                # Fallback: –ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è micro F1\n                tp = np.sum((preds == 1) & (labels == 1))\n                fp = np.sum((preds == 1) & (labels == 0))\n                fn = np.sum((preds == 0) & (labels == 1))\n                precision = tp / (tp + fp + 1e-10)\n                recall = tp / (tp + fn + 1e-10)\n                f1 = 2 * precision * recall / (precision + recall + 1e-10)\n                return {\"f1_micro\": float(f1)}\n    \n    return compute\n\ndef build_multilabel_metrics_all(threshold: float = 0.5):\n    \"\"\"\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è multi-label classification.\n    \"\"\"\n    def compute(p):\n        logits = p.predictions\n        labels = p.label_ids\n        \n        probs = 1 / (1 + np.exp(-logits))\n        preds = (probs >= threshold).astype(int)\n        \n        results = {}\n        \n        # Subset accuracy\n        exact_match = np.all(preds == labels, axis=1)\n        results[\"accuracy\"] = float(np.mean(exact_match))\n        \n        # Hamming loss\n        results[\"hamming_loss\"] = float(np.mean(preds != labels))\n        \n        # F1 scores\n        try:\n            from sklearn.metrics import f1_score\n            results[\"f1_micro\"] = float(f1_score(labels, preds, average='micro', zero_division=0))\n            results[\"f1_macro\"] = float(f1_score(labels, preds, average='macro', zero_division=0))\n            results[\"f1_samples\"] = float(f1_score(labels, preds, average='samples', zero_division=0))\n        except ImportError:\n            tp = np.sum((preds == 1) & (labels == 1))\n            fp = np.sum((preds == 1) & (labels == 0))\n            fn = np.sum((preds == 0) & (labels == 1))\n            precision = tp / (tp + fp + 1e-10)\n            recall = tp / (tp + fn + 1e-10)\n            f1 = 2 * precision * recall / (precision + recall + 1e-10)\n            results[\"f1_micro\"] = float(f1)\n        \n        return results\n    \n    return compute\n\n# train/recipes.py\nfrom transformers import CLIPTokenizer, CLIPImageProcessor, AutoTokenizer, AutoImageProcessor, ClapProcessor, AutoProcessor\n# from mmkit.processors.text import TextProcessor\n# from mmkit.processors.image import ImageProcessor\n# from mmkit.processors.audio import ClapAudioProcessor, Wav2VecAudioProcessor, Wav2ClipAudioProcessor\n\n# from mmkit.encoders.text_clip import CLIPTextEncoder\n# from mmkit.encoders.text_auto import AutoTextEncoder\n# from mmkit.encoders.image_clip import CLIPImageEncoder\n# from mmkit.encoders.image_auto import AutoImageEncoder\n# from mmkit.encoders.audio_clap import CLAPAudioEncoder\n# from mmkit.encoders.audio_wav2vec import Wav2Vec2Encoder\n# from mmkit.encoders.audio_wav2clip import Wav2ClipEncoder\n\n# from mmkit.aggregation.item_pool import ItemAggregator\n# from mmkit.aggregation.fusion import ConcatFusion, MeanFusion\n# from mmkit.heads.regression import RegressionHead\n# from mmkit.model.multimodal import MultiModalModel\n\ndef build_auto_regression(modalities,\n                          fusion=\"concat\",\n                          # text config\n                          text_model_type=\"clip\", text_checkpoint=\"openai/clip-vit-base-patch32\", text_max_length=77, text_padding=\"max_length\",\n                          # image config\n                          image_model_type=\"clip\", image_checkpoint=\"openai/clip-vit-base-patch32\", max_images=1, image_agg=\"concat\",\n                          # audio config\n                          audio_model_type=\"clap\", audio_checkpoint=\"laion/clap-htsat-unfused\", audio_sr=48000, max_audios=1, audio_agg=\"concat\",\n                          # head config\n                          out_dim=1, hidden=256, dropout=0.1,\n                          cache_dir=\"./model_cache\"):\n    encoders = {}\n    processors = {}\n    # text\n    if \"text\" in modalities:\n        if text_model_type == \"clip\":\n            tok = CLIPTokenizer.from_pretrained(text_checkpoint, cache_dir=cache_dir)\n            processors[\"text\"] = TextProcessor(tok, max_length=text_max_length, padding=text_padding)\n            encoders[\"text\"] = CLIPTextEncoder(text_checkpoint, cache_dir=cache_dir)\n            text_dim = encoders[\"text\"].embed_dim\n        else:\n            tok = AutoTokenizer.from_pretrained(text_checkpoint, cache_dir=cache_dir)\n            processors[\"text\"] = TextProcessor(tok, max_length=text_max_length, padding=text_padding)\n            encoders[\"text\"] = AutoTextEncoder(text_checkpoint, cache_dir=cache_dir)\n            text_dim = encoders[\"text\"].embed_dim\n    # image\n    image_cfg = None\n    if \"image\" in modalities:\n        if image_model_type == \"clip\":\n            ip = CLIPImageProcessor.from_pretrained(image_checkpoint, cache_dir=cache_dir)\n            processors[\"image\"] = ImageProcessor(ip)\n            encoders[\"image\"] = CLIPImageEncoder(image_checkpoint, cache_dir=cache_dir)\n        else:\n            ip = AutoImageProcessor.from_pretrained(image_checkpoint, cache_dir=cache_dir)\n            processors[\"image\"] = ImageProcessor(ip)\n            encoders[\"image\"] = AutoImageEncoder(image_checkpoint, cache_dir=cache_dir)\n        image_dim = encoders[\"image\"].embed_dim\n        image_cfg = {\"max_items\": max_images, \"how\": image_agg}\n    # audio\n    audio_cfg = None\n    if \"audio\" in modalities:\n        if audio_model_type == \"clap\":\n            ap = ClapProcessor.from_pretrained(audio_checkpoint, cache_dir=cache_dir)\n            processors[\"audio\"] = ClapAudioProcessor(ap, sr=audio_sr)\n            encoders[\"audio\"] = CLAPAudioEncoder(audio_checkpoint, cache_dir=cache_dir)\n        elif audio_model_type == \"wav2vec\":\n            ap = AutoProcessor.from_pretrained(audio_checkpoint, cache_dir=cache_dir)\n            processors[\"audio\"] = Wav2VecAudioProcessor(ap, sr=audio_sr)\n            encoders[\"audio\"] = Wav2Vec2Encoder(audio_checkpoint, cache_dir=cache_dir)\n        else:  # wav2clip\n            processors[\"audio\"] = Wav2ClipAudioProcessor(sr=audio_sr)\n            encoders[\"audio\"] = Wav2ClipEncoder()\n        audio_dim = encoders[\"audio\"].embed_dim\n        audio_cfg = {\"max_items\": max_audios, \"how\": audio_agg}\n\n    agg = ItemAggregator()\n    fusion_mod = ConcatFusion() if fusion==\"concat\" else MeanFusion()\n    # compute in_dim\n    dims = []\n    for m in [\"image\",\"text\",\"audio\"]:\n        if m in encoders:\n            dims.append(encoders[m].embed_dim)\n    if fusion==\"concat\":\n        in_dim = sum(dims)\n    else:\n        assert len(set(dims))==1, \"–î–ª—è fusion=mean —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\"\n        in_dim = dims[0]\n    head = RegressionHead(in_dim, out_dim=out_dim, hidden=hidden, dropout=dropout)\n    model = MultiModalModel(encoders, agg, fusion_mod, head, image_cfg=image_cfg, audio_cfg=audio_cfg)\n    return processors, model\n\n# pipelines/regression.py\nimport math\nimport gc\nfrom typing import Any, Dict, List, Optional\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nfrom transformers import TrainingArguments\n# from mmkit.core.utils import set_seed\n# from mmkit.data.dataset import MultiModalDataset\n# from mmkit.data.collate import MultiModalCollator\n# from mmkit.train.trainer_hf import MSETrainer\n# from mmkit.train.callbacks import PbarConsoleLogger\n# from mmkit.train.metrics import build_regression_metrics\n# from mmkit.train.recipes import build_auto_regression\n\nclass MultiModalRegressionPipeline:\n    \"\"\"\n    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (multi-target).\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: text/image/audio, dynamic/max padding, –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, —á–∞–Ω–∫–∏, RMSE/MAE/R2.\n    \"\"\"\n    def __init__(self,\n                 modalities: List[str],\n                 target_column_names: List[str],\n                 text_columns: Optional[List[str]] = None,\n                 image_columns: Optional[List[str]] = None,\n                 audio_columns: Optional[List[str]] = None,\n                 # recipe params\n                 backend: str = \"auto\",\n                 clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n                 clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n                 text_model_config: Optional[Dict[str, Any]] = None,\n                 image_model_config: Optional[Dict[str, Any]] = None,\n                 audio_model_config: Optional[Dict[str, Any]] = None,\n                 fusion: str = \"concat\",\n                 # tokenizer\n                 text_padding: str = \"max_length\",\n                 text_max_length: int = 256,\n                 use_batch_tokenizer: bool = True,\n                 pretokenize_data: bool = True,\n                 pretokenize_batch_size: int = 256,\n                 tokenizer_cache_size: int = 10000,\n                 max_pretokenize_samples: int = 100000,\n                 local_cache_dir: str = \"./model_cache\"):\n        self.modalities = sorted(list(set(modalities)))\n        self.target_column_names = list(target_column_names)\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n\n        self.text_padding = text_padding\n        self.text_max_length = text_max_length\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n\n        self._target_vec_col = \"__target_vector__\"\n        self.processors = {}\n        self.model = None\n        self.trainer = None\n\n    def _attach_target_vector(self, df: pd.DataFrame, fill_zeros: bool = False) -> pd.DataFrame:\n        df_c = df.copy()\n        K = len(self.target_column_names)\n        if fill_zeros:\n            df_c[self._target_vec_col] = [np.zeros(K, dtype=np.float32) for _ in range(len(df_c))]\n        else:\n            def _row_to_vec(row):\n                vals = [row[c] for c in self.target_column_names]\n                return np.asarray(vals, dtype=np.float32)\n            df_c[self._target_vec_col] = df_c.apply(_row_to_vec, axis=1)\n        return df_c\n\n    def _validate_modalities(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            assert self.text_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ text, –Ω–æ text_columns –ø—É—Å—Ç–æ–π\"\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing: raise ValueError(f\"–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {missing}\")\n        if \"image\" in self.modalities:\n            assert self.image_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ image, –Ω–æ image_columns –ø—É—Å—Ç\"\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing: raise ValueError(f\"–ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {missing}\")\n        if \"audio\" in self.modalities:\n            assert self.audio_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ audio, –Ω–æ audio_columns –ø—É—Å—Ç\"\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing: raise ValueError(f\"–ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –∞—É–¥–∏–æ: {missing}\")\n\n    def _validate_targets(self, df: pd.DataFrame):\n        miss = [c for c in self.target_column_names if c not in df.columns]\n        if miss:\n            raise ValueError(f\"–í DataFrame –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {miss}\")\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _build_model(self, num_outputs: int, image_cfg: Dict[str, Any], audio_cfg: Dict[str, Any]):\n        # –ê–≤—Ç–æ-—Ä–µ—Ü–µ–ø—Ç: CLIP –¥–ª—è text+image, CLAP –¥–ª—è audio (–∏–ª–∏ —è–≤–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏)\n        if self.backend_name == \"auto\":\n            # auto defaults\n            txt_type = \"clip\" if \"image\" in self.modalities else (\"auto\" if \"text\" in self.modalities else None)\n            img_type = \"clip\" if \"image\" in self.modalities else None\n            aud_type = \"clap\" if \"audio\" in self.modalities else None\n\n            txt_cp = self.clip_checkpoint if txt_type==\"clip\" else (self.text_model_config[\"checkpoint\"] if self.text_model_config else \"bert-base-multilingual-cased\")\n            img_cp = self.clip_checkpoint if img_type==\"clip\" else (self.image_model_config[\"checkpoint\"] if self.image_model_config else \"google/vit-base-patch16-224\")\n            aud_cp = self.clap_checkpoint if aud_type==\"clap\" else (self.audio_model_config[\"checkpoint\"] if self.audio_model_config else \"facebook/wav2vec2-base-960h\")\n\n            if aud_type is None and \"audio\" in self.modalities and self.audio_model_config:\n                aud_type = self.audio_model_config.get(\"model_type\",\"clap\")\n\n            processors, model = build_auto_regression(\n                modalities=self.modalities,\n                fusion=self.fusion,\n                text_model_type=\"clip\" if txt_type==\"clip\" else \"auto\",\n                text_checkpoint=txt_cp,\n                text_max_length=self.text_max_length,\n                text_padding=self.text_padding,\n                image_model_type=\"clip\" if img_type==\"clip\" else \"auto\",\n                image_checkpoint=img_cp,\n                max_images=image_cfg.get(\"max_images\", 1),\n                image_agg=image_cfg.get(\"image_agg\", \"concat\"),\n                audio_model_type=aud_type if aud_type else \"clap\",\n                audio_checkpoint=aud_cp,\n                audio_sr=audio_cfg.get(\"sr\", 48000),\n                max_audios=audio_cfg.get(\"max_audios\", 1),\n                audio_agg=audio_cfg.get(\"audio_agg\", \"concat\"),\n                out_dim=num_outputs,\n                hidden=256,\n                dropout=0.1,\n                cache_dir=self.local_cache_dir\n            )\n            return processors, model\n\n        # –ò–Ω–∞—á–µ ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Ñ–∞–±—Ä–∏–∫—É –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤ (–æ–ø—É—â–µ–Ω–æ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏)\n        raise NotImplementedError(\"–†—É—á–Ω–∞—è —Å–±–æ—Ä–∫–∞ (–Ω–µ auto) –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤ —ç—Ç–æ–π –≤–µ—Ä—Å–∏–∏\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"rmse\",       # \"rmse\" | \"mae\" | \"r2\"\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result_reg\",\n        seed: int = 42,\n        gradient_checkpointing: bool = False\n    ):\n        import os\n        from transformers import TrainingArguments\n    \n        # 1) –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞\n        self._validate_modalities(train_data)\n        self._validate_targets(train_data)\n        set_seed(seed)\n    \n        df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n        if test_data is not None:\n            self._validate_targets(test_data)\n    \n        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –≤ –æ–¥–Ω—É –≤–µ–∫—Ç–æ—Ä-–∫–æ–ª–æ–Ω–∫—É\n        df_train_ext = self._attach_target_vector(df_train, fill_zeros=False)\n        df_eval_ext  = self._attach_target_vector(df_eval,  fill_zeros=False)\n    \n        # 2) –ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã (auto-—Ä–µ—Ü–µ–ø—Ç)\n        image_cfg = {\"max_images\": 1, \"image_agg\": \"concat\"}\n        audio_cfg = {\"sr\": 48000, \"max_audios\": 1, \"audio_agg\": \"concat\"}\n    \n        self.processors, self.model = self._build_model(\n            num_outputs=len(self.target_column_names),\n            image_cfg=image_cfg,\n            audio_cfg=audio_cfg\n        )\n    \n        if gradient_checkpointing:\n            try:\n                self.model.gradient_checkpointing_enable()\n            except Exception:\n                pass\n    \n        # 3) Datasets (–±–µ–∑ —á–∞–Ω–∫–æ–≤)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        pretokenize_train = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_train_ext) <= self.max_pretokenize_samples)\n        pretokenize_eval  = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_eval_ext)  <= self.max_pretokenize_samples)\n    \n        ds_train = MultiModalDataset(\n            df=df_train_ext,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_train) else None,\n            pretokenize=pretokenize_train,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        ds_eval = MultiModalDataset(\n            df=df_eval_ext,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_eval) else None,\n            pretokenize=pretokenize_eval,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n    \n        collate = MultiModalCollator(self.processors, task=\"regression\")\n        compute_metrics = build_regression_metrics(metric_name)\n    \n        # 4) –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è (eval_strategy)\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.0,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=False\n        )\n    \n        # 5) Trainer –∏ —Å—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è\n        self.trainer = MSETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train,\n            eval_dataset=ds_eval,\n            data_collator=collate,\n            compute_metrics=compute_metrics\n        )\n    \n        try:\n            from transformers.trainer_callback import PrinterCallback\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n    \n        self.trainer.train()\n    \n        if has_bt:\n            try: self.processors[\"text\"].clear_cache()\n            except Exception: pass\n    \n        return self\n\n    def predict(self, df: pd.DataFrame, batch_size: Optional[int] = None) -> np.ndarray:\n        if self.trainer is None:\n            raise RuntimeError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ .fit().\")\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        collate = MultiModalCollator(self.processors, task=\"regression\")\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch - 1) // effective_batch\n        print(f\"Running predictions (batch_size={effective_batch}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds, metric_key_prefix=\"predict\")\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        ds.clear_cache()\n        y = preds.predictions\n        y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n        return y\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ .fit().\")\n        device = next(self.trainer.model.parameters()).device\n        self.model.to(device).eval()\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=False\n        )\n        collate = MultiModalCollator(self.processors, task=\"regression\")\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = batch[\"backend_inputs\"]\n                def move_to_device(obj):\n                    if torch.is_tensor(obj): return obj.to(device)\n                    if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n                    if isinstance(obj, list): return [move_to_device(v) for v in obj]\n                    return obj\n                bi = move_to_device(bi)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            print(f\"‚úì Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"‚úì Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"‚úì {m.capitalize()} embeddings shape: {arr.shape}\")\n        return fused_arr, per_mod\n\n# pipelines/classification.py\nimport os\nimport math\nimport gc\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import TrainingArguments\n\n# from mmkit.core.utils import set_seed\n# from mmkit.data.dataset import MultiModalDataset\n# from mmkit.data.collate import MultiModalCollator\n\n# from mmkit.train.trainer_hf import WeightedCETrainer, MultiLabelBCETrainer, ASLTrainer, FocalLossTrainer\n# from mmkit.train.callbacks import PbarConsoleLogger\n# from mmkit.train.metrics import build_multilabel_metrics, build_multilabel_metrics_all\n\n# from mmkit.aggregation.item_pool import ItemAggregator\n# from mmkit.aggregation.fusion import ConcatFusion, MeanFusion\n# from mmkit.heads.classification import ClassificationHead\n# from mmkit.model.multimodal import MultiModalModel\n\n# –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã\nfrom transformers import (\n    CLIPTokenizer,\n    CLIPImageProcessor,\n    AutoTokenizer,\n    AutoImageProcessor,\n    ClapProcessor,\n    AutoProcessor,\n)\n\n# from mmkit.processors.text import TextProcessor\n# from mmkit.processors.image import ImageProcessor\n# from mmkit.processors.audio import (\n#     ClapAudioProcessor,\n#     Wav2VecAudioProcessor,\n#     Wav2ClipAudioProcessor,\n# )\n\n# # –≠–Ω–∫–æ–¥–µ—Ä—ã\n# from mmkit.encoders.text_clip import CLIPTextEncoder\n# from mmkit.encoders.text_auto import AutoTextEncoder\n# from mmkit.encoders.image_clip import CLIPImageEncoder\n# from mmkit.encoders.image_auto import AutoImageEncoder\n# from mmkit.encoders.audio_clap import CLAPAudioEncoder\n# from mmkit.encoders.audio_wav2vec import Wav2Vec2Encoder\n# from mmkit.encoders.audio_wav2clip import Wav2ClipEncoder\n\n\nclass MultiModalClassificationPipeline:\n    \"\"\"\n    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:\n      - Single-label classification (–æ–±—ã—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –æ–¥–∏–Ω –∫–ª–∞—Å—Å –Ω–∞ –æ–±—Ä–∞–∑–µ—Ü)\n      - Multi-label classification (–º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –æ–±—Ä–∞–∑–µ—Ü)\n    \n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç text/image/audio, dynamic/max padding, –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é,\n    accuracy/f1 –º–µ—Ç—Ä–∏–∫–∏, class weights, predict –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n\n    –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é (backend=\"auto\"):\n      - –µ—Å–ª–∏ –µ—Å—Ç—å image: CLIP –¥–ª—è text –∏ image;\n      - –µ—Å–ª–∏ –µ—Å—Ç—å audio: CLAP (–∏–ª–∏ wav2vec/wav2clip –ø–æ –∂–µ–ª–∞–Ω–∏—é),\n      - –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: Auto (BERT) –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.\n\n    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n      - modalities: —Å–ø–∏—Å–æ–∫ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π —Å—Ä–µ–¥–∏ [\"text\",\"image\",\"audio\"]\n      - target_column_name: –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –º–µ—Ç–∫–∞–º–∏\n          * –¥–ª—è single-label: —Å—Ç—Ä–æ–∫–∏/–∏–Ω—Ç—ã (–æ–¥–Ω–∞ –º–µ—Ç–∫–∞ –Ω–∞ –æ–±—Ä–∞–∑–µ—Ü)\n          * –¥–ª—è multi-label: —Å–ø–∏—Å–∫–∏ –º–µ—Ç–æ–∫ –∏–ª–∏ –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã\n      - multi_label: True –¥–ª—è multi-label classification, False –¥–ª—è single-label (default: False)\n      - num_labels: —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤\n          * –¥–ª—è single-label: –µ—Å–ª–∏ None ‚Äî –≤–æ–∑—å–º—ë—Ç—Å—è –∏–∑ train_data –ø–æ unique\n          * –¥–ª—è multi-label: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑–∞—Ç—å –∏–ª–∏ –±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∏–∑ –¥–∞–Ω–Ω—ã—Ö\n      - label_names: —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è multi-label)\n      - trainer_type: —Ç–∏–ø —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è multi-label\n          * \"bce\" - BCEWithLogitsLoss (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)\n          * \"asl\" - Asymmetric Loss (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)\n          * \"focal\" - Focal Loss\n      - text_columns, image_columns, audio_columns: –∫–æ–ª–æ–Ω–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n      - fusion: \"concat\" | \"mean\" (–¥–ª—è mean —Ä–∞–∑–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å)\n      - text_padding: \"max_length\" | \"dynamic\"\n      - text_max_length: –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤\n      - pretokenize_data: –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (–≤—ã–∫–ª—é—á–∏—Ç—Å—è –ø—Ä–∏ dynamic padding)\n      - tokenizer_cache_size: LRU-–∫—ç—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n      - max_pretokenize_samples: –ª–∏–º–∏—Ç –Ω–∞ –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é\n      - local_cache_dir: –∫—ç—à HF\n\n      - audio_backend: \"clap\" | \"wav2vec\" | \"wav2clip\" (–¥–ª—è audio)\n      - audio_sr: sampling rate –¥–ª—è –∞—É–¥–∏–æ-–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤\n      - max_images_per_sample, image_agg: –∞–≥—Ä–µ–≥–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫ (\"concat\"/\"mean\")\n      - max_audios_per_sample, audio_agg: –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–ª—è –∞—É–¥–∏–æ\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        target_column_name: str,\n        multi_label: bool = False,\n        num_labels: Optional[int] = None,\n        label_names: Optional[List[str]] = None,\n        trainer_type: str = \"bce\",  # \"bce\" | \"asl\" | \"focal\" (–¥–ª—è multi-label) –∏–ª–∏ \"ce\" (–¥–ª—è single-label)\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n\n        # backend/–º–æ–¥–µ–ª–∏\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n\n        fusion: str = \"concat\",\n\n        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n        text_padding: str = \"max_length\",\n        text_max_length: int = 77,\n        use_batch_tokenizer: bool = True,\n        pretokenize_data: bool = True,\n        pretokenize_batch_size: int = 256,\n        tokenizer_cache_size: int = 10000,\n        max_pretokenize_samples: int = 100000,\n        local_cache_dir: str = \"./model_cache\",\n\n        # –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ/–∞—É–¥–∏–æ –∞–≥—Ä–µ–≥–∞—Ü–∏—è\n        max_images_per_sample: int = 1,\n        image_agg: str = \"concat\",\n        audio_backend: str = \"clap\",\n        audio_sr: int = 48000,\n        max_audios_per_sample: int = 1,\n        audio_agg: str = \"concat\",\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.target_column_name = target_column_name\n        self.multi_label = multi_label\n        self.num_labels = num_labels\n        self.label_names = label_names\n        self.trainer_type = trainer_type.lower()\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n\n        self.fusion = fusion\n        self.text_padding = text_padding\n        self.text_max_length = text_max_length\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.image_agg = image_agg\n        self.audio_backend = audio_backend\n        self.audio_sr = int(audio_sr)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n        self.audio_agg = audio_agg\n\n        # –í–∞–ª–∏–¥–∞—Ü–∏—è trainer_type\n        if self.multi_label:\n            valid_trainers = [\"bce\", \"asl\", \"focal\"]\n            if self.trainer_type not in valid_trainers:\n                raise ValueError(f\"–î–ª—è multi_label=True trainer_type –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–Ω–∏–º –∏–∑ {valid_trainers}, –ø–æ–ª—É—á–µ–Ω '{self.trainer_type}'\")\n        else:\n            valid_trainers = [\"ce\", \"weighted_ce\"]\n            if self.trainer_type not in valid_trainers:\n                print(f\"Warning: –¥–ª—è single-label —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è trainer_type='ce' –∏–ª–∏ 'weighted_ce', –ø–æ–ª—É—á–µ–Ω '{self.trainer_type}'. –ò—Å–ø–æ–ª—å–∑—É–µ–º 'weighted_ce'.\")\n                self.trainer_type = \"weighted_ce\"\n\n        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø–æ–ª—è\n        self.label2id: Optional[Dict[Any, int]] = None\n        self.id2label: Optional[Dict[int, str]] = None\n        self.processors: Dict[str, Any] = {}\n        self.model: Optional[MultiModalModel] = None\n        self.trainer: Optional[Union[WeightedCETrainer, MultiLabelBCETrainer, ASLTrainer, FocalLossTrainer]] = None\n        \n        # –î–ª—è multi-label\n        self._target_vec_col = \"__target_vector__\"\n\n    # --------------------------\n    # –í–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –ø–æ–¥—Å–æ–±–∫–∏\n    # --------------------------\n    def _validate_modalities(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            assert self.text_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ text, –Ω–æ text_columns –ø—É—Å—Ç–æ–π\"\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {missing}\")\n        if \"image\" in self.modalities:\n            assert self.image_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ image, –Ω–æ image_columns –ø—É—Å—Ç\"\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"–ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {missing}\")\n        if \"audio\" in self.modalities:\n            assert self.audio_columns, \"–í—ã –≤—ã–±—Ä–∞–ª–∏ audio, –Ω–æ audio_columns –ø—É—Å—Ç\"\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"–ù–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –∞—É–¥–∏–æ: {missing}\")\n\n    def _ensure_label_mapping_single(self, df_train: pd.DataFrame):\n        \"\"\"–î–ª—è single-label classification\"\"\"\n        classes = sorted(df_train[self.target_column_name].unique().tolist())\n        if self.num_labels is None:\n            self.num_labels = len(classes)\n        elif self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n    def _ensure_label_mapping_multi(self, df_train: pd.DataFrame):\n        \"\"\"–î–ª—è multi-label classification\"\"\"\n        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n        all_labels = set()\n        for val in df_train[self.target_column_name]:\n            if isinstance(val, (list, tuple, set)):\n                all_labels.update(val)\n            elif isinstance(val, str):\n                # –ú–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –∑–∞–ø—è—Ç—ã–º–∏\n                all_labels.update([v.strip() for v in val.split(',')])\n        \n        all_labels = sorted(list(all_labels))\n        \n        if self.label_names is not None:\n            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –∏–º–µ–Ω–∞\n            if self.num_labels is None:\n                self.num_labels = len(self.label_names)\n            self.label2id = {label: i for i, label in enumerate(self.label_names)}\n        else:\n            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏\n            if self.num_labels is None:\n                self.num_labels = len(all_labels)\n            self.label2id = {label: i for i, label in enumerate(all_labels)}\n        \n        self.id2label = {i: str(label) for label, i in self.label2id.items()}\n\n    def _attach_target_vector_multi(self, df: pd.DataFrame, fill_zeros: bool = False) -> pd.DataFrame:\n        \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç multi-label –º–µ—Ç–∫–∏ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã\"\"\"\n        df_c = df.copy()\n        K = self.num_labels\n        \n        if fill_zeros:\n            df_c[self._target_vec_col] = [np.zeros(K, dtype=np.float32) for _ in range(len(df_c))]\n        else:\n            def _row_to_vec(row):\n                vec = np.zeros(K, dtype=np.float32)\n                val = row[self.target_column_name]\n                labels = []\n                \n                if isinstance(val, (list, tuple, set)):\n                    labels = list(val)\n                elif isinstance(val, str):\n                    labels = [v.strip() for v in val.split(',')]\n                elif isinstance(val, np.ndarray):\n                    # –£–∂–µ –±–∏–Ω–∞—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä\n                    return val.astype(np.float32)\n                \n                for label in labels:\n                    if label in self.label2id:\n                        vec[self.label2id[label]] = 1.0\n                \n                return vec\n            \n            df_c[self._target_vec_col] = df_c.apply(_row_to_vec, axis=1)\n        \n        return df_c\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _ensure_label_column(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"–î–ª—è inference: –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—É—é –∫–æ–ª–æ–Ω–∫—É –º–µ—Ç–æ–∫ –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç\"\"\"\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            if self.multi_label:\n                # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫\n                df_c[self.target_column_name] = [[] for _ in range(len(df_c))]\n            else:\n                # –ü–µ—Ä–≤–∞—è –∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–µ—Ç–∫–∞\n                fake_label = next(iter(self.label2id.keys()))\n                df_c[self.target_column_name] = [fake_label] * len(df_c)\n        return df_c\n\n    # --------------------------\n    # –ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã (auto)\n    # --------------------------\n    def _build_model_and_processors(self) -> Tuple[Dict[str, Any], MultiModalModel]:\n        encoders = {}\n        processors = {}\n\n        # text\n        if \"text\" in self.modalities:\n            use_clip_text = (\"image\" in self.modalities)\n            if use_clip_text:\n                tok = CLIPTokenizer.from_pretrained(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n                processors[\"text\"] = TextProcessor(\n                    tok, max_length=self.text_max_length, padding=self.text_padding,\n                    cache_size=self.tokenizer_cache_size, batch_size=self.pretokenize_batch_size\n                )\n                encoders[\"text\"] = CLIPTextEncoder(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n            else:\n                tok = AutoTokenizer.from_pretrained(\n                    (self.text_model_config or {}).get(\"checkpoint\", \"bert-base-multilingual-cased\"),\n                    cache_dir=self.local_cache_dir\n                )\n                processors[\"text\"] = TextProcessor(\n                    tok, max_length=self.text_max_length, padding=self.text_padding,\n                    cache_size=self.tokenizer_cache_size, batch_size=self.pretokenize_batch_size\n                )\n                encoders[\"text\"] = AutoTextEncoder(\n                    (self.text_model_config or {}).get(\"checkpoint\", \"bert-base-multilingual-cased\"),\n                    cache_dir=self.local_cache_dir\n                )\n\n        # image\n        image_cfg = None\n        if \"image\" in self.modalities:\n            if (self.image_model_config or {}).get(\"model_type\", \"clip\") == \"clip\":\n                ip = CLIPImageProcessor.from_pretrained(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n                processors[\"image\"] = ImageProcessor(ip)\n                encoders[\"image\"] = CLIPImageEncoder(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n            else:\n                cp = (self.image_model_config or {}).get(\"checkpoint\", \"google/vit-base-patch16-224\")\n                ip = AutoImageProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n                processors[\"image\"] = ImageProcessor(ip)\n                encoders[\"image\"] = AutoImageEncoder(cp, cache_dir=self.local_cache_dir)\n            image_cfg = {\"max_items\": self.max_images_per_sample, \"how\": self.image_agg}\n\n        # audio\n        audio_cfg = None\n        if \"audio\" in self.modalities:\n            ab = (self.audio_model_config or {}).get(\"model_type\", self.audio_backend)\n            if ab == \"clap\":\n                cp = (self.audio_model_config or {}).get(\"checkpoint\", self.clap_checkpoint)\n                ap = ClapProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n                processors[\"audio\"] = ClapAudioProcessor(ap, sr=self.audio_sr)\n                encoders[\"audio\"] = CLAPAudioEncoder(cp, cache_dir=self.local_cache_dir)\n            elif ab == \"wav2vec\":\n                cp = (self.audio_model_config or {}).get(\"checkpoint\", \"facebook/wav2vec2-base-960h\")\n                ap = AutoProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n                processors[\"audio\"] = Wav2VecAudioProcessor(ap, sr=self.audio_sr)\n                encoders[\"audio\"] = Wav2Vec2Encoder(cp, cache_dir=self.local_cache_dir)\n            else:  # wav2clip\n                processors[\"audio\"] = Wav2ClipAudioProcessor(sr=self.audio_sr)\n                encoders[\"audio\"] = Wav2ClipEncoder()\n            audio_cfg = {\"max_items\": self.max_audios_per_sample, \"how\": self.audio_agg}\n\n        # Fusion + Head\n        agg = ItemAggregator()\n        fusion_mod = ConcatFusion() if self.fusion == \"concat\" else MeanFusion()\n        dims = [encoders[m].embed_dim for m in [\"image\", \"text\", \"audio\"] if m in encoders]\n        if self.fusion == \"concat\":\n            in_dim = sum(dims)\n        else:\n            assert len(set(dims)) == 1, \"–î–ª—è fusion='mean' —Ä–∞–∑–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å\"\n            in_dim = dims[0]\n        head = ClassificationHead(in_dim=in_dim, num_labels=self.num_labels, hidden=512, dropout=0.1)\n        model = MultiModalModel(encoders, agg, fusion_mod, head, image_cfg=image_cfg, audio_cfg=audio_cfg)\n        return processors, model\n\n    # --------------------------\n    # –ú–µ—Ç—Ä–∏–∫–∏\n    # --------------------------\n    def _build_compute_metrics(self, metric_name: str):\n        if self.multi_label:\n            # Multi-label –º–µ—Ç—Ä–∏–∫–∏\n            if metric_name == \"all\":\n                return build_multilabel_metrics_all()\n            else:\n                return build_multilabel_metrics(metric_name)\n        else:\n            # Single-label –º–µ—Ç—Ä–∏–∫–∏\n            metric_name = metric_name.lower()\n            if metric_name == \"f1\":\n                try:\n                    import evaluate\n                    f1_metric = evaluate.load(\"f1\")\n                    def compute(p):\n                        preds = p.predictions.argmax(-1)\n                        return f1_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\")\n                    return compute\n                except Exception:\n                    print(\"evaluate –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é accuracy\")\n                    metric_name = \"accuracy\"\n\n            if metric_name == \"accuracy\":\n                try:\n                    import evaluate\n                    acc = evaluate.load(\"accuracy\")\n                    def compute(p):\n                        preds = p.predictions.argmax(-1)\n                        return acc.compute(predictions=preds, references=p.label_ids)\n                    return compute\n                except Exception:\n                    def compute(p):\n                        preds = p.predictions.argmax(-1)\n                        y = p.label_ids\n                        return {\"accuracy\": float(np.mean(preds == y))}\n                    return compute\n\n            raise ValueError('metric_name –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å \"f1\" –∏–ª–∏ \"accuracy\"')\n\n    # --------------------------\n    # –û–±—É—á–µ–Ω–∏–µ\n    # --------------------------\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"accuracy\",   # single-label: \"accuracy\" | \"f1\"; multi-label: \"f1_micro\" | \"f1_macro\" | \"f1_samples\" | \"accuracy\" | \"all\"\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result_cls\",\n        seed: int = 42,\n        gradient_checkpointing: bool = False,\n        class_weights: Optional[np.ndarray] = None,\n        pos_weight: Optional[np.ndarray] = None,  # –¥–ª—è BCE/Focal: –≤–µ—Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n        # ASL –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n        asl_gamma_neg: float = 4.0,\n        asl_gamma_pos: float = 0.0,\n        asl_clip: float = 0.05,\n        # Focal Loss –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n        focal_alpha: float = 0.25,\n        focal_gamma: float = 2.0,\n    ):\n        import os\n        from transformers import TrainingArguments\n    \n        # 1) –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞\n        self._validate_modalities(train_data)\n        set_seed(seed)\n    \n        # train/val split\n        df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n    \n        # labels mapping\n        if self.multi_label:\n            self._ensure_label_mapping_multi(df_train)\n            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä—ã\n            df_train_ext = self._attach_target_vector_multi(df_train, fill_zeros=False)\n            df_eval_ext = self._attach_target_vector_multi(df_eval, fill_zeros=False)\n            target_col = self._target_vec_col\n            task = \"regression\"  # –¥–ª—è multi-label –∏—Å–ø–æ–ª—å–∑—É–µ–º task=\"regression\" –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ (—Ç.–∫. –º–µ—Ç–∫–∏ - float –≤–µ–∫—Ç–æ—Ä—ã)\n            \n            # –°—á–∏—Ç–∞–µ–º pos_weight –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω (–¥–ª—è BCE/Focal)\n            if pos_weight is None and self.trainer_type in [\"bce\", \"focal\"]:\n                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞: pos_weight = neg_count / pos_count\n                labels_matrix = np.vstack([df_train_ext[target_col].iloc[i] for i in range(len(df_train_ext))])\n                pos_counts = labels_matrix.sum(axis=0)\n                neg_counts = len(labels_matrix) - pos_counts\n                pos_weight = np.zeros(self.num_labels, dtype=np.float32)\n                for i in range(self.num_labels):\n                    if pos_counts[i] > 0:\n                        pos_weight[i] = neg_counts[i] / pos_counts[i]\n                    else:\n                        pos_weight[i] = 1.0\n        else:\n            self._ensure_label_mapping_single(df_train)\n            df_train_ext = df_train\n            df_eval_ext = df_eval\n            target_col = self.target_column_name\n            task = \"classification\"\n            \n            # class weights –¥–ª—è single-label\n            if class_weights is None:\n                y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n                counts = np.bincount(y_train_all, minlength=self.num_labels)\n                n_all = counts.sum()\n                cw = np.zeros(self.num_labels, dtype=np.float32)\n                nz = counts > 0\n                cw[nz] = n_all / (self.num_labels * counts[nz].astype(np.float32))\n                class_weights = cw\n    \n        # 2) –ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã (auto)\n        self.processors, self.model = self._build_model_and_processors()\n        if gradient_checkpointing:\n            try:\n                self.model.gradient_checkpointing_enable()\n            except Exception:\n                pass\n    \n        # 3) Datasets\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        pretokenize_train = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_train_ext) <= self.max_pretokenize_samples)\n        pretokenize_eval  = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_eval_ext)  <= self.max_pretokenize_samples)\n    \n        ds_train = MultiModalDataset(\n            df=df_train_ext,\n            target_col=target_col,\n            task=task,\n            label2id=self.label2id if not self.multi_label else None,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_train) else None,\n            pretokenize=pretokenize_train,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        ds_eval = MultiModalDataset(\n            df=df_eval_ext,\n            target_col=target_col,\n            task=task,\n            label2id=self.label2id if not self.multi_label else None,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_eval) else None,\n            pretokenize=pretokenize_eval,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n    \n        collate = MultiModalCollator(self.processors, task=task)\n        compute_metrics = self._build_compute_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name if metric_name != 'all' else 'f1_micro'}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=False\n        )\n    \n        # 5) –í—ã–±–æ—Ä Trainer –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç trainer_type\n        if self.multi_label:\n            if self.trainer_type == \"asl\":\n                print(f\"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ASL Trainer (gamma_neg={asl_gamma_neg}, gamma_pos={asl_gamma_pos}, clip={asl_clip})\")\n                self.trainer = ASLTrainer(\n                    model=self.model,\n                    args=args,\n                    train_dataset=ds_train,\n                    eval_dataset=ds_eval,\n                    data_collator=collate,\n                    compute_metrics=compute_metrics,\n                    gamma_neg=asl_gamma_neg,\n                    gamma_pos=asl_gamma_pos,\n                    clip=asl_clip\n                )\n            elif self.trainer_type == \"focal\":\n                print(f\"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Focal Loss Trainer (alpha={focal_alpha}, gamma={focal_gamma})\")\n                self.trainer = FocalLossTrainer(\n                    model=self.model,\n                    args=args,\n                    train_dataset=ds_train,\n                    eval_dataset=ds_eval,\n                    data_collator=collate,\n                    compute_metrics=compute_metrics,\n                    alpha=focal_alpha,\n                    gamma=focal_gamma\n                )\n            else:  # bce\n                print(f\"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è BCE Trainer\")\n                self.trainer = MultiLabelBCETrainer(\n                    model=self.model,\n                    args=args,\n                    train_dataset=ds_train,\n                    eval_dataset=ds_eval,\n                    data_collator=collate,\n                    compute_metrics=compute_metrics,\n                    class_weights=class_weights,\n                    pos_weight=pos_weight\n                )\n        else:\n            print(f\"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Weighted CE Trainer\")\n            self.trainer = WeightedCETrainer(\n                model=self.model,\n                args=args,\n                train_dataset=ds_train,\n                eval_dataset=ds_eval,\n                data_collator=collate,\n                compute_metrics=compute_metrics,\n                class_weights=class_weights\n            )\n    \n        try:\n            from transformers.trainer_callback import PrinterCallback\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n    \n        self.trainer.train()\n    \n        if has_bt:\n            try: self.processors[\"text\"].clear_cache()\n            except Exception: pass\n    \n        return self\n\n    # --------------------------\n    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n    # --------------------------\n    def predict(\n        self,\n        df: pd.DataFrame,\n        return_label_str: bool = False,\n        return_proba: bool = False,\n        batch_size: Optional[int] = None,\n        threshold: float = 0.5  # –¥–ª—è multi-label: –ø–æ—Ä–æ–≥ –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏\n    ) -> np.ndarray:\n        \"\"\"\n        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è single-label –∏–ª–∏ multi-label.\n        \n        –î–ª—è single-label:\n            - return_proba=False: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (int –∏–ª–∏ str)\n            - return_proba=True: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ [N, num_classes]\n        \n        –î–ª—è multi-label:\n            - return_proba=False: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ [N, num_classes] (0/1)\n            - return_proba=True: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ [N, num_classes] (sigmoid)\n            - threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.5)\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ .fit().\")\n\n        if self.multi_label:\n            df_c = self._attach_target_vector_multi(self._ensure_label_column(df), fill_zeros=True)\n            target_col = self._target_vec_col\n            task = \"regression\"\n        else:\n            df_c = self._ensure_label_column(df)\n            target_col = self.target_column_name\n            task = \"classification\"\n\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=target_col,\n            task=task,\n            label2id=self.label2id if not self.multi_label else None,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        collate = MultiModalCollator(self.processors, task=task)\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n        print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds, metric_key_prefix=\"predict\")\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        ds.clear_cache()\n\n        logits = preds.predictions  # [N, num_classes]\n\n        if self.multi_label:\n            # Multi-label classification\n            # –ü—Ä–∏–º–µ–Ω—è–µ–º sigmoid\n            probs = 1 / (1 + np.exp(-logits))\n            \n            if return_proba:\n                return probs\n            else:\n                # –ë–∏–Ω–∞—Ä–∏–∑—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É\n                binary_preds = (probs >= threshold).astype(int)\n                return binary_preds\n        else:\n            # Single-label classification\n            if return_proba:\n                exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n                probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n                return probabilities\n\n            y_pred = np.argmax(logits, axis=-1)\n            if return_label_str:\n                return np.array([self.id2label[int(i)] for i in y_pred])\n            return y_pred\n\n    # --------------------------\n    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏\n    # --------------------------\n    def get_embeddings(\n        self,\n        df: pd.DataFrame,\n        batch_size: int = 32,\n        return_per_modality: bool = False\n    ):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –í—ã–∑–æ–≤–∏—Ç–µ .fit().\")\n\n        device = next(self.trainer.model.parameters()).device\n        self.model.to(device).eval()\n\n        if self.multi_label:\n            df_c = self._attach_target_vector_multi(self._ensure_label_column(df), fill_zeros=True)\n            target_col = self._target_vec_col\n            task = \"regression\"\n        else:\n            df_c = self._ensure_label_column(df)\n            target_col = self.target_column_name\n            task = \"classification\"\n\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=target_col,\n            task=task,\n            label2id=self.label2id if not self.multi_label else None,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=False\n        )\n        collate = MultiModalCollator(self.processors, task=task)\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = batch[\"backend_inputs\"]\n\n                def move_to_device(obj):\n                    if torch.is_tensor(obj): return obj.to(device)\n                    if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n                    if isinstance(obj, list): return [move_to_device(v) for v in obj]\n                    return obj\n\n                bi = move_to_device(bi)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            print(f\"‚úì Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"‚úì Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"‚úì {m.capitalize()} embeddings shape: {arr.shape}\")\n\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ultralytics\n\nimport os\nimport cv2\nimport json\nimport shutil\nimport tempfile\nimport warnings\nimport random\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom ultralytics import YOLO\nfrom ultralytics.utils.downloads import attempt_download_asset\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import KFold\n\n\nclass YOLODetectionPipeline:\n    \"\"\"\n    YOLO-–ø–∞–π–ø–ª–∞–π–Ω –∏–∑ pandas DataFrame —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–¥—Å—á—ë—Ç–∞\n    (plain –∏–ª–∏ area-gated) –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª—å—é (Ridge –ø–æ —Ä–µ–∑–∏–¥—É–∞–ª—É).\n\n    –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (DataFrame):\n      - image_path (str): –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (jpg/png)\n      - boxes_col: GT-–±–æ–∫—Å—ã –≤ YOLO-–Ω–æ—Ä–º–∏—Ä–æ–≤–∫–µ [0..1] (—Å–ø–∏—Å–∫–∏/–º–∞—Å—Å–∏–≤—ã/—Å—Ç—Ä–æ–∫–∞)\n\n    –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π:\n      - fit(): –æ–±—É—á–∞–µ—Ç YOLO; (–æ–ø—Ü.) —Ç—é–Ω–∏–Ω–≥ –ø–æ—Ä–æ–≥–æ–≤ (plain/area-gated + iou/max_det);\n               (–æ–ø—Ü.) –∫–∞–ª–∏–±—Ä—É–µ—Ç Ridge; (–æ–ø—Ü.) –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç RMSE/MAE.\n      - predict(): –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ (boxes_json + count). –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ù–ï –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è.\n      - predict_counts(): –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–π –ø–æ–¥—Å—á—ë—Ç; –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ª—É—á—à–∏–µ –ø–æ—Ä–æ–≥–∏/–∫–∞–ª–∏–±—Ä–æ–≤–∫—É, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã.\n\n    –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–µ–º —Ç—é–Ω–∏–Ω–≥–∞:\n      - tune_val_subsample: –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (int ‚Äî –∫–æ–ª-–≤–æ, float ‚Äî –¥–æ–ª—è 0..1).\n      - tune_max_combinations: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —á–∏—Å–ª–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –∫–æ–º–±–æ (—Å–ª—É—á–∞–π–Ω–æ –∏–∑ —Å–µ—Ç–∫–∏).\n    \"\"\"\n\n    def __init__(self,\n                 model_ckpt: str = \"yolov8n.pt\",\n                 data_root: str | None = None,\n                 image_col: str = \"image_path\",\n                 boxes_col: str = \"boxes\",\n                 class_names: list[str] | None = None,\n                 use_symlinks: bool = True,\n                 verbose: bool = True,\n                 # –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª–∏\n                 enable_tuning: bool = True,\n                 enable_ridge: bool = True,\n                 validate_count: bool = True,\n                 # —Ä–µ–∂–∏–º —Ç—é–Ω–∏–Ω–≥–∞ plain vs area-gated\n                 enable_area_gate: bool = True,        # True ‚Üí —Ç—é–Ω–∏–º (conf_small, conf_big, area_thr) + (iou, max_det)\n                 enable_tta_flip: bool = False,        # True ‚Üí TTA flip –ø—Ä–∏ plain-–ø–æ–¥—Å—á—ë—Ç–µ\n                 # —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–µ–º —Ç—é–Ω–∏–Ω–≥–∞\n                 tune_val_subsample: int | float | None = None,  # int=–∫–æ–ª-–≤–æ; float=–¥–æ–ª—è [0..1]\n                 tune_max_combinations: int | None = 100,\n                 random_state: int = 42,\n                 # —Å–µ—Ç–∫–∏ –¥–ª—è plain-—Ç—é–Ω–∏–Ω–≥–∞\n                 tune_conf_grid = (0.20, 0.25, 0.30, 0.35),\n                 tune_iou_grid  = (0.55, 0.60),\n                 tune_max_det_grid = (300, 600),\n                 # —Å–µ—Ç–∫–∏ –¥–ª—è area-gated (–ø–æ–¥ –º–µ–ª–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã –∏–∑ –≤–∞—à–∏—Ö —Å—Ç–∞—Ç)\n                 tune_conf_small_grid = (0.10, 0.12, 0.14, 0.18),\n                 tune_conf_big_grid   = (0.30, 0.40, 0.50),\n                 tune_area_thr_grid   = (0.0008, 0.0010, 0.0012, 0.0015),\n                 gate_conf_base: float = 0.07,  # –±–∞–∑–æ–≤—ã–π conf –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø—Ä–∏ area-gated\n                 # —Å–µ—Ç–∫–∞ –¥–ª—è Ridge\n                 ridge_alpha_grid = (0.3, 1.0, 3.0),\n                 # –ø–æ—Ä–æ–≥–∏ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–ª–æ—â–∞–¥–∏ –¥–ª—è —Ñ–∏—á Ridge/plain\n                 small_thr: float = 0.0010,\n                 big_thr: float   = 0.003):\n        self.model_ckpt = model_ckpt\n        self.image_col = image_col\n        self.boxes_col = boxes_col\n        self.class_names = class_names or [\"obj\"]\n        self.use_symlinks = use_symlinks\n        self.verbose = verbose\n\n        self.enable_tuning = enable_tuning\n        self.enable_ridge = enable_ridge\n        self.validate_count = validate_count\n        self.enable_area_gate = enable_area_gate\n        self.enable_tta_flip = enable_tta_flip\n\n        self.tune_val_subsample = tune_val_subsample\n        self.tune_max_combinations = tune_max_combinations\n        self.random_state = random_state\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n        # —Ä–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞\n        self._tmpdir_owned = False\n        if data_root is None:\n            self.data_root = tempfile.mkdtemp(prefix=\"yolo_ds_\")\n            self._tmpdir_owned = True\n        else:\n            self.data_root = os.path.abspath(data_root)\n            os.makedirs(self.data_root, exist_ok=True)\n\n        self.dataset_yaml = os.path.join(self.data_root, \"dataset.yaml\")\n        self.model_path = None\n        self._model = None\n        self._device = None  # —Å—Ç—Ä–æ–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–∏ fit()\n\n        # —Å–µ—Ç–∫–∏ –∏ –ø–æ—Ä–æ–≥–∏\n        self.tune_conf_grid = tuple(float(x) for x in tune_conf_grid)\n        self.tune_iou_grid  = tuple(float(x) for x in tune_iou_grid)\n        self.tune_max_det_grid = tuple(int(x) for x in tune_max_det_grid)\n\n        self.tune_conf_small_grid = tuple(float(x) for x in tune_conf_small_grid)\n        self.tune_conf_big_grid   = tuple(float(x) for x in tune_conf_big_grid)\n        self.tune_area_thr_grid   = tuple(float(x) for x in tune_area_thr_grid)\n        self.gate_conf_base = float(gate_conf_base)\n\n        self.ridge_alpha_grid = tuple(float(x) for x in ridge_alpha_grid)\n        self.small_thr = float(small_thr)\n        self.big_thr   = float(big_thr)\n\n        # —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—é–Ω–∏–Ω–≥–∞/–∫–∞–ª–∏–±—Ä–æ–≤–∫–∏\n        self.calib_ = dict(\n            # plain —Ä–µ–∂–∏–º:\n            best_conf=None, best_iou=None, best_max_det=None,\n            # area-gated:\n            gate_conf_small=None, gate_conf_big=None, gate_area_thr=None, gate_conf_base=None,\n            # Ridge:\n            ridge_alpha=None, ridge_model=None, ridge_mu=None, ridge_sd=None,\n            # –æ–±—â–∏–π:\n            imgsz=None\n        )\n\n    # -------------------- device helpers --------------------\n    @staticmethod\n    def _resolve_device(device: str | int | None) -> str:\n        \"\"\"\n        –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:\n          - None / \"auto\": \"0,1,...,N-1\" –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ CUDA, –∏–Ω–∞—á–µ \"cpu\"\n          - –∏–Ω–∞—á–µ –≤–µ—Ä–Ω—É—Ç—å —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –µ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"0\" –∏–ª–∏ \"cpu\")\n        \"\"\"\n        if device is None or str(device).lower() == \"auto\":\n            if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n                n = torch.cuda.device_count()\n                return \",\".join(str(i) for i in range(n))\n            return \"cpu\"\n        return str(device)\n\n    def _infer_device(self) -> str:\n        \"\"\"\n        –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞/—Ç—é–Ω–∏–Ω–≥–∞:\n          - –µ—Å–ª–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ '0,1,...' ‚Üí –±–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –∫–∞—Ä—Ç—É '0'\n          - –µ—Å–ª–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ 'k' ‚Üí –µ—ë –∂–µ\n          - –∏–Ω–∞—á–µ –∞–≤—Ç–æ: '0' –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ CUDA, 'cpu' –±–µ–∑ GPU\n        \"\"\"\n        if getattr(self, \"_device\", None):\n            if isinstance(self._device, str) and \",\" in self._device:\n                return self._device.split(\",\")[0]\n            return self._device\n        return \"0\" if (torch.cuda.is_available() and torch.cuda.device_count() > 0) else \"cpu\"\n\n    # -------------------- helpers: —Ä–∞–∑–º–µ—Ç–∫–∞ ‚Üí YOLO-—Çxt --------------------\n    @staticmethod\n    def _is_nan_like(x):\n        if x is None: return True\n        if isinstance(x, float) and np.isnan(x): return True\n        if isinstance(x, str) and x.strip()==\"\": return True\n        return False\n\n    def _parse_boxes(self, row):\n        boxes_raw = row[self.boxes_col] if self.boxes_col in row else None\n        if self._is_nan_like(boxes_raw): return []\n        out = []\n        if isinstance(boxes_raw, (list, tuple, np.ndarray)):\n            for it in boxes_raw:\n                vals = list(map(float, it))\n                if len(vals) >= 4:\n                    x,y,w,h = vals[:4]\n                    if 0 <= x <= 1 and 0 <= y <= 1 and 0 < w <= 1 and 0 < h <= 1:\n                        out.append((0, x,y,w,h))\n        elif isinstance(boxes_raw, str):\n            lines = [ln.strip() for ln in boxes_raw.strip().splitlines() if ln.strip()]\n            for ln in lines:\n                parts = ln.split()\n                vals = list(map(float, parts))\n                if len(vals) == 4:\n                    x,y,w,h = vals\n                    out.append((0, x,y,w,h))\n                elif len(vals) >= 5:\n                    cls,x,y,w,h = int(vals[0]), *vals[1:5]\n                    out.append((cls, float(x),float(y),float(w),float(h)))\n        return out\n\n    def _link_or_copy(self, src, dst):\n        os.makedirs(os.path.dirname(dst), exist_ok=True)\n        if self.use_symlinks:\n            try:\n                if os.path.lexists(dst): os.remove(dst)\n                os.symlink(os.path.abspath(src), dst)\n                return\n            except Exception:\n                pass\n        shutil.copy2(src, dst)\n\n    def _write_label_file(self, label_path, boxes):\n        os.makedirs(os.path.dirname(label_path), exist_ok=True)\n        with open(label_path, \"w\", encoding=\"utf-8\") as f:\n            for cls, x,y,w,h in boxes:\n                f.write(f\"{int(cls)} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n\n    def _materialize(self, train_df, val_df, train_split=\"train\", val_split=\"val\"):\n        for split_name, df in [(train_split, train_df), (val_split, val_df)]:\n            img_dir = os.path.join(self.data_root, \"images\", split_name)\n            lbl_dir = os.path.join(self.data_root, \"labels\", split_name)\n            os.makedirs(img_dir, exist_ok=True); os.makedirs(lbl_dir, exist_ok=True)\n            it = df.iterrows()\n            if self.verbose: it = tqdm(it, total=len(df), desc=f\"[build] {split_name}\")\n            for _, row in it:\n                src = row[self.image_col]\n                if not os.path.exists(src):\n                    raise FileNotFoundError(f\"Image not found: {src}\")\n                fname = os.path.basename(src)\n                stem, _ = os.path.splitext(fname)\n                self._link_or_copy(src, os.path.join(img_dir, fname))\n                self._write_label_file(os.path.join(lbl_dir, stem + \".txt\"), self._parse_boxes(row))\n\n        with open(self.dataset_yaml, \"w\", encoding=\"utf-8\") as f:\n            f.write(f\"path: {self.data_root}\\ntrain: images/{train_split}\\nval: images/{val_split}\\nnames:\\n\")\n            for i, name in enumerate(self.class_names):\n                f.write(f\"  {i}: {name}\\n\")\n\n    # -------------------- fit: train + (tune/ridge/validate) --------------------\n    def fit(self,\n            train_df: pd.DataFrame,\n            val_df: pd.DataFrame | None = None,\n            test_size: float = 0.2,\n            epochs: int = 50,\n            imgsz: int = 640,\n            batch: int = 16,\n            device: str | int | None = \"auto\",\n            workers: int = 4,\n            patience: int = 50,\n            optimizer: str = \"auto\",\n            augment: bool = True,\n            seed: int = 42,\n            close_mosaic: int | None = 10,\n            cos_lr: bool = True,\n            rect: bool = False,\n            iou: float = 0.7,\n            **extra_train_kwargs):\n\n        # –≤—ã–±—Ä–∞—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å\n        self._device = self._resolve_device(device)\n        if self.verbose:\n            print(f\"[device] training device='{self._device}'\")\n\n        np.random.seed(seed); random.seed(seed)\n\n        # –µ—Å–ª–∏ val_df –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –¥–µ–ª–∞–µ–º –ø—Ä–æ—Å—Ç—É—é —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ –±–∏–Ω–∞–º count\n        if val_df is None:\n            tmp = train_df.copy()\n            counts = [len(self._parse_boxes(r)) for _, r in tmp.iterrows()]\n            tmp[\"_bins\"] = np.clip((np.array(counts)//5).astype(int), 0, 50)\n            val_mask = tmp.groupby(\"_bins\", group_keys=False).apply(\n                lambda g: g.sample(frac=test_size, random_state=seed)).index\n            val_df = train_df.loc[val_mask]\n            train_df = train_df.drop(index=val_mask)\n            train_df = train_df.reset_index(drop=True); val_df = val_df.reset_index(drop=True)\n\n        self._materialize(train_df, val_df)\n\n        # –∑–∞–≥—Ä—É–∑–∏—Ç—å/—Å–∫–∞—á–∞—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç\n        if not os.path.exists(self.model_ckpt) and self.model_ckpt.endswith(\".pt\"):\n            try:\n                if self.verbose: print(f\"Checkpoint '{self.model_ckpt}' not found. Attempting to download...\")\n                attempt_download_asset(self.model_ckpt)\n            except Exception as e:\n                raise FileNotFoundError(f\"Failed to download '{self.model_ckpt}'. Error: {e}\")\n\n        # train args\n        train_args = {\n            'data': self.dataset_yaml, 'epochs': epochs, 'imgsz': imgsz, 'batch': batch,\n            'device': self._device, 'workers': workers, 'patience': patience, 'optimizer': optimizer,\n            'augment': augment, 'seed': seed, 'close_mosaic': close_mosaic, 'cos_lr': cos_lr,\n            'rect': rect, 'iou': iou, 'verbose': self.verbose\n        }\n        train_args.update(extra_train_kwargs)\n\n        # –æ–±—É—á–µ–Ω–∏–µ\n        model = YOLO(self.model_ckpt)\n        model.train(**train_args)\n\n        # –ì–ê–†–ê–ù–¢–ò–†–û–í–ê–ù–ù–û –±–µ—Ä—ë–º –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç\n        best_path = None\n        if hasattr(model, \"trainer\") and getattr(model.trainer, \"best\", None):\n            best_path = str(model.trainer.best)     # .../runs/detect/exp/weights/best.pt\n        elif getattr(model, \"ckpt_path\", None):\n            best_path = str(model.ckpt_path)\n        else:\n            best_path = self.model_ckpt\n        self.model_path = best_path\n        if self.verbose:\n            print(f\"[fit] best model: {self.model_path}\")\n\n        # —Ç—é–Ω–∏–Ω–≥/–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞/–≤–∞–ª–∏–¥–∞—Ü–∏—è\n        try:\n            self._tune_and_or_calibrate(val_df, imgsz=imgsz)\n            if self.validate_count:\n                self._validate_counting(val_df)\n        except Exception as e:\n            if self.verbose:\n                print(f\"[post-fit] skipped tuning/calibration/validation due to: {e}\")\n\n        return self.model_path\n\n    # -------------------- –∏–Ω—Ñ–µ—Ä–µ–Ω—Å-—Ö–µ–ª–ø–µ—Ä—ã --------------------\n    def _ensure_model(self):\n        if self._model is None:\n            path = self.model_path or self.model_ckpt\n            self._model = YOLO(path)\n\n    @torch.no_grad()\n    def _raw_counts(self, paths, imgsz, conf, iou, max_det):\n        \"\"\"Plain len(detections). –ï—Å–ª–∏ enable_tta_flip=True ‚Äî —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Å augment=True.\"\"\"\n        self._ensure_model()\n        out = []\n        dev = self._infer_device()\n        if not self.enable_tta_flip:\n            for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[counts]\"):\n                batch = paths[i:i+64]\n                res = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                          max_det=max_det, device=dev, verbose=False)\n                for r in res:\n                    out.append(int(len(r.boxes) if (r.boxes is not None) else 0))\n        else:\n            for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[counts-tta]\"):\n                batch = paths[i:i+64]\n                r1 = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                         max_det=max_det, device=dev, verbose=False)\n                r2 = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                         max_det=max_det, device=dev, verbose=False, augment=True)\n                for a, b in zip(r1, r2):\n                    n1 = int(len(a.boxes) if (a.boxes is not None) else 0)\n                    n2 = int(len(b.boxes) if (b.boxes is not None) else 0)\n                    out.append(0.5 * (n1 + n2))\n        return np.array(out, dtype=float)\n\n    @torch.no_grad()\n    def _yolo_feats(self, paths, imgsz, conf, iou, max_det):\n        \"\"\"–§–∏—á–∏ –¥–ª—è Ridge: [n, conf_sum, conf_mean, conf_max, area_mean, frac_small, frac_mid, frac_big].\"\"\"\n        self._ensure_model()\n        rows = []\n        dev = self._infer_device()\n        for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[feats]\"):\n            batch = paths[i:i+64]\n            res = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                      max_det=max_det, device=dev, verbose=False)\n            for r in res:\n                if r.boxes is None or len(r.boxes) == 0:\n                    rows.append(dict(n=0, conf_sum=0, conf_mean=0, conf_max=0,\n                                     area_mean=0, frac_small=0, frac_mid=0, frac_big=0))\n                    continue\n                confs = r.boxes.conf.cpu().numpy()\n                xywhn = r.boxes.xywhn.cpu().numpy()\n                areas = (xywhn[:, 2] * xywhn[:, 3]).clip(0, 1)\n                rows.append(dict(\n                    n=len(confs),\n                    conf_sum=float(confs.sum()),\n                    conf_mean=float(confs.mean()),\n                    conf_max=float(confs.max()),\n                    area_mean=float(areas.mean()),\n                    frac_small=float((areas < self.small_thr).mean()),\n                    frac_mid=float(((areas >= self.small_thr) & (areas <= self.big_thr)).mean()),\n                    frac_big=float((areas > self.big_thr).mean())\n                ))\n        return pd.DataFrame(rows).to_numpy()\n\n    @torch.no_grad()\n    def _detect_conf_area(self, paths, imgsz, conf_base, iou, max_det):\n        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–∞—Å—Å–∏–≤–æ–≤ Nx2 [conf, area] –ø—Ä–∏ –±–∞–∑–æ–≤–æ–º –ø–æ—Ä–æ–≥–µ conf_base (–¥–ª—è area-gated).\"\"\"\n        self._ensure_model()\n        dev = self._infer_device()\n        out = []\n        for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[boxes]\"):\n            batch = paths[i:i+64]\n            res = self._model.predict(batch, imgsz=imgsz, conf=conf_base, iou=iou,\n                                      max_det=max_det, device=dev, verbose=False)\n            for r in res:\n                if r.boxes is None or len(r.boxes) == 0:\n                    out.append(np.empty((0, 2), dtype=np.float32))\n                    continue\n                confs = r.boxes.conf.cpu().numpy()\n                xywhn = r.boxes.xywhn.cpu().numpy()\n                areas = (xywhn[:, 2] * xywhn[:, 3]).clip(0, 1)\n                out.append(np.stack([confs, areas], axis=1))\n        return out\n\n    @staticmethod\n    def _count_with_area_gate(conf_area_list, conf_small, conf_big, area_thr):\n        \"\"\"–ü–æ–¥—Å—á—ë—Ç —Å –¥–≤—É–ø–æ—Ä–æ–≥–æ–≤–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –ø–ª–æ—â–∞–¥–∏.\"\"\"\n        counts = []\n        for ca in conf_area_list:\n            if ca.size == 0:\n                counts.append(0); continue\n            conf = ca[:, 0]; area = ca[:, 1]\n            small_mask = (area < area_thr)  & (conf >= conf_small)\n            big_mask   = (area >= area_thr) & (conf >= conf_big)\n            counts.append(int(small_mask.sum() + big_mask.sum()))\n        return np.array(counts, dtype=float)\n\n    # -------------------- –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ --------------------\n    def _subset_val(self, val_df: pd.DataFrame) -> pd.DataFrame:\n        if self.tune_val_subsample is None:\n            return val_df\n        n = len(val_df)\n        if isinstance(self.tune_val_subsample, float):\n            k = max(1, int(round(n * self.tune_val_subsample)))\n        else:\n            k = int(self.tune_val_subsample)\n        k = min(k, n)\n        return val_df.sample(n=k, random_state=self.random_state).reset_index(drop=True)\n\n    # -------------------- Ridge –ø–æ —Ä–µ–∑–∏–¥—É–∞–ª—É —Å K-fold CV --------------------\n    def _fit_ridge_cv_on_residual(self, X: np.ndarray, y_true: np.ndarray, y_plain: np.ndarray):\n        alphas = self.ridge_alpha_grid\n        k = min(5, len(y_true)) if len(y_true) >= 3 else 2\n        kf = KFold(n_splits=k, shuffle=True, random_state=self.random_state)\n\n        # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Ñ–∏—á\n        mu = X.mean(axis=0)\n        sd = X.std(axis=0); sd[sd == 0] = 1.0\n        Xs = (X - mu) / sd\n        r = y_true - y_plain\n\n        best_alpha, best_cv = None, 1e9\n        for a in alphas:\n            cv_scores = []\n            for tr, va in kf.split(Xs):\n                m = Ridge(alpha=float(a)).fit(Xs[tr], r[tr])\n                pr = m.predict(Xs[va])\n                cv_scores.append(mean_squared_error(r[va], pr, squared=False))\n            cv_rmse = float(np.mean(cv_scores))\n            if cv_rmse < best_cv:\n                best_alpha, best_cv = float(a), cv_rmse\n\n        # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–≥–æ–Ω–∫–∞ –Ω–∞ –≤—Å–µ—Ö\n        model = Ridge(alpha=best_alpha).fit(Xs, r)\n        return dict(model=model, alpha=best_alpha, mu=mu, sd=sd, cv_rmse=best_cv)\n\n    # -------------------- —Ç—é–Ω–∏–Ω–≥ –∏/–∏–ª–∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ --------------------\n    def _tune_and_or_calibrate(self, val_df: pd.DataFrame, imgsz: int):\n        val_sub = self._subset_val(val_df)\n        paths = val_sub[self.image_col].tolist()\n        y_true = np.array([len(self._parse_boxes(r)) for _, r in val_sub.iterrows()], dtype=float)\n\n        # 1) –¢—é–Ω–∏–Ω–≥ plain –∏–ª–∏ area-gated\n        if self.enable_tuning:\n            if self.enable_area_gate:\n                # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –±–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å–±–æ—Ä–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º cs\n                min_cs = min(self.tune_conf_small_grid) if len(self.tune_conf_small_grid) else 0.10\n                base_collect = max(0.03, min(self.gate_conf_base, min_cs - 0.02))\n                if self.verbose:\n                    print(f\"[tune-gate] base_collect={base_collect:.3f} (min_cs={min_cs:.3f})\")\n\n                # 1) –ü—Ä–µ–¥—Ä–∞—Å—á—ë—Ç —Å–ø–∏—Å–∫–æ–≤ [conf, area] –¥–ª—è –≤—Å–µ—Ö (iou, max_det)\n                iou_grid = tuple(self.tune_iou_grid)\n                md_grid  = tuple(self.tune_max_det_grid)\n                conf_area_by_key = {}\n                total_prepasses = len(iou_grid) * len(md_grid)\n                if self.verbose:\n                    print(f\"[tune-gate] precomputing boxes for {total_prepasses} (iou,max_det) pairs...\")\n                for iou_ in iou_grid:\n                    for md_ in md_grid:\n                        conf_area_by_key[(iou_, md_)] = self._detect_conf_area(\n                            paths, imgsz=imgsz,\n                            conf_base=float(base_collect),\n                            iou=float(iou_), max_det=int(md_)\n                        )\n\n                # 2) –ü–æ–¥–±–æ—Ä cs/cb/area_thr + iou/max_det\n                all_combos = []\n                for iou_ in iou_grid:\n                    for md_ in md_grid:\n                        for cs in self.tune_conf_small_grid:\n                            # —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞ (–Ω–∏–∂–µ cs)\n                            base_eff = max(0.03, min(float(self.gate_conf_base), float(cs) - 0.02))\n                            for cb in self.tune_conf_big_grid:\n                                for at in self.tune_area_thr_grid:\n                                    all_combos.append((float(iou_), int(md_), float(cs), float(cb), float(at), float(base_eff)))\n\n                random.shuffle(all_combos)\n                full_space = len(all_combos)\n                if self.tune_max_combinations is not None:\n                    all_combos = all_combos[:int(self.tune_max_combinations)]\n                if self.verbose:\n                    print(f\"[tune-gate] search combos: {len(all_combos)} (cap), full={full_space}\")\n\n                best = dict(rmse=1e9, iou=None, max_det=None, cs=None, cb=None, at=None, base=None)\n                for (iou_, md_, cs, cb, at, base_eff) in all_combos:\n                    conf_area = conf_area_by_key[(iou_, md_)]\n                    y_pred = self._count_with_area_gate(conf_area, cs, cb, at)\n                    rmse = mean_squared_error(y_true, y_pred, squared=False)\n                    if rmse < best[\"rmse\"]:\n                        best.update(dict(rmse=rmse, iou=iou_, max_det=md_,\n                                         cs=cs, cb=cb, at=at, base=base_eff))\n\n                if self.verbose:\n                    print(f\"[tune-gate] best: cs={best['cs']:.3f}, cb={best['cb']:.3f}, \"\n                          f\"area_thr={best['at']:.4f}, iou={best['iou']}, max_det={best['max_det']}  RMSE={best['rmse']:.3f}\")\n\n                self.calib_.update(dict(\n                    best_conf=None, best_iou=float(best['iou']), best_max_det=int(best['max_det']),\n                    gate_conf_small=float(best['cs']), gate_conf_big=float(best['cb']),\n                    gate_area_thr=float(best['at']), gate_conf_base=float(best['base'])\n                ))\n            else:\n                combos = [(float(c), float(i), int(m))\n                          for i in self.tune_iou_grid\n                          for m in self.tune_max_det_grid\n                          for c in self.tune_conf_grid]\n                random.shuffle(combos)\n                full_space = len(combos)\n                if self.tune_max_combinations is not None:\n                    combos = combos[:int(self.tune_max_combinations)]\n                if self.verbose:\n                    print(f\"[tune-plain] search combos: {len(combos)} (cap), full={full_space}\")\n\n                best = dict(rmse=1e9, conf=None, iou=None, max_det=None)\n                for conf, iou_v, max_det in combos:\n                    y_pred = self._raw_counts(paths, imgsz=imgsz, conf=conf, iou=iou_v, max_det=max_det)\n                    rmse = mean_squared_error(y_true, y_pred, squared=False)\n                    if rmse < best[\"rmse\"]:\n                        best.update(dict(rmse=rmse, conf=conf, iou=iou_v, max_det=max_det))\n                if self.verbose:\n                    print(f\"[tune] best plain count: conf={best['conf']}, iou={best['iou']}, max_det={best['max_det']}  RMSE={best['rmse']:.3f}\")\n\n                self.calib_.update(dict(\n                    best_conf=best['conf'], best_iou=best['iou'], best_max_det=best['max_det'],\n                    gate_conf_small=None, gate_conf_big=None, gate_area_thr=None, gate_conf_base=None\n                ))\n        else:\n            # –±–µ–∑ —Ç—é–Ω–∏–Ω–≥–∞ ‚Äî –¥–µ—Ñ–æ–ª—Ç—ã –¥–ª—è plain\n            self.calib_.update(dict(\n                best_conf=0.25, best_iou=0.5, best_max_det=1000,\n                gate_conf_small=None, gate_conf_big=None, gate_area_thr=None, gate_conf_base=None\n            ))\n\n        # 2) –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ Ridge (–ø–æ —Ä–µ–∑–∏–¥—É–∞–ª—É, —Å CV)\n        ridge_model, ridge_alpha = None, None\n        if self.enable_ridge:\n            # —Å—Ç—Ä–æ–∏–º y_plain –Ω–∞ —Ç–æ–º –∂–µ —Å–∞–±—Å–µ—Ç–µ val_sub\n            if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n                iou_use = float(self.calib_[\"best_iou\"])\n                md_use  = int(self.calib_[\"best_max_det\"])\n                conf_area = self._detect_conf_area(paths, imgsz=imgsz,\n                                                   conf_base=float(self.calib_[\"gate_conf_base\"]),\n                                                   iou=iou_use, max_det=md_use)\n                y_plain = self._count_with_area_gate(conf_area,\n                                                     float(self.calib_[\"gate_conf_small\"]),\n                                                     float(self.calib_[\"gate_conf_big\"]),\n                                                     float(self.calib_[\"gate_area_thr\"]))\n                X = self._yolo_feats(paths, imgsz=imgsz,\n                                     conf=float(self.calib_[\"gate_conf_base\"]),\n                                     iou=iou_use, max_det=md_use)\n            else:\n                conf_use = float(self.calib_[\"best_conf\"] if self.calib_.get(\"best_conf\") is not None else 0.25)\n                iou_use  = float(self.calib_[\"best_iou\"]  if self.calib_.get(\"best_iou\")  is not None else 0.5)\n                md_use   = int(self.calib_[\"best_max_det\"] if self.calib_.get(\"best_max_det\") is not None else 1000)\n                y_plain  = self._raw_counts(paths, imgsz=imgsz, conf=conf_use, iou=iou_use, max_det=md_use)\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=conf_use, iou=iou_use, max_det=md_use)\n\n            pack = self._fit_ridge_cv_on_residual(X, y_true, y_plain)\n            ridge_model, ridge_alpha = pack[\"model\"], pack[\"alpha\"]\n            if self.verbose:\n                print(f\"[calib] Ridge(residual) alpha={ridge_alpha}  CV-RMSE(resid)={pack['cv_rmse']:.3f}\")\n\n            # —Å–æ—Ö—Ä–∞–Ω–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—é\n            self.calib_.update(dict(\n                ridge_alpha=ridge_alpha, ridge_model=ridge_model,\n                ridge_mu=pack[\"mu\"], ridge_sd=pack[\"sd\"], imgsz=imgsz\n            ))\n        else:\n            self.calib_.update(dict(ridge_alpha=None, ridge_model=None, imgsz=imgsz))\n\n    # -------------------- —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–¥—Å—á—ë—Ç–∞ --------------------\n    def _validate_counting(self, val_df: pd.DataFrame):\n        paths = val_df[self.image_col].tolist()\n        y_true = np.array([len(self._parse_boxes(r)) for _, r in val_df.iterrows()], dtype=float)\n\n        imgsz = self.calib_['imgsz'] or 640\n\n        # plain/gate\n        if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n            iou_use = float(self.calib_[\"best_iou\"])\n            md_use  = int(self.calib_[\"best_max_det\"])\n            conf_area = self._detect_conf_area(paths, imgsz=imgsz, conf_base=self.calib_[\"gate_conf_base\"],\n                                               iou=iou_use, max_det=md_use)\n            y_plain = self._count_with_area_gate(conf_area,\n                                                 self.calib_[\"gate_conf_small\"],\n                                                 self.calib_[\"gate_conf_big\"],\n                                                 self.calib_[\"gate_area_thr\"])\n        else:\n            conf  = self.calib_['best_conf'] if self.enable_tuning else 0.25\n            iou_v   = self.calib_['best_iou']  if self.enable_tuning else 0.5\n            max_det = self.calib_['best_max_det'] if self.enable_tuning else 1000\n            y_plain = self._raw_counts(paths, imgsz, conf, iou_v, max_det)\n\n        rmse_plain = mean_squared_error(y_true, y_plain, squared=False)\n        mae_plain  = mean_absolute_error(y_true, y_plain)\n\n        # calibrated\n        if self.enable_ridge and self.calib_['ridge_model'] is not None:\n            if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=float(self.calib_[\"gate_conf_base\"]),\n                                     iou=float(self.calib_[\"best_iou\"]), max_det=int(self.calib_[\"best_max_det\"]))\n            else:\n                X = self._yolo_feats(paths, imgsz=imgsz,\n                                     conf=(self.calib_[\"best_conf\"] if self.calib_[\"best_conf\"] is not None else 0.25),\n                                     iou=(self.calib_[\"best_iou\"] if self.calib_[\"best_iou\"] is not None else 0.5),\n                                     max_det=(self.calib_[\"best_max_det\"] if self.calib_[\"best_max_det\"] is not None else 1000))\n            mu = self.calib_.get(\"ridge_mu\"); sd = self.calib_.get(\"ridge_sd\")\n            Xs = (X - mu) / sd\n            resid = self.calib_['ridge_model'].predict(Xs)\n            y_cal = np.clip(y_plain + resid, 0, None)\n            rmse_cal = mean_squared_error(y_true, y_cal, squared=False)\n            mae_cal  = mean_absolute_error(y_true, y_cal)\n            print(f\"[val-count] plain: RMSE={rmse_plain:.3f}, MAE={mae_plain:.3f}  |  calibrated: RMSE={rmse_cal:.3f}, MAE={mae_cal:.3f}\")\n        else:\n            print(f\"[val-count] plain: RMSE={rmse_plain:.3f}, MAE={mae_plain:.3f}  |  calibrated: (disabled)\")\n\n    # -------------------- –ø—É–±–ª–∏—á–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å: –¥–µ—Ç–µ–∫—Ü–∏–∏ --------------------\n    @torch.no_grad()\n    def predict(self, df: pd.DataFrame,\n                conf: float = 0.25, iou: float = 0.6,\n                imgsz: int = 640, device: str | int | None = \"auto\",\n                max_det: int = 300, agnostic_nms: bool = False) -> pd.DataFrame:\n        \"\"\"–î–µ—Ç–µ–∫—Ü–∏–∏ (–∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è).\"\"\"\n        assert self.image_col in df.columns\n        if self._model is None:\n            self._model = YOLO(self.model_path or self.model_ckpt)\n\n        dev = self._resolve_device(device if device is not None else self._infer_device())\n\n        paths = df[self.image_col].tolist()\n        preds = []\n        for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[predict]\"):\n            batch = paths[i:i+64]\n            res = self._model(batch, conf=conf, iou=iou, imgsz=imgsz,\n                              device=dev, verbose=False, max_det=max_det,\n                              agnostic_nms=agnostic_nms)\n            for r in res:\n                boxes = []\n                if r.boxes is not None and len(r.boxes):\n                    xywhn = r.boxes.xywhn.cpu().numpy()\n                    confv = r.boxes.conf.cpu().numpy()\n                    clsv  = r.boxes.cls.cpu().numpy().astype(int)\n                    for (x,y,w,h), c, k in zip(xywhn, confv, clsv):\n                        boxes.append({\"cls\": int(k), \"conf\": float(c),\n                                      \"x\": float(x), \"y\": float(y), \"w\": float(w), \"h\": float(h)})\n                preds.append({\n                    self.image_col: r.path,\n                    \"count\": len(boxes),\n                    \"boxes_json\": json.dumps(boxes, ensure_ascii=False)\n                })\n        return pd.DataFrame(preds)\n\n    # -------------------- –ø—É–±–ª–∏—á–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å: –ø–æ–¥—Å—á—ë—Ç --------------------\n    @torch.no_grad()\n    def predict_counts(self, df: pd.DataFrame,\n                       imgsz: int | None = None,\n                       conf: float | None = None,\n                       iou: float | None = None,\n                       max_det: int | None = None,\n                       device: str | int | None = \"auto\",\n                       clamp_nonneg: bool = True,\n                       do_round: bool = False) -> pd.DataFrame:\n        \"\"\"\n        –ü–æ–¥—Å—á—ë—Ç –æ–±—ä–µ–∫—Ç–æ–≤.\n          - –ï—Å–ª–∏ enable_ridge=True –∏ –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω ‚Üí y = y_plain + Ridge(residual).\n          - –ò–Ω–∞—á–µ ‚Üí plain len(dets).\n          - –ï—Å–ª–∏ enable_area_gate=True –∏ —Ç—é–Ω–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω ‚Üí –¥–≤—É–ø–æ—Ä–æ–≥–æ–≤–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (conf_small/conf_big) –ø–æ area.\n        \"\"\"\n        assert self.image_col in df.columns\n        if self._model is None:\n            self._model = YOLO(self.model_path or self.model_ckpt)\n\n        dev = self._resolve_device(device if device is not None else self._infer_device())\n        imgsz = imgsz or self.calib_['imgsz'] or 640\n        paths = df[self.image_col].tolist()\n\n        # area-gated –ø—É—Ç—å\n        if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n            conf_base = float(self.calib_[\"gate_conf_base\"])\n            iou_use   = float(self.calib_[\"best_iou\"])\n            max_det_use = int(self.calib_[\"best_max_det\"])\n            conf_area = self._detect_conf_area(paths, imgsz=imgsz, conf_base=conf_base, iou=iou_use, max_det=max_det_use)\n            cs, cb, at = float(self.calib_[\"gate_conf_small\"]), float(self.calib_[\"gate_conf_big\"]), float(self.calib_[\"gate_area_thr\"])\n            y_plain = self._count_with_area_gate(conf_area, cs, cb, at)\n\n            if self.enable_ridge and self.calib_.get(\"ridge_model\") is not None:\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=conf_base, iou=iou_use, max_det=max_det_use)\n                # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–∑–∏–¥—É–∞–ª–∞\n                mu = self.calib_.get(\"ridge_mu\"); sd = self.calib_.get(\"ridge_sd\")\n                Xs = (X - mu) / sd\n                resid = self.calib_['ridge_model'].predict(Xs)\n                y = y_plain + resid\n            else:\n                y = y_plain\n\n        else:\n            # –æ–±—ã—á–Ω—ã–π plain –ø—É—Ç—å\n            if self.enable_tuning and self.calib_.get(\"best_conf\") is not None:\n                conf_def, iou_def, max_det_def = self.calib_['best_conf'], self.calib_['best_iou'], self.calib_['best_max_det']\n            else:\n                conf_def, iou_def, max_det_def = 0.25, 0.5, 1000\n\n            use_conf  = conf    if conf    is not None else conf_def\n            use_iou   = iou     if iou     is not None else iou_def\n            use_maxdet= max_det if max_det is not None else max_det_def\n\n            if self.enable_ridge and self.calib_.get(\"ridge_model\") is not None:\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=use_conf, iou=use_iou, max_det=use_maxdet)\n                mu = self.calib_.get(\"ridge_mu\"); sd = self.calib_.get(\"ridge_sd\")\n                Xs = (X - mu) / sd\n                resid = self.calib_['ridge_model'].predict(Xs)\n                # –±–∞–∑–æ–≤—ã–π plain-—Å—á—ë—Ç –¥–ª—è —Å–ª–æ–∂–µ–Ω–∏—è —Å —Ä–µ–∑–∏–¥—É–∞–ª–æ–º:\n                y_plain = self._raw_counts(paths, imgsz=imgsz, conf=use_conf, iou=use_iou, max_det=use_maxdet)\n                y = y_plain + resid\n            else:\n                y = self._raw_counts(paths, imgsz=imgsz, conf=use_conf, iou=use_iou, max_det=use_maxdet)\n\n        if clamp_nonneg: y = np.clip(y, 0, None)\n        if do_round:     y = np.rint(y)\n\n        out = df[[self.image_col]].copy()\n        out[\"label\"] = y\n        return out\n\n    # -------------------- housekeeping --------------------\n    def cleanup(self):\n        if self._tmpdir_owned and os.path.isdir(self.data_root):\n            shutil.rmtree(self.data_root, ignore_errors=True)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 1: –ë–∞–∑–æ–≤–∞—è —Ç–µ–∫—Å—Ç-—Ä–µ–≥—Ä–µ—Å—Å–∏—è","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–π—Ç–∏–Ω–≥–∞ –æ—Ç–∑—ã–≤–æ–≤ (0-10) –ø–æ —Ç–µ–∫—Å—Ç—É –æ—Ç–∑—ã–≤–∞.\n# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ BERT –¥–ª—è —Ç–µ–∫—Å—Ç–∞, fusion=\"concat\", 3 —ç–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è.\n# –î–∞–Ω–Ω—ã–µ: df —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['review_text', 'rating']\n\ndf = pd.DataFrame({\n    'review_text': ['Great. I enjoed it place.', 'This hotel is most bad which i have seen!'],\n    'rating': [8, 1]\n})\n\nmodel = MultiModalRegressionPipeline(\n    modalities=['text'],\n    target_column_names=['rating'],\n    text_columns=['review_text'],\n    text_model_config={\n        'checkpoint': 'google-bert/bert-base-uncased'\n    },\n    fusion='concat'\n)\nmodel.fit(df, epochs=3)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 2: –ú—É–ª—å—Ç–∏—Ç–∞—Ä–≥–µ—Ç —Ä–µ–≥—Ä–µ—Å—Å–∏—è","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¢–†–ï–• –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ç–∞:\n# [—Ü–µ–Ω–∞, –∫–∞—á–µ—Å—Ç–≤–æ, —É–¥–æ–±—Å—Ç–≤–æ] –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —Ç–æ–≤–∞—Ä–∞.\n# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CLIP –¥–ª—è text+image, fusion=\"mean\", dynamic padding.\n# –î–∞–Ω–Ω—ã–µ: df —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['description', 'product_image', 'price', 'quality', 'convenience']\n\n# df –≥–¥–µ-—Ç–æ –∑–¥–µ—Å—å –µ—Å—Ç—å\n\nmodel = MultiModalRegressionPipeline(\n    modalities=['text', 'image'],\n    text_columns=['description']\n    image_columns=['product_image']\n    target_column_names=['price', 'quality', 'convenience'],\n    text_max_length=77,\n    pretokenize_data=False,\n    clip_checkpoint='jinaai/jina-clip-v2',  # –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π clip\n    fusion='mean',\n    text_padding='dynamic'\n)\nmodel.fit(df)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 3: –ê—É–¥–∏–æ-—Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å CLAP","metadata":{}},{"cell_type":"code","source":"# –ü—Ä–µ–¥—Å–∫–∞–∂–∏—Ç–µ —É—Ä–æ–≤–µ–Ω—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ—á–∏ (0-1) –ø–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É.\n# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CLAP –¥–ª—è –∞—É–¥–∏–æ, audio_sr=48000, 5 —ç–ø–æ—Ö.\n# –î–∞–Ω–Ω—ã–µ: df —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['audio_path', 'emotion_score']\\\n\nmodel = MultiModalRegressionPipeline(\n    modalities=['audio'],\n    audio_model_config={\n        'checkpoin': 'laion/clap-htsat-unfused',  # model_type –º–æ–∂–Ω–æ –Ω–µ —É–∫–∞–∑—ã–≤–∞—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n        'model_type': 'clap',\n        'audio_sr': 48000\n    }\n    target_column_names=['emotion_score'],\n    audio_columns=['audio_path'],\n)\nmodel.fit(df, epochs=5)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 4: –¢—Ä–∏–ø–ª–µ—Ç –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π —Å –ø—Ä–µ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π","metadata":{}},{"cell_type":"code","source":"# –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (0-100) –ø–æ:\n# - —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –∑–∞–≥–æ–ª–æ–≤–∫—É\n# - –æ–±–ª–æ–∂–∫–µ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)\n# - –ø—Ä–µ–≤—å—é-–∞—É–¥–∏–æ\n# CLIP –¥–ª—è text+image, CLAP –¥–ª—è audio, fusion=\"concat\",\n# pretokenize=True, text_padding=\"max_length\", 10 —ç–ø–æ—Ö.\n# –î–∞–Ω–Ω—ã–µ: df —Å ['title', 'description', 'cover_image', 'preview_audio', 'quality_score']\n\npipeline = MultiModalRegressionPipeline(\n    modalities=['text', 'image', 'audio'],\n    target_column_names=['quality_score'],\n    text_columns=['title', 'description'],\n    image_columns=['cover_image'],\n    audio_columns=['preview_audio'],\n    fusion='concat',\n    text_padding='max_length',\n    pretokenize=True\n)\npipeline.fit(df, epochs=10)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 5: –†–µ–≥—Ä–µ—Å—Å–∏—è —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏","metadata":{}},{"cell_type":"code","source":"# –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–æ–≤–∞—Ä–∞ (10 –∫–ª–∞—Å—Å–æ–≤) –ø–æ —Ñ–æ—Ç–æ –∏ –∫—Ä–∞—Ç–∫–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é.\n# CLIP –¥–ª—è text+image, fusion=\"mean\", class_weights=None (–∞–≤—Ç–æ-—Ä–∞—Å—á–µ—Ç),\n# metric=\"accuracy\", gradient_checkpointing=True.\n# –î–∞–Ω–Ω—ã–µ: df —Å ['product_image', 'short_desc', 'category']\n\nmodel = MultiModalClassificationPipeline(\n    modalities=['text', 'image'],\n    fusion='mean',\n    target_column_name='category',\n    image_columns=['product_image'],\n    text_columns=['short_desc']\n)\nmodel.fit(df, metric_name='accuracy', gradient_checkpointing=True)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# –ü—Ä–µ–¥—Å–∫–∞–∂–∏—Ç–µ —Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –ø–æ 5 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º –æ–±—ä–µ–∫—Ç–∞ –∏ –æ–ø–∏—Å–∞–Ω–∏—é.\n# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ max_images=5, image_agg=\"mean\", CLIP, fusion=\"concat\".\n# –î–∞–Ω–Ω—ã–µ: df —Å ['description', 'photos' (—Å–ø–∏—Å–æ–∫ –∏–∑ 5 –ø—É—Ç–µ–π), 'price']\n\nmodel = MultiModalRegressionPipeline(\n    modalities=['text', 'image'],\n    target_column_names=['price'],\n    text_columns=['description'],\n    image_columns=['photos'],\n    fusion='concat',\n    image_agg='mean',\n    max_images_per_sample=5\n)\nmodel.fit(df)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 6: –ë–∞–∑–æ–≤–∞—è text-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è","metadata":{}},{"cell_type":"code","source":"# –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ –æ—Ç–∑—ã–≤—ã –Ω–∞ 3 –∫–ª–∞—Å—Å–∞: positive, neutral, negative.\n# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Auto (BERT) –¥–ª—è —Ç–µ–∫—Å—Ç–∞, metric=\"f1\", 5 —ç–ø–æ—Ö.\n# –î–∞–Ω–Ω—ã–µ: df —Å ['review_text', 'sentiment']\n\nmodel = MultiModalClassificationPipeline(\n    modalities=['text'],\n    target_column_name=['sentiment'],\n    text_columns=['review_text']\n)\nmodel.fit(df, epochs=5, metric='f1')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 7: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ + —Ç–µ–∫—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è","metadata":{}},{"cell_type":"markdown","source":"# –ê—É–¥–∏–æ –≤ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport librosa\n\naudio = np.random.randn(22050)\nD = librosa.stft(audio)\nspectrogram = librosa.amplitude_to_db(np.abs(D), ref=np.max)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}