{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Дообучение encoder'а для классификации токенов.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  numpy==1.26.4 \\\n  pandas==2.2.3 \\\n  tqdm==4.67.1 \\\n  transformers==4.51.3 \\\n  evaluate==0.4.5 \\\n  torch==2.6.0+cu124 \\\n  seqeval==1.2.2\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport gc\nimport math\nfrom typing import List, Dict, Any, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport evaluate\nfrom transformers import (\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForTokenClassification,\n    TrainerCallback,\n    PrinterCallback,\n    EarlyStoppingCallback,\n)\nfrom transformers.modeling_outputs import ModelOutput\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\n\ndef set_seed(seed: int = 42):\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                try:\n                    parts.append(f\"{k.replace('eval_', '')} {float(v):.4f}\")\n                except Exception:\n                    pass\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    try:\n                        extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n                    except Exception:\n                        pass\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\nclass WeightedTokenCETrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n        self._warned_label_tiling = False\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n\n        if logits.size(0) != labels.size(0):\n            ngpu = torch.cuda.device_count()\n            if ngpu > 1 and logits.size(0) == labels.size(0) * ngpu:\n                labels = labels.repeat_interleave(ngpu, dim=0)\n                if not self._warned_label_tiling:\n                    print(f\"[Warning] DataParallel удвоил batch для logits. \"\n                          f\"Повторяем labels x{ngpu}. logits: {tuple(logits.shape)}, labels: {tuple(labels.shape)}\")\n                    self._warned_label_tiling = True\n            else:\n                raise ValueError(f\"Batch size mismatch: logits {tuple(logits.shape)} vs labels {tuple(labels.shape)}\")\n\n        loss_fct = torch.nn.CrossEntropyLoss(\n            weight=(self.class_weights.to(logits.device) if self.class_weights is not None else None),\n            ignore_index=-100,\n        )\n        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n\nclass TokenClassification:\n    \"\"\"\n    Класс-обёртка для обучения и инференса моделей токен-классификации (NER/POS и т.д.)\n    на базе Hugging Face Transformers. Поддерживает обучение со «скользящим окном»,\n    выравнивание меток слов с субтокенами, расчёт весов классов по словам, раннюю остановку,\n    агрегирование логитов по перекрывающимся окнам и извлечение эмбеддингов слов.\n    \"\"\"\n\n    def __init__(\n        self,\n        checkpoint: str,\n        label2id: Dict[str, int],\n        tokens_column_name: str,\n        tags_column_name: str\n    ):\n        \"\"\"\n        Инициализирует модель, токенайзер и инфраструктуру для обучения/инференса.\n\n        :param checkpoint: имя/путь модели в Hugging Face (например, 'bert-base-cased').\n        :param label2id: словарь отображения строковых меток в целочисленные id.\n        :param tokens_column_name: имя колонки DataFrame с токенами (словами).\n        :param tags_column_name: имя колонки DataFrame с метками (строки или уже id).\n        :return: None\n        :raises: ValueError при некорректных входных параметрах (например, пустой label2id).\n        \"\"\"\n        self.id2label = {v: k for k, v in label2id.items()}\n        self.label2id = label2id\n\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            checkpoint,\n            num_labels=len(self.id2label),\n            id2label=self.id2label,\n            label2id=self.label2id,\n            ignore_mismatched_sizes=True\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.tokens_column_name = tokens_column_name\n        self.tags_column_name = tags_column_name\n\n        # Градиентный чекпоинтинг (если поддерживается моделью)\n        try:\n            self.model.gradient_checkpointing_enable()\n        except Exception:\n            pass\n\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.data_collator = DataCollatorForTokenClassification(tokenizer=self.tokenizer)\n        self.progress_callback: Optional[TrainerCallback] = None\n\n    # ------------------------------\n    # Вспомогательные хелперы\n    # ------------------------------\n    @staticmethod\n    def _labels_are_strings(labels_col_list) -> bool:\n        \"\"\"\n        Определяет, представлены ли метки строками (а не id).\n\n        :param labels_col_list: итерируемая коллекция списков меток (по документам).\n        :return: True, если метки строковые; False, если уже id или все пусто.\n        :raises: None\n        \"\"\"\n        for tags in labels_col_list:\n            if isinstance(tags, (list, tuple)) and len(tags) > 0:\n                return isinstance(tags[0], str)\n        return False\n\n    def _label_to_id(self, tag: str) -> int:\n        \"\"\"\n        Преобразует строковую метку в id согласно self.label2id.\n\n        :param tag: строковая метка.\n        :return: целочисленный id метки.\n        :raises ValueError: если метка отсутствует в label2id.\n        \"\"\"\n        if tag not in self.label2id:\n            raise ValueError(\n                f\"Unknown label encountered: '{tag}'. \"\n                f\"Known labels: {sorted(self.label2id.keys())}\"\n            )\n        return int(self.label2id[tag])\n\n    def _assert_tokens_labels_same_len(self, tokens_seq, labels_seq):\n        \"\"\"\n        Проверяет совпадение длины списков токенов и меток для каждого документа.\n\n        :param tokens_seq: iterable со списками токенов (по документам).\n        :param labels_seq: iterable со списками меток (по документам).\n        :return: None\n        :raises ValueError: если типы неверны или длины не совпадают.\n        \"\"\"\n        for i, (toks, labs) in enumerate(zip(tokens_seq, labels_seq)):\n            if not isinstance(toks, (list, tuple)) or not isinstance(labs, (list, tuple)):\n                raise ValueError(\n                    f\"Row {i}: tokens/labels must be lists, got \"\n                    f\"{type(toks).__name__} and {type(labs).__name__}\"\n                )\n            if len(toks) != len(labs):\n                raise ValueError(\n                    f\"Row {i}: tokens and labels length mismatch: \"\n                    f\"{len(toks)} vs {len(labs)}\"\n                )\n\n    @staticmethod\n    def _to_token_list(obj):\n        \"\"\"\n        Приводит значение ячейки к списку токенов.\n\n        :param obj: значение колонки токенов (list/tuple/np.ndarray/None/другое).\n        :return: список токенов (или пустой список при неподдерживаемом типе).\n        :raises: None\n        \"\"\"\n        if obj is None:\n            return []\n        if isinstance(obj, (list, tuple)):\n            return list(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return []\n\n    def _get_effective_max_length(self) -> int:\n        \"\"\"\n        Возвращает безопасную максимальную длину контекста:\n        min(model.config.max_position_embeddings, tokenizer.model_max_length),\n        игнорируя «бесконечные» значения токенайзера.\n\n        :return: целочисленное значение безопасной максимальной длины.\n        :raises: None\n        \"\"\"\n        m_conf = int(getattr(self.model.config, \"max_position_embeddings\", 512) or 512)\n        m_tok = int(getattr(self.tokenizer, \"model_max_length\", 512) or 512)\n        if m_tok > 100000:\n            return m_conf\n        return min(m_conf, m_tok)\n\n    @staticmethod\n    def _sanitize_stride(stride: int, max_length: int) -> int:\n        \"\"\"\n        Ограничивает stride до [0, max_length - 2], учитывая спецтокены.\n\n        :param stride: желаемый страйд перекрытия.\n        :param max_length: безопасная максимальная длина контекста.\n        :return: целочисленное безопасное значение stride.\n        :raises: None\n        \"\"\"\n        stride = int(max(0, stride))\n        return int(min(stride, max(0, max_length - 2)))\n\n    # ------------------------------\n    # Алгоритмика\n    # ------------------------------\n    @staticmethod\n    def _align_labels_with_word_ids(labels_ids: List[int], word_ids: List[Optional[int]]) -> List[int]:\n        \"\"\"\n        Выравнивает метки слов по субтокенам: первый субтокен слова получает метку,\n        последующие субтокены — -100 (игнор в CrossEntropy).\n\n        :param labels_ids: список меток по словам (id), длина = числу слов.\n        :param word_ids: список индексов слов для каждого субтокена (tokenizer.word_ids()).\n        :return: список меток по длине субтокенов, с -100 для игнорируемых позиций.\n        :raises: None\n        \"\"\"\n        new_labels = []\n        prev_word_id = None\n        L = len(labels_ids)\n        for wid in word_ids:\n            if wid is None or wid < 0 or wid >= L:\n                new_labels.append(-100)\n            else:\n                if wid != prev_word_id:\n                    new_labels.append(labels_ids[wid])\n                else:\n                    new_labels.append(-100)\n            prev_word_id = wid\n        return new_labels\n\n    def _tokenize_and_align_chunk(\n        self,\n        docs_tokens: List[List[str]],\n        docs_labels_ids: List[List[int]],\n        max_length: int,\n        stride: int\n    ) -> Dataset:\n        \"\"\"\n        Токенизирует документы с разбиением на окна и выравниванием меток.\n\n        :param docs_tokens: списки токенов по документам.\n        :param docs_labels_ids: списки меток (id) по документам.\n        :param max_length: безопасная максимальная длина контекста.\n        :param stride: перекрытие между окнами.\n        :return: HF Dataset с полями input_ids, attention_mask, labels.\n        :raises: None\n        \"\"\"\n        enc = self.tokenizer(\n            docs_tokens,\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True\n        )\n        mapping = enc.pop(\"overflow_to_sample_mapping\")\n        num_chunks = len(enc[\"input_ids\"])\n\n        all_labels = []\n        for i in range(num_chunks):\n            doc_idx = int(mapping[i])\n            word_ids = enc.word_ids(batch_index=i)\n            aligned = self._align_labels_with_word_ids(docs_labels_ids[doc_idx], word_ids)\n            all_labels.append(aligned)\n\n        return Dataset.from_dict({\n            \"input_ids\": enc[\"input_ids\"],\n            \"attention_mask\": enc[\"attention_mask\"],\n            \"labels\": all_labels\n        })\n\n    def _count_total_chunks(\n        self,\n        docs_tokens: List[List[str]],\n        max_length: int,\n        stride: int,\n        batch_docs: int = 64\n    ) -> int:\n        \"\"\"\n        Подсчитывает число чанков (окон) после токенизации набора документов.\n\n        :param docs_tokens: списки токенов по документам.\n        :param max_length: безопасная максимальная длина контекста.\n        :param stride: перекрытие между окнами.\n        :param batch_docs: размер батча документов при токенизации.\n        :return: общее число чанков (int).\n        :raises: None\n        \"\"\"\n        total = 0\n        for i in range(0, len(docs_tokens), batch_docs):\n            batch = docs_tokens[i:i + batch_docs]\n            enc = self.tokenizer(\n                batch,\n                is_split_into_words=True,\n                return_overflowing_tokens=True,\n                max_length=max_length,\n                stride=stride,\n                truncation=True\n            )\n            total += len(enc[\"input_ids\"])\n        return total\n\n    def _compute_class_weights_over_words(self, docs_labels_ids: List[List[int]]) -> np.ndarray:\n        \"\"\"\n        Считает веса классов по словам (без влияния overlap-окон).\n\n        :param docs_labels_ids: списки меток (id) по документам.\n        :return: массив весов классов shape=(num_labels,), dtype=float32.\n        :raises: None\n        \"\"\"\n        num_labels = len(self.id2label)\n        counts = np.zeros(num_labels, dtype=np.int64)\n        for labs in docs_labels_ids:\n            if isinstance(labs, (list, tuple)) and len(labs) > 0:\n                arr = np.asarray(labs, dtype=np.int64)\n                arr = arr[(arr >= 0) & (arr < num_labels)]\n                if arr.size > 0:\n                    counts += np.bincount(arr, minlength=num_labels)\n        N = counts.sum()\n        weights = np.zeros(num_labels, dtype=np.float32)\n        if N > 0:\n            nonzero = counts > 0\n            weights[nonzero] = N / (num_labels * counts[nonzero].astype(np.float32))\n        return weights\n\n    @staticmethod\n    def _normalize_clip_weights(w: np.ndarray, clip: float = 5.0) -> np.ndarray:\n        \"\"\"\n        Нормирует и клипует веса классов: клип сверху до clip и нормировка\n        положительных весов к среднему ~1.0.\n\n        :param w: исходные веса классов.\n        :param clip: верхняя граница клипа (None/<=0 — без клипа).\n        :return: нормированные веса dtype=float32.\n        :raises: None\n        \"\"\"\n        w = np.asarray(w, dtype=np.float32)\n        if clip is not None and clip > 0:\n            w = np.minimum(w, clip)\n        mask = w > 0\n        mean = float(np.mean(w[mask])) if np.any(mask) else 1.0\n        if mean > 0:\n            w = w / mean\n        return w\n\n    def _setup_compute_metrics(self):\n        \"\"\"\n        Создаёт и сохраняет функцию метрик для seqeval (self.compute_metrics).\n\n        Метрики:\n        - precision/recall/f1/accuracy — агрегированные;\n        - f1_{entity} — по каждой сущности.\n\n        :return: None\n        :raises: None\n        \"\"\"\n        metric = evaluate.load(\"seqeval\")\n\n        def compute_seqeval_metrics(p):\n            if isinstance(p, (tuple, list)):\n                predictions, labels = p\n            else:\n                predictions, labels = p.predictions, p.label_ids\n\n            predictions = np.argmax(predictions, axis=2)\n\n            true_predictions = [\n                [self.id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            true_labels = [\n                [self.id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n\n            results = metric.compute(predictions=true_predictions, references=true_labels)\n\n            out = {\n                \"precision\": results.get(\"overall_precision\", 0.0),\n                \"recall\": results.get(\"overall_recall\", 0.0),\n                \"f1\": results.get(\"overall_f1\", 0.0),\n                \"accuracy\": results.get(\"overall_accuracy\", 0.0),\n            }\n            for ent, vals in results.items():\n                if isinstance(vals, dict) and \"f1\" in vals:\n                    out[f\"f1_{ent}\"] = float(vals[\"f1\"])\n            return out\n\n        self.compute_metrics = compute_seqeval_metrics\n\n    def _prepare_dataset_with_sliding_window(self, df: pd.DataFrame, max_length: int, stride: int) -> Dataset:\n        \"\"\"\n        Готовит HF Dataset для оценки/валидации со «скользящим окном».\n\n        :param df: DataFrame с колонками токенов и меток.\n        :param max_length: безопасная максимальная длина контекста.\n        :param stride: перекрытие между окнами.\n        :return: HF Dataset с полями input_ids, attention_mask, labels.\n        :raises ValueError: при неверных типах или несовпадении длины токенов и меток.\n        \"\"\"\n        docs_tokens = df[self.tokens_column_name].tolist()\n        docs_labels = df[self.tags_column_name].tolist()\n\n        if self._labels_are_strings(docs_labels):\n            docs_labels = [[self._label_to_id(tag) for tag in tags] for tags in docs_labels]\n\n        filtered_tokens, filtered_labels = [], []\n        for i, (toks, labs) in enumerate(zip(docs_tokens, docs_labels)):\n            if not isinstance(toks, (list, tuple)) or not isinstance(labs, (list, tuple)):\n                raise ValueError(\n                    f\"Row {i}: tokens/labels must be lists, got \"\n                    f\"{type(toks).__name__} and {type(labs).__name__}\"\n                )\n            if len(toks) == 0 and len(labs) == 0:\n                continue\n            if len(toks) != len(labs):\n                raise ValueError(\n                    f\"Row {i}: tokens and labels length mismatch: \"\n                    f\"{len(toks)} vs {len(labs)}\"\n                )\n            filtered_tokens.append(list(toks))\n            filtered_labels.append(list(labs))\n\n        if len(filtered_tokens) == 0:\n            return Dataset.from_dict({\"input_ids\": [], \"attention_mask\": [], \"labels\": []})\n\n        return self._tokenize_and_align_chunk(filtered_tokens, filtered_labels, max_length, stride)\n\n    # ------------------------------\n    # Обучение\n    # ------------------------------\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        test_size: float = 0.2,\n        learning_rate: float = 2e-5,\n        fp16: bool = True,\n        stride: int = 128,\n        logging_steps: int = 50,\n        eval_steps: int = 100,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        fit_chunk_size_docs: Optional[int] = None,\n        early_stopping_patience: Optional[int] = 3,\n        early_stopping_threshold: float = 0.0,\n    ):\n        \"\"\"\n        Обучает модель токен-классификации на данных.\n\n        :param train_data: DataFrame с колонками токенов и меток.\n        :param epochs: число эпох (проходов) по обучающему набору документов.\n        :param per_device_train_batch_size: размер батча на устройство при обучении.\n        :param gradient_accumulation_steps: число шагов аккумуляции градиента.\n        :param test_size: доля/размер валидации; при слишком малом наборе eval отключается автоматически.\n        :param learning_rate: скорость обучения (LR).\n        :param fp16: использовать ли fp16 (если bf16 не используется и доступен CUDA).\n        :param stride: перекрытие между окнами для токенизации длинных документов.\n        :param logging_steps: частота логирования в шагах.\n        :param eval_steps: частота валидации/сохранения (если есть eval).\n        :param output_dir: директория для артефактов обучения.\n        :param seed: seed для воспроизводимости.\n        :param fit_chunk_size_docs: сколько документов обучать за один «кусок» перед сменой train_dataset (None = все).\n        :param early_stopping_patience: количество подряд неулучшающихся точек валидации до остановки;\n                                       если None или <= 0 — ранняя остановка не используется.\n        :param early_stopping_threshold: минимальное относительное улучшение метрики, требуемое для сброса счётчика patience.\n        :return: self (для чейнинга).\n        :raises ValueError: при несогласованных данных (тип/длина токенов и меток).\n        \"\"\"\n        set_seed(seed)\n\n        max_length = self._get_effective_max_length()\n        stride = self._sanitize_stride(stride, max_length)\n\n        df_all = train_data.copy()\n        if self._labels_are_strings(df_all[self.tags_column_name].tolist()):\n            df_all[self.tags_column_name] = df_all[self.tags_column_name].apply(\n                lambda tags: [self._label_to_id(tag) for tag in tags]\n            )\n\n        self._assert_tokens_labels_same_len(\n            df_all[self.tokens_column_name].tolist(),\n            df_all[self.tags_column_name].tolist()\n        )\n\n        # Робастный train/val split\n        n_total = len(df_all)\n        use_eval = False\n        test_size_abs = 0\n        if n_total >= 2 and test_size and float(test_size) > 0:\n            if isinstance(test_size, float):\n                test_size_abs = int(round(n_total * float(test_size)))\n            else:\n                test_size_abs = int(test_size)\n            if test_size_abs <= 0:\n                test_size_abs = 1\n            if test_size_abs >= n_total:\n                test_size_abs = n_total - 1\n            use_eval = test_size_abs > 0\n\n        if use_eval:\n            df_train, df_eval = train_test_split(df_all, test_size=test_size_abs, random_state=seed, shuffle=True)\n        else:\n            df_train = df_all\n            df_eval = df_all.iloc[0:0]\n\n        eval_dataset = None\n        if len(df_eval) > 0:\n            eval_dataset = self._prepare_dataset_with_sliding_window(df_eval, max_length, stride)\n\n        train_docs_tokens = df_train[self.tokens_column_name].tolist()\n        train_docs_labels = df_train[self.tags_column_name].tolist()\n\n        class_weights = self._compute_class_weights_over_words(train_docs_labels)\n        class_weights = self._normalize_clip_weights(class_weights, clip=5.0)\n\n        self._setup_compute_metrics()\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\" if eval_dataset is not None else \"no\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\" if eval_dataset is not None else \"no\",\n            save_steps=eval_steps,\n            load_best_model_at_end=bool(eval_dataset is not None),\n            metric_for_best_model=\"eval_f1\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=bool(fp16 and torch.cuda.is_available()),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            disable_tqdm=True,\n            dataloader_pin_memory=True,\n            gradient_checkpointing=True,\n        )\n\n        data_collator = self.data_collator\n\n        def steps_for_size(n_samples: int, bsz: int, accum: int) -> int:\n            return max(0, math.ceil(math.ceil(n_samples / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(n_docs: int, chunk_docs: int):\n            for i in range(0, n_docs, chunk_docs):\n                yield slice(i, min(i + chunk_docs, n_docs))\n\n        n_docs = len(train_docs_tokens)\n        chunk_docs = int(fit_chunk_size_docs) if (fit_chunk_size_docs and fit_chunk_size_docs > 0) else n_docs\n\n        total_steps = 0\n        rng = np.random.default_rng(seed)\n        doc_indices = np.arange(n_docs)\n        for _ in range(epochs):\n            rng.shuffle(doc_indices)\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = doc_indices[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                n_samples = self._count_total_chunks(toks_chunk, max_length, stride, batch_docs=64)\n                total_steps += steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n\n        if n_docs > 0:\n            init_chunk_ds = self._tokenize_and_align_chunk(\n                [train_docs_tokens[0]], [train_docs_labels[0]], max_length, stride\n            )\n        else:\n            init_chunk_ds = Dataset.from_dict({\"input_ids\": [], \"attention_mask\": [], \"labels\": []})\n\n        self.trainer = WeightedTokenCETrainer(\n            model=self.model,\n            args=args,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics if eval_dataset is not None else None,\n            train_dataset=init_chunk_ds,\n            eval_dataset=eval_dataset,\n            tokenizer=self.tokenizer,\n            class_weights=class_weights\n        )\n        try:\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n\n        # Ранняя остановка (если есть eval и запрошена)\n        if eval_dataset is not None and (early_stopping_patience is not None) and (early_stopping_patience > 0):\n            early_cb = EarlyStoppingCallback(\n                early_stopping_patience=int(early_stopping_patience),\n                early_stopping_threshold=float(early_stopping_threshold),\n            )\n            self.trainer.add_callback(early_cb)\n\n        self.progress_callback = cb\n\n        steps_done = 0\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            order = np.arange(n_docs)\n            rng.shuffle(order)\n\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = order[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                labs_chunk = [train_docs_labels[i] for i in idx]\n\n                ds_chunk = self._tokenize_and_align_chunk(toks_chunk, labs_chunk, max_length, stride)\n                self.trainer.train_dataset = ds_chunk\n\n                n_samples = len(ds_chunk)\n                chunk_steps = steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                del ds_chunk\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n        return self\n\n    # ------------------------------\n    # Инференс\n    # ------------------------------\n    def _predict_single_document(self, tokens: List[str], stride: int) -> List[str]:\n        \"\"\"\n        Предсказывает метки для одного документа со «скользящим окном».\n\n        :param tokens: список слов (токенов) документа.\n        :param stride: перекрытие между окнами.\n        :return: список строковых меток той же длины, что и tokens.\n        :raises: None\n        \"\"\"\n        if not isinstance(tokens, (list, tuple)) or len(tokens) == 0:\n            return []\n\n        max_length = self._get_effective_max_length()\n        stride = self._sanitize_stride(stride, max_length)\n\n        tokenized_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n        )\n        tokenized_inputs.pop(\"overflow_to_sample_mapping\", None)\n\n        default_id = int(min(self.id2label.keys())) if len(self.id2label) else 0\n        default_label = self.id2label.get(default_id, str(default_id))\n\n        if not isinstance(tokenized_inputs.get(\"input_ids\", None), list) or len(tokenized_inputs[\"input_ids\"]) == 0:\n            return [default_label] * len(tokens)\n\n        chunk_dataset = Dataset.from_dict(tokenized_inputs)\n        outputs = self.trainer.predict(chunk_dataset)\n\n        if hasattr(outputs, \"predictions\"):\n            preds = outputs.predictions\n        else:\n            preds = outputs[\"predictions\"]\n\n        num_original_words = len(tokens)\n\n        # Основной путь: 3D логиты (num_chunks, seq_len, num_labels)\n        if isinstance(preds, np.ndarray) and preds.ndim == 3:\n            num_labels = preds.shape[-1]\n            word_logits = np.zeros((num_original_words, num_labels), dtype=np.float32)\n            word_counts = np.zeros((num_original_words,), dtype=np.float32)\n\n            for i in range(preds.shape[0]):\n                chunk_logits = preds[i]\n                try:\n                    chunk_word_ids = tokenized_inputs.word_ids(batch_index=i)\n                except Exception:\n                    continue\n                if chunk_word_ids is None:\n                    continue\n\n                for token_pos, word_id in enumerate(chunk_word_ids):\n                    if word_id is None:\n                        continue\n                    if token_pos == 0 or chunk_word_ids[token_pos - 1] != word_id:\n                        if 0 <= word_id < num_original_words:\n                            word_logits[word_id] += chunk_logits[token_pos]\n                            word_counts[word_id] += 1.0\n\n            mask = word_counts > 0\n            if np.any(mask):\n                word_logits[mask] /= word_counts[mask, None]\n\n            pred_ids = np.full(num_original_words, default_id, dtype=np.int32)\n            if np.any(mask):\n                pred_ids[mask] = word_logits[mask].argmax(-1)\n\n            filled = [self.id2label.get(int(x), str(int(x))) for x in pred_ids]\n            return filled\n\n        # Fallback: если preds не 3D\n        if isinstance(preds, np.ndarray):\n            if preds.ndim == 2:\n                predictions = preds\n            elif preds.ndim == 1:\n                predictions = preds[None, :]\n            else:\n                predictions = preds.reshape(len(tokenized_inputs[\"input_ids\"]), -1)\n        else:\n            predictions = np.asarray(preds)\n\n        final_predictions = np.full(num_original_words, -1, dtype=np.int32)\n        num_chunks = predictions.shape[0]\n        for i in range(num_chunks):\n            chunk_preds = predictions[i]\n            try:\n                chunk_word_ids = tokenized_inputs.word_ids(batch_index=i)\n            except Exception:\n                continue\n            if chunk_word_ids is None:\n                continue\n\n            chunk_len = len(chunk_preds)\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if token_pos >= chunk_len:\n                    break\n                if word_id is None:\n                    continue\n                if 0 <= word_id < num_original_words and final_predictions[word_id] == -1:\n                    final_predictions[word_id] = int(chunk_preds[token_pos])\n\n        filled = [\n            self.id2label.get(pid, default_label) if pid != -1 else default_label\n            for pid in final_predictions\n        ]\n        return filled\n\n    def predict(self, df: pd.DataFrame, stride: int = 128) -> List[List[str]]:\n        \"\"\"\n        Предсказывает метки для всех документов из DataFrame.\n\n        :param df: DataFrame с колонкой токенов.\n        :param stride: перекрытие между окнами.\n        :return: список документов, каждый — список строковых меток по словам.\n        :raises RuntimeError: если модель не обучена и документы непустые.\n        \"\"\"\n        all_final_labels = []\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Предсказание (sliding window)\"):\n            try:\n                original_tokens = row.get(self.tokens_column_name, None)\n            except Exception:\n                original_tokens = None\n\n            tokens = self._to_token_list(original_tokens)\n\n            if len(tokens) == 0:\n                all_final_labels.append([])\n                continue\n\n            if self.trainer is None or self.trainer.model is None:\n                raise RuntimeError(\"Модель не обучена. Вызовите .fit() перед .predict() для непустых документов.\")\n\n            document_labels = self._predict_single_document(tokens, stride)\n            all_final_labels.append(document_labels)\n        return all_final_labels\n\n    # ------------------------------\n    # Эмбеддинги\n    # ------------------------------\n    def _get_embeddings_single_document(self, tokens: List[str], stride: int, device: torch.device) -> np.ndarray:\n        \"\"\"\n        Извлекает эмбеддинги слов для одного документа.\n\n        :param tokens: список слов документа.\n        :param stride: перекрытие между окнами.\n        :param device: устройство модели (CPU/GPU).\n        :return: массив формы (num_words, hidden_size), dtype=float32.\n        :raises: None\n        \"\"\"\n        max_length = self._get_effective_max_length()\n        stride = self._sanitize_stride(stride, max_length)\n        num_original_words = len(tokens)\n\n        chunk_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        chunk_inputs.pop(\"overflow_to_sample_mapping\")\n\n        with torch.no_grad():\n            base_model = getattr(self.trainer.model, self.trainer.model.base_model_prefix)\n            outputs = base_model(**chunk_inputs)\n\n        chunk_embeddings = outputs.last_hidden_state\n\n        hidden_size = int(self.model.config.hidden_size)\n        final_word_embeddings = torch.zeros(num_original_words, hidden_size, device=device)\n        word_counts = torch.zeros(num_original_words, device=device)\n\n        for i in range(len(chunk_embeddings)):\n            chunk_embeds = chunk_embeddings[i]\n            chunk_word_ids = chunk_inputs.word_ids(batch_index=i)\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if word_id is not None:\n                    final_word_embeddings[word_id] += chunk_embeds[token_pos]\n                    if token_pos == 0 or chunk_word_ids[token_pos - 1] != word_id:\n                        word_counts[word_id] += 1\n\n        average_embeddings = final_word_embeddings / (word_counts.unsqueeze(1) + 1e-8)\n        return average_embeddings.detach().cpu().numpy()\n\n    def get_embeddings(self, df: pd.DataFrame, stride: int = 128) -> List[np.ndarray]:\n        \"\"\"\n        Извлекает эмбеддинги слов для каждого документа в DataFrame.\n\n        :param df: DataFrame с колонкой токенов.\n        :param stride: перекрытие между окнами.\n        :return: список массивов эмбеддингов, по одному на документ (num_words, hidden_size).\n        :raises RuntimeError: если модель не обучена.\n        \"\"\"\n        if self.trainer is None or self.trainer.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        self.trainer.model.eval()\n        device = self.trainer.model.device\n        all_final_embeddings = []\n\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Генерация эмбеддингов (sliding window)\"):\n            original_tokens = row[self.tokens_column_name]\n            if not original_tokens:\n                all_final_embeddings.append(np.zeros((0, int(self.model.config.hidden_size)), dtype=np.float32))\n                continue\n\n            document_embeddings = self._get_embeddings_single_document(original_tokens, stride, device)\n            all_final_embeddings.append(document_embeddings)\n\n        return all_final_embeddings","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Данные: строковые метки (будут конвертированы в id внутри .fit)\ntokens_col, tags_col = \"tokens\", \"tags\"\nlabel2id = {\n    \"O\": 0,\n    \"B-PER\": 1, \"I-PER\": 2,\n    \"B-LOC\": 3, \"I-LOC\": 4,\n    \"B-ORG\": 5, \"I-ORG\": 6,\n}\n\ndf_train = pd.DataFrame([\n    {tokens_col: [\"John\", \"Doe\", \"lives\", \"in\", \"Berlin\"], tags_col: [\"B-PER\",\"I-PER\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"Mary\", \"works\", \"at\", \"Google\"], tags_col: [\"B-PER\",\"O\",\"O\",\"B-ORG\"]},\n    {tokens_col: [\"Alice\", \"is\", \"from\", \"Paris\"], tags_col: [\"B-PER\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"IBM\", \"is\", \"in\", \"Armonk\"], tags_col: [\"B-ORG\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"Bob\", \"moved\", \"to\", \"London\"],  tags_col: [\"B-PER\",\"O\",\"O\",\"B-LOC\"]},\n    {tokens_col: [\"Google\", \"is\", \"in\", \"California\"], tags_col: [\"B-ORG\",\"O\",\"O\",\"B-LOC\"]},\n])\n\n# Инициализация (минимум, что требует класс)\nCKPT = \"prajjwal1/bert-tiny\"\ntc = TokenClassification(\n    checkpoint=CKPT,\n    label2id=label2id,\n    tokens_column_name=tokens_col,\n    tags_column_name=tags_col\n)\n\n# Обучение с максимальной параметризацией\ntc.fit(\n    train_data=df_train,\n    epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    test_size=0.33,             # включаем валидацию\n    learning_rate=3e-5,\n    fp16=True,                  # если есть CUDA — включит fp16\n    stride=64,                  # скользящее окно\n    logging_steps=1,\n    eval_steps=2,\n    output_dir=\"./tokcls_max_param\",\n    seed=123,\n    fit_chunk_size_docs=2,      # обучаемся «кусками» по 2 документа\n    early_stopping_patience=2,  # ранняя остановка после 2 неулучшений\n    early_stopping_threshold=0.0,\n)\n\n# Метрики от Trainer (включая per-entity F1: eval_f1_PER/LOC/ORG и т.д., если встретились)\nmetrics = tc.trainer.evaluate()\nprint(\"Eval metrics (subset):\", {k: float(v) for k, v in metrics.items() if isinstance(v, (int, float))})\n\n# Предсказание на части данных (с отдельным stride на инференсе)\ndf_infer = df_train.iloc[:3]\npreds = tc.predict(df_infer, stride=32)\nfor i, (tokens, pred) in enumerate(zip(df_infer[tokens_col], preds), 1):\n    print(f\"Doc {i}:\")\n    print(list(zip(tokens, pred)))\n\n# Эмбеддинги слов (каждый документ -> массив [num_words, hidden_size])\nembs = tc.get_embeddings(df_infer, stride=32)\nprint(\"Embeddings shapes:\", [e.shape for e in embs])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntokens_col, tags_col = \"tokens\", \"tags\"\nlabel2id = {\"O\": 0, \"B-PER\": 1}  # минимальный набор меток\n\n# Данные уже в id (минимальная разметка)\ndf_small = pd.DataFrame([\n    {tokens_col: [\"John\", \"works\"], tags_col: [1, 0]},  # [\"B-PER\",\"O\"]\n    {tokens_col: [\"Mary\", \"smiles\"], tags_col: [1, 0]},  # [\"B-PER\",\"O\"]\n])\n\n# Инициализация\nCKPT = \"prajjwal1/bert-tiny\"\ntc = TokenClassification(\n    checkpoint=CKPT,\n    label2id=label2id,\n    tokens_column_name=tokens_col,\n    tags_column_name=tags_col\n)\n\n# Обучение — все параметры по умолчанию\ntc.fit(train_data=df_small)\n\n# Базовый предикт — тоже по умолчанию\npreds = tc.predict(df_small)\nprint(\"Preds:\", preds)\n\n# При необходимости — эмбеддинги (тоже с параметрами по умолчанию)\nembs = tc.get_embeddings(df_small)\nprint(\"Embeddings shape for doc 0:\", embs[0].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  pillow==11.1.0 \\\n  numpy==1.26.4 \\\n  pandas==2.2.3 \\\n  tqdm==4.67.1 \\\n  transformers==4.51.3 \\\n  evaluate==0.4.5 \\\n  wav2clip==0.1.0 \\\n  torch==2.6.0+cu124 \\\n  torchaudio==2.6.0+cu124\n# !pip install evaluate wav2clip\n\nimport os\nimport time\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport math\nimport random\nimport gc\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, Generator, List, Optional, Union\n\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers.trainer_callback import TrainerCallback, PrinterCallback, EarlyStoppingCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport evaluate\n\n# =========================\n# Утилиты\n# =========================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef to_pil(x: Union[str, np.ndarray, 'Image.Image']) -> 'Image.Image':\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\ndef safe_load(component_cls, checkpoint: str, local_cache_dir: str = \"./model_cache\",\n              local_files_only: Optional[bool] = None, **kwargs):\n    if local_files_only is None:\n        local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n    name = getattr(component_cls, \"__name__\", \"\")\n    if \"Tokenizer\" in name:\n        kwargs.setdefault(\"use_fast\", True)\n    return component_cls.from_pretrained(\n        checkpoint, cache_dir=local_cache_dir, local_files_only=local_files_only, **kwargs\n    )\n\n\n# =========================\n# Токенизатор батчевый\n# =========================\n\nclass BatchTokenizer:\n    def __init__(\n        self,\n        tokenizer,\n        max_length: int = 512,\n        cache_size: int = 10000,\n        batch_size: int = 256,\n        use_fast: bool = True,\n        device: str = \"cpu\",\n        padding_strategy: str = \"max_length\"  # \"max_length\" или \"dynamic\"\n    ):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.use_fast = use_fast\n        self.device = device\n        self.padding_strategy = padding_strategy\n        if self.padding_strategy not in {\"max_length\", \"dynamic\"}:\n            raise ValueError(\"padding_strategy должен быть 'max_length' или 'dynamic'\")\n        self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n        self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n        if self.is_fast:\n            print(\"✓ Используется Fast Tokenizer\")\n\n    def _tokenize_single(self, text: str) -> tuple:\n        # В кэше храним только версии с фиксированной длиной — иначе нельзя будет склеивать батчи\n        result = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        return tuple((k, v.squeeze(0).cpu().numpy()) for k, v in result.items())\n\n    def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n        # Динамический паддинг: всегда токенизируем списком целиком (без кэша по-элементно)\n        if self.padding_strategy == \"dynamic\":\n            result = self.tokenizer(\n                texts,\n                padding=True,  # паддинг до «longest» в батче\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n        # Фиксированный паддинг — как раньше, но явно padding=\"max_length\"\n        if use_cache and len(texts) < 100:\n            results = [dict(self._cache(text)) for text in texts]\n            keys = results[0].keys()\n            batch_dict = {}\n            for key in keys:\n                dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                batch_dict[key] = torch.tensor(np.stack([r[key] for r in results]), dtype=dtype)\n            return batch_dict\n        else:\n            result = self.tokenizer(\n                texts,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n    def tokenize_dataset_lazy(\n        self,\n        texts: List[str],\n        batch_size: Optional[int] = None\n    ) -> Generator[Dict[str, torch.Tensor], None, None]:\n        batch_size = batch_size or self.batch_size\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            yield self.tokenize_batch(batch, use_cache=False)\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n\n# =========================\n# Универсальный датасет\n# =========================\n\nclass MultiComboDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        label2id: Dict[Any, int],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer: Optional[Any] = None,         # BatchTokenizer\n        text_tokenizer_fn: Optional[Callable] = None, # custom fn -> dict of tensors\n        special_tokens: Optional[Dict[str, Any]] = None,\n        pretokenize: bool = True,\n        pretokenize_batch_size: int = 1024,\n        tokenizer_returns_tensors: bool = True,\n        deduplicate_texts: bool = True,\n        max_cache_size=None\n    ):\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.label2id = label2id\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        # Один путь: BatchTokenizer; другой: custom fn\n        self.batch_tokenizer = text_tokenizer\n        self.text_tokenizer_fn = text_tokenizer_fn\n\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n        self._N = len(self.df)\n\n        # labels сразу в тензор [N]\n        if self.target_col in self.df.columns:\n            y = self.df[self.target_col].map(self.label2id).fillna(0).astype(int).values\n        else:\n            # если на инференсе метки нет — пусть будут 0\n            y = np.zeros(self._N, dtype=np.int64)\n        self._labels = torch.tensor(y, dtype=torch.long)\n\n        # Предсобранные списки изображений/аудио (чтобы не дёргать pandas в __getitem__)\n        self._image_lists = None\n        if self.image_columns:\n            self._image_lists = self._collect_multi_values(self.df, self.image_columns)\n\n        self._audio_lists = None\n        if self.audio_columns:\n            self._audio_lists = self._collect_multi_values(self.df, self.audio_columns)\n\n        # Предтокенизированные банки: dict(key -> torch.Tensor [N, ...])\n        self._tok_bank: Optional[Dict[str, torch.Tensor]] = None\n\n        # Предтокенизация текста (ускоряет обучение на порядки)\n        self._has_text = bool(self.text_columns)\n\n        # dynamic-паддинг несовместим с предтокенизацией (формы в батчах будут разные)\n        if pretokenize and self.batch_tokenizer is not None and getattr(self.batch_tokenizer, \"padding_strategy\", \"max_length\") == \"dynamic\":\n            print(\"⚠ Предтокенизация отключена: выбран dynamic-паддинг для текста.\")\n            pretokenize = False\n\n        if self._has_text and pretokenize:\n            t0 = time.time()\n            if self.batch_tokenizer is not None and self.text_tokenizer_fn is None:\n                # BatchTokenizer путь\n                self._pretokenize_with_batch_tokenizer(pretokenize_batch_size)\n            elif self.text_tokenizer_fn is not None:\n                # Custom fn путь (equal-split и т.п.)\n                self._pretokenize_with_custom_fn(pretokenize_batch_size, deduplicate_texts=deduplicate_texts)\n            else:\n                # Ни BatchTokenizer, ни custom fn — оставляем без токенов (коллатор потом сам токенизирует из строк)\n                pass\n            t1 = time.time()\n            if self._tok_bank is not None:\n                shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n                print(f\"✓ Предтокенизация завершена: {self._N} образцов за {t1 - t0:.2f}s | keys={list(self._tok_bank.keys())}, shapes={shapes}\")\n\n    def __len__(self) -> int:\n        return self._N\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item: Dict[str, Any] = {}\n        item[\"labels\"] = int(self._labels[idx])\n\n        # Текст: если есть предтокенизированные банки — просто слайсим\n        if self._tok_bank is not None:\n            item[\"text_tokens\"] = {k: v[idx] for k, v in self._tok_bank.items()}\n        elif self._has_text:\n            # Фоллбек: отдаём строку (коллатор бэкенда сам батчево токенизирует)\n            item[\"text\"] = self._join_text(self.df.iloc[idx])  # быстрый fallback; лучше всегда pretokenize\n\n        # Изображения/аудио — просто отдаём подготовленные списки\n        if self._image_lists is not None:\n            item[\"images\"] = self._image_lists[idx]\n        if self._audio_lists is not None:\n            item[\"audios\"] = self._audio_lists[idx]\n\n        return item\n\n    # --------------------------\n    # Вспомогательные методы\n    # --------------------------\n\n    def clear_cache(self):\n        # Освободить предтокенизированные банки (для экономии RAM между чанками)\n        self._tok_bank = None\n        torch.cuda.empty_cache()\n\n    def get_cache_stats(self) -> Dict[str, Any]:\n        has = self._tok_bank is not None\n        sizes = {k: tuple(v.shape) for k, v in (self._tok_bank or {}).items()}\n        return {\"has_pretokenized\": has, \"shapes\": sizes, \"N\": self._N}\n\n    def _join_text(self, row: pd.Series) -> str:\n        sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n        return sep.join([(\"\" if pd.isna(row[c]) else str(row[c])) for c in self.text_columns])\n\n    @staticmethod\n    def _as_list(v):\n        if v is None or (isinstance(v, float) and math.isnan(v)):\n            return []\n        if isinstance(v, (list, tuple)):\n            return list(v)\n        return [v]\n\n    def _collect_multi_values(self, df: pd.DataFrame, columns: List[str]) -> List[List[Any]]:\n        out = []\n        as_list = self._as_list\n        for _, row in df.iterrows():\n            lst: List[Any] = []\n            for c in columns:\n                if c in row:\n                    lst.extend([x for x in as_list(row[c]) if x is not None])\n            out.append(lst)\n        return out\n\n    # --------------------------\n    # Предтокенизация: BatchTokenizer\n    # --------------------------\n    def _pretokenize_with_batch_tokenizer(self, batch_size: int):\n        texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n        banks: Dict[str, List[torch.Tensor]] = {}\n\n        for start in range(0, self._N, batch_size):\n            batch = texts[start:start + batch_size]\n            tok = self.batch_tokenizer.tokenize_batch(batch, use_cache=False)  # dict[str, torch.Tensor [B, L]]\n            # Нормализуем типы\n            for k in tok:\n                if k in (\"input_ids\", \"attention_mask\", \"token_type_ids\"):\n                    tok[k] = tok[k].long()\n                else:\n                    tok[k] = tok[k].to(torch.float32)\n\n            # Сохраняем\n            for k, v in tok.items():\n                banks.setdefault(k, []).append(v)\n\n        # Склеиваем по первой оси\n        self._tok_bank = {k: torch.cat(v_parts, dim=0).contiguous() for k, v_parts in banks.items()}\n\n    # --------------------------\n    # Предтокенизация: custom fn\n    # --------------------------\n    def _pretokenize_with_custom_fn(self, batch_size: int, deduplicate_texts: bool = True):\n        # Подготовим «сырые» тексты как списки (без pandas в горячем цикле)\n        cols = self.text_columns\n        col_arrays = [self.df[c].astype(str).where(~self.df[c].isna(), other=\"\").tolist() for c in cols]\n\n        # Детектируем форму по первому примеру\n        first_td = {c: col_arrays[i][0] for i, c in enumerate(cols)}\n        first_tok = self.text_tokenizer_fn(first_td, self.special_tokens)\n        if not isinstance(first_tok, dict):\n            raise ValueError(\"custom text_tokenizer_fn должна возвращать dict тензоров\")\n\n        # Проверим одинаковую длину для всех ключей\n        shapes = {k: tuple(t.shape) for k, t in first_tok.items()}\n        if any(len(s) == 0 for s in shapes.values()):\n            raise ValueError(\"text_tokenizer_fn должна возвращать тензоры с размерностью хотя бы [L]\")\n\n        # Предвыделим банки\n        bank: Dict[str, torch.Tensor] = {}\n        for k, t in first_tok.items():\n            dtype = t.dtype if torch.is_tensor(t) else torch.long\n            bank[k] = torch.empty((self._N, *t.shape), dtype=dtype)\n\n        # Заполним первую строку\n        for k, t in first_tok.items():\n            bank[k][0].copy_(t if torch.is_tensor(t) else torch.tensor(t))\n\n        # Дедупликация (опционально)\n        cache: Dict[tuple, Dict[str, torch.Tensor]] = {}\n        if deduplicate_texts:\n            key0 = tuple(first_td.get(c, \"\") for c in cols)\n            cache[key0] = {k: (v.clone() if v.is_floating_point() else v.clone()) for k, v in first_tok.items()}\n\n        # Основной цикл: батчами формируем text_data и токенизируем per-sample (но единожды)\n        for start in range(1, self._N, batch_size):\n            end = min(self._N, start + batch_size)\n            for i in range(start, end):\n                # Сформировать text_data для i-й строки\n                td = {c: col_arrays[j][i] for j, c in enumerate(cols)}\n                if deduplicate_texts:\n                    key = tuple(td.get(c, \"\") for c in cols)\n                    got = cache.get(key)\n                    if got is None:\n                        tok = self.text_tokenizer_fn(td, self.special_tokens)\n                        cache[key] = tok\n                    else:\n                        tok = got\n                else:\n                    tok = self.text_tokenizer_fn(td, self.special_tokens)\n\n                # Записать в банки\n                for k, t in tok.items():\n                    if not torch.is_tensor(t):\n                        t = torch.tensor(t)\n                    bank[k][i].copy_(t)\n\n        # Сохраняем банки\n        self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n\n\n# =========================\n# Универсальный бэкенд\n# =========================\n\nclass BaseBackend(nn.Module):\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n    text_tokenizer_fn: Optional[Callable] = None\n    batch_tokenizer: Optional[BatchTokenizer] = None\n    special_tokens: Dict[str, str] = {}\n    tokenizer_returns_tensors: bool = False\n    local_cache_dir: str = \"./model_cache\"\n    text_padding_strategy: str = \"max_length\"  # стратегия паддинга текста\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        raise NotImplementedError\n\n    def freeze_all(self):\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n    def set_text_tokenizer(self, tokenizer_fn: Optional[Callable], special_tokens: Optional[Dict[str, str]] = None,\n                           returns_tensors: bool = False):\n        self.text_tokenizer_fn = tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = returns_tensors\n\n    def set_batch_tokenizer(self, tokenizer, max_length: int = 512,\n                            cache_size: int = 10000, batch_size: int = 256,\n                            padding_strategy: str = \"max_length\"):\n        self.text_padding_strategy = padding_strategy\n        self.batch_tokenizer = BatchTokenizer(\n            tokenizer=tokenizer,\n            max_length=max_length,\n            cache_size=cache_size,\n            batch_size=batch_size,\n            use_fast=True,\n            padding_strategy=padding_strategy\n        )\n\n\nclass UniversalMultiBackend(BaseBackend):\n    name = \"universal\"\n\n    class _ParamDeviceProxy(nn.Module):\n        def __init__(self, base, device: torch.device):\n            super().__init__()\n            self.base = base if isinstance(base, nn.Module) else None\n            self._callable = base if not isinstance(base, nn.Module) else None\n            self._dummy = nn.Parameter(torch.empty(0), requires_grad=False)\n            with torch.no_grad():\n                self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n\n        def forward(self, *args, **kwargs):\n            target = self.base if self.base is not None else self._callable\n            return target(*args, **kwargs)\n\n        def to(self, device, *args, **kwargs):\n            self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n            return super().to(device, *args, **kwargs)\n\n    def _preferred_device(self) -> torch.device:\n        if torch.cuda.is_available():\n            return torch.device(\"cuda\")\n        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n            return torch.device(\"mps\")\n        return torch.device(\"cpu\")\n\n    def _wrap_if_parameterless(self, model, device: torch.device):\n        try:\n            it = model.parameters() if hasattr(model, \"parameters\") else iter(())\n            next(it)\n            return model\n        except StopIteration:\n            return self._ParamDeviceProxy(model, device)\n        except Exception:\n            return self._ParamDeviceProxy(model, device)\n\n    def __init__(\n        self,\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        freeze: bool = True,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        use_batch_tokenizer: bool = True,\n        tokenizer_cache_size: int = 10000,\n        tokenizer_batch_size: int = 256,\n        local_cache_dir: str = \"./model_cache\",\n        text_padding_strategy: str = \"max_length\"  # стратегия паддинга текста\n    ):\n        super().__init__()\n        self.supported = set()\n        self.out_dim_per_modality = {}\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.tokenizer_batch_size = tokenizer_batch_size\n        self.local_cache_dir = local_cache_dir\n        self.text_padding_strategy = text_padding_strategy\n\n        self.text_model = None\n        self.text_processor = None\n        self.text_config = text_model_config or {}\n        if text_model_config:\n            self._init_text_model(text_model_config)\n            self.supported.add(\"text\")\n\n        self.image_model = None\n        self.image_processor = None\n        self.image_config = image_model_config or {}\n        if image_model_config:\n            self._init_image_model(image_model_config)\n            self.supported.add(\"image\")\n\n        self.audio_model = None\n        self.audio_processor = None\n        self.audio_config = audio_model_config or {}\n        if audio_model_config:\n            self._init_audio_model(audio_model_config)\n            self.supported.add(\"audio\")\n\n        if freeze:\n            self.freeze_all()\n\n    def _ensure_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        if x is None:\n            return None\n        if x.dim() == 1:\n            return x.unsqueeze(0)\n        if x.dim() > 2:\n            return x.view(x.size(0), -1)\n        return x\n\n    def _normalize_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        x = self._ensure_2d(x)\n        return F.normalize(x, dim=-1, eps=1e-12) if x is not None and x.numel() > 0 else x\n\n    def _init_text_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoTokenizer, CLIPModel, CLIPTokenizer, ClapModel, ClapProcessor\n\n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n\n        print(f\"Загрузка текстовой модели {checkpoint}...\")\n\n        if model_type == 'clip':\n            self.text_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(CLIPTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.projection_dim\n        elif model_type == 'clap':\n            self.text_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            proc = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = getattr(proc, 'tokenizer', None) or safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = getattr(self.text_model.config, \"projection_dim\", 512)\n        else:\n            self.text_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.text_model  = self._wrap_if_parameterless(self.text_model, dev)\n\n        if self.use_batch_tokenizer and self.text_processor is not None:\n            self.set_batch_tokenizer(\n                self.text_processor,\n                max_length=config.get('max_length', 512),\n                cache_size=self.tokenizer_cache_size,\n                batch_size=self.tokenizer_batch_size,\n                padding_strategy=self.text_padding_strategy\n            )\n\n        self.text_config['max_length'] = config.get('max_length', 512)\n        self.text_config['dim'] = dim\n        self.text_config['model_type'] = model_type\n        self.out_dim_per_modality['text'] = dim\n\n    def _init_image_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoImageProcessor, CLIPModel, CLIPImageProcessor\n\n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n\n        print(f\"Загрузка визуальной модели {checkpoint}...\")\n\n        if model_type == 'clip':\n            self.image_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(CLIPImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.projection_dim\n        else:\n            self.image_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(AutoImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.image_model = self._wrap_if_parameterless(self.image_model, dev)\n\n        self.image_config['max_images'] = config.get('max_images', 1)\n        self.image_config['image_agg'] = config.get('image_agg', 'concat')\n        self.image_config['dim'] = dim\n        self.image_config['model_type'] = model_type\n\n        self.out_dim_per_modality['image'] = (dim * self.image_config['max_images']) if self.image_config['image_agg'] == 'concat' else dim\n\n    def _init_audio_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoProcessor, ClapModel, ClapProcessor\n\n        model_type = config.get('model_type', 'auto').lower()\n        checkpoint = config.get('checkpoint', None)\n\n        print(f\"Загрузка аудио модели (type={model_type})...\")\n\n        if model_type == 'wav2clip':\n            import wav2clip as w2c\n            self._w2c = w2c\n\n            w2c_model = None\n            if hasattr(w2c, \"get_model\"):\n                w2c_model = w2c.get_model()\n            elif hasattr(w2c, \"model\"):\n                m = w2c.model\n                w2c_model = m() if callable(m) else m\n            else:\n                raise RuntimeError(\"wav2clip не содержит get_model()/model. Обновите пакет wav2clip.\")\n\n            self.audio_model = w2c_model\n\n            try:\n                if isinstance(self.audio_model, torch.nn.Module) and torch.cuda.is_available():\n                    self.audio_model = self.audio_model.to(\"cuda\")\n            except Exception:\n                pass\n\n            self.audio_processor = None\n            dim = 512\n            sr = config.get('sr', 16000)\n\n        elif model_type == 'clap':\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для CLAP\")\n            self.audio_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = getattr(self.audio_model.config, \"projection_dim\", 512)\n            sr = getattr(self.audio_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.audio_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n\n        else:\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для аудио-моделей, кроме wav2clip\")\n            self.audio_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(AutoProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.audio_model.config.hidden_size\n            fe = getattr(self.audio_processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 16000) if fe is not None else 16000\n\n        dev = self._preferred_device()\n        self.audio_model = self._wrap_if_parameterless(self.audio_model, dev)\n\n        self.audio_config['sr'] = config.get('sr', sr)\n        self.audio_config['max_audios'] = config.get('max_audios', 1)\n        self.audio_config['audio_agg'] = config.get('audio_agg', 'concat')\n        self.audio_config['dim'] = dim\n        self.audio_config['model_type'] = model_type\n\n        self.out_dim_per_modality['audio'] = (\n            dim * self.audio_config['max_audios']\n            if self.audio_config['audio_agg'] == 'concat' else dim\n        )\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        labels = []\n        for b in batch:\n            labels.append(torch.tensor(b.get(\"labels\", 0), dtype=torch.long))\n        labels = torch.stack(labels)\n\n        backend_inputs: Dict[str, Any] = {}\n        batch_size = len(batch)\n\n        if self.text_model is not None:\n            if \"text_tokens\" in batch[0]:\n                text_inputs = {}\n                for key in batch[0][\"text_tokens\"].keys():\n                    if torch.is_tensor(batch[0][\"text_tokens\"][key]):\n                        text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n                    else:\n                        dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                        text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n                backend_inputs[\"text_inputs\"] = text_inputs\n            else:\n                texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n                if self.batch_tokenizer:\n                    text_inputs = self.batch_tokenizer.tokenize_batch(texts, use_cache=True)\n                else:\n                    pad = \"max_length\" if getattr(self, \"text_padding_strategy\", \"max_length\") == \"max_length\" else True\n                    text_inputs = self.text_processor(\n                        texts, padding=pad, truncation=True,\n                        max_length=self.text_config.get('max_length', 512),\n                        return_tensors=\"pt\"\n                    )\n                backend_inputs[\"text_inputs\"] = {k: v for k, v in text_inputs.items()}\n\n        if self.image_model is not None:\n            images_lists = [b.get(\"images\", []) for b in batch]\n            flat_images, img_counts = [], []\n            for lst in images_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [img for img in lst if img is not None]\n                img_counts.append(len(lst))\n                for img in lst:\n                    flat_images.append(to_pil(img))\n\n            if len(flat_images) > 0:\n                img_proc = self.image_processor(images=flat_images, return_tensors=\"pt\")\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": img_proc[\"pixel_values\"]}\n            else:\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": None}\n\n            backend_inputs[\"image_counts\"] = torch.tensor(img_counts, dtype=torch.long)\n\n        if self.audio_model is not None:\n            audios_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios, aud_counts = [], []\n            for lst in audios_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_config['sr']))\n                    elif isinstance(a, np.ndarray):\n                        aa = np.asarray(a, dtype=np.float32)\n                        if aa.ndim > 1:\n                            aa = np.squeeze(aa)\n                        if aa.ndim > 1:\n                            aa = aa.reshape(-1)\n                        flat_audios.append(aa)\n            if self.audio_config.get('model_type') == 'wav2clip':\n                backend_inputs[\"audio_inputs\"] = {\"raw_audios\": flat_audios}\n            elif len(flat_audios) > 0:\n                if self.audio_config.get('model_type') == 'clap':\n                    aud_proc = self.audio_processor(\n                        audios=flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_features\": aud_proc[\"input_features\"]}\n                else:\n                    aud_proc = self.audio_processor(\n                        flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_values\": aud_proc[\"input_values\"]}\n            else:\n                backend_inputs[\"audio_inputs\"] = {\"input_features\": None, \"input_values\": None, \"raw_audios\": []}\n\n            backend_inputs[\"audio_counts\"] = torch.tensor(aud_counts, dtype=torch.long)\n\n        backend_inputs[\"batch_size\"] = batch_size\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _aggregate_embeddings(\n        self,\n        embs: Optional[torch.Tensor],\n        counts: List[int],\n        max_k: int,\n        dim_hint: int,\n        agg_type: str,\n        batch_size: int,\n        device: torch.device\n    ) -> torch.Tensor:\n        if embs is None or (torch.is_tensor(embs) and embs.numel() == 0):\n            feat_dim = int(dim_hint) if dim_hint is not None else 0\n            out_dim = feat_dim * max_k if agg_type == 'concat' else feat_dim\n            return torch.zeros((batch_size, out_dim), device=device, dtype=torch.float32)\n\n        if not torch.is_tensor(embs):\n            embs = torch.as_tensor(embs, device=device, dtype=torch.float32)\n        if embs.dim() == 1:\n            embs = embs.unsqueeze(0)\n        elif embs.dim() > 2:\n            embs = embs.view(embs.size(0), -1)\n\n        N, D = embs.size()\n        out_dim = (D * max_k) if agg_type == 'concat' else D\n        out = torch.zeros((batch_size, out_dim), device=device, dtype=embs.dtype)\n\n        offset = 0\n        for i, c in enumerate(counts):\n            if c <= 0 or offset >= N:\n                continue\n            take_n = min(c, N - offset)\n            sample = embs[offset:offset + take_n]\n            offset += take_n\n\n            if agg_type == 'concat':\n                take = sample[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n\n        return F.normalize(out, dim=-1, eps=1e-12) if out.size(1) > 0 else out\n\n    @torch.no_grad()\n    def _wav2clip_embed(self, arr: np.ndarray, device: torch.device) -> torch.Tensor:\n        arr = np.asarray(arr, dtype=np.float32)\n        if arr.ndim > 1:\n            arr = np.squeeze(arr)\n        if arr.ndim > 1:\n            arr = arr.reshape(-1)\n        if arr.size < 512:\n            arr = np.pad(arr, (0, 512 - arr.size), mode=\"constant\")\n\n        try:\n            emb = self._w2c.embed_audio(arr, self.audio_model)\n            emb = np.asarray(emb)\n        except Exception:\n            x = torch.from_numpy(arr).float().unsqueeze(0).to(device)\n            y = self.audio_model(x)\n            if isinstance(y, (tuple, list)):\n                y = y[0]\n            if torch.is_tensor(y):\n                if y.dim() == 2 and y.size(0) == 1:\n                    y = y.squeeze(0)\n                emb = y.detach().cpu().numpy()\n            else:\n                emb = np.asarray(y)\n\n        if emb.ndim > 1:\n            emb = emb.reshape(-1)\n        return torch.as_tensor(emb, device=device, dtype=torch.float32)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        results: Dict[str, torch.Tensor] = {}\n        batch_size = int(backend_inputs.get(\"batch_size\", 1))\n\n        if self.text_model is not None and \"text_inputs\" in backend_inputs:\n            text_inputs = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n            if hasattr(self.text_model, \"get_text_features\"):\n                text_z = self.text_model.get_text_features(**text_inputs)\n            else:\n                outputs = self.text_model(**text_inputs)\n                text_z = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n            results[\"text\"] = self._normalize_2d(text_z)\n\n        if self.image_model is not None and \"image_inputs\" in backend_inputs:\n            pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n            counts = backend_inputs[\"image_counts\"].tolist()\n            total_images_needed = sum(counts)\n\n            img_flat = None\n            actual_img_dim = self.image_config.get(\"dim\", 768)\n\n            if pi is not None and pi.numel() > 0 and total_images_needed > 0:\n                pi = pi.to(device)\n                if pi.size(0) > total_images_needed:\n                    pi = pi[:total_images_needed]\n\n                if hasattr(self.image_model, \"get_image_features\"):\n                    img_flat = self.image_model.get_image_features(pixel_values=pi)\n                else:\n                    outputs = self.image_model(pixel_values=pi)\n                    img_flat = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state[:, 0]\n\n                img_flat = self._normalize_2d(img_flat)\n                actual_img_dim = img_flat.size(1) if img_flat is not None else actual_img_dim\n\n            img_z = self._aggregate_embeddings(\n                img_flat, counts,\n                self.image_config[\"max_images\"],\n                actual_img_dim,\n                self.image_config[\"image_agg\"],\n                len(counts),\n                device\n            )\n\n            if actual_img_dim != self.image_config.get(\"dim\"):\n                self.image_config[\"dim\"] = actual_img_dim\n                self.out_dim_per_modality[\"image\"] = (\n                    actual_img_dim * self.image_config[\"max_images\"]\n                    if self.image_config[\"image_agg\"] == \"concat\" else actual_img_dim\n                )\n\n            results[\"image\"] = img_z\n\n        if self.audio_model is not None and \"audio_inputs\" in backend_inputs:\n            counts = backend_inputs[\"audio_counts\"].tolist()\n            total_audios_needed = sum(counts)\n\n            aud_flat = None\n            actual_aud_dim = self.audio_config.get(\"dim\", 768)\n            model_type = self.audio_config.get(\"model_type\")\n\n            if total_audios_needed > 0:\n                if model_type == \"clap\":\n                    af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n                    if af is not None and af.numel() > 0:\n                        af = af.to(device)\n                        if af.size(0) > total_audios_needed:\n                            af = af[:total_audios_needed]\n                        with torch.cuda.amp.autocast(enabled=False):\n                            aud_flat = self.audio_model.get_audio_features(input_features=af.float())\n                        aud_flat = self._normalize_2d(aud_flat.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n                elif model_type == \"wav2clip\":\n                    raw_list = backend_inputs[\"audio_inputs\"].get(\"raw_audios\", [])\n                    if len(raw_list) > total_audios_needed:\n                        raw_list = raw_list[:total_audios_needed]\n                    if len(raw_list) > 0:\n                        embs = [self._wav2clip_embed(arr, device) for arr in raw_list]\n                        aud_flat = torch.stack(embs, dim=0)\n                        aud_flat = self._normalize_2d(aud_flat)\n                        actual_aud_dim = aud_flat.size(1)\n\n                else:\n                    av = backend_inputs[\"audio_inputs\"][\"input_values\"]\n                    if av is not None and av.numel() > 0:\n                        av = av.to(device)\n                        if av.size(0) > total_audios_needed:\n                            av = av[:total_audios_needed]\n                        av = av.clamp_(-1.0, 1.0)\n                        with torch.cuda.amp.autocast(enabled=False):\n                            outputs = self.audio_model(input_values=av.float())\n                            feats = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n                        aud_flat = self._normalize_2d(feats.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n            aud_z = self._aggregate_embeddings(\n                aud_flat, counts,\n                self.audio_config[\"max_audios\"],\n                actual_aud_dim,\n                self.audio_config[\"audio_agg\"],\n                len(counts),\n                device\n            )\n\n            if aud_flat is not None and actual_aud_dim != self.audio_config.get(\"dim\"):\n                self.audio_config[\"dim\"] = actual_aud_dim\n                self.out_dim_per_modality[\"audio\"] = (\n                    actual_aud_dim * self.audio_config[\"max_audios\"]\n                    if self.audio_config[\"audio_agg\"] == \"concat\" else actual_aud_dim\n                )\n\n            results[\"audio\"] = aud_z\n\n        if results:\n            bs_list = [v.size(0) for v in results.values()]\n            if len(set(bs_list)) != 1:\n                raise RuntimeError(f\"Inconsistent batch sizes across modalities: {bs_list}\")\n\n        return results\n\n\n# =========================\n# Классификатор\n# =========================\n\nclass SingleBackboneClassifier(nn.Module):\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_labels: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError('Для fusion=\"mean\" размеры модальностей должны совпадать')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                has_trainable = any(p.requires_grad for p in m.parameters()) if hasattr(m, \"parameters\") else False\n            except Exception:\n                has_trainable = False\n            if not has_trainable:\n                continue\n            try:\n                cfg = getattr(m, \"config\", None)\n                if cfg is not None and hasattr(cfg, \"use_cache\"):\n                    cfg.use_cache = False\n            except Exception:\n                pass\n            try:\n                if hasattr(m, \"gradient_checkpointing_enable\"):\n                    try:\n                        if gradient_checkpointing_kwargs is not None:\n                            m.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                        else:\n                            m.gradient_checkpointing_enable()\n                    except TypeError:\n                        m.gradient_checkpointing_enable()\n            except Exception:\n                pass\n\n    def gradient_checkpointing_disable(self):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                if hasattr(m, \"gradient_checkpointing_disable\"):\n                    m.gradient_checkpointing_disable()\n            except Exception:\n                pass\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        batch_size = None\n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() == 3:\n                    t = t.mean(dim=1)\n                elif t.dim() > 3:\n                    t = t.view(t.size(0), -1)\n                feats.append(t)\n                if batch_size is None:\n                    batch_size = t.size(0)\n        if batch_size is not None:\n            feats = [f[:batch_size] for f in feats]\n        if self.fusion == \"concat\":\n            out = torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            out = torch.stack(feats, dim=0).mean(dim=0)\n        return out\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        if not z:\n            raise ValueError(\"Backend не вернул эмбеддинги\")\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\n# =========================\n# Trainer с весами классов\n# =========================\n\nclass WeightedCETrainer(Trainer):\n    def __init__(self, *args, num_labels=None, train_labels=None, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        if class_weights is not None:\n            self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32)\n        elif train_labels is not None and num_labels is not None:\n            y = np.asarray(train_labels).astype(int)\n            counts = np.bincount(y, minlength=num_labels)\n            n = counts.sum()\n            w = np.zeros(num_labels, dtype=np.float32)\n            nz = counts > 0\n            w[nz] = n / (num_labels * counts[nz].astype(np.float32))\n            self.class_weights = torch.tensor(w, dtype=torch.float32)\n        else:\n            self.class_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        if logits.size(0) != labels.size(0):\n            raise ValueError(f\"Batch size mismatch: logits batch={logits.size(0)} vs labels batch={labels.size(0)}\")\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = F.cross_entropy(logits, labels.long(), weight=weight)\n        return (loss, outputs) if return_outputs else loss\n\n\n# =========================\n# Прогресс-логгер\n# =========================\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n        self.tqdm = tqdm\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            self.tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\n# =========================\n# Основной пайплайн\n# =========================\n\nclass SingleModelMultiComboClassification:\n    \"\"\"\n    Универсальный пайплайн для мульти-модальной классификации (text / image / audio) поверх моделей Hugging Face.\n    Поддерживает автоматический выбор бэкенда (CLIP/CLAP/Auto), батчевую токенизацию текста, кэширование,\n    чанковую тренировку, раннюю остановку, взвешивание классов, предсказание и извлечение эмбеддингов.\n\n    Основные возможности:\n    - Автоматическая сборка бэкенда: CLIP для связки text+image, CLAP для text+audio, либо произвольные Auto-модели.\n    - Работа с тремя модальностями: text, image, audio (любой поднабор).\n    - Батчевая токенизация текста с кэшем и опциональной предварительной токенизацией датасета.\n    - Чанковая тренировка очень больших датасетов без перегрузки памяти.\n    - Сбалансированная (взвешенная) кросс-энтропия на основе частот классов в тренировочных данных.\n    - Прогресс-бар, ранняя остановка и выбор лучшей модели по метрике.\n    - Предсказания и извлечение эмбеддингов (в том числе по модальностям).\n\n    :param modalities: Список используемых модальностей из {\"text\", \"image\", \"audio\"}.\n    :param num_labels: Число классов в задаче классификации.\n    :param target_column_name: Имя колонки с целевой меткой в DataFrame.\n    :param text_columns: Список текстовых колонок (используются, если выбрана модальность \"text\"). Значения будут\n                         конкатенированы через special_tokens[\"sep\"] при подготовке примеров.\n    :param image_columns: Список колонок с изображениями (пути к файлам, PIL.Image, np.ndarray или списки таких объектов),\n                          используется, если выбрана модальность \"image\".\n    :param audio_columns: Список колонок с аудио (пути к файлам или массивы np.ndarray; моно, float32),\n                          используется, если выбрана модальность \"audio\". Для чтения из файлов требуется torchaudio.\n    :param text_tokenizer_fn: Кастомная функция токенизации текста (если не используется встроенный BatchTokenizer).\n                              Сигнатура: fn(text_dict: Dict[str, str], special_tokens: Dict[str, str]) -> Union[Dict[str, Tensor], str].\n                              Если возвращает dict с ключами вроде 'input_ids', считается, что функция сразу возвращает тензоры токенов;\n                              иначе строку для последующей стандартной токенизации.\n    :param special_tokens: Спец. токены/разделители для подготовки текста. По умолчанию {\"sep\": \" [SEP] \"}.\n    :param tokenizer_returns_tensors: Флаг, сигнализирующий, что custom text_tokenizer_fn возвращает уже тензоры\n                                      (dict c 'input_ids', 'attention_mask' и т.д.). Влияет на коллатор.\n    :param backend: Режим сборки бэкенда. \"auto\" — подобрать оптимальные модели по модальностям;\n                    \"clip\" — CLIP для текста и изображений; \"clap\" — CLAP для текста и аудио; любое иное — ручные конфиги.\n    :param clip_checkpoint: Чекпойнт CLIP (используется при auto/clip), по умолчанию \"openai/clip-vit-base-patch32\".\n    :param clap_checkpoint: Чекпойнт CLAP (используется при auto/clap), по умолчанию \"laion/clap-htsat-unfused\".\n    :param text_model_config: Конфиг текстовой модели. Минимум: {\"checkpoint\": \"...\", \"model_type\": \"...\"}.\n                              Дополнительно: \"max_length\" и т.д. Примеры model_type: \"clip\", \"clap\", \"bert\", \"auto\".\n    :param image_model_config: Конфиг визуальной модели. Минимум: {\"checkpoint\": \"...\", \"model_type\": \"...\"}.\n                               Дополнительно: \"max_images\", \"image_agg\" (\"concat\"|\"mean\") и т.д. Примеры model_type: \"clip\", \"vit\", \"auto\".\n    :param audio_model_config: Конфиг аудио-модели. Минимум: {\"checkpoint\": \"...\", \"model_type\": \"...\"} (кроме \"wav2clip\").\n                               Дополнительно: \"max_audios\", \"audio_agg\" (\"concat\"|\"mean\"), \"sr\". Примеры model_type: \"clap\", \"wav2clip\", \"auto\".\n    :param fusion: Способ слияния модальностей в классификаторе: \"concat\" или \"mean\".\n                   При \"mean\" размеры эмбеддингов всех модальностей должны совпадать.\n    :param freeze_backbone: Если True — бэкенды заморожены (тренируется только классификационная \"голова\").\n    :param clip_max_length: Максимальная длина текста для CLIP-токенизатора (по умолчанию 77).\n    :param max_images_per_sample: Сколько изображений брать на сэмпл (усреднение или конкатенация задаются в image_model_config[\"image_agg\"]).\n    :param max_audios_per_sample: Сколько аудио брать на сэмпл (аналично image, параметр audio_model_config[\"audio_agg\"]).\n    :param use_batch_tokenizer: Использовать BatchTokenizer для текста (ускоряет токенизацию и кэширует результаты).\n    :param pretokenize_data: Предварительно токенизировать текст датасета (в памяти) для ускорения обучения/инференса.\n    :param pretokenize_batch_size: Батч-размер при предварительной токенизации.\n    :param tokenizer_cache_size: Размер LRU-кэша в BatchTokenizer.\n    :param max_pretokenize_samples: Максимум сэмплов для предварительной токенизации на чанк/датасет.\n    :param local_cache_dir: Локальная директория кэша моделей/процессоров HF.\n    :param text_padding: \"max_length\" — паддинг до фиксированной длины; \"dynamic\" — паддинг до максимальной длины в батче.\n                         При \"dynamic\" предтокенизация текста автоматически отключается.\n\n    :return: None\n\n    :raises ValueError: Если бэкенд не поддерживает выбранные модальности (внутренняя проверка соответствия).\n                        Также возможны ошибки конфигов, например fusion=\"mean\" при несовпадающих размерах эмбеддингов.\n    :raises OSError: Ошибки загрузки моделей/процессоров из Hugging Face Hub (сетевые/офлайн проблемы, отсутствующие чекпойнты).\n    :raises RuntimeError: Проблемы с устройством/драйвером (CUDA/MPS), несовместимость версий зависимостей и т.п.\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        num_labels: int,\n        target_column_name: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1,\n        use_batch_tokenizer: bool = True,\n        pretokenize_data: bool = True,\n        pretokenize_batch_size: int = 256,\n        tokenizer_cache_size: int = 10000,\n        max_pretokenize_samples: int = 100000,\n        local_cache_dir: str = \"./model_cache\",\n        text_padding: str = \"max_length\"  # \"max_length\" или \"dynamic\"\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.num_labels = num_labels\n        self.target_column_name = target_column_name\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n        self.text_padding = text_padding\n\n        self.label2id: Optional[Dict[Any, int]] = None\n        self.id2label: Optional[Dict[int, str]] = None\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[PbarConsoleLogger] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        mods = set(self.modalities)\n        name = self.backend_name\n\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n                }\n                self.image_model_config = self.image_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n                }\n            elif mods == {\"text\", \"audio\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n                }\n                self.audio_model_config = self.audio_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n                }\n            else:\n                if \"text\" in mods and self.text_model_config is None:\n                    self.text_model_config = {'checkpoint': 'bert-base-multilingual-cased', 'model_type': 'bert', 'max_length': 512}\n                if \"image\" in mods and self.image_model_config is None:\n                    self.image_model_config = {'checkpoint': 'google/vit-base-patch16-224', 'model_type': 'vit', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'}\n                if \"audio\" in mods and self.audio_model_config is None:\n                    self.audio_model_config = {'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000}\n\n        elif name == \"clip\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n            }\n            self.image_model_config = self.image_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n            }\n        elif name == \"clap\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n            }\n            self.audio_model_config = self.audio_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n            }\n        else:\n            pass\n\n        self.backend = UniversalMultiBackend(\n            text_model_config=self.text_model_config if \"text\" in mods else None,\n            image_model_config=self.image_model_config if \"image\" in mods else None,\n            audio_model_config=self.audio_model_config if \"audio\" in mods else None,\n            freeze=self.freeze_backbone,\n            text_tokenizer_fn=self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors,\n            use_batch_tokenizer=self.use_batch_tokenizer,\n            tokenizer_cache_size=self.tokenizer_cache_size,\n            tokenizer_batch_size=self.pretokenize_batch_size,\n            local_cache_dir=self.local_cache_dir,\n            text_padding_strategy=self.text_padding\n        )\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}\")\n\n    def _setup_metrics(self, metric_name: str):\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            metric = evaluate.load(\"f1\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids, average=\"weighted\")\n        elif metric_name == \"accuracy\":\n            metric = evaluate.load(\"accuracy\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids)\n        else:\n            raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"f1\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1,\n        gradient_checkpointing: bool = False,\n        fit_chunk_size: Optional[int] = None,\n        clear_cache_every_n_chunks: int = 10\n    ):\n        \"\"\"\n        Обучает классификатор на заданном DataFrame с поддержкой валидации, ранней остановки и чанковой тренировки.\n        Внутренне использует WeightedCETrainer (кросс-энтропия с весами классов по обратной частоте в train_data),\n        логгер прогресса и, при необходимости, предварительную токенизацию батчей текста.\n\n        Если test_data не передан, train_data разделяется на train/eval по test_size (стратификации нет).\n        При очень больших датасетах обучение проводится чанками (fit_chunk_size), чтобы ограничить потребление памяти.\n\n        :param train_data: Обучающий DataFrame. Должен содержать столбец target_column_name, а также столбцы по выбранным модальностям:\n                           - text_columns для \"text\" (строки, допускаются NaN),\n                           - image_columns для \"image\" (строки путей к файлам, PIL.Image, np.ndarray или списки этих типов),\n                           - audio_columns для \"audio\" (пути к аудиофайлам или np.ndarray моно-сигнала; для путей требуется torchaudio).\n        :param epochs: Количество эпох обучения.\n        :param test_size: Доля данных на валидацию, если test_data не задан.\n        :param test_data: Отдельный DataFrame для валидации. Если указан, параметр test_size игнорируется.\n        :param per_device_train_batch_size: Батч-размер на устройство для обучения.\n        :param gradient_accumulation_steps: Шаги аккумулирования градиента (эффективный батч = batch_size * steps).\n        :param learning_rate: Начальная скорость обучения (оптимизатор и шедулер создаются внутри Trainer).\n        :param metric_name: Метрика ранней остановки/выбора лучшей модели: \"f1\" (weighted) или \"accuracy\".\n        :param fp16: Использовать ли полуточность (только при наличии CUDA).\n        :param logging_steps: Периодичность логирования в шагах.\n        :param eval_steps: Периодичность валидации/сохранения модели в шагах.\n        :param output_dir: Директория для чекпойнтов/логов.\n        :param seed: Начальное зерно для воспроизводимости.\n        :param hidden: Размер скрытого слоя классификационной головы.\n        :param dropout: Дропаут в классификационной голове.\n        :param gradient_checkpointing: Включить gradient checkpointing в бэкендах (если они обучаемые).\n        :param fit_chunk_size: Размер чанка для поэтапной тренировки (None — весь датасет за эпоху без разбиения).\n        :param clear_cache_every_n_chunks: Каждые N чанков очищать кэш токенизации (для экономии памяти).\n\n        :return: self (для чейнинга вызовов).\n\n        :raises ValueError:\n            - Отсутствуют обязательные колонки по модальностям в train_data (внутренняя проверка _validate_data).\n            - Невозможно собрать классификатор с fusion=\"mean\" при разных размерах эмбеддингов модальностей.\n        :raises RuntimeError: Ошибки, возникающие внутри transformers.Trainer (например, рассогласование батчей/логитов),\n                              проблемы с устройством (CUDA OOM, MPS), ошибки чтения аудио/изображений.\n        :raises OSError: Ошибки чтения исходных файлов данных (изображения/аудио) или кэша моделей.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        classes = sorted(train_data[self.target_column_name].unique().tolist())\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n        if test_data is None:\n            df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n        else:\n            df_train, df_eval = train_data, test_data\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds_eval = MultiComboDataset(\n            df=df_eval,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_eval) < 50000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_eval), self.max_pretokenize_samples),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n        counts = np.bincount(y_train_all, minlength=self.num_labels)\n        n_all = counts.sum()\n        class_weights = np.zeros(self.num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        class_weights[nonzero] = n_all / (self.num_labels * counts[nonzero].astype(np.float32))\n\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        self._setup_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def data_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        n_train = len(df_train)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        dummy_idx = np.arange(min(len(df_train), 1))\n        ds_train_init = (\n            MultiComboDataset(\n                df=df_train.iloc[dummy_idx],\n                target_col=self.target_column_name,\n                label2id=self.label2id,\n                text_columns=self.text_columns,\n                image_columns=self.image_columns,\n                audio_columns=self.audio_columns,\n                text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                special_tokens=self.special_tokens,\n                pretokenize=False,\n                tokenizer_returns_tensors=self.tokenizer_returns_tensors\n            ) if len(dummy_idx) > 0 else ds_eval\n        )\n\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            num_labels=self.num_labels,\n            train_labels=y_train_all,\n            class_weights=class_weights\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        steps_done = 0\n        chunk_counter = 0\n\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n\n            for slc in chunk_slices(shuffled, chunk_size):\n                chunk_df = df_train.iloc[slc]\n                should_pretokenize = (\n                    self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                    and len(slc) < self.max_pretokenize_samples and len(slc) > 100\n                )\n\n                ds_chunk = MultiComboDataset(\n                    df=chunk_df,\n                    target_col=self.target_column_name,\n                    label2id=self.label2id,\n                    text_columns=self.text_columns,\n                    image_columns=self.image_columns,\n                    audio_columns=self.audio_columns,\n                    text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                    text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                    special_tokens=self.special_tokens,\n                    pretokenize=should_pretokenize,\n                    pretokenize_batch_size=self.pretokenize_batch_size,\n                    max_cache_size=min(len(slc), self.max_pretokenize_samples),\n                    tokenizer_returns_tensors=self.tokenizer_returns_tensors\n                )\n\n                self.trainer.train_dataset = ds_chunk\n\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk, chunk_df\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                chunk_counter += 1\n                if chunk_counter % clear_cache_every_n_chunks == 0:\n                    if hasattr(ds_chunk, 'clear_cache'):\n                        ds_chunk.clear_cache()\n                        print(f\"✓ Очищен кэш токенизации после {chunk_counter} чанков\")\n\n                del ds_chunk, chunk_df\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n\n        if getattr(self.backend, \"batch_tokenizer\", None):\n            self.backend.batch_tokenizer.clear_cache()\n\n        return self\n\n    def predict(\n        self,\n        df: pd.DataFrame,\n        return_label_str: bool = False,\n        return_proba: bool = False,\n        batch_size: Optional[int] = None\n    ) -> np.ndarray:\n        \"\"\"\n        Выполняет инференс на новом DataFrame и возвращает предсказания.\n        Если в df отсутствует столбец target_column_name, он будет добавлен фиктивными значениями.\n\n        :param df: DataFrame с теми же колонками по выбранным модальностям, что и при обучении.\n                   - text_columns: строки,\n                   - image_columns: пути к изображениям, PIL.Image, np.ndarray или списки таких элементов,\n                   - audio_columns: пути к аудиофайлам или np.ndarray (моно, float32).\n        :param return_label_str: Если True — вернуть массив строковых меток (id2label), иначе — индексы классов.\n        :param return_proba: Если True — вернуть распределения вероятностей (softmax) формы [N, num_labels].\n                             При включении этого флага игнорируется return_label_str.\n        :param batch_size: Переопределяет per_device_eval_batch_size на время инференса (опционально).\n\n        :return:\n            - Если return_proba=True: np.ndarray формы [N, num_labels] — вероятности классов.\n            - Иначе:\n                - Если return_label_str=True: np.ndarray формы [N] со строковыми метками,\n                - Иначе: np.ndarray формы [N] с индексами предсказанных классов.\n\n        :raises RuntimeError: Если модель не обучена (trainer отсутствует).\n        :raises ValueError: Ошибки приведения данных (например, несоответствие ожидаемым колонкам/типам),\n                            внутренние ошибки коллатора/бэкенда (рассогласование размеров батчей).\n        :raises OSError: Ошибки чтения исходных файлов (изображения/аудио).\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n\n        print(f\"Preparing dataset for prediction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_c), 10000),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n\n        print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds)\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        if hasattr(ds, 'clear_cache'):\n            ds.clear_cache()\n\n        if return_proba:\n            logits = preds.predictions\n            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n            return probabilities\n\n        y_pred = np.argmax(preds.predictions, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    def get_embeddings(\n        self,\n        df: pd.DataFrame,\n        batch_size: int = 32,\n        return_per_modality: bool = False\n    ):\n        \"\"\"\n        Извлекает эмбеддинги для входного DataFrame. Возвращает склеенный (fused) эмбеддинг,\n        а при необходимости — также эмбеддинги по каждой модальности.\n        Схема слияния (concat/mean) и размеры зависят от настроек бэкенда и параметра fusion.\n\n        Если в df отсутствует столбец target_column_name, он будет добавлен фиктивными значениями.\n\n        :param df: DataFrame с данными по модальностям (аналогично predict()).\n        :param batch_size: Батч-размер при извлечении эмбеддингов.\n        :param return_per_modality: Если True — вернуть дополнительно словарь с эмбеддингами по модальностям.\n\n        :return:\n            - Если return_per_modality=False:\n                np.ndarray формы [N, D_fused], где D_fused — размерность эмбеддинга после слияния.\n            - Если return_per_modality=True:\n                (fused, per_mod) — кортеж:\n                    - fused: np.ndarray [N, D_fused],\n                    - per_mod: Dict[str, np.ndarray], где ключ — модальность (\"text\"/\"image\"/\"audio\"),\n                               значение — эмбеддинги этой модальности формы [N, D_mod].\n\n        :raises RuntimeError: Если модель не обучена или не готова (trainer/model отсутствуют).\n        :raises ValueError: Если бэкенд не вернул эмбеддинги (например, неверные настройки модальностей/данных).\n        :raises OSError: Ошибки чтения исходных файлов (изображения/аудио).\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n\n        print(f\"Preparing dataset for embeddings extraction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=False,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device: torch.device):\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        print(\"Concatenating embeddings...\")\n        fused_arr = np.vstack(fused_list)\n\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Функции для создания фиктивных данных.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef make_rand_image(h=512, w=512):\n    return (np.random.rand(h, w, 3) * 255).astype(\"uint8\")\n\ndef make_sine_audio(sr=48000, seconds=1.0, freq=440.0):\n    t = np.linspace(0, seconds, int(sr * seconds), endpoint=False)\n    return (0.1 * np.sin(2 * np.pi * freq * t)).astype(np.float32)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Данные (строковые метки → красиво отобразятся в predict(return_label_str=True))\ndf_clip_clap = pd.DataFrame([\n    {\"text\": \"A man riding a bike\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 440.0), \"label\": \"sports\"},\n    {\"text\": \"A cat lying on sofa\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 330.0), \"label\": \"lifestyle\"},\n    {\"text\": \"Stock market is volatile\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 550.0), \"label\": \"business\"},\n    {\"text\": \"Runner on the track\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 660.0), \"label\": \"sports\"},\n    {\"text\": \"New cafe opens downtown\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 220.0), \"label\": \"lifestyle\"},\n    {\"text\": \"Company reports revenue\", \"image\": make_rand_image(), \"audio\": make_sine_audio(48000, 1.0, 770.0), \"label\": \"business\"},\n])\n\npipe1 = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\", \"audio\"],\n    num_labels=3,                          # == числу уникальных меток\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    image_columns=[\"image\"],\n    audio_columns=[\"audio\"],\n    # Явные конфиги бэкендов\n    text_model_config={\n        \"checkpoint\": \"openai/clip-vit-base-patch32\",\n        \"model_type\": \"clip\",\n        \"max_length\": 77\n    },\n    image_model_config={\n        \"checkpoint\": \"openai/clip-vit-base-patch32\",\n        \"model_type\": \"clip\",\n        \"max_images\": 1,\n        \"image_agg\": \"mean\"\n    },\n    audio_model_config={\n        \"checkpoint\": \"laion/clap-htsat-unfused\",\n        \"model_type\": \"clap\",\n        \"sr\": 48000,\n        \"max_audios\": 1,\n        \"audio_agg\": \"mean\"\n    },\n    fusion=\"mean\",                         # у всех 512 → mean\n    freeze_backbone=False,                  # linear probing\n    use_batch_tokenizer=True,              # быстрый токенизатор\n    pretokenize_data=True,                 # предварительная токенизация\n    pretokenize_batch_size=128,\n    tokenizer_cache_size=5000,\n    max_pretokenize_samples=100000,\n    local_cache_dir=\"./model_cache\"\n)\n\npipe1.fit(\n    train_data=df_clip_clap,\n    epochs=2,\n    test_size=0.33,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=1e-4,\n    metric_name=\"f1\",\n    fp16=True,                 # если есть CUDA\n    logging_steps=1,\n    eval_steps=2,\n    output_dir=\"./mc_max_clip_clap\",\n    seed=42,\n    hidden=512,\n    dropout=0.2,\n    gradient_checkpointing=True,\n    fit_chunk_size=2,          # чанки по 2 сэмпла\n    clear_cache_every_n_chunks=5,\n)\n\n# Предсказания — вероятности\nprobas1 = pipe1.predict(df_clip_clap.iloc[:3], return_proba=True)\nprint(\"Probas shape:\", probas1.shape)\n\n# Предсказания — строковые метки\nlabels1 = pipe1.predict(df_clip_clap.iloc[:3], return_label_str=True)\nprint(\"Labels:\", labels1)\n\n# Эмбеддинги (fused + по модальностям)\nfused1, per1 = pipe1.get_embeddings(df_clip_clap.iloc[:3], batch_size=2, return_per_modality=True)\nprint(\"Fused shape:\", fused1.shape)\nfor m, arr in per1.items():\n    print(f\"{m} emb shape:\", arr.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef build_binary_multimodal_df(n_per_class: int = 50, sr: int = 16000) -> pd.DataFrame:\n    # Генерация текстов «pets»\n    pet_animals = [\"кошка\", \"собака\", \"щенок\", \"кот\", \"котёнок\", \"пёс\", \"питомец\", \"котик\", \"пёсик\", \"котяра\"]\n    pet_actions = [\"сидит\", \"лежит\", \"играет\", \"смотрит\", \"прячется\", \"спит\", \"тянется\", \"мурлычет\", \"исследует\", \"охотится\"]\n    pet_places = [\"на подоконнике\", \"на диване\", \"на ковре\", \"на кухне\", \"в коробке\", \"на кресле\", \"у окна\", \"в саду\", \"на полу\", \"на стуле\"]\n\n    # Генерация текстов «news»\n    news_subjects = [\"власти города\", \"жители района\", \"аналитики\", \"эксперты\", \"журналисты\", \"компания\", \"департамент\", \"учёные\", \"инженеры\", \"ведомство\"]\n    news_verbs = [\"обсудили\", \"обновили\", \"сообщили\", \"рассказали\", \"анонсировали\", \"заявили\", \"подтвердили\", \"планируют\", \"запустили\", \"увеличили\"]\n    news_topics = [\"экономику\", \"политику\", \"транспорт\", \"погоду\", \"технологии\", \"культуру\", \"спорт\", \"здравоохранение\", \"образование\", \"экологию\"]\n\n    rows = []\n\n    # PETS класс\n    pet_freqs = [260.0, 280.0, 300.0, 320.0, 340.0, 360.0]\n    for _ in range(n_per_class):\n        text = f\"{random.choice(pet_animals).capitalize()} {random.choice(pet_actions)} {random.choice(pet_places)}\"\n        img = make_rand_image()\n        aud = make_sine_audio(sr, 1.0, random.choice(pet_freqs))\n        rows.append({\"text\": text, \"image\": img, \"audio\": aud, \"label\": \"pets\"})\n\n    # NEWS класс\n    news_freqs = [560.0, 580.0, 600.0, 620.0, 640.0, 660.0]\n    for _ in range(n_per_class):\n        text = f\"{random.choice(news_subjects).capitalize()} {random.choice(news_verbs)} новости про {random.choice(news_topics)}\"\n        img = make_rand_image()\n        aud = make_sine_audio(sr, 1.0, random.choice(news_freqs))\n        rows.append({\"text\": text, \"image\": img, \"audio\": aud, \"label\": \"news\"})\n\n    random.shuffle(rows)\n    df = pd.DataFrame(rows)\n    return df\n\ntrain_data = build_binary_multimodal_df(n_per_class=12)\ndf_train, df_eval = train_test_split(\n    train_data, test_size=0.3, random_state=42, shuffle=True,\n    stratify=train_data['label'])\n\npipe3 = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\", \"audio\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    image_columns=[\"image\"],\n    audio_columns=[\"audio\"],\n    text_model_config={\n        \"checkpoint\": \"DeepPavlov/rubert-base-cased\",\n        \"model_type\": \"bert\",\n        \"max_length\": 256\n    },\n    image_model_config={\n        \"checkpoint\": \"google/vit-base-patch16-224\",\n        \"model_type\": \"vit\",\n        \"max_images\": 1,\n        \"image_agg\": \"mean\"\n    },\n    audio_model_config={\n        \"checkpoint\": \"facebook/wav2vec2-base-960h\",\n        \"model_type\": \"auto\",   # AutoModel + AutoProcessor\n        \"sr\": 16000,\n        \"max_audios\": 1,\n        \"audio_agg\": \"mean\"\n    },\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=True,\n    pretokenize_data=True,\n    pretokenize_batch_size=128,\n    tokenizer_cache_size=5000,\n    local_cache_dir=\"./model_cache\"\n)\n\npipe3.fit(\n    train_data=df_train,\n    test_data=df_eval,\n    epochs=2,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-4,\n    metric_name=\"f1\",\n    fp16=True,\n    logging_steps=1,\n    eval_steps=1,\n    output_dir=\"./mc_max_rubert_vit_w2v2\",\n    seed=2025,\n    hidden=512,\n    dropout=0.2,\n    gradient_checkpointing=True,\n    fit_chunk_size=8,\n    clear_cache_every_n_chunks=3\n)\n\nprint(\"Pred (labels):\", pipe3.predict(df_eval, return_label_str=True))\nfused3, per3 = pipe3.get_embeddings(df_eval, batch_size=2, return_per_modality=True)\nprint(\"Fused:\", fused3.shape, \"| text:\", per3[\"text\"].shape, \"| image:\", per3[\"image\"].shape, \"| audio:\", per3[\"audio\"].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 3.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_min = pd.DataFrame([\n    {\"text\": \"Привет, как дела?\", \"label\": \"greet\"},\n    {\"text\": \"Сегодня отличная погода\", \"label\": \"weather\"},\n    {\"text\": \"До встречи!\", \"label\": \"greet\"},\n])\n\npipe_min = SingleModelMultiComboClassification(\n    modalities=[\"text\"],\n    num_labels=2,                           # ровно столько, сколько уникальных меток в df_min\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    # Достаточно указать только текстовую модель\n    text_model_config={\n        \"checkpoint\": \"DeepPavlov/rubert-base-cased\",\n        \"model_type\": \"bert\",\n        \"max_length\": 128\n    },\n    fusion=\"concat\",                        # неважно для single-modality\n    freeze_backbone=True\n)\n\n# Минимальный fit: всё по умолчанию (без ранней остановки, без чанков)\npipe_min.fit(train_data=df_min)\n\nprint(\"Pred (ids):\", pipe_min.predict(df_min))\nprint(\"Pred (labels):\", pipe_min.predict(df_min, return_label_str=True))\nemb_min = pipe_min.get_embeddings(df_min)\nprint(\"Embeddings shape:\", emb_min.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 4.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom huggingface_hub import login\nlogin('hf_ьдшин4хфэюц2\\хкпсзэыэкпзмцфх3мххи ихщ т игщ йхт ъйм\\укщи хъ4хрьх84им о\\фуео\\щ рэрь')  # ваш токен\n\ndf_min = pd.DataFrame([\n    {\"text\": \"Привет, как дела?\", \"label\": \"greet\"},\n    {\"text\": \"Сегодня отличная погода\", \"label\": \"weather\"},\n    {\"text\": \"До встречи!\", \"label\": \"greet\"},\n])\n\npipe_min = SingleModelMultiComboClassification(\n    modalities=[\"text\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"text\"],\n    text_model_config={\n        \"checkpoint\": \"google/embeddinggemma-300m\",\n        \"max_length\": 2048\n    },\n    text_padding='max_length',\n    freeze_backbone=False\n)\n\npipe_min.fit(train_data=df_min)\n\nprint(\"Pred (ids):\", pipe_min.predict(df_min))\nprint(\"Pred (labels):\", pipe_min.predict(df_min, return_label_str=True))\nemb_min = pipe_min.get_embeddings(df_min)\nprint(\"Embeddings shape:\", emb_min.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение регрессора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация регрессора.","metadata":{}},{"cell_type":"code","source":"# !pip install --upgrade --no-cache-dir \\\n#   --extra-index-url https://download.pytorch.org/whl/cu124 \\\n#   pillow==11.1.0 \\\n#   numpy==1.26.4 \\\n#   pandas==2.2.3 \\\n#   tqdm==4.67.1 \\\n#   transformers==4.51.3 \\\n#   evaluate==0.4.5 \\\n#   wav2clip==0.1.0 \\\n#   torch==2.6.0+cu124 \\\n#   torchaudio==2.6.0+cu124\n!pip install evaluate wav2clip\n\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport math\nimport random\nimport gc\nfrom functools import lru_cache\nfrom typing import Any, Callable, Dict, Generator, List, Optional, Union\n\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\nfrom transformers import TrainingArguments, Trainer\nfrom transformers.trainer_callback import TrainerCallback, PrinterCallback, EarlyStoppingCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport evaluate\n\n\n# =========================\n# Утилиты\n# =========================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef to_pil(x: Union[str, np.ndarray, 'Image.Image']) -> 'Image.Image':\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\ndef safe_load(component_cls, checkpoint: str, local_cache_dir: str = \"./model_cache\",\n              local_files_only: Optional[bool] = None, **kwargs):\n    if local_files_only is None:\n        local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n    name = getattr(component_cls, \"__name__\", \"\")\n    if \"Tokenizer\" in name:\n        kwargs.setdefault(\"use_fast\", True)\n    return component_cls.from_pretrained(\n        checkpoint, cache_dir=local_cache_dir, local_files_only=local_files_only, **kwargs\n    )\n\n\n# =========================\n# Токенизатор батчевый (с поддержкой стратегии паддинга)\n# =========================\n\nclass BatchTokenizer:\n    def __init__(\n        self,\n        tokenizer,\n        max_length: int = 512,\n        cache_size: int = 10000,\n        batch_size: int = 256,\n        use_fast: bool = True,\n        device: str = \"cpu\",\n        padding_strategy: str = \"max_length\"  # \"max_length\" или \"dynamic\"\n    ):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.use_fast = use_fast\n        self.device = device\n        self.padding_strategy = padding_strategy\n        if self.padding_strategy not in {\"max_length\", \"dynamic\"}:\n            raise ValueError(\"padding_strategy должен быть 'max_length' или 'dynamic'\")\n        self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n        self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n        if self.is_fast:\n            print(\"✓ Используется Fast Tokenizer\")\n\n    def _tokenize_single(self, text: str) -> tuple:\n        # Кэшируем только фиксированный паддинг — иначе батчи не склеить\n        result = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        return tuple((k, v.squeeze(0).cpu().numpy()) for k, v in result.items())\n\n    def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n        # Динамический паддинг — токенизируем сразу список (без поэлементного кэша, иначе формы различаются)\n        if self.padding_strategy == \"dynamic\":\n            result = self.tokenizer(\n                texts,\n                padding=True,  # 'longest'\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n        # Фиксированный паддинг — используем кэш для коротких батчей\n        if use_cache and len(texts) < 100:\n            results = [dict(self._cache(text)) for text in texts]\n            keys = results[0].keys()\n            batch_dict = {}\n            for key in keys:\n                dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                batch_dict[key] = torch.tensor(np.stack([r[key] for r in results]), dtype=dtype)\n            return batch_dict\n        else:\n            result = self.tokenizer(\n                texts,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            for key in result:\n                if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n                    result[key] = result[key].long()\n            return result\n\n    def tokenize_dataset_lazy(\n        self,\n        texts: List[str],\n        batch_size: Optional[int] = None\n    ) -> Generator[Dict[str, torch.Tensor], None, None]:\n        batch_size = batch_size or self.batch_size\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            yield self.tokenize_batch(batch, use_cache=False)\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n\n# =========================\n# Универсальный датасет (ускорённый)\n# =========================\n\nclass MultiComboDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer: Optional[BatchTokenizer] = None,          # BatchTokenizer\n        text_tokenizer_fn: Optional[Callable] = None,              # кастомная fn -> dict тензоров ИЛИ строка\n        text_tokenizer_fn_batched: Optional[Callable] = None,      # батчевая кастомная fn: List[dict] -> dict тензоров\n        special_tokens: Optional[Dict[str, Any]] = None,\n        pretokenize: bool = False,\n        pretokenize_batch_size: int = 256,\n        max_cache_size: int = 100000,                              # совместимость; не используется банками напрямую\n        tokenizer_returns_tensors: bool = False,\n        cache_dir: Optional[str] = None,\n        deduplicate_texts: bool = True\n    ):\n        \"\"\"\n        Быстрый датасет:\n        - Предтокенизирует текст батчами в банки тензоров (если возможно).\n        - В __getitem__ просто делает слайс по банкам (O(1)), либо возвращает сырую строку.\n        - Предсобирает списки изображений/аудио и метки.\n\n        Если выбран dynamic padding в BatchTokenizer — предтокенизация выключается.\n        \"\"\"\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.batch_tokenizer = text_tokenizer\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.text_tokenizer_fn_batched = text_tokenizer_fn_batched\n\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n        self._N = len(self.df)\n\n        # labels в тензор [N, K] или [N]\n        self._labels = self._prepare_labels(self.df, self.target_col)\n\n        # Предсобранные списки изображений/аудио (без pandas в __getitem__)\n        self._image_lists = self._collect_multi_values(self.df, self.image_columns) if self.image_columns else None\n        self._audio_lists = self._collect_multi_values(self.df, self.audio_columns) if self.audio_columns else None\n\n        # Банки предтокенизированных текстов: dict(key -> torch.Tensor [N, L])\n        self._tok_bank: Optional[Dict[str, torch.Tensor]] = None\n        self._has_text = bool(self.text_columns)\n\n        # Dynamic-паддинг несовместим с предтокенизацией (формы различаются)\n        if pretokenize and self.batch_tokenizer is not None and getattr(self.batch_tokenizer, \"padding_strategy\", \"max_length\") == \"dynamic\":\n            print(\"⚠ Предтокенизация отключена: выбран dynamic-паддинг для текста.\")\n            pretokenize = False\n\n        # Предтокенизация (батчами) — либо BatchTokenizer, либо кастомная функция (batched/single)\n        if self._has_text and pretokenize:\n            if self.batch_tokenizer is not None and self.text_tokenizer_fn is None and self.text_tokenizer_fn_batched is None:\n                self._pretokenize_with_batch_tokenizer(pretokenize_batch_size)\n            else:\n                self._pretokenize_with_custom_fn(pretokenize_batch_size, deduplicate_texts=deduplicate_texts)\n\n    # --------------------------\n    # Метки\n    # --------------------------\n    def _prepare_labels(self, df: pd.DataFrame, target_col: str) -> torch.Tensor:\n        if target_col not in df.columns:\n            return torch.zeros((len(df), 1), dtype=torch.float32)\n        labels_list = []\n        for i in range(len(df)):\n            v = df.iloc[i][target_col]\n            if isinstance(v, (list, tuple, np.ndarray)):\n                arr = np.asarray(v, dtype=np.float32)\n            else:\n                try:\n                    arr = np.asarray([float(v)], dtype=np.float32)\n                except Exception:\n                    arr = np.asarray([0.0], dtype=np.float32)\n            labels_list.append(arr)\n        # выравниваем K по максимуму\n        K = max(a.shape[0] for a in labels_list) if labels_list else 1\n        out = np.zeros((len(labels_list), K), dtype=np.float32)\n        for i, a in enumerate(labels_list):\n            out[i, :a.shape[0]] = a\n        return torch.tensor(out, dtype=torch.float32)\n\n    # --------------------------\n    # Хелперы по тексту/мультимедиа\n    # --------------------------\n    def _join_text(self, row: pd.Series) -> str:\n        sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n        return sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n    @staticmethod\n    def _as_list(v):\n        if v is None or (isinstance(v, float) and np.isnan(v)):\n            return []\n        if isinstance(v, (list, tuple)):\n            return list(v)\n        return [v]\n\n    def _collect_multi_values(self, df: pd.DataFrame, columns: List[str]) -> List[List[Any]]:\n        out = []\n        as_list = self._as_list\n        for _, row in df.iterrows():\n            lst: List[Any] = []\n            for c in columns:\n                if c in row:\n                    lst.extend([x for x in as_list(row[c]) if x is not None])\n            out.append(lst)\n        return out\n\n    # --------------------------\n    # Предтокенизация: BatchTokenizer\n    # --------------------------\n    def _pretokenize_with_batch_tokenizer(self, batch_size: int):\n        print(\"Предтокенизация с BatchTokenizer...\")\n        texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n        banks: Dict[str, List[torch.Tensor]] = {}\n\n        for start in range(0, self._N, batch_size):\n            batch = texts[start:start + batch_size]\n            tok = self.batch_tokenizer.tokenize_batch(batch, use_cache=False)  # dict[str, torch.Tensor [B, L]]\n            # типы\n            for k in tok:\n                if k in (\"input_ids\", \"attention_mask\", \"token_type_ids\"):\n                    tok[k] = tok[k].long()\n                else:\n                    tok[k] = tok[k].to(torch.float32)\n            for k, v in tok.items():\n                banks.setdefault(k, []).append(v)\n\n        self._tok_bank = {k: torch.cat(v_parts, dim=0).contiguous() for k, v_parts in banks.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"✓ Предтокенизация завершена: {self._N} образцов | keys={list(self._tok_bank.keys())}, shapes={shapes}\")\n\n    # --------------------------\n    # Предтокенизация: кастомные функции (batched / single + дедуп)\n    # --------------------------\n    def _pretokenize_with_custom_fn(self, batch_size: int, deduplicate_texts: bool = True):\n        if not self._has_text:\n            return\n\n        cols = list(self.text_columns)\n\n        # Если есть batched-функция — используем её\n        if self.text_tokenizer_fn_batched is not None:\n            print(\"Предтокенизация кастомной batched-функцией...\")\n            # первая порция для выяснения формы\n            first_end = min(self._N, max(8, batch_size))\n            batch_data = []\n            for i in range(first_end):\n                row = self.df.iloc[i]\n                d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                batch_data.append(d)\n            first_tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n            if not isinstance(first_tok, dict):\n                raise ValueError(\"text_tokenizer_fn_batched должна возвращать dict тензоров [B, L]\")\n            # выделяем банки по форме первого батча\n            bank: Dict[str, torch.Tensor] = {}\n            for k, t in first_tok.items():\n                if not torch.is_tensor(t): t = torch.tensor(t)\n                bank[k] = torch.empty((self._N, t.size(1)), dtype=t.dtype)\n                bank[k][:first_end] = t[:first_end]\n\n            # оставшаяся часть\n            for start in range(first_end, self._N, batch_size):\n                end = min(self._N, start + batch_size)\n                batch_data = []\n                for i in range(start, end):\n                    row = self.df.iloc[i]\n                    d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                    batch_data.append(d)\n                tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): t = torch.tensor(t)\n                    bank[k][start:end] = t\n\n            self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n            shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n            print(f\"✓ Предтокенизация кастомной batched-функцией завершена: shapes={shapes}\")\n            return\n\n        # Иначе — single-функция + дедуп\n        print(\"Предтокенизация кастомной single-функцией...\")\n        # подготовим строки (без pandas в горячем цикле)\n        col_arrays = [self.df[c].astype(str).where(~self.df[c].isna(), other=\"\").tolist() for c in cols]\n\n        # детектируем форму\n        first_td = {c: col_arrays[i][0] for i, c in enumerate(cols)}\n        first_tok = self.text_tokenizer_fn(first_td, self.special_tokens)\n        if not isinstance(first_tok, dict):\n            raise ValueError(\"custom text_tokenizer_fn должна возвращать dict тензоров\")\n        for k, t in first_tok.items():\n            if not torch.is_tensor(t): first_tok[k] = torch.tensor(t)\n        # банки\n        bank: Dict[str, torch.Tensor] = {k: torch.empty((self._N, *t.shape), dtype=t.dtype) for k, t in first_tok.items()}\n        for k, t in first_tok.items():\n            bank[k][0].copy_(t)\n\n        cache: Dict[tuple, Dict[str, torch.Tensor]] = {}\n        if deduplicate_texts:\n            key0 = tuple(first_td.get(c, \"\") for c in cols)\n            cache[key0] = {k: v.clone() for k, v in first_tok.items()}\n\n        for i in range(1, self._N):\n            td = {c: col_arrays[j][i] for j, c in enumerate(cols)}\n            if deduplicate_texts:\n                key = tuple(td.get(c, \"\") for c in cols)\n                tok = cache.get(key)\n                if tok is None:\n                    tok = self.text_tokenizer_fn(td, self.special_tokens)\n                    for k, t in tok.items():\n                        if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n                    cache[key] = {k: v.clone() for k, v in tok.items()}\n            else:\n                tok = self.text_tokenizer_fn(td, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n            for k, t in tok.items():\n                bank[k][i].copy_(t)\n\n        self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"✓ Предтокенизация кастомной single-функцией завершена: shapes={shapes}\")\n\n    # --------------------------\n    # Интерфейс Dataset\n    # --------------------------\n    def __len__(self) -> int:\n        return self._N\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item: Dict[str, Any] = {}\n\n        # Метки (вектор [K] или [1])\n        item[\"labels\"] = self._labels[idx]\n\n        # Текст: либо слайс из банка, либо сырая строка (коллатор потом батчево токенизирует)\n        if self.text_columns:\n            if self._tok_bank is not None:\n                item[\"text_tokens\"] = {k: v[idx] for k, v in self._tok_bank.items()}\n            else:\n                item[\"text\"] = self._join_text(self.df.iloc[idx])\n\n        # Изображения/аудио — предсобранные списки\n        if self._image_lists is not None:\n            item[\"images\"] = self._image_lists[idx]\n        if self._audio_lists is not None:\n            item[\"audios\"] = self._audio_lists[idx]\n\n        return item\n\n    # --------------------------\n    # Сервис\n    # --------------------------\n    def get_cache_stats(self) -> Dict[str, Any]:\n        has = self._tok_bank is not None\n        sizes = {k: tuple(v.shape) for k, v in (self._tok_bank or {}).items()}\n        return {\"has_pretokenized\": has, \"shapes\": sizes, \"N\": self._N}\n\n    def clear_cache(self):\n        self._tok_bank = None\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n\n# =========================\n# Универсальный бэкенд\n# =========================\n\nclass BaseBackend(nn.Module):\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n    text_tokenizer_fn: Optional[Callable] = None\n    batch_tokenizer: Optional[BatchTokenizer] = None\n    special_tokens: Dict[str, str] = {}\n    tokenizer_returns_tensors: bool = False\n    local_cache_dir: str = \"./model_cache\"\n    text_padding_strategy: str = \"max_length\"  # стратегия паддинга текста\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        raise NotImplementedError\n\n    def freeze_all(self):\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n    def set_text_tokenizer(self, tokenizer_fn: Optional[Callable], special_tokens: Optional[Dict[str, str]] = None,\n                           returns_tensors: bool = False):\n        self.text_tokenizer_fn = tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = returns_tensors\n\n    def set_batch_tokenizer(self, tokenizer, max_length: int = 512,\n                            cache_size: int = 10000, batch_size: int = 256,\n                            padding_strategy: str = \"max_length\"):\n        self.text_padding_strategy = padding_strategy\n        self.batch_tokenizer = BatchTokenizer(\n            tokenizer=tokenizer,\n            max_length=max_length,\n            cache_size=cache_size,\n            batch_size=batch_size,\n            use_fast=True,\n            padding_strategy=padding_strategy\n        )\n\n\nclass UniversalMultiBackend(BaseBackend):\n    name = \"universal\"\n\n    class _ParamDeviceProxy(nn.Module):\n        def __init__(self, base, device: torch.device):\n            super().__init__()\n            self.base = base if isinstance(base, nn.Module) else None\n            self._callable = base if not isinstance(base, nn.Module) else None\n            self._dummy = nn.Parameter(torch.empty(0), requires_grad=False)\n            with torch.no_grad():\n                self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n\n        def forward(self, *args, **kwargs):\n            target = self.base if self.base is not None else self._callable\n            return target(*args, **kwargs)\n\n        def to(self, device, *args, **kwargs):\n            self._dummy.data = self._dummy.data.to(device)\n            try:\n                target = self.base if self.base is not None else self._callable\n                if hasattr(target, \"to\"):\n                    target.to(device)\n            except Exception:\n                pass\n            return super().to(device, *args, **kwargs)\n\n    def _model_device(self, model, default: torch.device) -> torch.device:\n        try:\n            return next(model.parameters()).device\n        except StopIteration:\n            pass\n        try:\n            buf = next(model.buffers())\n            return buf.device\n        except StopIteration:\n            pass\n        return default\n\n    def _preferred_device(self) -> torch.device:\n        if torch.cuda.is_available():\n            return torch.device(\"cuda\")\n        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n            return torch.device(\"mps\")\n        return torch.device(\"cpu\")\n\n    def _wrap_if_parameterless(self, model, device: torch.device):\n        try:\n            it = model.parameters() if hasattr(model, \"parameters\") else iter(())\n            next(it)\n            return model\n        except StopIteration:\n            return self._ParamDeviceProxy(model, device)\n        except Exception:\n            return self._ParamDeviceProxy(model, device)\n\n    def __init__(\n        self,\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        freeze: bool = True,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        use_batch_tokenizer: bool = True,\n        tokenizer_cache_size: int = 10000,\n        tokenizer_batch_size: int = 256,\n        local_cache_dir: str = \"./model_cache\",\n        text_padding_strategy: str = \"max_length\"\n    ):\n        super().__init__()\n        self.supported = set()\n        self.out_dim_per_modality = {}\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.tokenizer_batch_size = tokenizer_batch_size\n        self.local_cache_dir = local_cache_dir\n        self.text_padding_strategy = text_padding_strategy\n\n        self.text_model = None\n        self.text_processor = None\n        self.text_config = text_model_config or {}\n        if text_model_config:\n            self._init_text_model(text_model_config)\n            self.supported.add(\"text\")\n\n        self.image_model = None\n        self.image_processor = None\n        self.image_config = image_model_config or {}\n        if image_model_config:\n            self._init_image_model(image_model_config)\n            self.supported.add(\"image\")\n\n        self.audio_model = None\n        self.audio_processor = None\n        self.audio_config = audio_model_config or {}\n        if audio_model_config:\n            self._init_audio_model(audio_model_config)\n            self.supported.add(\"audio\")\n\n        if freeze:\n            self.freeze_all()\n\n    def _ensure_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        if x is None:\n            return None\n        if x.dim() == 1:\n            return x.unsqueeze(0)\n        if x.dim() > 2:\n            return x.view(x.size(0), -1)\n        return x\n\n    def _normalize_2d(self, x: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n        x = self._ensure_2d(x)\n        return F.normalize(x, dim=-1, eps=1e-12) if x is not None and x.numel() > 0 else x\n\n    def _init_text_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoTokenizer, CLIPModel, CLIPTokenizer, ClapModel, ClapProcessor\n\n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n\n        print(f\"Загрузка текстовой модели {checkpoint}...\")\n\n        if model_type == 'clip':\n            self.text_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(CLIPTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.projection_dim\n        elif model_type == 'clap':\n            self.text_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            proc = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = getattr(proc, 'tokenizer', None) or safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = getattr(self.text_model.config, \"projection_dim\", 512)\n        else:\n            self.text_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.text_processor = safe_load(AutoTokenizer, checkpoint, local_cache_dir=self.local_cache_dir, use_fast=True)\n            dim = self.text_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.text_model  = self._wrap_if_parameterless(self.text_model, dev)\n\n        if self.use_batch_tokenizer and self.text_processor is not None:\n            self.set_batch_tokenizer(\n                self.text_processor,\n                max_length=config.get('max_length', 512),\n                cache_size=self.tokenizer_cache_size,\n                batch_size=self.tokenizer_batch_size,\n                padding_strategy=self.text_padding_strategy\n            )\n\n        self.text_config['max_length'] = config.get('max_length', 512)\n        self.text_config['dim'] = dim\n        self.text_config['model_type'] = model_type\n        self.out_dim_per_modality['text'] = dim\n\n    def _init_image_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoImageProcessor, CLIPModel, CLIPImageProcessor\n\n        checkpoint = config['checkpoint']\n        model_type = config.get('model_type', 'auto').lower()\n\n        print(f\"Загрузка визуальной модели {checkpoint}...\")\n\n        if model_type == 'clip':\n            self.image_model = safe_load(CLIPModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(CLIPImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.projection_dim\n        else:\n            self.image_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.image_processor = safe_load(AutoImageProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.image_model.config.hidden_size\n\n        dev = self._preferred_device()\n        self.image_model = self._wrap_if_parameterless(self.image_model, dev)\n\n        self.image_config['max_images'] = config.get('max_images', 1)\n        self.image_config['image_agg'] = config.get('image_agg', 'concat')\n        self.image_config['dim'] = dim\n        self.image_config['model_type'] = model_type\n\n        self.out_dim_per_modality['image'] = (dim * self.image_config['max_images']) if self.image_config['image_agg'] == 'concat' else dim\n\n    def _init_audio_model(self, config: Dict[str, Any]):\n        from transformers import AutoModel, AutoProcessor, ClapModel, ClapProcessor\n\n        model_type = config.get('model_type', 'auto').lower()\n        checkpoint = config.get('checkpoint', None)\n\n        print(f\"Загрузка аудио модели (type={model_type})...\")\n\n        if model_type == 'wav2clip':\n            import wav2clip as w2c\n            self._w2c = w2c\n\n            w2c_model = None\n            if hasattr(w2c, \"get_model\"):\n                w2c_model = w2c.get_model()\n            elif hasattr(w2c, \"model\"):\n                m = w2c.model\n                w2c_model = m() if callable(m) else m\n            else:\n                raise RuntimeError(\"wav2clip не содержит get_model()/model. Обновите пакет wav2clip.\")\n\n            self.audio_model = w2c_model\n\n            try:\n                if isinstance(self.audio_model, torch.nn.Module) and torch.cuda.is_available():\n                    self.audio_model = self.audio_model.to(\"cuda\")\n            except Exception:\n                pass\n\n            self.audio_processor = None\n            dim = 512\n            sr = config.get('sr', 16000)\n\n        elif model_type == 'clap':\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для CLAP\")\n            self.audio_model = safe_load(ClapModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(ClapProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = getattr(self.audio_model.config, \"projection_dim\", 512)\n            sr = getattr(self.audio_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.audio_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n\n        else:\n            if checkpoint is None:\n                raise ValueError(\"audio_model_config['checkpoint'] обязателен для аудио-моделей, кроме wav2clip\")\n            self.audio_model = safe_load(AutoModel, checkpoint, local_cache_dir=self.local_cache_dir)\n            self.audio_processor = safe_load(AutoProcessor, checkpoint, local_cache_dir=self.local_cache_dir)\n            dim = self.audio_model.config.hidden_size\n            fe = getattr(self.audio_processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 16000) if fe is not None else 16000\n\n        dev = self._preferred_device()\n        self.audio_model = self._wrap_if_parameterless(self.audio_model, dev)\n\n        self.audio_config['sr'] = config.get('sr', sr)\n        self.audio_config['max_audios'] = config.get('max_audios', 1)\n        self.audio_config['audio_agg'] = config.get('audio_agg', 'concat')\n        self.audio_config['dim'] = dim\n        self.audio_config['model_type'] = model_type\n\n        self.out_dim_per_modality['audio'] = (\n            dim * self.audio_config['max_audios']\n            if self.audio_config['audio_agg'] == 'concat' else dim\n        )\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        # Метки (регрессия: вектор или скаляр)\n        labels = []\n        for b in batch:\n            labels.append(torch.as_tensor(b.get(\"labels\", 0.0), dtype=torch.float32))\n        labels = torch.stack(labels)\n\n        backend_inputs: Dict[str, Any] = {}\n        batch_size = len(batch)\n\n        # Текст\n        if self.text_model is not None:\n            if \"text_tokens\" in batch[0]:\n                text_inputs = {}\n                for key in batch[0][\"text_tokens\"].keys():\n                    if torch.is_tensor(batch[0][\"text_tokens\"][key]):\n                        text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n                    else:\n                        dtype = torch.long if key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] else torch.float32\n                        text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n                backend_inputs[\"text_inputs\"] = text_inputs\n            else:\n                texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n                if self.batch_tokenizer:\n                    text_inputs = self.batch_tokenizer.tokenize_batch(texts, use_cache=True)\n                else:\n                    pad = \"max_length\" if getattr(self, \"text_padding_strategy\", \"max_length\") == \"max_length\" else True\n                    text_inputs = self.text_processor(\n                        texts, padding=pad, truncation=True,\n                        max_length=self.text_config.get('max_length', 512),\n                        return_tensors=\"pt\"\n                    )\n                backend_inputs[\"text_inputs\"] = {k: v for k, v in text_inputs.items()}\n\n        # Изображения\n        if self.image_model is not None:\n            images_lists = [b.get(\"images\", []) for b in batch]\n            flat_images, img_counts = [], []\n            for lst in images_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [img for img in lst if img is not None]\n                img_counts.append(len(lst))\n                for img in lst:\n                    flat_images.append(to_pil(img))\n\n            if len(flat_images) > 0:\n                img_proc = self.image_processor(images=flat_images, return_tensors=\"pt\")\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": img_proc[\"pixel_values\"]}\n            else:\n                backend_inputs[\"image_inputs\"] = {\"pixel_values\": None}\n\n            backend_inputs[\"image_counts\"] = torch.tensor(img_counts, dtype=torch.long)\n\n        # Аудио\n        if self.audio_model is not None:\n            audios_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios, aud_counts = [], []\n            for lst in audios_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_config['sr']))\n                    elif isinstance(a, np.ndarray):\n                        aa = np.asarray(a, dtype=np.float32)\n                        if aa.ndim > 1: aa = np.squeeze(aa)\n                        if aa.ndim > 1: aa = aa.reshape(-1)\n                        flat_audios.append(aa)\n            if self.audio_config.get('model_type') == 'wav2clip':\n                backend_inputs[\"audio_inputs\"] = {\"raw_audios\": flat_audios}\n            elif len(flat_audios) > 0:\n                if self.audio_config.get('model_type') == 'clap':\n                    aud_proc = self.audio_processor(\n                        audios=flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_features\": aud_proc[\"input_features\"]}\n                else:\n                    aud_proc = self.audio_processor(\n                        flat_audios, sampling_rate=self.audio_config['sr'], padding=True, return_tensors=\"pt\"\n                    )\n                    backend_inputs[\"audio_inputs\"] = {\"input_values\": aud_proc[\"input_values\"]}\n            else:\n                backend_inputs[\"audio_inputs\"] = {\"input_features\": None, \"input_values\": None, \"raw_audios\": []}\n\n            backend_inputs[\"audio_counts\"] = torch.tensor(aud_counts, dtype=torch.long)\n\n        backend_inputs[\"batch_size\"] = batch_size\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _aggregate_embeddings(\n        self,\n        embs: Optional[torch.Tensor],\n        counts: List[int],\n        max_k: int,\n        dim_hint: int,\n        agg_type: str,\n        batch_size: int,\n        device: torch.device\n    ) -> torch.Tensor:\n        if embs is None or (torch.is_tensor(embs) and embs.numel() == 0):\n            feat_dim = int(dim_hint) if dim_hint is not None else 0\n            out_dim = feat_dim * max_k if agg_type == 'concat' else feat_dim\n            return torch.zeros((batch_size, out_dim), device=device, dtype=torch.float32)\n\n        if not torch.is_tensor(embs):\n            embs = torch.as_tensor(embs, device=device, dtype=torch.float32)\n        if embs.dim() == 1:\n            embs = embs.unsqueeze(0)\n        elif embs.dim() > 2:\n            embs = embs.view(embs.size(0), -1)\n\n        N, D = embs.size()\n        out_dim = (D * max_k) if agg_type == 'concat' else D\n        out = torch.zeros((batch_size, out_dim), device=device, dtype=embs.dtype)\n\n        offset = 0\n        for i, c in enumerate(counts):\n            if c <= 0 or offset >= N:\n                continue\n            take_n = min(c, N - offset)\n            sample = embs[offset:offset + take_n]\n            offset += take_n\n\n            if agg_type == 'concat':\n                take = sample[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n\n        return F.normalize(out, dim=-1, eps=1e-12) if out.size(1) > 0 else out\n\n    @torch.no_grad()\n    def _wav2clip_embed(self, arr: np.ndarray, device: torch.device) -> torch.Tensor:\n        arr = np.asarray(arr, dtype=np.float32)\n        if arr.ndim > 1:\n            arr = np.squeeze(arr)\n        if arr.ndim > 1:\n            arr = arr.reshape(-1)\n        if arr.size < 512:\n            arr = np.pad(arr, (0, 512 - arr.size), mode=\"constant\")\n\n        try:\n            emb = self._w2c.embed_audio(arr, self.audio_model)\n            emb = np.asarray(emb)\n        except Exception:\n            model_dev = self._model_device(self.audio_model, default=device)\n            x = torch.from_numpy(arr).float().unsqueeze(0).to(model_dev)\n            y = self.audio_model(x)\n            if isinstance(y, (tuple, list)):\n                y = y[0]\n            if torch.is_tensor(y):\n                if y.dim() == 2 and y.size(0) == 1:\n                    y = y.squeeze(0)\n                emb = y.detach().cpu().numpy()\n            else:\n                emb = np.asarray(y)\n\n        if emb.ndim > 1:\n            emb = emb.reshape(-1)\n        return torch.as_tensor(emb, device=device, dtype=torch.float32)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        results: Dict[str, torch.Tensor] = {}\n        batch_size = int(backend_inputs.get(\"batch_size\", 1))\n\n        # Текст\n        if self.text_model is not None and \"text_inputs\" in backend_inputs:\n            text_inputs = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n            if hasattr(self.text_model, \"get_text_features\"):\n                text_z = self.text_model.get_text_features(**text_inputs)\n            else:\n                outputs = self.text_model(**text_inputs)\n                text_z = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n            results[\"text\"] = self._normalize_2d(text_z)\n\n        # Изображения\n        if self.image_model is not None and \"image_inputs\" in backend_inputs:\n            pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n            counts = backend_inputs[\"image_counts\"].tolist()\n            total_images_needed = sum(counts)\n\n            img_flat = None\n            actual_img_dim = self.image_config.get(\"dim\", 768)\n\n            if pi is not None and pi.numel() > 0 and total_images_needed > 0:\n                pi = pi.to(device)\n                if pi.size(0) > total_images_needed:\n                    pi = pi[:total_images_needed]\n\n                if hasattr(self.image_model, \"get_image_features\"):\n                    img_flat = self.image_model.get_image_features(pixel_values=pi)\n                else:\n                    outputs = self.image_model(pixel_values=pi)\n                    img_flat = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state[:, 0]\n\n                img_flat = self._normalize_2d(img_flat)\n                actual_img_dim = img_flat.size(1) if img_flat is not None else actual_img_dim\n\n            img_z = self._aggregate_embeddings(\n                img_flat, counts,\n                self.image_config[\"max_images\"],\n                actual_img_dim,\n                self.image_config[\"image_agg\"],\n                len(counts),\n                device\n            )\n\n            if actual_img_dim != self.image_config.get(\"dim\"):\n                self.image_config[\"dim\"] = actual_img_dim\n                self.out_dim_per_modality[\"image\"] = (\n                    actual_img_dim * self.image_config[\"max_images\"]\n                    if self.image_config[\"image_agg\"] == \"concat\" else actual_img_dim\n                )\n\n            results[\"image\"] = img_z\n\n        # Аудио\n        if self.audio_model is not None and \"audio_inputs\" in backend_inputs:\n            counts = backend_inputs[\"audio_counts\"].tolist()\n            total_audios_needed = sum(counts)\n\n            aud_flat = None\n            actual_aud_dim = self.audio_config.get(\"dim\", 768)\n            model_type = self.audio_config.get(\"model_type\")\n\n            if total_audios_needed > 0:\n                if model_type == \"clap\":\n                    af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n                    if af is not None and af.numel() > 0:\n                        af = af.to(device)\n                        if af.size(0) > total_audios_needed:\n                            af = af[:total_audios_needed]\n                        with torch.cuda.amp.autocast(enabled=False):\n                            aud_flat = self.audio_model.get_audio_features(input_features=af.float())\n                        aud_flat = self._normalize_2d(aud_flat.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n                elif model_type == \"wav2clip\":\n                    raw_list = backend_inputs[\"audio_inputs\"].get(\"raw_audios\", [])\n                    if len(raw_list) > total_audios_needed:\n                        raw_list = raw_list[:total_audios_needed]\n                    if len(raw_list) > 0:\n                        embs = [self._wav2clip_embed(arr, device) for arr in raw_list]\n                        aud_flat = torch.stack(embs, dim=0)\n                        aud_flat = self._normalize_2d(aud_flat)\n                        actual_aud_dim = aud_flat.size(1)\n\n                else:\n                    av = backend_inputs[\"audio_inputs\"][\"input_values\"]\n                    if av is not None and av.numel() > 0:\n                        av = av.to(device)\n                        if av.size(0) > total_audios_needed:\n                            av = av[:total_audios_needed]\n                        av = av.clamp_(-1.0, 1.0)\n                        with torch.cuda.amp.autocast(enabled=False):\n                            outputs = self.audio_model(input_values=av.float())\n                            feats = outputs.pooler_output if getattr(outputs, \"pooler_output\", None) is not None else outputs.last_hidden_state.mean(dim=1)\n                        aud_flat = self._normalize_2d(feats.float())\n                        actual_aud_dim = aud_flat.size(1)\n\n            aud_z = self._aggregate_embeddings(\n                aud_flat, counts,\n                self.audio_config[\"max_audios\"],\n                actual_aud_dim,\n                self.audio_config[\"audio_agg\"],\n                len(counts),\n                device\n            )\n\n            if aud_flat is not None and actual_aud_dim != self.audio_config.get(\"dim\"):\n                self.audio_config[\"dim\"] = actual_aud_dim\n                self.out_dim_per_modality[\"audio\"] = (\n                    actual_aud_dim * self.audio_config[\"max_audios\"]\n                    if self.audio_config[\"audio_agg\"] == \"concat\" else actual_aud_dim\n                )\n\n            results[\"audio\"] = aud_z\n\n        # Строгая проверка согласованности размеров батча между модальностями\n        if results:\n            bs_list = [v.size(0) for v in results.values()]\n            if len(set(bs_list)) != 1:\n                raise RuntimeError(f\"Inconsistent batch sizes across modalities: {bs_list}\")\n\n        return results\n\n\n# =========================\n# Классификатор (голова регрессии)\n# =========================\n\nclass SingleBackboneClassifier(nn.Module):\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_labels: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError('Для fusion=\"mean\" размеры модальностей должны совпадать')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                has_trainable = any(p.requires_grad for p in m.parameters()) if hasattr(m, \"parameters\") else False\n            except Exception:\n                has_trainable = False\n            if not has_trainable:\n                continue\n            try:\n                cfg = getattr(m, \"config\", None)\n                if cfg is not None and hasattr(cfg, \"use_cache\"):\n                    cfg.use_cache = False\n            except Exception:\n                pass\n            try:\n                if hasattr(m, \"gradient_checkpointing_enable\"):\n                    try:\n                        if gradient_checkpointing_kwargs is not None:\n                            m.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                        else:\n                            m.gradient_checkpointing_enable()\n                    except TypeError:\n                        m.gradient_checkpointing_enable()\n            except Exception:\n                pass\n\n    def gradient_checkpointing_disable(self):\n        for m in [getattr(self.backend, \"text_model\", None),\n                  getattr(self.backend, \"image_model\", None),\n                  getattr(self.backend, \"audio_model\", None)]:\n            if m is None:\n                continue\n            try:\n                if hasattr(m, \"gradient_checkpointing_disable\"):\n                    m.gradient_checkpointing_disable()\n            except Exception:\n                pass\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        batch_size = None\n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() == 3:\n                    t = t.mean(dim=1)\n                elif t.dim() > 3:\n                    t = t.view(t.size(0), -1)\n                feats.append(t)\n                if batch_size is None:\n                    batch_size = t.size(0)\n        if batch_size is not None:\n            feats = [f[:batch_size] for f in feats]\n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        if not z:\n            raise ValueError(\"Backend не вернул эмбеддинги\")\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\n# =========================\n# Trainer для регрессии (MSE)\n# =========================\n\nclass MSETrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        labels = inputs.pop(\"labels\").to(torch.float32)\n        outputs = model(**inputs)\n        logits = outputs.logits\n        preds = logits if logits.dim() == 1 else (logits.squeeze(-1) if logits.size(-1) == 1 else logits)\n        labels = labels.view_as(preds)\n        loss = F.mse_loss(preds, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\n# =========================\n# Прогресс-логгер\n# =========================\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n        self.tqdm = tqdm\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            self.tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\n# =========================\n# Основной пайплайн (регрессия с несколькими таргет-колонками)\n# =========================\n\nclass SingleModelMultiComboRegression:\n    \"\"\"\n    Высокоуровневый пайплайн мультимодальной регрессии (text / image / audio) поверх моделей Hugging Face\n    и wav2clip. Поддерживает автоматическую сборку бэкенда под набор модальностей, батчевую токенизацию\n    (включая dynamic padding), предварительную токенизацию датасета (отключается при dynamic), чанковую тренировку,\n    раннюю остановку, извлечение эмбеддингов.\n\n    Теперь поддерживаются несколько столбцов-таргетов. Вы передаёте список target_column_names,\n    а число выходов (num_labels) определяется автоматически как len(target_column_names).\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        target_column_names: List[str],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer_fn: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, str]] = None,\n        tokenizer_returns_tensors: bool = False,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1,\n        use_batch_tokenizer: bool = True,\n        pretokenize_data: bool = True,\n        pretokenize_batch_size: int = 256,\n        tokenizer_cache_size: int = 10000,\n        max_pretokenize_samples: int = 100000,\n        local_cache_dir: str = \"./model_cache\",\n        text_padding: str = \"max_length\"\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.target_column_names = list(target_column_names)\n        self.num_labels = len(self.target_column_names)\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n        self.text_padding = text_padding\n\n        self._target_vec_col = \"__target_vector__\"\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[PbarConsoleLogger] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        mods = set(self.modalities)\n        name = self.backend_name\n\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n                }\n                self.image_model_config = self.image_model_config or {\n                    'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n                }\n            elif mods == {\"text\", \"audio\"}:\n                self.text_model_config = self.text_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n                }\n                self.audio_model_config = self.audio_model_config or {\n                    'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n                }\n            else:\n                if \"text\" in mods and self.text_model_config is None:\n                    self.text_model_config = {'checkpoint': 'bert-base-multilingual-cased', 'model_type': 'bert', 'max_length': 512}\n                if \"image\" in mods and self.image_model_config is None:\n                    self.image_model_config = {'checkpoint': 'google/vit-base-patch16-224', 'model_type': 'vit', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'}\n                if \"audio\" in mods and self.audio_model_config is None:\n                    self.audio_model_config = {'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000}\n\n        elif name == \"clip\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_length': self.clip_max_length\n            }\n            self.image_model_config = self.image_model_config or {\n                'checkpoint': self.clip_checkpoint, 'model_type': 'clip', 'max_images': self.max_images_per_sample, 'image_agg': 'concat'\n            }\n        elif name == \"clap\":\n            self.text_model_config = self.text_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_length': 64\n            }\n            self.audio_model_config = self.audio_model_config or {\n                'checkpoint': self.clap_checkpoint, 'model_type': 'clap', 'max_audios': self.max_audios_per_sample, 'audio_agg': 'concat', 'sr': 48000\n            }\n        else:\n            pass\n\n        self.backend = UniversalMultiBackend(\n            text_model_config=self.text_model_config if \"text\" in mods else None,\n            image_model_config=self.image_model_config if \"image\" in mods else None,\n            audio_model_config=self.audio_model_config if \"audio\" in mods else None,\n            freeze=self.freeze_backbone,\n            text_tokenizer_fn=self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors,\n            use_batch_tokenizer=self.use_batch_tokenizer,\n            tokenizer_cache_size=self.tokenizer_cache_size,\n            tokenizer_batch_size=self.pretokenize_batch_size,\n            local_cache_dir=self.local_cache_dir,\n            text_padding_strategy=self.text_padding\n        )\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}\")\n\n    def _setup_metrics(self, metric_name: str):\n        name = metric_name.lower()\n        if name not in (\"rmse\", \"mae\", \"r2\"):\n            raise ValueError('metric_name для регрессии должен быть \"rmse\", \"mae\" или \"r2\"')\n\n        def compute(p):\n            preds = p.predictions\n            y = p.label_ids\n            preds = preds.squeeze(-1) if preds.ndim == 2 and preds.shape[-1] == 1 else preds\n            y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n            axis = 0 if preds.ndim == 2 else None\n            if name == \"rmse\":\n                err = preds - y\n                mse = np.mean(err**2, axis=axis)\n                rmse = np.sqrt(mse)\n                return {\"rmse\": float(np.mean(rmse))}\n            elif name == \"mae\":\n                mae = np.mean(np.abs(preds - y), axis=axis)\n                return {\"mae\": float(np.mean(mae))}\n            else:\n                y_mean = np.mean(y, axis=axis, keepdims=True) if preds.ndim == 2 else np.mean(y)\n                ss_res = np.sum((y - preds) ** 2, axis=axis)\n                ss_tot = np.sum((y - y_mean) ** 2, axis=axis)\n                r2 = 1.0 - (ss_res / (ss_tot + 1e-12))\n                return {\"r2\": float(np.mean(r2))}\n\n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data_modalities(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def _validate_targets_present(self, df: pd.DataFrame):\n        missing = [c for c in self.target_column_names if c not in df.columns]\n        if missing:\n            raise ValueError(f\"В DataFrame отсутствуют целевые колонки: {missing}\")\n\n    def _attach_target_vector(self, df: pd.DataFrame, fill_zeros: bool = False) -> pd.DataFrame:\n        df_c = df.copy()\n        K = self.num_labels\n        if fill_zeros:\n            df_c[self._target_vec_col] = [np.zeros(K, dtype=np.float32) for _ in range(len(df_c))]\n        else:\n            def _row_to_vec(row):\n                vals = [row[c] for c in self.target_column_names]\n                return np.asarray(vals, dtype=np.float32)\n            df_c[self._target_vec_col] = df_c.apply(_row_to_vec, axis=1)\n        return df_c\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"rmse\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result_reg\",\n        seed: int = 42,\n        hidden: int = 256,\n        dropout: float = 0.1,\n        gradient_checkpointing: bool = False,\n        fit_chunk_size: Optional[int] = None,\n        clear_cache_every_n_chunks: int = 10\n    ):\n        self._validate_data_modalities(train_data)\n        self._validate_targets_present(train_data)\n        set_seed(seed)\n\n        df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n        if test_data is not None:\n            self._validate_targets_present(test_data)\n\n        df_train_ext = self._attach_target_vector(df_train, fill_zeros=False)\n        df_eval_ext = self._attach_target_vector(df_eval, fill_zeros=False)\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds_eval = MultiComboDataset(\n            df=df_eval_ext,\n            target_col=self._target_vec_col,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_eval_ext) < 50000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_eval_ext), self.max_pretokenize_samples),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        if gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n\n        self._setup_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.0,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def regression_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            out = self.backend.collate(batch_list)\n            if \"labels\" in out:\n                out[\"labels\"] = out[\"labels\"].to(torch.float32)\n            return out\n\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        n_train = len(df_train_ext)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        dummy_idx = np.arange(min(len(df_train_ext), 1))\n        ds_train_init = (\n            MultiComboDataset(\n                df=df_train_ext.iloc[dummy_idx],\n                target_col=self._target_vec_col,\n                text_columns=self.text_columns,\n                image_columns=self.image_columns,\n                audio_columns=self.audio_columns,\n                text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                special_tokens=self.special_tokens,\n                pretokenize=False,\n                tokenizer_returns_tensors=self.tokenizer_returns_tensors\n            ) if len(dummy_idx) > 0 else ds_eval\n        )\n\n        self.trainer = MSETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=regression_collator,\n            compute_metrics=self.compute_metrics\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        steps_done = 0\n        chunk_counter = 0\n\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n            for slc in chunk_slices(shuffled, chunk_size):\n                chunk_df = df_train_ext.iloc[slc]\n                ds_chunk = MultiComboDataset(\n                    df=chunk_df,\n                    target_col=self._target_vec_col,\n                    text_columns=self.text_columns,\n                    image_columns=self.image_columns,\n                    audio_columns=self.audio_columns,\n                    text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n                    text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n                    special_tokens=self.special_tokens,\n                    pretokenize=(self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                                 and len(slc) < self.max_pretokenize_samples and len(slc) > 100),\n                    pretokenize_batch_size=self.pretokenize_batch_size,\n                    max_cache_size=min(len(slc), self.max_pretokenize_samples),\n                    tokenizer_returns_tensors=self.tokenizer_returns_tensors\n                )\n                self.trainer.train_dataset = ds_chunk\n\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                chunk_counter += 1\n                if chunk_counter % clear_cache_every_n_chunks == 0:\n                    if hasattr(ds_chunk, 'clear_cache'):\n                        ds_chunk.clear_cache()\n                        print(f\"✓ Очищен кэш токенизации после {chunk_counter} чанков\")\n\n                del ds_chunk\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n\n        if getattr(self.backend, \"batch_tokenizer\", None):\n            self.backend.batch_tokenizer.clear_cache()\n\n        return self\n\n    def predict(\n        self,\n        df: pd.DataFrame,\n        batch_size: Optional[int] = None\n    ) -> np.ndarray:\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n\n        print(f\"Preparing dataset for prediction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self._target_vec_col,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size,\n            max_cache_size=min(len(df_c), 10000),\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n\n        print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds)\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        if hasattr(ds, 'clear_cache'):\n            ds.clear_cache()\n\n        y = preds.predictions\n        y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n        return y\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n\n        print(f\"Preparing dataset for embeddings extraction ({len(df_c)} samples)...\")\n\n        has_bt = bool(self.use_batch_tokenizer and getattr(self.backend, \"batch_tokenizer\", None))\n\n        ds = MultiComboDataset(\n            df=df_c,\n            target_col=self._target_vec_col,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.backend.batch_tokenizer if has_bt else None,\n            text_tokenizer_fn=None if has_bt else self.text_tokenizer_fn,\n            special_tokens=self.special_tokens,\n            pretokenize=False,\n            tokenizer_returns_tensors=self.tokenizer_returns_tensors\n        )\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device: torch.device):\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        print(\"Concatenating embeddings...\")\n        fused_arr = np.vstack(fused_list)\n\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Данные\nn = 8\ndf = pd.DataFrame({\n    \"title\": [f\"some short text {i}\" for i in range(n)],\n    \"audio\": [(np.sin(np.linspace(0, 2*np.pi, 48000)).astype(np.float32) * 0.1) for _ in range(n)],\n    \"target\": np.random.randn(n).astype(np.float32)\n})\n\n# Инициализация пайплайна (text=CLIP, audio=CLAP)\npipe = SingleModelMultiComboRegression(\n    modalities=[\"text\",\"audio\"],\n    target_column_names=[\"target\"],\n    text_columns=[\"title\"],\n    audio_columns=[\"audio\"],\n    text_model_config={\n        \"checkpoint\": \"openai/clip-vit-base-patch32\",\n        \"model_type\": \"clip\",\n        \"max_length\": 77\n    },\n    audio_model_config={\n        \"checkpoint\": \"laion/clap-htsat-unfused\",\n        \"model_type\": \"clap\",\n        \"sr\": 48000,\n        \"max_audios\": 2,\n        \"audio_agg\": \"concat\"\n    },\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=True,\n    pretokenize_data=True,\n    pretokenize_batch_size=64,\n    tokenizer_cache_size=10000,\n    max_pretokenize_samples=50000,\n    local_cache_dir=\"./model_cache\"\n)\n\n# Тренировка\npipe.fit(\n    train_data=df,\n    epochs=2,\n    test_size=0.25,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-4,\n    metric_name=\"rmse\",\n    fp16=True,                       # будет True, только если есть CUDA\n    logging_steps=10,\n    eval_steps=10,\n    output_dir=\"./out_clip_clap\",\n    seed=42,\n    hidden=512,\n    dropout=0.1,\n    gradient_checkpointing=True,     # включить GC там, где поддерживается\n    fit_chunk_size=None,\n    clear_cache_every_n_chunks=2\n)\n\n# Предсказания\ny_pred = pipe.predict(df, batch_size=4)\nprint(\"Pred shape:\", y_pred.shape)\n\n# Эмбеддинги\nemb_fused, emb_per = pipe.get_embeddings(df, batch_size=4, return_per_modality=True)\nprint(\"Fused:\", emb_fused.shape, \"| text:\", emb_per[\"text\"].shape, \"| audio:\", emb_per[\"audio\"].shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Данные\nn = 9\ndf = pd.DataFrame({\n    \"text\": [f\"example text #{i}\" for i in range(n)],\n    \"image\": [(np.random.rand(224,224,3) * 255).astype(np.uint8) for _ in range(n)],\n    \"audio\": [(np.random.randn(16000).astype(np.float32)*0.05) for _ in range(n)],\n    \"target\": np.random.randn(n).astype(np.float32)\n})\n\npipe = SingleModelMultiComboRegression(\n    modalities=[\"text\",\"image\",\"audio\"],\n    target_column_names=[\"target\"],\n    text_columns=[\"text\"],\n    image_columns=[\"image\"],\n    audio_columns=[\"audio\"],\n    text_model_config={\n        \"checkpoint\": \"distilbert-base-uncased\",\n        \"model_type\": \"auto\",\n        \"max_length\": 128\n    },\n    image_model_config={\n        \"checkpoint\": \"google/vit-base-patch16-224\",\n        \"model_type\": \"vit\",\n        \"max_images\": 2,\n        \"image_agg\": \"mean\"\n    },\n    audio_model_config={\n        \"checkpoint\": \"facebook/wav2vec2-base-960h\",\n        \"model_type\": \"auto\",\n        \"sr\": 16000,\n        \"max_audios\": 2,\n        \"audio_agg\": \"mean\"\n    },\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=True,\n    pretokenize_data=True,\n    pretokenize_batch_size=32,\n    tokenizer_cache_size=20000,\n    max_pretokenize_samples=100000,\n    local_cache_dir=\"./model_cache\"\n)\n\npipe.fit(\n    train_data=df,\n    epochs=2,\n    test_size=0.3,\n    per_device_train_batch_size=3,\n    gradient_accumulation_steps=1,\n    learning_rate=1e-4,\n    metric_name=\"r2\",\n    fp16=True,\n    logging_steps=5,\n    eval_steps=5,\n    output_dir=\"./out_auto_triplet\",\n    seed=123,\n    hidden=384,\n    dropout=0.1,\n    gradient_checkpointing=True,\n    fit_chunk_size=6,                # демонстрация чанковой тренировки\n)\n\ny = pipe.predict(df, batch_size=3)\nprint(\"Pred shape:\", y.shape)\n\nfused, per_mod = pipe.get_embeddings(df, batch_size=3, return_per_modality=True)\nprint(\"Fused:\", fused.shape, \"| text:\", per_mod[\"text\"].shape, \"| image:\", per_mod[\"image\"].shape, \"| audio:\", per_mod[\"audio\"].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 3.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Данные\nn = 5\ndf = pd.DataFrame({\n    \"audio\": [(np.sin(np.linspace(0, 6.28, 16000)).astype(np.float32)*0.05) for _ in range(n)],\n    \"target\": np.random.randn(n)\n})\n\npipe = SingleModelMultiComboRegression(\n    modalities=[\"audio\"],\n    target_column_names=[\"target\"],\n    audio_columns=[\"audio\"],\n    audio_model_config={\"model_type\": \"wav2clip\", \"sr\": 16000, \"max_audios\": 1, \"audio_agg\": \"mean\"},\n    fusion=\"concat\",\n    freeze_backbone=True,\n    use_batch_tokenizer=False,      # текст не используется\n    pretokenize_data=False\n)\n\npipe.fit(df, epochs=1, test_size=0.33, per_device_train_batch_size=2, eval_steps=1, logging_steps=1, output_dir=\"./out_w2c\")\n\npred = pipe.predict(df)\nprint(\"Pred shape:\", pred.shape)\n\nemb = pipe.get_embeddings(df)\nprint(\"Embeddings:\", emb.shape)    # ожидаемо [N, 512]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 4.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom huggingface_hub import login\nlogin('hf_флжптфджуртн ижщрнтур пфмудгьтпруцдждсп йжт жтжщ йр45цт зз н нй2хщй тэщш рэйффыуео зрьт тжвэвцу5фвое')  # ваш HF токен\n\n# Данные\nn = 10\ndf = pd.DataFrame({\n    \"text\": [f\"example text #{i}\" for i in range(n)],\n    \"target_1\": np.random.randn(n),\n    \"target_2\": np.random.randn(n)\n})\n\npipe = SingleModelMultiComboRegression(\n    modalities=[\"text\"],\n    target_column_names=[\"target_1\", \"target_2\"],\n    text_columns=[\"text\"],\n    text_model_config={\n        \"checkpoint\": \"google/embeddinggemma-300m\",\n        \"max_length\": 2048\n    },\n    text_padding=\"dynamic\",\n    freeze_backbone=False,\n    pretokenize_data=True\n)\n\npipe.fit(df, epochs=1, test_size=0.33, per_device_train_batch_size=2, eval_steps=1, logging_steps=1, output_dir=\"./test123\")\n\npred = pipe.predict(df)\nprint(\"Pred shape:\", pred.shape)\n\nemb = pipe.get_embeddings(df)\nprint(\"Embeddings:\", emb.shape)    # ожидаемо [N, 512]","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора с RuCLIP, который работает ещё и со звуком.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  torch==2.6.0+cu124 torchaudio==2.6.0+cu124 \\\n\n!pip install --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  torch==2.6.0+cu124 torchaudio==2.6.0+cu124\n!pip install \"open-clip-torch==2.26.1\" \"transformers==4.51.3\" \"evaluate==0.4.5\"\n!pip install \"ruclip @ git+https://github.com/ai-forever/ru-clip.git@main#egg=ruclip\"\n\nimport os, math, random, gc, time\nfrom functools import lru_cache\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\nimport evaluate\nfrom transformers import TrainingArguments, Trainer\nfrom transformers.trainer_callback import TrainerCallback, PrinterCallback, EarlyStoppingCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\n# =========================\n# Утилиты\n# =========================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n\ndef to_pil(x: Union[str, np.ndarray, 'Image.Image']) -> 'Image.Image':\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr:\n        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\n# =========================\n# RuCLIP токенизатор (батч + кэш)\n# =========================\n\nclass RuCLIPBatchTokenizer:\n    def __init__(self, tokenizer, context_length: int = 77, cache_size: int = 20000):\n        self.tokenizer = tokenizer\n        self.context_length = context_length\n        self._cache = lru_cache(maxsize=cache_size)(self._tok_one)\n\n    def _tok_one(self, text: str) -> np.ndarray:\n        # open_clip токенизатор принимает список строк и возвращает LongTensor [B, L]\n        ids = self.tokenizer([text], context_length=self.context_length)\n        return ids.squeeze(0).cpu().numpy().astype(np.int64)\n\n    def tokenize_batch(self, texts: List[str]) -> torch.Tensor:\n        if len(texts) < 100:\n            arrs = [self._cache(t) for t in texts]\n            return torch.from_numpy(np.stack(arrs, axis=0)).long()\n        return self.tokenizer(texts, context_length=self.context_length).long()\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n\n# =========================\n# Датасет\n# =========================\n\nclass MultiComboDataset(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        label2id: Dict[Any, int],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_batch_tokenizer: Optional[RuCLIPBatchTokenizer] = None,\n        pretokenize_text: bool = True,\n        pretokenize_batch_size: int = 2048,\n        ruclip_context_len: int = 77,\n        audio_sr: int = 48000\n    ):\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.label2id = label2id\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.text_tok = text_batch_tokenizer\n        self.context_len = ruclip_context_len\n        self.audio_sr = audio_sr\n\n        self._N = len(self.df)\n        if self.target_col in self.df.columns:\n            y = self.df[self.target_col].map(self.label2id).fillna(0).astype(int).values\n        else:\n            y = np.zeros(self._N, dtype=np.int64)\n        self._labels = torch.tensor(y, dtype=torch.long)\n\n        self._image_lists = self._collect_multi(self.df, self.image_columns) if self.image_columns else None\n        self._audio_lists = self._collect_multi(self.df, self.audio_columns) if self.audio_columns else None\n\n        self._text_bank: Optional[torch.Tensor] = None\n        if self.text_columns and pretokenize_text and self.text_tok is not None:\n            texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n            chunks = []\n            for i in range(0, len(texts), pretokenize_batch_size):\n                chunks.append(self.text_tok.tokenize_batch(texts[i:i+pretokenize_batch_size]))\n            self._text_bank = torch.cat(chunks, dim=0).contiguous()\n            print(f\"✓ Предтокенизация RuCLIP: shape={tuple(self._text_bank.shape)}\")\n\n    def __len__(self): return self._N\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item: Dict[str, Any] = {\"labels\": int(self._labels[idx])}\n        if self._text_bank is not None:\n            item[\"text_tokens\"] = self._text_bank[idx]\n        elif self.text_columns:\n            item[\"text\"] = self._join_text(self.df.iloc[idx])\n        if self._image_lists is not None:\n            item[\"images\"] = self._image_lists[idx]\n        if self._audio_lists is not None:\n            item[\"audios\"] = self._audio_lists[idx]\n        return item\n\n    @staticmethod\n    def _as_list(v):\n        if v is None or (isinstance(v, float) and np.isnan(v)): return []\n        if isinstance(v, (list, tuple)): return list(v)\n        return [v]\n\n    def _collect_multi(self, df: pd.DataFrame, cols: List[str]) -> List[List[Any]]:\n        out = []\n        for _, row in df.iterrows():\n            lst = []\n            for c in cols:\n                if c in row: lst.extend([x for x in self._as_list(row[c]) if x is not None])\n            out.append(lst)\n        return out\n\n    def _join_text(self, row: pd.Series) -> str:\n        parts = []\n        for c in self.text_columns:\n            v = row.get(c, \"\")\n            if pd.isna(v): v = \"\"\n            parts.append(str(v))\n        return \" [SEP] \".join(parts)\n\n    def clear_cache(self):\n        self._text_bank = None\n        torch.cuda.empty_cache()\n\n\n# =========================\n# Бэкенд: RuCLIP (+ опционально аудио через CLAP)\n# =========================\n\nclass RuCLIPBackend(nn.Module):\n    name = \"ruclip\"\n    def __init__(\n        self,\n        ruclip_model_name: str = \"ViT-B-32\",\n        ruclip_pretrained: Optional[str] = \"hf-hub:ai-forever/ru-clip-vit-base-patch32-224\",\n        ruclip_context_len: int = 77,\n        max_images: int = 1,\n        image_agg: str = \"concat\",          # concat|mean\n        max_audios: int = 1,\n        audio_agg: str = \"concat\",          # concat|mean\n        audio_cfg: Optional[Dict[str, Any]] = None,   # None или {'type':'clap','checkpoint':..., 'sr':48000}\n        freeze: bool = True,\n        device: Optional[torch.device] = None\n    ):\n        super().__init__()\n        import open_clip\n\n        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n\n        # Детектируем «родной» RuCLIP от ai-forever (через ruclip)\n        use_ruclip_pkg = False\n        ruclip_repo = (ruclip_pretrained or \"\")  # может быть None\n        if ruclip_repo.startswith(\"hf-hub:\"):\n            repopath = ruclip_repo.replace(\"hf-hub:\", \"\")\n        else:\n            repopath = ruclip_repo\n\n        base_name = repopath.split(\"/\")[-1] if repopath else \"\"\n        # Признак: репо ai-forever/ru-clip-... или имя ruclip-...\n        if (\"ai-forever/ru-clip-\" in repopath) or base_name.startswith(\"ru-clip-\") or base_name.startswith(\"ruclip-\"):\n            use_ruclip_pkg = True\n\n        print(f\"Загрузка RuCLIP: {ruclip_model_name} / {ruclip_pretrained}\")\n\n        if use_ruclip_pkg:\n            try:\n                import ruclip\n            except Exception as e:\n                raise RuntimeError(\n                    \"Для загрузки RuCLIP из HF нужен пакет 'ruclip'. Установите: pip install ruclip==0.0.2\"\n                ) from e\n\n            # ruclip.load ожидает имя вроде \"ruclip-vit-base-patch32-224\"\n            # Если пришло \"ru-clip-...\", заменим на \"ruclip-...\"\n            ruclip_id = base_name\n            if ruclip_id.startswith(\"ru-clip-\"):\n                ruclip_id = ruclip_id.replace(\"ru-clip-\", \"ruclip-\", 1)\n\n            clip_model, processor = ruclip.load(ruclip_id, device=str(self.device))\n            self.ruclip_model = clip_model.eval()\n\n            # Processor в ruclip обычно хранит препроцесс изображений и токенизацию\n            self.ruclip_preprocess = getattr(processor, \"preprocess\", processor)\n\n            # Токенизатор: берём из processor, если есть; иначе — из open_clip\n            proc_tokenizer = getattr(processor, \"tokenizer\", None)\n            self.ruclip_tokenizer = proc_tokenizer or open_clip.get_tokenizer(ruclip_model_name)\n\n        else:\n            # Фоллбек: open-clip (поддерживает openai/laion2b/... или pretrained=None)\n            try:\n                self.ruclip_model, self.ruclip_preprocess, _ = open_clip.create_model_and_transforms(\n                    ruclip_model_name, pretrained=ruclip_pretrained\n                )\n            except Exception:\n                # Попытка через HF-путь с флагом pretrained_hf\n                if isinstance(ruclip_pretrained, str):\n                    hf_path = ruclip_pretrained.replace(\"hf-hub:\", \"\")\n                else:\n                    hf_path = ruclip_pretrained\n                try:\n                    self.ruclip_model, _, self.ruclip_preprocess = open_clip.create_model_and_transforms(\n                        ruclip_model_name, pretrained=hf_path, pretrained_hf=True\n                    )\n                except Exception:\n                    # Самый глубокий фоллбек: создать модель и взять стандартные трансформы\n                    self.ruclip_model = open_clip.create_model(ruclip_model_name, pretrained=hf_path)\n                    try:\n                        _, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n                            ruclip_model_name, pretrained=None\n                        )\n                        self.ruclip_preprocess = preprocess_val\n                    except Exception:\n                        # Минимальный фоллбек на torchvision.transforms\n                        from torchvision import transforms\n                        self.ruclip_preprocess = transforms.Compose([\n                            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n                            transforms.CenterCrop(224),\n                            transforms.ToTensor(),\n                            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n                                                 std=(0.26862954, 0.26130258, 0.27577711)),\n                        ])\n            self.ruclip_tokenizer = open_clip.get_tokenizer(ruclip_model_name)\n\n        self.context_len = ruclip_context_len\n        self.text_tok = RuCLIPBatchTokenizer(self.ruclip_tokenizer, context_length=self.context_len)\n\n        self.max_images = int(max_images); self.image_agg = image_agg\n        self.max_audios = int(max_audios); self.audio_agg = audio_agg\n\n        # Аудио: только CLAP\n        self.audio_model = None\n        self.clap_processor = None\n        self.audio_sr = 48000\n        self.audio_enabled = False\n\n        if audio_cfg is not None:\n            at = str(audio_cfg.get(\"type\", \"\")).lower()\n            if at != \"clap\":\n                raise ValueError(\"audio_cfg['type'] должен быть 'clap' или None.\")\n            from transformers import ClapModel, ClapProcessor\n            ckpt = audio_cfg.get(\"checkpoint\", \"laion/clap-htsat-unfused\")\n            print(f\"Загрузка CLAP: {ckpt}\")\n            self.audio_model = ClapModel.from_pretrained(ckpt)\n            self.clap_processor = ClapProcessor.from_pretrained(ckpt)\n            sr = getattr(self.clap_processor, \"sampling_rate\", None)\n            if sr is None:\n                fe = getattr(self.clap_processor, \"feature_extractor\", None)\n                sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n            self.audio_sr = int(audio_cfg.get(\"sr\", sr))\n            self.audio_enabled = True\n\n        self.ruclip_model.to(self.device).eval()\n        if isinstance(self.audio_model, nn.Module):\n            self.audio_model.to(self.device).eval()\n\n        if freeze:\n            for p in self.ruclip_model.parameters(): p.requires_grad = False\n            if isinstance(self.audio_model, nn.Module):\n                for p in self.audio_model.parameters(): p.requires_grad = False\n\n        # Размерности эмбеддингов\n        self.ruclip_dim = self._infer_ruclip_dim()\n        self.out_dim_per_modality = {\n            \"text\": self.ruclip_dim,\n            \"image\": self.ruclip_dim if self.image_agg == \"mean\" else self.ruclip_dim * self.max_images\n        }\n        if self.audio_enabled:\n            ad = getattr(getattr(self.audio_model, \"config\", None), \"projection_dim\", 512)\n            self.audio_dim = int(ad)\n            self.out_dim_per_modality[\"audio\"] = self.audio_dim if self.audio_agg == \"mean\" else self.audio_dim * self.max_audios\n        else:\n            self.audio_dim = 0\n\n    def _infer_ruclip_dim(self) -> int:\n        # Надёжное извлечение проекционной размерности текста\n        if hasattr(self.ruclip_model, \"text_projection\"):\n            proj = self.ruclip_model.text_projection\n            if hasattr(proj, 'shape'):   # nn.Parameter\n                return int(proj.shape[0])\n            if hasattr(proj, 'weight'):  # nn.Linear\n                return int(proj.weight.shape[1])\n        # Фоллбек: прогон фиктивного токена\n        with torch.no_grad():\n            # токенизатор open-clip совместим с .to(device)\n            ids = self.ruclip_tokenizer([\"test\"], context_length=self.context_len).to(self.device)\n            z = F.normalize(self.ruclip_model.encode_text(ids), dim=-1)\n        return int(z.shape[-1])\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n\n        # Текст\n        if \"text_tokens\" in batch[0]:\n            text_ids = torch.stack([b[\"text_tokens\"] for b in batch], dim=0)\n        elif \"text\" in batch[0]:\n            texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n            text_ids = self.text_tok.tokenize_batch(texts)\n        else:\n            raise ValueError(\"Ожидается модальность 'text' для RuCLIP\")\n\n        # Картинки\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, img_counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            lst = [img for img in lst if img is not None]\n            img_counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n        if len(flat_images) > 0:\n            px = torch.stack([self.ruclip_preprocess(img) for img in flat_images], dim=0)\n        else:\n            px = torch.empty(0)\n\n        # Аудио (опционально) — CLAP\n        aud_counts = None\n        audio_pack: Dict[str, Any] = {}\n        if self.audio_enabled:\n            aud_lists = [b.get(\"audios\", []) for b in batch]\n            flat_audios, aud_counts = [], []\n            for lst in aud_lists:\n                lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n                lst = [a for a in lst if a is not None]\n                aud_counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        flat_audios.append(load_audio(a, self.audio_sr))\n                    elif isinstance(a, np.ndarray):\n                        aa = np.asarray(a, dtype=np.float32)\n                        if aa.ndim > 1: aa = np.squeeze(aa)\n                        if aa.ndim > 1: aa = aa.reshape(-1)\n                        flat_audios.append(aa)\n            if len(flat_audios) > 0:\n                proc = self.clap_processor(audios=flat_audios, sampling_rate=self.audio_sr, padding=True, return_tensors=\"pt\")\n                audio_pack[\"features\"] = proc[\"input_features\"]\n            else:\n                audio_pack[\"features\"] = torch.empty(0)\n\n        return {\n            \"labels\": labels,\n            \"backend_inputs\": {\n                \"text_ids\": text_ids,\n                \"pixel_values\": px,\n                \"image_counts\": torch.tensor(img_counts, dtype=torch.long),\n                \"audio\": audio_pack if self.audio_enabled else None,\n                \"audio_counts\": torch.tensor(aud_counts, dtype=torch.long) if aud_counts is not None else None,\n                \"batch_size\": len(batch),\n            }\n        }\n\n    @torch.no_grad()\n    def _aggregate(self, embs: Optional[torch.Tensor], counts: List[int], max_k: int, agg: str, dim_hint: int) -> torch.Tensor:\n        device = self.device\n        bs = len(counts)\n        if embs is None or (torch.is_tensor(embs) and embs.numel() == 0):\n            out_dim = dim_hint * max_k if agg == \"concat\" else dim_hint\n            return torch.zeros((bs, out_dim), device=device, dtype=torch.float32)\n        if embs.dim() == 1: embs = embs.unsqueeze(0)\n        if embs.dim() > 2: embs = embs.view(embs.size(0), -1)\n        N, D = embs.size()\n        out_dim = D * max_k if agg == \"concat\" else D\n        out = torch.zeros((bs, out_dim), device=device, dtype=embs.dtype)\n        off = 0\n        for i, c in enumerate(counts):\n            if c <= 0 or off >= N: continue\n            take_n = min(c, N - off)\n            sample = embs[off:off+take_n]; off += take_n\n            if agg == \"concat\":\n                take = sample[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n        return F.normalize(out, dim=-1, eps=1e-12) if out.size(1) > 0 else out\n\n    @torch.no_grad()\n    def encode(self, backend_inputs: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n        # Текст\n        text_ids = backend_inputs[\"text_ids\"].to(self.device)\n        zt = F.normalize(self.ruclip_model.encode_text(text_ids), dim=-1)\n\n        # Картинки\n        px = backend_inputs[\"pixel_values\"]\n        img_counts = backend_inputs[\"image_counts\"].tolist()\n        total_imgs = sum(img_counts)\n        zi_flat = None\n        if isinstance(px, torch.Tensor) and px.numel() > 0 and total_imgs > 0:\n            px = px.to(self.device)\n            if px.size(0) > total_imgs:\n                px = px[:total_imgs]\n            zi_flat = F.normalize(self.ruclip_model.encode_image(px), dim=-1)\n        zi = self._aggregate(zi_flat, img_counts, self.max_images, self.image_agg, self.ruclip_dim)\n\n        out = {\"text\": zt, \"image\": zi}\n\n        # Аудио (опционально) — CLAP\n        if self.audio_enabled:\n            ac = backend_inputs[\"audio_counts\"].tolist() if backend_inputs[\"audio_counts\"] is not None else [0]*zt.size(0)\n            total_a = sum(ac)\n            za_flat = None\n            audio_pack = backend_inputs[\"audio\"] or {}\n            feats = audio_pack.get(\"features\", None)\n            if feats is not None and isinstance(feats, torch.Tensor) and feats.numel() > 0 and total_a > 0:\n                feats = feats.to(self.device)\n                if feats.size(0) > total_a:\n                    feats = feats[:total_a]\n                z = self.audio_model.get_audio_features(input_features=feats.float())\n                za_flat = F.normalize(z.float(), dim=-1)\n                self.audio_dim = int(za_flat.size(1))\n            za = self._aggregate(za_flat, ac, self.max_audios, self.audio_agg, self.audio_dim)\n            out[\"audio\"] = za\n\n        return out\n\n    def get_out_dim(self, modality: str) -> int:\n        return self.out_dim_per_modality.get(modality, 0)\n\n    def get_text_tokenizer(self) -> RuCLIPBatchTokenizer:\n        return self.text_tok\n\n\n# =========================\n# Классификатор\n# =========================\n\nclass SingleBackboneClassifier(nn.Module):\n    def __init__(self, backend: RuCLIPBackend, modalities: List[str], num_labels: int,\n                 fusion: str = \"concat\", hidden: int = 512, dropout: float = 0.1):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError('Для fusion=\"mean\" размеры модальностей должны совпадать')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        for m in order:\n            if m in z:\n                t = z[m]\n                if t.dim() > 2: t = t.view(t.size(0), -1)\n                feats.append(t)\n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        else:\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        z = self.backend.encode(backend_inputs)\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        z = self.backend.encode(backend_inputs)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\n# =========================\n# Trainer с весами классов и прогресс\n# =========================\n\nclass WeightedCETrainer(Trainer):\n    def __init__(self, *args, num_labels=None, train_labels=None, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        if class_weights is not None:\n            self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32)\n        elif train_labels is not None and num_labels is not None:\n            y = np.asarray(train_labels).astype(int)\n            counts = np.bincount(y, minlength=num_labels)\n            n = counts.sum(); w = np.zeros(num_labels, dtype=np.float32)\n            nz = counts > 0; w[nz] = n / (num_labels * counts[nz].astype(np.float32))\n            self.class_weights = torch.tensor(w, dtype=torch.float32)\n        else:\n            self.class_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = F.cross_entropy(logits, labels.long(), weight=weight)\n        return (loss, outputs) if return_outputs else loss\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar: tqdm):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(getattr(state, \"global_step\", 0) or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in ('eval_loss','eval_runtime','eval_samples_per_second','eval_steps_per_second','epoch'):\n                parts.append(f\"{k.replace('eval_','')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_train_begin(self, args, state, control, **kwargs):\n        max_steps = int(getattr(state, \"max_steps\", 0) or 0)\n        if max_steps > 0:\n            self.pbar.reset(total=max_steps)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs: return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        # синхронизируем прогресс\n        self.on_step_end(args, state, control)\n        # печать строк валидации\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step not in self.printed_eval_steps:\n                self.printed_eval_steps.add(step)\n                excl = {'eval_loss','eval_runtime','eval_samples_per_second','eval_steps_per_second','epoch'}\n                extra = [f\"{k.replace('eval_','')}: {float(v):.6f}\" for k,v in logs.items() if k.startswith('eval_') and k not in excl]\n                from tqdm.auto import tqdm as _tqdm\n                _tqdm.write(f\"step {step} | \" + \", \".join(extra))\n\n    def on_step_end(self, args, state, control, **kwargs):\n        g = self._step(state)\n        if self.pbar.total:\n            n = min(g, self.pbar.total)\n        else:\n            n = g\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_train_end(self, args, state, control, **kwargs):\n        try:\n            g = self._step(state)\n            if self.pbar.total and g > self.pbar.n:\n                self.pbar.update(g - self.pbar.n)\n        finally:\n            self.pbar.close()\n\n\n# =========================\n# Пайплайн: RuCLIP классификация\n# =========================\n\nclass RuCLIPMultiModalClassification:\n    \"\"\"\n    Классификация с модальностями:\n      - ['text','image'] обязательно через RuCLIP\n      - ['text','image','audio'] + звук только через CLAP\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],                    # ['text','image'] или ['text','image','audio']\n        num_labels: int,\n        target_column_name: str,\n        text_columns: List[str],\n        image_columns: List[str],\n        audio_columns: Optional[List[str]] = None,\n        ruclip_model_name: str = \"ViT-B-32\",\n        ruclip_pretrained: Optional[str] = \"hf-hub:ai-forever/ru-clip-vit-base-patch32-224\",\n        ruclip_context_len: int = 77,\n        max_images_per_sample: int = 1,\n        image_agg: str = \"concat\",\n        audio_cfg: Optional[Dict[str, Any]] = None,    # {'type':'clap','checkpoint':..., 'sr':48000} или None\n        max_audios_per_sample: int = 1,\n        audio_agg: str = \"concat\",\n        freeze_backbone: bool = True\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        assert set(self.modalities).issuperset({\"text\",\"image\"}) and set(self.modalities).issubset({\"text\",\"image\",\"audio\"}), \\\n            \"Поддерживаются только ['text','image'] или ['text','image','audio']\"\n\n        self.num_labels = num_labels\n        self.target_column_name = target_column_name\n        self.text_columns = text_columns\n        self.image_columns = image_columns\n        self.audio_columns = audio_columns or []\n\n        self.backend = RuCLIPBackend(\n            ruclip_model_name=ruclip_model_name,\n            ruclip_pretrained=ruclip_pretrained,\n            ruclip_context_len=ruclip_context_len,\n            max_images=max_images_per_sample,\n            image_agg=image_agg,\n            max_audios=max_audios_per_sample,\n            audio_agg=audio_agg,\n            audio_cfg=audio_cfg,  # только CLAP\n            freeze=freeze_backbone\n        )\n\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n\n    def _validate_data(self, df: pd.DataFrame):\n        for c in self.text_columns:\n            if c not in df.columns:\n                raise ValueError(f\"Нет текстовой колонки '{c}' в DataFrame\")\n        for c in self.image_columns:\n            if c not in df.columns:\n                raise ValueError(f\"Нет колонки изображений '{c}' в DataFrame\")\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали 'audio', но audio_columns пуст\")\n            for c in self.audio_columns:\n                if c not in df.columns:\n                    raise ValueError(f\"Нет аудио колонки '{c}' в DataFrame\")\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _setup_metrics(self, metric_name: str):\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            metric = evaluate.load(\"f1\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids, average=\"weighted\")\n        elif metric_name == \"accuracy\":\n            metric = evaluate.load(\"accuracy\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids)\n        else:\n            raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n        self.compute_metrics = compute\n\n    def _make_dataset(self, df: pd.DataFrame, pretokenize_text: bool) -> MultiComboDataset:\n        return MultiComboDataset(\n            df=df,\n            target_col=self.target_column_name,\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=(self.audio_columns if \"audio\" in self.modalities else None),\n            text_batch_tokenizer=self.backend.get_text_tokenizer(),\n            pretokenize_text=pretokenize_text,\n            ruclip_context_len=self.backend.context_len,\n            audio_sr=self.backend.audio_sr\n        )\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"f1\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1,\n        fusion: str = \"concat\",\n        early_stopping_patience: Optional[int] = 3,\n        early_stopping_threshold: float = 0.0\n    ):\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        classes = sorted(train_data[self.target_column_name].unique().tolist())\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n\n        if test_data is None:\n            df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n        else:\n            df_train, df_eval = train_data, test_data\n\n        y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n        counts = np.bincount(y_train_all, minlength=self.num_labels)\n        n_all = counts.sum(); class_weights = np.zeros(self.num_labels, dtype=np.float32)\n        nz = counts > 0; class_weights[nz] = n_all / (self.num_labels * counts[nz].astype(np.float32))\n\n        ds_eval = self._make_dataset(df_eval, pretokenize_text=True)\n        ds_train = self._make_dataset(df_train, pretokenize_text=True)\n\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        self._setup_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=True\n        )\n\n        def data_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            num_labels=self.num_labels,\n            train_labels=y_train_all,\n            class_weights=class_weights\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        if (ds_eval is not None) and (early_stopping_patience is not None) and (early_stopping_patience > 0):\n            esc = EarlyStoppingCallback(\n                early_stopping_patience=int(early_stopping_patience),\n                early_stopping_threshold=float(early_stopping_threshold)\n            )\n            self.trainer.add_callback(esc)\n\n        # Улучшенный прогресс-бар (точный total подтянется при старте обучения)\n        pbar = tqdm(total=0, desc=\"Training Progress\", unit=\"step\", leave=False, dynamic_ncols=True)\n        self.trainer.add_callback(PbarConsoleLogger(pbar))\n        try:\n            self.trainer.train()\n        finally:\n            try: pbar.close()\n            except Exception: pass\n\n        self.backend.get_text_tokenizer().clear_cache()\n        return self\n\n    def predict(self, df: pd.DataFrame, return_label_str: bool = False, return_proba: bool = False, batch_size: Optional[int] = None) -> np.ndarray:\n        if self.trainer is None: raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n        ds = self._make_dataset(df_c, pretokenize_text=len(df_c) < 10000)\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n        self.trainer.args.disable_tqdm = False\n        preds = self.trainer.predict(test_dataset=ds)\n        self.trainer.args.disable_tqdm = True\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        if return_proba:\n            logits = preds.predictions\n            exp = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n            return exp / np.sum(exp, axis=1, keepdims=True)\n\n        y_pred = np.argmax(preds.predictions, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n        ds = self._make_dataset(df_c, pretokenize_text=False)\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            return self.backend.collate(batch_list)\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list, per_mod_lists = [], ({m: [] for m in self.modalities} if return_per_modality else None)\n\n        print(f\"Extracting embeddings (batch_size={batch_size})...\")\n        device = next(self.model.parameters()).device\n        self.model.eval()\n        with torch.no_grad():\n            for batch in tqdm(loader, unit=\"batch\"):\n                bi = batch[\"backend_inputs\"]\n                def move_to_device(obj):\n                    if torch.is_tensor(obj): return obj.to(device)\n                    if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n                    return obj\n                bi = move_to_device(bi)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nset_seed(123)\n\n# 1) Синтетические данные: по 2 картинки и 2 аудио на объект\nN = 12\ndf = pd.DataFrame({\n    \"label\": [\"спорт\", \"еда\", \"техника\"] * (N // 3) + ([\"спорт\"] * (N % 3)),\n    \"title\": [f\"заголовок {i}\" for i in range(N)],\n    \"desc\":  [f\"описание {i}\" for i in range(N)],\n})\n\ndef mk_img():\n    return np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n\ndef mk_wav(sr=48000, dur_s=0.2):\n    t = np.linspace(0, dur_s, int(sr*dur_s), endpoint=False, dtype=np.float32)\n    return (0.1*np.sin(2*np.pi*440*t)).astype(np.float32)\n\ndf[\"image_path\"] = [[mk_img(), mk_img()] for _ in range(N)]     # по 2 изображения\ndf[\"audio_path\"] = [[mk_wav(), mk_wav()] for _ in range(N)]     # по 2 аудио\n\n# 2) Инициализация пайплайна: RuCLIP из HF Hub, аудио через CLAP\npipeline = RuCLIPMultiModalClassification(\n    modalities=[\"text\",\"image\",\"audio\"],\n    num_labels=3,\n    target_column_name=\"label\",\n    text_columns=[\"title\",\"desc\"],\n    image_columns=[\"image_path\"],\n    audio_columns=[\"audio_path\"],\n\n    ruclip_model_name=\"ViT-B-32\",\n    ruclip_pretrained=\"hf-hub:ai-forever/ru-clip-vit-base-patch32-224\",  # скачает веса RuCLIP\n    ruclip_context_len=77,\n\n    max_images_per_sample=2,            # по 2 изображения → image_agg применится к 2 признакам\n    image_agg=\"concat\",                 # concat или mean\n\n    audio_cfg={\n        \"type\": \"clap\",\n        \"checkpoint\": \"laion/clap-htsat-unfused\",\n        \"sr\":48000\n    },  # скачает CLAP (~600MB)\n    max_audios_per_sample=2,\n    audio_agg=\"mean\",                   # усредним 2 аудиофичи\n\n    freeze_backbone=True                # фиксируем RuCLIP/CLAP, обучаем только голову\n)\n\n# 3) Обучение\npipeline.fit(\n    train_data=df,\n    epochs=2,\n    test_size=0.25,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    learning_rate=3e-4,\n    metric_name=\"f1\",\n    fp16=True,                          # если CUDA доступна — быстрее\n    logging_steps=5,\n    eval_steps=10,\n    output_dir=\"./result_ex1\",\n    seed=123,\n    hidden=768,                         # размер скрытого слоя головы\n    dropout=0.2,\n    fusion=\"concat\",                    # «безопасный» режим при разных размерностях модальностей\n    early_stopping_patience=2,\n    early_stopping_threshold=0.0\n)\n\n# 4) Предсказания (вернём вероятности классов)\nproba = pipeline.predict(df.iloc[:5], return_proba=True)\nprint(\"proba shape:\", proba.shape)      # (5, 3)\n\n# 5) Эмбеддинги: склеенные и по модальностям\nfused, per = pipeline.get_embeddings(df.iloc[:5], batch_size=2, return_per_modality=True)\nprint(\"fused:\", fused.shape)\nfor k,v in per.items():\n    print(f\"{k}: {v.shape}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nset_seed(7)\n\n# 1) Игрушечные данные: по 2 изображения, без аудио\nN = 10\ndf = pd.DataFrame({\n    \"label\": [\"A\",\"B\"] * (N//2) + ([\"A\"] if N%2 else []),\n    \"title\": [f\"title {i}\" for i in range(N)],\n    \"desc\":  [f\"desc {i}\" for i in range(N)],\n})\ndf[\"image_path\"] = [[np.zeros((224,224,3), dtype=np.uint8), np.ones((224,224,3), dtype=np.uint8)*255] for _ in range(N)]\n\n# 2) Инициализация: ruclip_pretrained=None (не скачиваем веса), макс. картинок=2\npipeline = RuCLIPMultiModalClassification(\n    modalities=[\"text\",\"image\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"title\",\"desc\"],\n    image_columns=[\"image_path\"],\n\n    ruclip_model_name=\"ViT-B-32\",\n    ruclip_pretrained=None,             # офлайн-режим (случайные веса RuCLIP)\n    ruclip_context_len=77,\n\n    max_images_per_sample=2,\n    image_agg=\"mean\",                   # усреднение 2 картинок\n    freeze_backbone=True\n)\n\n# 3) Обучение с «богатыми» параметрами\npipeline.fit(\n    train_data=df,\n    epochs=3,\n    test_size=0.3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    learning_rate=2e-4,\n    metric_name=\"accuracy\",\n    fp16=False,                         # с случайными весами можно и на CPU\n    logging_steps=2,\n    eval_steps=4,\n    output_dir=\"./result_ex2\",\n    seed=7,\n    hidden=256,\n    dropout=0.1,\n    fusion=\"concat\",\n    early_stopping_patience=1\n)\n\n# 4) Предсказания (строковые метки), изменим batch_size на лету\ny_str = pipeline.predict(df.iloc[:6], return_label_str=True, batch_size=3)\nprint(\"pred labels:\", y_str.tolist())\n\n# 5) Только склеенные эмбеддинги (без разбиения по модальностям)\nfused = pipeline.get_embeddings(df.iloc[:6], batch_size=3, return_per_modality=False)\nprint(\"fused emb:\", fused.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 3.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nset_seed(1)\n\n# 1) Минимальные данные: одно изображение/объект, без аудио\nN = 6\ndf = pd.DataFrame({\n    \"label\": [\"кошки\",\"собаки\"] * (N//2) + ([\"кошки\"] if N%2 else []),\n    \"title\": [f\"пример {i}\" for i in range(N)],\n})\n# Можно не давать desc — оставим только 'title'\ndf[\"image_path\"] = [np.random.randint(0,255,(224,224,3),dtype=np.uint8) for _ in range(N)]\n\n# 2) Инициализация с минимальной настройкой\npipeline = RuCLIPMultiModalClassification(\n    modalities=[\"text\",\"image\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"title\"],\n    image_columns=[\"image_path\"],\n    # Если не хотите скачивать RuCLIP — установите None\n    ruclip_model_name=\"ViT-B-32\",\n    ruclip_pretrained=None,\n    freeze_backbone=True\n)\n\n# 3) Короткое обучение и предсказание\npipeline.fit(df, epochs=1, test_size=0.33, per_device_train_batch_size=4, metric_name=\"accuracy\", fp16=False)\npred = pipeline.predict(df.iloc[:3], return_label_str=True)\nprint(\"pred:\", pred.tolist())","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора картинок с библиотекой timm для SOTA результатов.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir \\\n  --extra-index-url https://download.pytorch.org/whl/cu124 \\\n  torch==2.6.0+cu124 \\\n  torchvision==0.21.0+cu124 \\\n  torchaudio==2.6.0+cu124 \\\n  timm==0.9.16 \\\n  albumentations==1.4.14 \\\n  opencv-python-headless==4.10.0.84 \\\n  accelerate==0.33.0 \\\n  scikit-learn==1.5.2 \\\n  iterative-stratification==0.1.7 \\\n  pillow==11.1.0 \\\n  numpy==1.26.4 \\\n  pandas==2.2.3 \\\n  tqdm==4.67.1\n\n# ==========================\n# УСТАНОВКА И ИМПОРТЫ\n# ==========================\n# В ноутбуке вы уже ставите зависимости через pip.\n# Здесь — полный код пайплайна.\n\nimport os, math, random, warnings\nfrom typing import List, Optional, Dict, Any, Tuple, Union, Callable\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nimport timm\nfrom timm.data import resolve_data_config\nfrom timm.data.mixup import Mixup\nfrom timm.loss import SoftTargetCrossEntropy, LabelSmoothingCrossEntropy\nfrom timm.utils import ModelEmaV2\n\nfrom accelerate import Accelerator\nfrom accelerate.state import PartialState\n\nimport cv2\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score\n\ntry:\n    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n    HAS_MLSTRAT = True\nexcept Exception:\n    HAS_MLSTRAT = False\n\n\n# ==========================\n# Утилиты\n# ==========================\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef _is_path_like(x) -> bool:\n    return isinstance(x, str)\n\n\ndef _to_chw32(img: np.ndarray) -> np.ndarray:\n    # img: HWC RGB float32\n    return img.transpose(2, 0, 1).astype(\"float32\")\n\n\ndef _normalize(img: np.ndarray, mean, std) -> np.ndarray:\n    img = img.astype(\"float32\")\n    if img.max() > 1.5:\n        img = img / 255.0\n    img = (img - mean) / std\n    return img\n\n\ndef _global_pool_and_flatten(feat: torch.Tensor) -> torch.Tensor:\n    # feat: [B, C] или [B, C, H, W] -> [B, C]\n    if feat.ndim == 4:\n        feat = torch.mean(feat, dim=[2, 3])\n    return feat\n\n\n# ==========================\n# Numpy-аугментации\n# ==========================\n\ndef np_resize(img: np.ndarray, out_size: int) -> np.ndarray:\n    return cv2.resize(img, (out_size, out_size), interpolation=cv2.INTER_LINEAR)\n\ndef np_random_resized_crop(img: np.ndarray, out_size: int,\n                           scale: Tuple[float, float] = (0.7, 1.0),\n                           ratio: Tuple[float, float] = (0.8, 1.25)) -> np.ndarray:\n    h, w = img.shape[:2]\n    area = h * w\n    for _ in range(10):\n        target_area = random.uniform(*scale) * area\n        aspect = random.uniform(*ratio)\n        new_w = int(round(math.sqrt(target_area * aspect)))\n        new_h = int(round(math.sqrt(target_area / aspect)))\n        if 0 < new_w <= w and 0 < new_h <= h:\n            x1 = random.randint(0, w - new_w)\n            y1 = random.randint(0, h - new_h)\n            crop = img[y1:y1+new_h, x1:x1+new_w, :]\n            return cv2.resize(crop, (out_size, out_size), interpolation=cv2.INTER_LINEAR)\n    # fallback: центр-кроп -> ресайз\n    return np_resize(img, out_size)\n\ndef np_hflip(img: np.ndarray, p: float = 0.5) -> np.ndarray:\n    if random.random() < p:\n        return cv2.flip(img, 1)\n    return img\n\ndef np_shift_scale_rotate(img: np.ndarray,\n                          shift_limit: float = 0.05,\n                          scale_limit: float = 0.1,\n                          rotate_limit: int = 15,\n                          p: float = 0.7) -> np.ndarray:\n    if random.random() >= p:\n        return img\n    h, w = img.shape[:2]\n    angle = random.uniform(-rotate_limit, rotate_limit)\n    scale = 1.0 + random.uniform(-scale_limit, scale_limit)\n    tx = random.uniform(-shift_limit, shift_limit) * w\n    ty = random.uniform(-shift_limit, shift_limit) * h\n    M = cv2.getRotationMatrix2D((w/2, h/2), angle, scale)\n    M[0,2] += tx\n    M[1,2] += ty\n    return cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n\ndef _adjust_brightness(img: np.ndarray, factor: float) -> np.ndarray:\n    out = img.astype(np.float32) * factor\n    return np.clip(out, 0, 255).astype(np.uint8)\n\ndef _adjust_contrast(img: np.ndarray, factor: float) -> np.ndarray:\n    mean = np.mean(img, axis=(0,1), keepdims=True)\n    out = (img.astype(np.float32) - mean) * factor + mean\n    return np.clip(out, 0, 255).astype(np.uint8)\n\ndef _adjust_saturation(img: np.ndarray, factor: float) -> np.ndarray:\n    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    hsv = hsv.astype(np.float32)\n    hsv[...,1] = np.clip(hsv[...,1] * factor, 0, 255)\n    hsv = hsv.astype(np.uint8)\n    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n\ndef _adjust_hue(img: np.ndarray, delta: float) -> np.ndarray:\n    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    hsv = hsv.astype(np.int32)\n    shift = int(delta * 180.0)  # OpenCV H в [0,180]\n    hsv[...,0] = (hsv[...,0] + shift) % 180\n    hsv = np.clip(hsv, 0, 255).astype(np.uint8)\n    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n\ndef np_color_jitter(img: np.ndarray,\n                    brightness: float = 0.2,\n                    contrast: float = 0.2,\n                    saturation: float = 0.2,\n                    hue: float = 0.1,\n                    p: float = 0.5) -> np.ndarray:\n    if random.random() >= p:\n        return img\n    ops = []\n    if brightness > 0: ops.append(lambda x: _adjust_brightness(x, random.uniform(1 - brightness, 1 + brightness)))\n    if contrast > 0: ops.append(lambda x: _adjust_contrast(x, random.uniform(1 - contrast, 1 + contrast)))\n    if saturation > 0: ops.append(lambda x: _adjust_saturation(x, random.uniform(1 - saturation, 1 + saturation)))\n    if hue > 0: ops.append(lambda x: _adjust_hue(x, random.uniform(-hue, hue)))\n    random.shuffle(ops)\n    out = img\n    for f in ops:\n        out = f(out)\n    return out\n\ndef np_coarse_dropout(img: np.ndarray,\n                      max_holes: int = 1,\n                      max_h_frac: float = 0.3,\n                      max_w_frac: float = 0.3,\n                      p: float = 0.5,\n                      fill_value: Optional[Tuple[int,int,int]] = None) -> np.ndarray:\n    if random.random() >= p:\n        return img\n    h, w = img.shape[:2]\n    out = img.copy()\n    holes = random.randint(1, max_holes)\n    if fill_value is None:\n        # средний цвет\n        fill_value = tuple(int(c) for c in out.reshape(-1,3).mean(axis=0))\n    for _ in range(holes):\n        hh = random.randint(1, max(1, int(max_h_frac * h)))\n        ww = random.randint(1, max(1, int(max_w_frac * w)))\n        y1 = random.randint(0, max(0, h - hh))\n        x1 = random.randint(0, max(0, w - ww))\n        out[y1:y1+hh, x1:x1+ww, :] = fill_value\n    return out\n\nclass NpTransformPipeline:\n    \"\"\"Простая последовательность numpy-аугментаций.\"\"\"\n    def __init__(self, ops: List[Callable[[np.ndarray], np.ndarray]]):\n        self.ops = ops\n    def __call__(self, img: np.ndarray) -> np.ndarray:\n        out = img\n        for op in self.ops:\n            out = op(out)\n        return out\n\n\n# ==========================\n# Датасет\n# ==========================\n\nclass PandasImageDataset(Dataset):\n    \"\"\"\n    Универсальный датасет под pandas.\n    image_column: может хранить путь (str) ИЛИ уже массив numpy (HWC RGB).\n    target_column: индекс класса (multiclass) или строка/список меток (multilabel).\n    transforms: callable(img: HWC uint8 RGB) -> HWC uint8 RGB (без нормализации).\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        image_column: str,\n        target_column: Optional[str],\n        transforms: Optional[Callable[[np.ndarray], np.ndarray]],\n        classes: Optional[List[str]] = None,\n        multilabel: bool = False,\n        label_sep: str = \" \",\n        mean: Tuple[float, float, float] = (0.485, 0.456, 0.406),\n        std: Tuple[float, float, float] = (0.229, 0.224, 0.225),\n        return_targets: bool = True,\n    ):\n        self.df = df.reset_index(drop=True)\n        self.image_column = image_column\n        self.target_column = target_column\n        self.transforms = transforms\n        self.multilabel = multilabel\n        self.label_sep = label_sep\n        self.mean = np.array(mean, dtype=\"float32\")\n        self.std = np.array(std, dtype=\"float32\")\n        self.return_targets = return_targets\n\n        self.classes = classes\n        self.class_to_idx = {c: i for i, c in enumerate(self.classes)} if self.classes else None\n\n    def __len__(self):\n        return len(self.df)\n\n    def _read_image(self, src):\n        if _is_path_like(src):\n            img = cv2.imread(src)\n            if img is None:\n                raise FileNotFoundError(f\"Cannot read image at path: {src}\")\n            return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        elif isinstance(src, np.ndarray):\n            img = src\n            if img.ndim == 2:\n                img = np.stack([img]*3, axis=-1)\n            if img.ndim == 3 and img.shape[2] == 4:\n                img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n            if img.dtype != np.uint8 and img.max() <= 1.5:\n                img = (img * 255.0).astype(np.uint8)\n            return img\n        else:\n            raise ValueError(\"image_column должен содержать путь (str) или numpy.ndarray\")\n\n    def _encode_target(self, row) -> Union[int, np.ndarray]:\n        if self.target_column is None:\n            return None\n        y = row[self.target_column]\n        if not self.multilabel:\n            return int(self.class_to_idx[y]) if self.class_to_idx else int(y)\n        # multilabel\n        if isinstance(y, (list, tuple, set)):\n            labels = list(y)\n        elif isinstance(y, str):\n            labels = [lab for lab in y.split(self.label_sep) if lab]\n        else:\n            labels = [y]\n        vec = np.zeros(len(self.classes), dtype=\"float32\")\n        for lab in labels:\n            if lab in self.class_to_idx:\n                vec[self.class_to_idx[lab]] = 1.0\n        return vec\n\n    def __getitem__(self, idx):\n        r = self.df.iloc[idx]\n        img = self._read_image(r[self.image_column])\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        img = _normalize(img, self.mean, self.std)\n        img_t = torch.from_numpy(_to_chw32(img))\n\n        if not self.return_targets or self.target_column is None:\n            return img_t\n\n        tgt = self._encode_target(r)\n        if self.multilabel:\n            y_t = torch.from_numpy(tgt)\n        else:\n            y_t = torch.tensor(tgt, dtype=torch.long)\n        return img_t, y_t\n\n\n# ==========================\n# Лоссы\n# ==========================\n\nclass FocalLossMultiLabel(nn.Module):\n    def __init__(self, gamma: float = 2.0, reduction: str = \"mean\"):\n        super().__init__()\n        self.gamma = gamma\n        self.reduction = reduction\n    def forward(self, logits, targets):\n        p = torch.sigmoid(logits)\n        ce = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n        p_t = p * targets + (1 - p) * (1 - targets)\n        loss = ce * ((1 - p_t) ** self.gamma)\n        if self.reduction == \"mean\": return loss.mean()\n        if self.reduction == \"sum\": return loss.sum()\n        return loss\n\nclass AsymmetricLossMultiLabel(nn.Module):\n    def __init__(self, gamma_pos=0, gamma_neg=4, clip=0.05, eps=1e-8, reduction=\"mean\"):\n        super().__init__()\n        self.gamma_pos, self.gamma_neg, self.clip, self.eps, self.reduction = gamma_pos, gamma_neg, clip, eps, reduction\n    def forward(self, logits, targets):\n        x_sigmoid = torch.sigmoid(logits)\n        xs_pos, xs_neg = x_sigmoid, 1 - x_sigmoid\n        if self.clip and self.clip > 0:\n            xs_neg = (xs_neg + self.clip).clamp(max=1)\n        loss = targets * torch.log(xs_pos.clamp(min=self.eps)) + (1 - targets) * torch.log(xs_neg.clamp(min=self.eps))\n        if self.gamma_pos > 0 or self.gamma_neg > 0:\n            pt = xs_pos * targets + xs_neg * (1 - targets)\n            asym_w = (1 - pt) ** (self.gamma_pos * targets + self.gamma_neg * (1 - targets))\n            loss *= asym_w\n        loss = -loss\n        if self.reduction == \"mean\": return loss.mean()\n        if self.reduction == \"sum\": return loss.sum()\n        return loss\n\n\n# ==========================\n# SAM (опционально)\n# ==========================\n\nclass SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n        if rho <= 0: raise ValueError(\"rho should be > 0\")\n        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n        super().__init__(params, defaults)\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group['rho'] / (grad_norm + 1e-12)\n            for p in group['params']:\n                if p.grad is None: continue\n                e_w = (torch.pow(p, 2) if group['adaptive'] else 1.0) * p.grad * scale\n                p.add_(e_w)\n                self.state[p]['e_w'] = e_w\n        if zero_grad: self.zero_grad()\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None: continue\n                p.sub_(self.state[p]['e_w'])\n        self.base_optimizer.step()\n        if zero_grad: self.zero_grad()\n    def step(self, closure=None): raise NotImplementedError(\"SAM doesn't use step(). Use first_step() and second_step().\")\n    def zero_grad(self): self.base_optimizer.zero_grad()\n    def _grad_norm(self):\n        shared_device = self.param_groups[0]['params'][0].device\n        norms = [p.grad.norm(p=2).to(shared_device) for group in self.param_groups for p in group['params'] if p.grad is not None]\n        return torch.norm(torch.stack(norms), p=2) if norms else torch.tensor(0.0, device=shared_device)\n\n\n# ==========================\n# Пайплайн\n# ==========================\n\nclass ImageClassificationPipeline:\n    \"\"\"\n    Универсальный пайплайн для задач классификации изображений (multiclass/multilabel)\n    на PyTorch + timm с аугментациями на numpy/OpenCV, Accelerate, EMA, SAM, LLRD и TTA.\n\n    Параметры инициализации касаются только данных/модели/инференса.\n    Параметры обучения и валидации задаются в методе fit(...).\n\n    :param target_column_name: имя колонки с таргетом в DataFrame\n    :param image_column_name: имя колонки с изображением (путь к файлу или numpy HWC RGB)\n    :param model_name: имя модели для timm.create_model (например, \"resnet18\", \"vit_tiny_patch16_224\")\n    :param pretrained: использовать предобученные веса timm\n    :param drop_path_rate: коэффициент stochastic depth (DropPath)\n    :param multilabel: True — задача мультилейбл (multi-hot), False — мультикласс\n    :param label_sep: разделитель меток в строковых таргетах для мультилейбл\n    :param model_img_size: фиксированный размер входа для некоторых моделей (например, ViT 224)\n    :param tta_hflip: использовать горизонтальный флип в TTA\n    :param tta_scales: список размеров для multi-scale TTA; если None — без масштабов\n    :param tta_crop_size: размер кропа для 5-crop TTA; если None — без мультикропов\n    :param seed: базовое случайное зерно\n    :param amp: включить mixed precision (\"fp16\") для Accelerate; False — \"no\"\n    \"\"\"\n    def __init__(\n        self,\n        target_column_name: str,\n        image_column_name: str,\n        model_name: str = \"convnextv2_base\",\n        pretrained: bool = True,\n        drop_path_rate: float = 0.2,\n        multilabel: bool = False,\n        label_sep: str = \" \",\n        model_img_size: Optional[int] = None,\n        tta_hflip: bool = True,\n        tta_scales: Optional[List[int]] = None,\n        tta_crop_size: Optional[int] = None,\n        seed: int = 42,\n        amp: bool = True,\n    ):\n        # Параметры модели/данных/инференса\n        self.target_col = target_column_name\n        self.image_col = image_column_name\n        self.model_name = model_name\n        self.pretrained = pretrained\n        self.drop_path_rate = drop_path_rate\n        self.multilabel = multilabel\n        self.label_sep = label_sep\n        self.model_img_size = model_img_size\n\n        # Параметры TTA (инференс)\n        self.tta_hflip = tta_hflip\n        self.tta_scales = tta_scales\n        self.tta_crop_size = tta_crop_size\n\n        # Служебные\n        self.seed = seed\n        self.amp = amp\n\n        # Атрибуты, заполняемые в fit()\n        self.classes_: Optional[List[str]] = None\n        self.models_: List[Any] = []\n        self.oof_pred_proba_: Optional[np.ndarray] = None\n        self.oof_targets_: Optional[np.ndarray] = None\n        self.oof_fold_: Optional[np.ndarray] = None\n        self.thresholds_: Optional[np.ndarray] = None\n        self._data_config: Optional[Dict[str, Any]] = None\n\n        # Динамические параметры обучения (заполняются в fit и используются после)\n        self.img_sizes: List[int] = [224]\n        self.stage_epochs: Optional[List[int]] = None\n        self.aug_strength: str = \"medium\"\n        self.epochs: int = 0\n        self.batch_size: int = 0\n        self.num_workers: int = 0\n        self.lr: float = 0.0\n        self.weight_decay: float = 0.0\n        self.warmup_epochs: float = 0.0\n        self.grad_clip: float = 0.0\n        self.ema_decay: Optional[float] = None\n        self.use_sam: bool = False\n        self.sam_rho: float = 0.0\n        self.mixup_alpha: float = 0.0\n        self.cutmix_alpha: float = 0.0\n        self.label_smoothing: float = 0.0\n        self.disable_mix_last_n_epochs: int = 0\n        self.class_weights_in_loss: bool = False\n        self.use_weighted_sampler: bool = False\n        self.layer_decay: Optional[float] = None\n        self.val_metric: str = \"f1_macro\"\n        self.optimize_thresholds: bool = True\n        self.n_folds: int = 0\n        self.fold_column: Optional[str] = None\n        self.group_column: Optional[str] = None\n        self.stratify: bool = True\n        self.class_weights_: Optional[List[float]] = None\n        self.grad_accum_steps: int = 1\n        self._warned_sam_accum: bool = False  # внутр. флаг предупреждения SAM+accum\n\n        # Инициализация Accelerate с учётом уже инициализированного состояния\n        ps = PartialState()\n        already_init = getattr(ps, \"initialized\", False)\n        if already_init:\n            self.accelerator = Accelerator()\n            try:\n                cur_mp = self.accelerator.state.mixed_precision\n                want_mp = \"fp16\" if self.amp else \"no\"\n                if cur_mp != want_mp:\n                    warnings.warn(\n                        f\"Accelerate is already initialized with mixed_precision='{cur_mp}'. \"\n                        f\"Requested amp={'True' if self.amp else 'False'} (='{want_mp}') will be ignored for this instance.\"\n                    )\n            except Exception:\n                pass\n        else:\n            self.accelerator = Accelerator(mixed_precision=(\"fp16\" if self.amp else \"no\"))\n\n    # ---------------------- Трансформы ----------------------\n\n    def _build_train_tfms(self, img_size: int) -> Callable[[np.ndarray], np.ndarray]:\n        \"\"\"\n        Создаёт последовательность numpy-аугментаций для тренировки.\n\n        :param img_size: итоговый размер стороны изображения (квадрат)\n        :return: callable(img: np.ndarray HWC uint8 RGB) -> np.ndarray HWC uint8 RGB\n        \"\"\"\n        if self.aug_strength == \"light\":\n            ops = [\n                lambda im: np_random_resized_crop(im, img_size, scale=(0.85, 1.0)),\n                lambda im: np_hflip(im, p=0.5),\n            ]\n        elif self.aug_strength == \"heavy\":\n            ops = [\n                lambda im: np_random_resized_crop(im, img_size, scale=(0.6, 1.0)),\n                lambda im: np_hflip(im, p=0.5),\n                lambda im: np_shift_scale_rotate(im, 0.05, 0.2, 20, p=0.7),\n                lambda im: np_color_jitter(im, 0.3, 0.3, 0.3, 0.15, p=0.6),\n                lambda im: np_coarse_dropout(im, max_holes=1, max_h_frac=0.3, max_w_frac=0.3, p=0.5),\n            ]\n        else:\n            ops = [\n                lambda im: np_random_resized_crop(im, img_size, scale=(0.7, 1.0)),\n                lambda im: np_hflip(im, p=0.5),\n                lambda im: np_shift_scale_rotate(im, 0.05, 0.1, 15, p=0.6),\n                lambda im: np_color_jitter(im, 0.2, 0.2, 0.2, 0.1, p=0.5),\n                lambda im: np_coarse_dropout(im, max_holes=1, max_h_frac=0.3, max_w_frac=0.3, p=0.5),\n            ]\n        return NpTransformPipeline(ops)\n\n    def _build_valid_tfms(self, img_size: int) -> Callable[[np.ndarray], np.ndarray]:\n        \"\"\"\n        Создаёт преобразование валидации/инференса (только Resize).\n\n        :param img_size: итоговый размер стороны изображения (квадрат)\n        :return: callable(img: np.ndarray HWC uint8 RGB) -> np.ndarray HWC uint8 RGB\n        \"\"\"\n        return lambda im: np_resize(im, img_size)\n\n    # ---------------------- Сплиты ----------------------\n\n    def _make_folds(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Создаёт колонку fold c номером фолда (Stratified/Group/Multilabel).\n\n        :param df: исходный DataFrame\n        :return: копия df с колонкой \"fold\"\n        \"\"\"\n        if self.fold_column and self.fold_column in df.columns:\n            return df.copy()\n\n        df = df.copy()\n        df[\"fold\"] = -1\n\n        if self.group_column:\n            gkf = GroupKFold(n_splits=self.n_folds)\n            for k, (_, val_idx) in enumerate(gkf.split(df, groups=df[self.group_column])):\n                df.loc[df.index[val_idx], \"fold\"] = k\n            return df\n\n        if self.multilabel and HAS_MLSTRAT:\n            Y = df[self.target_col]\n            if isinstance(Y.iloc[0], str):\n                Y_bin = Y.str.get_dummies(sep=self.label_sep)\n            elif isinstance(Y.iloc[0], (list, tuple, set)):\n                uniq = sorted({lab for labs in Y for lab in (labs if isinstance(labs, (list, tuple, set)) else [labs])})\n                Y_bin = pd.DataFrame(\n                    [[1 if u in (y if isinstance(y, (list, tuple, set)) else [y]) else 0 for u in uniq] for y in Y],\n                    columns=uniq\n                )\n            else:\n                raise ValueError(\"Для multilabel ожидается строка меток или список/множество в target_column.\")\n            mskf = MultilabelStratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.seed)\n            for k, (_, val_idx) in enumerate(mskf.split(df, Y_bin)):\n                df.loc[df.index[val_idx], \"fold\"] = k\n            return df\n\n        if self.stratify and not self.multilabel:\n            skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.seed)\n            for k, (_, val_idx) in enumerate(skf.split(df, y=df[self.target_col])):\n                df.loc[df.index[val_idx], \"fold\"] = k\n        else:\n            kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.seed)\n            for k, (_, val_idx) in enumerate(kf.split(df)):\n                df.loc[df.index[val_idx], \"fold\"] = k\n        return df\n\n    # ---------------------- LLRD ----------------------\n\n    def _param_groups_llrd(self, model, base_lr: float, weight_decay: float):\n        \"\"\"\n        Формирует группы параметров с layer-wise lr decay для ViT/Swin.\n\n        :param model: модель timm\n        :param base_lr: базовое значение learning rate\n        :param weight_decay: коэфф. L2-регуляризации для AdamW\n        :return: список групп параметров (dict)\n        \"\"\"\n        layer_decay = self.layer_decay\n        if not layer_decay or layer_decay >= 1.0:\n            return [{\"params\": [p for p in model.parameters() if p.requires_grad], \"lr\": base_lr, \"weight_decay\": weight_decay}]\n\n        layers = []\n        if hasattr(model, \"blocks\"):  # ViT/DeiT\n            layers = list(model.blocks)\n        elif hasattr(model, \"layers\"):  # Swin\n            for l in model.layers:\n                if hasattr(l, \"blocks\"):\n                    layers += list(l.blocks)\n                else:\n                    layers.append(l)\n\n        if not layers:\n            warnings.warn(\"LLRD: не найден blocks/layers — один LR для всех.\")\n            return [{\"params\": [p for p in model.parameters() if p.requires_grad], \"lr\": base_lr, \"weight_decay\": weight_decay}]\n\n        n = len(layers) + 1\n        layer_map = {}\n        for i, layer in enumerate(layers):\n            for name, p in layer.named_parameters(recurse=True):\n                if p.requires_grad:\n                    layer_map[id(p)] = i\n\n        head_params = []\n        body_params = [[] for _ in range(n)]\n        for name, p in model.named_parameters():\n            if not p.requires_grad:\n                continue\n            if id(p) in layer_map:\n                body_params[layer_map[id(p)]].append(p)\n            else:\n                head_params.append(p)\n\n        groups = []\n        for i, params in enumerate(body_params):\n            if not params:\n                continue\n            lr_i = base_lr * (layer_decay ** (n - i - 1))\n            groups.append({\"params\": params, \"lr\": lr_i, \"weight_decay\": weight_decay})\n        if head_params:\n            groups.append({\"params\": head_params, \"lr\": base_lr, \"weight_decay\": weight_decay})\n        return groups\n\n    # ---------------------- Метрики и пороги ----------------------\n\n    def _compute_metric(self, y_true, y_proba) -> float:\n        \"\"\"\n        Считает метрику валидации/OOF согласно self.val_metric.\n\n        :param y_true: истинные метки (1D для мультикласса, 2D (N, C) для мультилейбл)\n        :param y_proba: вероятности модели (2D (N, C))\n        :return: значение метрики\n        \"\"\"\n        if self.multilabel:\n            if self.val_metric == \"map_macro\":\n                return average_precision_score(y_true, y_proba, average=\"macro\")\n            elif self.val_metric == \"auc_ovr\":\n                try:\n                    return roc_auc_score(y_true, y_proba, average=\"macro\")\n                except Exception:\n                    return average_precision_score(y_true, y_proba, average=\"macro\")\n            thr = getattr(self, \"thresholds_\", None)\n            if thr is None:\n                y_pred = (y_proba >= 0.5).astype(int)\n            else:\n                thr_arr = np.array(thr)[None, :] if not np.isscalar(thr) else thr\n                y_pred = (y_proba >= thr_arr).astype(int)\n            return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n        else:\n            y_pred = y_proba.argmax(1)\n            if self.val_metric == \"accuracy\":\n                return accuracy_score(y_true, y_pred)\n            else:\n                return f1_score(y_true, y_pred, average=\"macro\")\n\n    def _tune_thresholds(self, y_true: np.ndarray, y_proba: np.ndarray):\n        \"\"\"\n        Подбирает per-class пороги для мультилейбл по OOF (грид [0.05..0.95]).\n\n        :param y_true: истинные метки мультилейбл (N, C)\n        :param y_proba: вероятности модели (N, C)\n        \"\"\"\n        if not self.multilabel:\n            self.thresholds_ = None\n            return\n        grid = np.linspace(0.05, 0.95, 19)\n        best_per_class = []\n        for c in range(y_proba.shape[1]):\n            best_c, sc = 0.5, -1\n            y_t = y_true[:, c]\n            y_p = y_proba[:, c]\n            for t in grid:\n                s = f1_score(y_t, (y_p >= t).astype(int), average=\"binary\", zero_division=0)\n                if s > sc:\n                    sc, best_c = s, t\n            best_per_class.append(best_c)\n        self.thresholds_ = np.array(best_per_class, dtype=\"float32\")\n\n    # ---------------------- LR schedule ----------------------\n\n    def _build_scheduler(self, optimizer, steps_per_epoch: int, epochs: int):\n        \"\"\"\n        Строит LambdaLR: линейный warmup -> косинусное убывание.\n\n        :param optimizer: оптимизатор, к которому привязан шедулер\n        :param steps_per_epoch: число шагов на эпоху\n        :param epochs: число эпох для данной стадии\n        :return: torch.optim.lr_scheduler.LambdaLR\n        \"\"\"\n        warmup_steps = int(max(1, self.warmup_epochs * steps_per_epoch))\n        total_steps = int(max(1, epochs * steps_per_epoch))\n\n        def lr_lambda(step):\n            if step < warmup_steps:\n                return float(step) / float(max(1, warmup_steps))\n            t = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n            return 0.5 * (1.0 + math.cos(math.pi * t))\n\n        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    # ---------------------- Обучение / Валидация ----------------------\n\n    def _create_model_safe(self, pretrained: bool, img_size: Optional[int], num_classes: int):\n        \"\"\"\n        Безопасно создаёт модель timm, игнорируя img_size там, где он не поддерживается.\n\n        :param pretrained: использовать предобученные веса timm\n        :param img_size: фиксированный размер входа (если нужен)\n        :param num_classes: число классов для классификатора\n        :return: torch.nn.Module\n        \"\"\"\n        kw = dict(pretrained=pretrained, num_classes=num_classes, drop_path_rate=self.drop_path_rate)\n        if img_size is not None:\n            kw[\"img_size\"] = img_size\n        try:\n            return timm.create_model(self.model_name, **kw)\n        except TypeError as e:\n            if \"img_size\" in str(e):\n                kw.pop(\"img_size\", None)\n                return timm.create_model(self.model_name, **kw)\n            raise\n\n    def _train_one_epoch(self, model, train_loader, loss_fn, optimizer, scheduler, ema=None, mixup_fn=None):\n        \"\"\"\n        Одна эпоха обучения с поддержкой AMP, SAM, EMA, mixup/cutmix, grad clipping и аккумуляции градиентов.\n\n        :param model: обучаемая модель (после accelerator.prepare)\n        :param train_loader: DataLoader с обучающими батчами\n        :param loss_fn: функция потерь для тренинга\n        :param optimizer: оптимизатор (SAM или обычный)\n        :param scheduler: шедулер LR\n        :param ema: EMA-объект (ModelEmaV2) или None\n        :param mixup_fn: timm Mixup (или None)\n        :return: средний train loss за эпоху\n        \"\"\"\n        model.train()\n        total_loss = 0.0\n\n        if isinstance(loss_fn, SoftTargetCrossEntropy) or (hasattr(loss_fn, '__class__') and 'SoftTarget' in str(loss_fn.__class__)):\n            fallback_loss_fn = nn.CrossEntropyLoss().to(self.accelerator.device)\n        else:\n            fallback_loss_fn = loss_fn\n\n        pbar = tqdm(train_loader, desc=\"Training\", leave=False, disable=not self.accelerator.is_main_process)\n\n        use_accum = max(1, getattr(self, \"grad_accum_steps\", 1))\n        if isinstance(optimizer, SAM) and use_accum > 1 and not self._warned_sam_accum:\n            warnings.warn(\"SAM с grad_accum_steps>1 не поддерживается корректно; аккумуляция будет проигнорирована (используется 1).\")\n            self._warned_sam_accum = True\n\n        for imgs, targets in pbar:\n            def step_once():\n                current_loss_fn = loss_fn if mixup_fn is not None else fallback_loss_fn\n                x, y = imgs, targets\n                if mixup_fn is not None:\n                    x, y = mixup_fn(x, y)\n\n                with self.accelerator.autocast():\n                    preds = model(x)\n                    loss = current_loss_fn(preds, y)\n\n                if isinstance(optimizer, SAM):\n                    self.accelerator.backward(loss)\n                    if self.grad_clip:\n                        self.accelerator.clip_grad_norm_(self.accelerator.unwrap_model(model).parameters(), self.grad_clip)\n                    optimizer.first_step(zero_grad=True)\n\n                    with self.accelerator.autocast():\n                        preds2 = model(x)\n                        loss2 = current_loss_fn(preds2, y)\n                    self.accelerator.backward(loss2)\n                    if self.grad_clip:\n                        self.accelerator.clip_grad_norm_(self.accelerator.unwrap_model(model).parameters(), self.grad_clip)\n                    optimizer.second_step(zero_grad=True)\n                else:\n                    with self.accelerator.accumulate(model):\n                        self.accelerator.backward(loss)\n                        if self.grad_clip:\n                            self.accelerator.clip_grad_norm_(model.parameters(), self.grad_clip)\n                        if self.accelerator.sync_gradients:\n                            optimizer.step()\n                            optimizer.zero_grad()\n                            if scheduler is not None:\n                                scheduler.step()\n                return loss\n\n            loss = step_once()\n            total_loss += loss.item()\n            if ema is not None:\n                ema.update(self.accelerator.unwrap_model(model))\n            if isinstance(optimizer, SAM) and scheduler is not None:\n                scheduler.step()\n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n        pbar.close()\n        return total_loss / max(1, len(train_loader))\n\n    @torch.no_grad()\n    def _validate(self, model, valid_loader, loss_fn):\n        \"\"\"\n        Валидация: считает лосс и возвращает вероятности/таргеты.\n\n        :param model: модель для инференса (ema.module или обычная, после prepare)\n        :param valid_loader: DataLoader валидации\n        :param loss_fn: функция потерь валидации\n        :return: (val_loss, probs, targets) — лосс (float), вероятности (np.ndarray), таргеты (np.ndarray)\n        \"\"\"\n        model.eval()\n        probs, tgts, losses = [], [], []\n        pbar = tqdm(valid_loader, desc=\"Validating\", leave=False, disable=not self.accelerator.is_main_process)\n\n        for imgs, targets in pbar:\n            with self.accelerator.autocast():\n                logits = model(imgs)\n                loss = loss_fn(logits, targets)\n\n            loss_gathered = self.accelerator.gather(loss.detach())\n            if loss_gathered.ndim == 0:\n                loss_gathered = loss_gathered.unsqueeze(0)\n            losses.append(loss_gathered.cpu())\n\n            p = torch.sigmoid(logits) if self.multilabel else torch.softmax(logits, dim=1)\n            probs.append(self.accelerator.gather(p.detach()).cpu())\n            tgts.append(self.accelerator.gather(targets.detach()).cpu())\n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n        pbar.close()\n        probs = torch.cat(probs).numpy()\n        tgts = torch.cat(tgts).numpy()\n        vloss = torch.cat(losses).mean().item() if losses else 0.0\n        return vloss, probs, tgts\n\n    # ---------------------- Fit (обучение) ----------------------\n\n    def fit(\n        self,\n        df: pd.DataFrame,\n        test_size: Optional[float] = None,\n        n_folds: int = 5,\n        fold_column: Optional[str] = None,\n        group_column: Optional[str] = None,\n        stratify: bool = True,\n        img_sizes: Union[List[int], Tuple[int, ...]] = (224, 384),\n        stage_epochs: Optional[List[int]] = None,\n        aug_strength: str = \"medium\",\n        epochs: int = 20,\n        batch_size: int = 32,\n        num_workers: int = 4,\n        lr: float = 2e-3,\n        weight_decay: float = 1e-4,\n        warmup_epochs: float = 3.0,\n        grad_clip: float = 1.0,\n        ema_decay: Optional[float] = 0.9999,\n        use_sam: bool = False,\n        sam_rho: float = 0.05,\n        mixup_alpha: float = 0.8,\n        cutmix_alpha: float = 1.0,\n        label_smoothing: float = 0.1,\n        disable_mix_last_n_epochs: int = 3,\n        class_weights_in_loss: bool = False,\n        use_weighted_sampler: bool = False,\n        layer_decay: Optional[float] = None,\n        val_metric: str = \"f1_macro\",\n        optimize_thresholds: bool = True,\n        seed: Optional[int] = None,\n        holdout_seed: Optional[int] = None,\n        grad_accum_steps: int = 1,\n        bce_pos_weight: Optional[Union[float, List[float], np.ndarray]] = None,\n        grad_checkpointing: bool = False,\n    ):\n        \"\"\"\n        Обучает модели и сохраняет OOF-предсказания (holdout или K-fold).\n\n        :param df: DataFrame с колонками изображения и таргета (+ опц. group_column)\n        :param test_size: доля данных на валидацию в holdout-режиме (0 < test_size < 1); если None — K-fold\n        :param n_folds: число фолдов в K-fold режиме\n        :param fold_column: имя готовой колонки с номером фолда (если уже есть)\n        :param group_column: имя колонки групп для группового сплита (holdout/K-fold)\n        :param stratify: использовать стратификацию в мультиклассе\n        :param img_sizes: список размеров для прогрессивного ресайза\n        :param stage_epochs: список эпох на стадию; если None — распределяются равномерно\n        :param aug_strength: сила аугментаций: \"light\" | \"medium\" | \"heavy\"\n        :param epochs: общее число эпох (сумма по стадиям)\n        :param batch_size: размер батча\n        :param num_workers: число воркеров DataLoader\n        :param lr: learning rate для AdamW\n        :param weight_decay: weight decay (L2) для AdamW\n        :param warmup_epochs: длительность warmup (в “эпохах”) для линейного разогрева LR\n        :param grad_clip: максимум L2-нормы градиента (0/None — выключить)\n        :param ema_decay: коэффициент EMA (None/0 — выключить EMA)\n        :param use_sam: включить SAM (Sharpness-Aware Minimization)\n        :param sam_rho: радиус окна для SAM\n        :param mixup_alpha: параметр mixup (0 — выключить)\n        :param cutmix_alpha: параметр cutmix (0 — выключить)\n        :param label_smoothing: сглаживание меток для CrossEntropy\n        :param disable_mix_last_n_epochs: число финальных эпох, где mixup/cutmix отключён\n        :param class_weights_in_loss: использовать веса классов в CrossEntropy (только мультикласс)\n        :param use_weighted_sampler: балансировка батчей через WeightedRandomSampler (только мультикласс)\n        :param layer_decay: LLRD коэффициент (для ViT/Swin), например 0.75–0.8; None — выключено\n        :param val_metric: метрика валидации: \"accuracy\" | \"f1_macro\" | \"auc_ovr\" | \"map_macro\"\n        :param optimize_thresholds: подбирать per-class пороги для мультилейбл по OOF\n        :param seed: случайное зерно обучения; если None — используется значение из __init__\n        :param holdout_seed: зерно для holdout-сплита; если None — используется seed\n        :param grad_accum_steps: число микробатчей для аккумуляции градиентов (для обычного оптимизатора)\n        :param bce_pos_weight: pos_weight для BCEWithLogitsLoss (float | список | np.array) для мультилейбл;\n                               если None — считается автоматически по train split; игнорируется для focal/asl\n        :param grad_checkpointing: включить gradient checkpointing, если модель timm поддерживает set_grad_checkpointing\n        :return: self\n        \"\"\"\n        if seed is not None:\n            self.seed = seed\n        set_seed(self.seed)\n\n        # Синхронизация параметров обучения c self.*\n        self.n_folds = n_folds\n        self.fold_column = fold_column\n        self.group_column = group_column\n        self.stratify = stratify\n\n        self.img_sizes = list(img_sizes) if isinstance(img_sizes, (list, tuple)) else [img_sizes]\n        self.stage_epochs = stage_epochs\n        self.aug_strength = aug_strength\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.warmup_epochs = warmup_epochs\n        self.grad_clip = grad_clip\n        self.ema_decay = ema_decay\n        self.use_sam = use_sam\n        self.sam_rho = sam_rho\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.label_smoothing = label_smoothing\n        self.disable_mix_last_n_epochs = disable_mix_last_n_epochs\n        self.class_weights_in_loss = class_weights_in_loss\n        self.use_weighted_sampler = use_weighted_sampler\n        self.layer_decay = layer_decay\n        self.val_metric = val_metric\n        self.optimize_thresholds = optimize_thresholds\n        self.grad_accum_steps = max(1, int(grad_accum_steps))\n\n        # Классы\n        if self.multilabel:\n            uniq = set()\n            for y in df[self.target_col]:\n                if isinstance(y, str):\n                    uniq.update([lab for lab in y.split(self.label_sep) if lab])\n                elif isinstance(y, (list, tuple, set)):\n                    uniq.update(list(y))\n                else:\n                    raise ValueError(\"Для multilabel ожидается строка меток или список/множество.\")\n            self.classes_ = sorted(list(uniq))\n        else:\n            self.classes_ = sorted(list(pd.unique(df[self.target_col])))\n        num_classes = len(self.classes_)\n        if num_classes <= 0:\n            raise ValueError(\"Не удалось определить список классов.\")\n\n        # Режим: holdout или k-fold\n        use_holdout = test_size is not None\n        if use_holdout:\n            if not (0.0 < test_size < 1.0):\n                raise ValueError(\"test_size должен быть в диапазоне (0, 1).\")\n            ho_seed = self.seed if holdout_seed is None else holdout_seed\n\n            df_use = df.copy().reset_index(drop=True)\n            df_use[\"fold\"] = -1\n\n            n = len(df_use)\n            val_size = test_size\n\n            if self.group_column:\n                from sklearn.model_selection import GroupShuffleSplit\n                gss = GroupShuffleSplit(n_splits=1, test_size=val_size, random_state=ho_seed)\n                train_idx, val_idx = next(gss.split(df_use, groups=df_use[self.group_column]))\n            elif self.multilabel:\n                try:\n                    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit as MLSSS  # type: ignore\n                    Y = df_use[self.target_col]\n                    if isinstance(Y.iloc[0], str):\n                        Y_bin = Y.str.get_dummies(sep=self.label_sep).values\n                    elif isinstance(Y.iloc[0], (list, tuple, set)):\n                        uniq = self.classes_\n                        Y_bin = np.stack([\n                            np.array([1.0 if u in (y if isinstance(y, (list, tuple, set)) else [y]) else 0.0 for u in uniq], dtype=np.float32)\n                            for y in Y\n                        ], axis=0)\n                    else:\n                        y0 = Y.iloc[0]\n                        if isinstance(y0, (np.ndarray, list, tuple)):\n                            Y_bin = np.stack(Y.values, axis=0).astype(np.float32)\n                        else:\n                            raise ValueError(\"Невозможно стратифицировать multilabel: непонятный формат target_column.\")\n                    msss = MLSSS(n_splits=1, test_size=val_size, random_state=ho_seed)\n                    train_idx, val_idx = next(msss.split(np.zeros((len(df_use), 1)), Y_bin))\n                except Exception:\n                    from sklearn.model_selection import ShuffleSplit\n                    ss = ShuffleSplit(n_splits=1, test_size=val_size, random_state=ho_seed)\n                    train_idx, val_idx = next(ss.split(df_use))\n            else:\n                from sklearn.model_selection import train_test_split\n                y = df_use[self.target_col].values\n                if self.stratify:\n                    train_idx, val_idx = train_test_split(\n                        np.arange(n), test_size=val_size, random_state=ho_seed, stratify=y\n                    )\n                else:\n                    train_idx, val_idx = train_test_split(\n                        np.arange(n), test_size=val_size, random_state=ho_seed, stratify=None\n                    )\n            df_use.loc[val_idx, \"fold\"] = 0\n            folds_iter = [0]\n            n_folds_eff = 1\n        else:\n            df_use = self._make_folds(df)\n            folds_iter = list(range(self.n_folds))\n            n_folds_eff = self.n_folds\n\n        # OOF\n        self.oof_pred_proba_ = np.zeros((len(df_use), num_classes), dtype=\"float32\")\n        self.oof_targets_ = np.zeros((len(df_use), num_classes if self.multilabel else 1), dtype=\"float32\")\n        self.oof_fold_ = np.full(len(df_use), -1, dtype=\"int32\")\n\n        # Эпохи по стадиям\n        if self.stage_epochs is None:\n            base = self.epochs // len(self.img_sizes)\n            rem = self.epochs - base * len(self.img_sizes)\n            self.stage_epochs = [base + (1 if i < rem else 0) for i in range(len(self.img_sizes))]\n\n        self.models_.clear()\n\n        # Цикл по фолдам\n        fold_pbar = tqdm(folds_iter, desc=\"Folds\", disable=not self.accelerator.is_main_process)\n        for fold in fold_pbar:\n            fold_pbar.set_description(f\"Fold {fold+1}/{n_folds_eff}\")\n\n            if \"fold\" not in df_use.columns:\n                raise RuntimeError(\"Нет fold-колонки. Проверьте _make_folds()/holdout-сплит.\")\n\n            if use_holdout:\n                df_tr = df_use[df_use[\"fold\"] != 0].reset_index(drop=True)\n                df_va = df_use[df_use[\"fold\"] == 0].reset_index()\n            else:\n                df_tr = df_use[df_use[\"fold\"] != fold].reset_index(drop=True)\n                df_va = df_use[df_use[\"fold\"] == fold].reset_index()\n\n            # timm default_cfg\n            ghost = self._create_model_safe(self.pretrained, self.model_img_size or self.img_sizes[-1], num_classes)\n            self._data_config = resolve_data_config({}, model=ghost)\n            mean = tuple(self._data_config.get(\"mean\", (0.485, 0.456, 0.406)))\n            std = tuple(self._data_config.get(\"std\", (0.229, 0.224, 0.225)))\n\n            # Модель\n            model = self._create_model_safe(self.pretrained, self.model_img_size or self.img_sizes[-1], num_classes)\n\n            # Включаем gradient checkpointing, если запрошено и модель поддерживает\n            if grad_checkpointing and hasattr(model, \"set_grad_checkpointing\"):\n                model.set_grad_checkpointing(True)\n                if self.accelerator.is_main_process:\n                    print(\"[GC] Gradient checkpointing enabled.\")\n\n            ema = None\n\n            # Балансировка (мультикласс): class weights\n            self.class_weights_ = None\n            if (not self.multilabel) and self.class_weights_in_loss:\n                counts = df_tr[self.target_col].value_counts().reindex(self.classes_, fill_value=0).values.astype(np.float32)\n                cw = (counts.sum() / (counts + 1e-6))\n                self.class_weights_ = (cw / cw.mean()).tolist()\n\n            # Sampler (мультикласс): WeightedRandomSampler\n            sampler = None\n            if self.use_weighted_sampler and not self.multilabel:\n                counts_map = df_tr[self.target_col].value_counts().reindex(self.classes_, fill_value=0).to_dict()\n                sample_weights = df_tr[self.target_col].map(lambda x: 1.0 / max(1, counts_map.get(x, 1))).values\n                sampler = WeightedRandomSampler(\n                    weights=torch.as_tensor(sample_weights, dtype=torch.double),\n                    num_samples=len(sample_weights),\n                    replacement=True\n                )\n\n            # pos_weight для мультилейбл BCE\n            pos_weight_tensor = None\n            if self.multilabel:\n                Y = df_tr[self.target_col]\n                if isinstance(Y.iloc[0], str):\n                    Y_bin = Y.str.get_dummies(sep=self.label_sep).reindex(columns=self.classes_, fill_value=0).values.astype(np.float32)\n                elif isinstance(Y.iloc[0], (list, tuple, set)):\n                    Y_bin = np.zeros((len(Y), num_classes), dtype=np.float32)\n                    for i, y in enumerate(Y):\n                        labs = list(y) if isinstance(y, (list, tuple, set)) else [y]\n                        for lab in labs:\n                            if lab in self.classes_:\n                                Y_bin[i, self.classes_.index(lab)] = 1.0\n                else:\n                    y0 = Y.iloc[0]\n                    if isinstance(y0, (np.ndarray, list, tuple)) and len(y0) == num_classes:\n                        Y_bin = np.stack(Y.values, axis=0).astype(np.float32)\n                    else:\n                        Y_bin = np.zeros((len(Y), num_classes), dtype=np.float32)\n\n                if bce_pos_weight is not None:\n                    if isinstance(bce_pos_weight, (list, tuple, np.ndarray)):\n                        pw = np.asarray(bce_pos_weight, dtype=np.float32)\n                        if pw.shape[0] != num_classes:\n                            raise ValueError(\"Длина bce_pos_weight должна совпадать с числом классов.\")\n                        pos_weight_tensor = torch.as_tensor(pw, dtype=torch.float32, device=self.accelerator.device)\n                    else:\n                        pos_weight_tensor = torch.full((num_classes,), float(bce_pos_weight), dtype=torch.float32, device=self.accelerator.device)\n                else:\n                    pc = Y_bin.sum(axis=0) + 1e-6\n                    Ntr = float(Y_bin.shape[0])\n                    pw = (Ntr - pc) / pc\n                    pw = np.clip(pw, 1.0, 100.0).astype(np.float32)\n                    pos_weight_tensor = torch.as_tensor(pw, dtype=torch.float32, device=self.accelerator.device)\n\n            # Лоссы\n            mixup_active = (self.mixup_alpha > 0 or self.cutmix_alpha > 0) and not self.multilabel\n            if self.multilabel:\n                loss_name = getattr(self, \"loss_name\", \"bce\")\n                if loss_name == \"focal\":\n                    train_loss_fn = FocalLossMultiLabel(gamma=2.0)\n                    val_loss_fn = nn.BCEWithLogitsLoss()\n                elif loss_name == \"asl\":\n                    train_loss_fn = AsymmetricLossMultiLabel(gamma_pos=0, gamma_neg=4, clip=0.05)\n                    val_loss_fn = nn.BCEWithLogitsLoss()\n                else:\n                    if pos_weight_tensor is not None:\n                        train_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n                        val_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n                    else:\n                        train_loss_fn = nn.BCEWithLogitsLoss()\n                        val_loss_fn = nn.BCEWithLogitsLoss()\n            else:\n                if mixup_active:\n                    train_loss_fn = SoftTargetCrossEntropy()\n                elif self.label_smoothing > 0:\n                    train_loss_fn = LabelSmoothingCrossEntropy(smoothing=self.label_smoothing)\n                elif self.class_weights_in_loss and self.class_weights_:\n                    train_loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(self.class_weights_, dtype=torch.float32))\n                else:\n                    train_loss_fn = nn.CrossEntropyLoss()\n                val_loss_fn = nn.CrossEntropyLoss()\n\n            # Оптимизатор / SAM (с LLRD при необходимости)\n            if self.layer_decay:\n                param_groups = self._param_groups_llrd(model, self.lr, self.weight_decay)\n                base_optimizer = torch.optim.AdamW(param_groups, lr=self.lr, weight_decay=self.weight_decay)\n                optimizer = SAM(param_groups, torch.optim.AdamW, rho=self.sam_rho, lr=self.lr, weight_decay=self.weight_decay) if self.use_sam else base_optimizer\n            else:\n                base_optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n                optimizer = SAM(model.parameters(), torch.optim.AdamW, rho=self.sam_rho, lr=self.lr, weight_decay=self.weight_decay) if self.use_sam else base_optimizer\n\n            best_score = -1.0\n            best_state = None\n\n            # Прогрессивный ресайз\n            for stage, (img_size, epc) in enumerate(zip(self.img_sizes, self.stage_epochs)):\n                train_tfms = self._build_train_tfms(img_size)\n                valid_tfms = self._build_valid_tfms(img_size)\n\n                train_ds = PandasImageDataset(\n                    df_tr, self.image_col, self.target_col, train_tfms,\n                    classes=self.classes_, multilabel=self.multilabel, label_sep=self.label_sep,\n                    mean=mean, std=std, return_targets=True\n                )\n                valid_ds = PandasImageDataset(\n                    df_va, self.image_col, self.target_col, valid_tfms,\n                    classes=self.classes_, multilabel=self.multilabel, label_sep=self.label_sep,\n                    mean=mean, std=std, return_targets=True\n                )\n\n                pin_mem = (self.accelerator.device.type == 'cuda')\n                train_loader = DataLoader(\n                    train_ds, batch_size=self.batch_size, shuffle=(sampler is None), sampler=sampler,\n                    num_workers=self.num_workers, pin_memory=pin_mem, drop_last=True, persistent_workers=False\n                )\n                valid_loader = DataLoader(\n                    valid_ds, batch_size=max(1, self.batch_size * 2), shuffle=False,\n                    num_workers=self.num_workers, pin_memory=pin_mem, drop_last=False, persistent_workers=False\n                )\n\n                mixup_fn = None\n                if mixup_active and (not self.multilabel):\n                    mixup_fn = Mixup(\n                        mixup_alpha=self.mixup_alpha, cutmix_alpha=self.cutmix_alpha,\n                        label_smoothing=self.label_smoothing, num_classes=num_classes\n                    )\n\n                # Scheduler\n                sched_target_opt = optimizer.base_optimizer if isinstance(optimizer, SAM) else optimizer\n                scheduler = self._build_scheduler(sched_target_opt, max(1, len(train_loader)), epc)\n\n                # Accelerate.prepare: если SAM, готовим только base_optimizer\n                if isinstance(optimizer, SAM):\n                    opt_to_prepare = optimizer.base_optimizer\n                else:\n                    opt_to_prepare = optimizer\n\n                model, opt_prepared, train_loader, valid_loader, scheduler = self.accelerator.prepare(\n                    model, opt_to_prepare, train_loader, valid_loader, scheduler\n                )\n\n                if isinstance(optimizer, SAM):\n                    optimizer.base_optimizer = opt_prepared\n                else:\n                    optimizer = opt_prepared\n\n                # EMA после prepare на правильном девайсе\n                if self.ema_decay:\n                    if ema is None:\n                        ema = ModelEmaV2(\n                            self.accelerator.unwrap_model(model),\n                            decay=self.ema_decay,\n                            device=self.accelerator.device\n                        )\n                    else:\n                        ema.set(self.accelerator.unwrap_model(model))\n\n                for epoch in range(epc):\n                    global_epoch_idx = sum(self.stage_epochs[:stage]) + epoch\n                    use_mix_now = mixup_fn is not None and global_epoch_idx < (self.epochs - self.disable_mix_last_n_epochs)\n\n                    train_loss = self._train_one_epoch(\n                        model, train_loader, train_loss_fn, optimizer, scheduler, ema,\n                        mixup_fn if use_mix_now else None\n                    )\n\n                    eval_model = ema.module if (ema is not None) else model\n                    vloss, vproba, vtgts = self._validate(eval_model, valid_loader, val_loss_fn)\n                    vt = vtgts if self.multilabel else vtgts.reshape(-1)\n                    score = self._compute_metric(vt, vproba)\n\n                    if self.accelerator.is_main_process:\n                        tqdm.write(\n                            f\"fold {fold+1}/{n_folds_eff} | stage {stage+1}/{len(self.img_sizes)} | \"\n                            f\"epoch {epoch+1}/{epc} | train_loss {train_loss:.4f} | val_loss {vloss:.4f} | score {score:.5f}\"\n                        )\n\n                    if score > best_score:\n                        best_score = score\n                        state = {k: v.cpu() for k, v in self.accelerator.unwrap_model(eval_model).state_dict().items()}\n                        best_state = state\n\n                # unwrap после стадии\n                model = self.accelerator.unwrap_model(model)\n\n            # Восстановить лучшую модель фолда\n            if best_state is None:\n                raise RuntimeError(\"best_state не был сохранён — проверьте цикл обучения.\")\n            best_model = self._create_model_safe(False, self.model_img_size or self.img_sizes[-1], num_classes)\n            best_model.load_state_dict(best_state, strict=True)\n            best_model = self.accelerator.prepare(best_model)\n            best_model.eval()\n\n            # OOF для вал. части\n            va_tfms = self._build_valid_tfms(self.img_sizes[-1])\n            va_ds = PandasImageDataset(\n                df_va, self.image_col, self.target_col, va_tfms,\n                classes=self.classes_, multilabel=self.multilabel, label_sep=self.label_sep,\n                mean=mean, std=std, return_targets=True\n            )\n            va_loader = DataLoader(\n                va_ds, batch_size=max(1, self.batch_size * 2), shuffle=False,\n                num_workers=self.num_workers, pin_memory=(self.accelerator.device.type == 'cuda'),\n                persistent_workers=False\n            )\n            va_loader = self.accelerator.prepare(va_loader)\n\n            all_probs, all_tgts = [], []\n            with torch.no_grad():\n                for imgs, tgts in va_loader:\n                    logits = best_model(imgs)\n                    probs = torch.sigmoid(logits) if self.multilabel else torch.softmax(logits, dim=1)\n                    all_probs.append(self.accelerator.gather(probs).cpu())\n                    all_tgts.append(self.accelerator.gather(tgts).cpu())\n            all_probs = torch.cat(all_probs).numpy()\n            all_tgts = torch.cat(all_tgts).numpy()\n\n            self.oof_pred_proba_[df_va.index.values] = all_probs\n            if self.multilabel:\n                self.oof_targets_[df_va.index.values] = all_tgts\n            else:\n                self.oof_targets_[df_va.index.values, 0] = all_tgts\n            self.oof_fold_[df_va.index.values] = fold if not use_holdout else 0\n\n            self.models_.append(self.accelerator.unwrap_model(best_model))\n            self.accelerator.wait_for_everyone()\n\n        # Пороги (multilabel)\n        if self.optimize_thresholds and self.multilabel:\n            mask = (self.oof_fold_ >= 0)\n            if mask.any():\n                self._tune_thresholds(self.oof_targets_[mask], self.oof_pred_proba_[mask])\n\n        # Итоговый CV по доступным OOF\n        mask = (self.oof_fold_ >= 0)\n        if mask.any():\n            if self.multilabel:\n                y_true, y_proba = self.oof_targets_[mask], self.oof_pred_proba_[mask]\n            else:\n                y_true, y_proba = self.oof_targets_[mask].reshape(-1), self.oof_pred_proba_[mask]\n            cv_score = self._compute_metric(y_true, y_proba)\n        else:\n            cv_score = float(\"nan\")\n\n        if self.accelerator.is_main_process:\n            print(f\"\\nCV {self.val_metric}: {cv_score:.5f}\" if not np.isnan(cv_score) else \"\\nCV: no validation predictions (mask empty)\")\n\n        return self\n\n    # ---------------------- TTA ----------------------\n\n    def _apply_tta(self, imgs: torch.Tensor, scales: List[int], crop_size: Optional[int]) -> List[torch.Tensor]:\n        \"\"\"\n        Генерирует TTA-вариации батча: multi-scale, 5-crop, hflip.\n\n        :param imgs: входной батч тензоров [B, C, H, W]\n        :param scales: список размеров для ресайза (квадрат)\n        :param crop_size: размер 5-crop (None — без multi-crop)\n        :return: список TTA-версий батча (тензоры)\n        \"\"\"\n        outs = []\n\n        def resize(imgs_, new_size):\n            return torch.nn.functional.interpolate(imgs_, size=(new_size, new_size), mode=\"bilinear\", align_corners=False)\n\n        def five_crops(imgs_, out_size):\n            H, W = imgs_.shape[-2:]\n            if out_size is None or out_size >= H or out_size >= W:\n                return [imgs_]\n            coords = [\n                (0, 0), (0, W - out_size),\n                (H - out_size, 0), (H - out_size, W - out_size),\n                ((H - out_size) // 2, (W - out_size) // 2)\n            ]\n            return [imgs_[..., y:y + out_size, x:x + out_size] for y, x in coords]\n\n        for s in scales:\n            r = resize(imgs, s)\n            crops = five_crops(r, crop_size) if crop_size is not None else [r]\n            for ci in crops:\n                outs.append(ci)\n                if self.tta_hflip:\n                    outs.append(torch.flip(ci, dims=[-1]))\n        return outs\n\n    # ---------------------- Predict ----------------------\n\n    @torch.no_grad()\n    def predict(self, df: pd.DataFrame, return_proba: bool = True, batch_size: Optional[int] = None,\n                tta: bool = True) -> Union[np.ndarray, np.ndarray]:\n        \"\"\"\n        Предсказывает для df, усредняя по фолдам (и по TTA при включении).\n\n        :param df: DataFrame с колонкой изображения (путь или numpy HWC RGB)\n        :param return_proba: True — вернуть вероятности (N×C); False — индексы классов (multiclass)\n        :param batch_size: размер батча на инференсе; если None — 2×тренировочный\n        :param tta: включить Test-Time Augmentation\n        :return: np.ndarray вероятностей (multилabel/multiclass) или индексов классов (multiclass, return_proba=False)\n        \"\"\"\n        if not self.models_:\n            raise RuntimeError(\"Сначала вызовите fit().\")\n        batch_size = batch_size or max(1, self.batch_size * 2)\n\n        mean = tuple(self._data_config.get(\"mean\", (0.485, 0.456, 0.406))) if self._data_config else (0.485, 0.456, 0.406)\n        std = tuple(self._data_config.get(\"std\", (0.229, 0.224, 0.225))) if self._data_config else (0.229, 0.224, 0.225)\n        base_size = self.img_sizes[-1]\n        valid_tfms = self._build_valid_tfms(base_size)\n\n        ds = PandasImageDataset(\n            df, self.image_col, None, valid_tfms,\n            classes=self.classes_, multilabel=self.multilabel, label_sep=self.label_sep,\n            mean=mean, std=std, return_targets=False\n        )\n        pin_mem = (self.accelerator.device.type == 'cuda')\n        dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=pin_mem, persistent_workers=False)\n        dl = self.accelerator.prepare(dl)\n\n        if self.model_img_size is not None:\n            scales = [self.model_img_size] if tta else [self.model_img_size]\n            crop_size = None\n        else:\n            scales = self.tta_scales if (tta and self.tta_scales) else [base_size]\n            crop_size = (self.tta_crop_size if (tta and self.tta_crop_size and self.tta_crop_size < max(scales)) else None)\n\n        all_fold_probs = []\n        for i, model in enumerate(self.models_):\n            model.to(self.accelerator.device)\n            model.eval()\n            fold_probs = []\n            pbar_desc = f\"Predicting fold {i+1}/{len(self.models_)}\"\n            for imgs in tqdm(dl, desc=pbar_desc, leave=False, disable=not self.accelerator.is_main_process):\n                aug_imgs = self._apply_tta(imgs, scales, crop_size) if tta else [imgs]\n                tta_preds = []\n                for batch_aug in aug_imgs:\n                    logits = model(batch_aug)\n                    p = torch.sigmoid(logits) if self.multilabel else torch.softmax(logits, dim=1)\n                    tta_preds.append(p)\n                tta_mean = torch.stack(tta_preds, dim=0).mean(dim=0)\n                fold_probs.append(self.accelerator.gather(tta_mean).cpu())\n            all_fold_probs.append(torch.cat(fold_probs, dim=0).numpy())\n\n        probs = np.mean(all_fold_probs, axis=0)\n\n        if self.multilabel:\n            return probs\n        if return_proba:\n            return probs\n        else:\n            return probs.argmax(1)\n\n    # ---------------------- Embeddings ----------------------\n\n    @torch.no_grad()\n    def get_embeddings(self, df: pd.DataFrame, batch_size: Optional[int] = None) -> np.ndarray:\n        \"\"\"\n        Извлекает эмбеддинги (признаки до классификатора), усредняя по фолдам.\n\n        :param df: DataFrame с колонкой изображения (путь или numpy HWC RGB)\n        :param batch_size: размер батча на инференсе эмбеддингов; если None — 2×тренировочный\n        :return: матрица эмбеддингов формы [N, D]\n        \"\"\"\n        if not self.models_:\n            raise RuntimeError(\"Сначала вызовите fit().\")\n        batch_size = batch_size or max(1, self.batch_size * 2)\n\n        mean = tuple(self._data_config.get(\"mean\", (0.485, 0.456, 0.406))) if self._data_config else (0.485, 0.456, 0.406)\n        std = tuple(self._data_config.get(\"std\", (0.229, 0.224, 0.225))) if self._data_config else (0.229, 0.224, 0.225)\n        img_size = self.img_sizes[-1]\n        tfms = self._build_valid_tfms(img_size)\n\n        ds = PandasImageDataset(\n            df, self.image_col, None, tfms,\n            classes=self.classes_, multilabel=self.multilabel, label_sep=self.label_sep,\n            mean=mean, std=std, return_targets=False\n        )\n        pin_mem = (self.accelerator.device.type == 'cuda')\n        dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=pin_mem, persistent_workers=False)\n        dl = self.accelerator.prepare(dl)\n\n        def extract_features(model, x):\n            if hasattr(model, \"forward_features\"):\n                feat = model.forward_features(x)\n            else:\n                feat = model(x)\n            if isinstance(feat, (list, tuple)):\n                feat = feat[-1]\n            if feat.ndim == 3:   # ViT/DeiT: [B, N, D] -> CLS\n                feat = feat[:, 0, :]\n            if feat.ndim == 4:   # CNN: [B, C, H, W] -> GAP\n                feat = torch.mean(feat, dim=[2, 3])\n            return feat\n\n        all_fold_embs = []\n        for i, model in enumerate(self.models_):\n            model.to(self.accelerator.device)\n            model.eval()\n            fold_embs = []\n            pbar_desc = f\"Embeddings fold {i+1}/{len(self.models_)}\"\n            for imgs in tqdm(dl, desc=pbar_desc, leave=False, disable=not self.accelerator.is_main_process):\n                feat = extract_features(model, imgs)\n                feat = feat.flatten(1)\n                fold_embs.append(self.accelerator.gather(feat).cpu())\n            all_fold_embs.append(torch.cat(fold_embs, dim=0).numpy())\n\n        return np.mean(all_fold_embs, axis=0)\n\n    def save(self, path: str):\n        \"\"\"\n        Сохраняет ансамбль моделей и метаданные пайплайна в один .pt файл.\n        Сохраняется только на главном процессе Accelerate.\n        \"\"\"\n        if not self.accelerator.is_main_process:\n            return\n        if not self.models_:\n            raise RuntimeError(\"Нет обученных моделей: сначала вызовите fit().\")\n    \n        num_classes = len(self.classes_ or [])\n        mean = tuple(self._data_config.get(\"mean\", (0.485, 0.456, 0.406))) if self._data_config else (0.485, 0.456, 0.406)\n        std  = tuple(self._data_config.get(\"std\",  (0.229, 0.224, 0.225)))  if self._data_config else (0.229, 0.224, 0.225)\n    \n        state_dicts = []\n        for m in self.models_:\n            # На всякий случай снимаем с DDP/AMP-обёрток (должны быть уже unwrap’нуты в вашем fit)\n            sd = {k: v.cpu() for k, v in m.state_dict().items()}\n            state_dicts.append(sd)\n    \n        ckpt = {\n            \"format\": \"ICPv1\",\n            \"saved_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"lib_versions\": {\n                \"torch\": torch.__version__,\n                \"timm\": getattr(timm, \"__version__\", \"unknown\"),\n                \"accelerate\": getattr(__import__(\"accelerate\"), \"__version__\", \"unknown\"),\n            },\n            # конструкторные параметры\n            \"target_column_name\": self.target_col,\n            \"image_column_name\": self.image_col,\n            \"model_name\": self.model_name,\n            \"pretrained\": self.pretrained,\n            \"drop_path_rate\": self.drop_path_rate,\n            \"multilabel\": self.multilabel,\n            \"label_sep\": self.label_sep,\n            \"model_img_size\": self.model_img_size,\n            \"tta_hflip\": self.tta_hflip,\n            \"tta_scales\": self.tta_scales,\n            \"tta_crop_size\": self.tta_crop_size,\n            # мета обучения/инференса\n            \"classes\": self.classes_,\n            \"num_classes\": num_classes,\n            \"img_sizes\": self.img_sizes,\n            \"val_metric\": self.val_metric,\n            \"thresholds\": (self.thresholds_.tolist() if self.thresholds_ is not None else None),\n            \"mean\": mean,\n            \"std\": std,\n            # веса ансамбля (список по фолдам)\n            \"state_dicts\": state_dicts,\n        }\n        torch.save(ckpt, path)\n        print(f\"[save] checkpoint saved to: {path}\")\n\n    @classmethod\n    def load(cls, path: str, map_location: Optional[str] = \"cpu\"):\n        try:\n            from torch.torch_version import TorchVersion\n            from torch.serialization import add_safe_globals\n            add_safe_globals([TorchVersion])\n            ckpt = torch.load(path, map_location=map_location) \n        except Exception as e:\n            ckpt = torch.load(path, map_location=map_location, weights_only=False)\n\n        pipe = cls(\n            target_column_name=ckpt.get(\"target_column_name\", \"label\"),\n            image_column_name=ckpt.get(\"image_column_name\", \"image\"),\n            model_name=ckpt[\"model_name\"],\n            pretrained=False,\n            multilabel=ckpt[\"multilabel\"],\n            drop_path_rate=ckpt.get(\"drop_path_rate\", 0.0),\n            label_sep=ckpt.get(\"label_sep\", \" \"),\n            model_img_size=ckpt.get(\"model_img_size\", None),\n            tta_hflip=ckpt.get(\"tta_hflip\", True),\n            tta_scales=ckpt.get(\"tta_scales\", None),\n            tta_crop_size=ckpt.get(\"tta_crop_size\", None),\n            amp=True,\n            seed=42,\n        )\n    \n        pipe.classes_ = ckpt[\"classes\"]\n        pipe.img_sizes = ckpt.get(\"img_sizes\", [224])\n        pipe.val_metric = ckpt.get(\"val_metric\", \"f1_macro\")\n        thr = ckpt.get(\"thresholds\", None)\n        pipe.thresholds_ = (torch.tensor(thr).numpy() if thr is not None else None)\n        pipe._data_config = {\n            \"mean\": tuple(ckpt.get(\"mean\", (0.485, 0.456, 0.406))),\n            \"std\":  tuple(ckpt.get(\"std\",  (0.229, 0.224, 0.225))),\n        }\n    \n        num_classes = ckpt[\"num_classes\"]\n        pipe.models_.clear()\n        for sd in ckpt[\"state_dicts\"]:\n            m = pipe._create_model_safe(\n                pretrained=False,\n                img_size=(pipe.model_img_size or pipe.img_sizes[-1]),\n                num_classes=num_classes\n            )\n            m.load_state_dict(sd, strict=True)\n            m.eval()\n            pipe.models_.append(m)\n    \n        print(f\"[load] loaded {len(pipe.models_)} model(s) from {path} ({ckpt['model_name']}); classes={num_classes}\")\n        return pipe","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Краш-тесты.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.GradScaler\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", message=\".*does not have many workers.*\")\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport tempfile\nimport os\n\n# ==================== ТЕСТ 1: Базовый multiclass с малым датасетом ====================\nprint(\"TEST 1: Tiny dataset (edge case)\")\ndef test_1_tiny_dataset():\n    rng = np.random.default_rng(1)\n    N = 10\n    imgs = [rng.integers(0, 255, (32, 32, 3), dtype=np.uint8) for _ in range(N)]\n    labels = rng.integers(0, 2, N)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n        amp=False,\n    )\n\n    clf.fit(\n        df,\n        img_sizes=[32],\n        epochs=2,\n        batch_size=3,\n        n_folds=2,\n        num_workers=0,\n        ema_decay=None,\n        mixup_alpha=0.0,\n        cutmix_alpha=0.0,\n    )\n    preds = clf.predict(df, return_proba=False, tta=False)\n    print(f\"✓ Test 1 passed. Predictions shape: {preds.shape}, Accuracy: {accuracy_score(labels, preds):.2f}\")\n\ntest_1_tiny_dataset()\n\n# ==================== ТЕСТ 2: Multilabel с разными форматами меток ====================\nprint(\"\\nTEST 2: Multilabel with various label formats\")\ndef test_2_multilabel_formats():\n    rng = np.random.default_rng(2)\n    N = 30\n\n    # Способ 1: строки с разделителями\n    imgs1 = [rng.integers(0, 255, (48, 48, 3), dtype=np.uint8) for _ in range(N)]\n    labels1 = []\n    for _ in range(N):\n        n_labels = rng.integers(1, 4)\n        labs = rng.choice(['cat', 'dog', 'bird', 'fish', 'mouse'], n_labels, replace=False)\n        labels1.append(\" \".join(sorted(labs)))\n    df1 = pd.DataFrame({\"img\": imgs1, \"tags\": labels1})\n\n    # Способ 2: списки\n    imgs2 = [rng.integers(0, 255, (48, 48, 3), dtype=np.uint8) for _ in range(N)]\n    labels2 = []\n    for _ in range(N):\n        n_labels = rng.integers(1, 4)\n        labs = rng.choice(['A', 'B', 'C', 'D'], n_labels, replace=False).tolist()\n        labels2.append(labs)\n    df2 = pd.DataFrame({\"img\": imgs2, \"tags\": labels2})\n\n    # Тест со строками\n    clf1 = ImageClassificationPipeline(\n        target_column_name=\"tags\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n        multilabel=True,\n        label_sep=\" \",\n    )\n    clf1.loss_name = \"focal\"\n    clf1.fit(\n        df1,\n        img_sizes=[48],\n        epochs=2,\n        batch_size=8,\n        n_folds=2,\n        num_workers=0,\n        val_metric=\"map_macro\",\n        optimize_thresholds=True,\n    )\n    proba1 = clf1.predict(df1[:5], tta=False)\n\n    # Тест со списками\n    clf2 = ImageClassificationPipeline(\n        target_column_name=\"tags\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n        multilabel=True,\n    )\n    clf2.loss_name = \"asl\"\n    clf2.fit(\n        df2,\n        img_sizes=[48],\n        epochs=2,\n        batch_size=8,\n        n_folds=2,\n        num_workers=0,\n        val_metric=\"map_macro\",\n    )\n    proba2 = clf2.predict(df2[:5], tta=False)\n\n    print(f\"✓ Test 2 passed. String labels shape: {proba1.shape}, List labels shape: {proba2.shape}\")\n    print(f\"  Thresholds found: {clf1.thresholds_}\")\n\ntest_2_multilabel_formats()\n\n# ==================== ТЕСТ 3: Работа с файлами изображений ====================\nprint(\"\\nTEST 3: Image file paths instead of arrays\")\ndef test_3_file_paths():\n    rng = np.random.default_rng(3)\n    N = 20\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        paths = []\n        labels = []\n        for i in range(N):\n            img = rng.integers(0, 255, (64, 64, 3), dtype=np.uint8)\n            path = os.path.join(tmpdir, f\"img_{i}.jpg\")\n            cv2.imwrite(path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n            paths.append(path)\n            labels.append(rng.choice(['class_a', 'class_b', 'class_c']))\n\n        df = pd.DataFrame({\"image_path\": paths, \"category\": labels})\n\n        clf = ImageClassificationPipeline(\n            target_column_name=\"category\",\n            image_column_name=\"image_path\",\n            model_name=\"resnet18\",\n            pretrained=False,\n        )\n\n        clf.fit(\n            df,\n            img_sizes=[64],\n            epochs=2,\n            batch_size=5,\n            n_folds=2,\n            num_workers=0,\n            stratify=True,\n        )\n        preds = clf.predict(df[:5], return_proba=True, tta=False)\n        print(f\"✓ Test 3 passed. File paths work. Predictions shape: {preds.shape}\")\n\ntest_3_file_paths()\n\n# ==================== ТЕСТ 4: Progressive resize с heavy augmentations ====================\nprint(\"\\nTEST 4: Progressive resize + heavy augmentations\")\ndef test_4_progressive_resize():\n    rng = np.random.default_rng(4)\n    N = 40\n\n    imgs = []\n    for _ in range(N):\n        size = rng.choice([64, 96, 128])\n        imgs.append(rng.integers(0, 255, (size, size, 3), dtype=np.uint8))\n    labels = rng.integers(0, 4, N)\n\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n    )\n\n    clf.fit(\n        df,\n        img_sizes=[64, 96, 128],\n        stage_epochs=[2, 2, 2],\n        aug_strength=\"heavy\",\n        epochs=6,\n        batch_size=8,\n        n_folds=2,\n        num_workers=0,\n    )\n    preds = clf.predict(df[:5], return_proba=False, tta=False)\n    print(f\"✓ Test 4 passed. Progressive resize works. Final predictions: {preds}\")\n\ntest_4_progressive_resize()\n\n# ==================== ТЕСТ 5: ViT с LLRD и SAM ====================\nprint(\"\\nTEST 5: Vision Transformer with LLRD and SAM\")\ndef test_5_vit_llrd_sam():\n    rng = np.random.default_rng(5)\n    N = 30\n    imgs = [rng.integers(0, 255, (224, 224, 3), dtype=np.uint8) for _ in range(N)]\n    labels = rng.integers(0, 3, N)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"vit_tiny_patch16_224\",\n        pretrained=False,\n        model_img_size=224,\n    )\n\n    clf.fit(\n        df,\n        img_sizes=[224],\n        epochs=3,\n        batch_size=8,\n        n_folds=2,\n        num_workers=0,\n        layer_decay=0.75,   # LLRD\n        use_sam=True,       # SAM optimizer\n        sam_rho=0.05,\n        lr=1e-3,\n    )\n\n    emb = clf.get_embeddings(df[:10])\n    print(f\"✓ Test 5 passed. ViT+LLRD+SAM works. Embeddings shape: {emb.shape}\")\n\ntest_5_vit_llrd_sam()\n\n# ==================== ТЕСТ 6: Mixup/Cutmix с отключением на последних эпохах ====================\nprint(\"\\nTEST 6: Mixup/Cutmix with disable on last epochs\")\ndef test_6_mixup_cutmix():\n    rng = np.random.default_rng(6)\n    N = 50\n\n    imgs = [rng.integers(0, 255, (64, 64, 3), dtype=np.uint8) for _ in range(N)]\n    labels = rng.integers(0, 5, N)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n    )\n\n    clf.fit(\n        df,\n        img_sizes=[64],\n        epochs=5,\n        batch_size=10,\n        n_folds=2,\n        num_workers=0,\n        mixup_alpha=0.8,\n        cutmix_alpha=1.0,\n        label_smoothing=0.1,\n        disable_mix_last_n_epochs=2,\n    )\n    preds = clf.predict(df[:5], return_proba=True, tta=False)\n    print(f\"✓ Test 6 passed. Mixup/Cutmix with disable works. Predictions shape: {preds.shape}\")\n\ntest_6_mixup_cutmix()\n\n# ==================== ТЕСТ 7: GroupKFold для предотвращения утечек ====================\nprint(\"\\nTEST 7: GroupKFold to prevent leakage\")\ndef test_7_group_kfold():\n    rng = np.random.default_rng(7)\n    N = 60\n    groups = np.repeat(np.arange(15), 4)\n    imgs = [rng.integers(0, 255, (48, 48, 3), dtype=np.uint8) for _ in range(N)]\n    labels = rng.integers(0, 2, N)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels, \"patient_id\": groups})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n    )\n\n    clf.fit(\n        df,\n        img_sizes=[48],\n        epochs=2,\n        batch_size=8,\n        n_folds=3,\n        num_workers=0,\n        group_column=\"patient_id\",\n    )\n\n    print(f\"✓ Test 7 passed. GroupKFold works. OOF shape: {clf.oof_pred_proba_.shape}\")\n\ntest_7_group_kfold()\n\n# ==================== ТЕСТ 8: Imbalanced dataset с WeightedSampler и class weights ====================\nprint(\"\\nTEST 8: Imbalanced dataset handling (multiclass)\")\ndef test_8_imbalanced():\n    rng = np.random.default_rng(8)\n\n    N_class0, N_class1, N_class2 = 100, 20, 10\n    imgs, labels = [], []\n    for _ in range(N_class0):\n        imgs.append(rng.integers(0, 255, (32, 32, 3), dtype=np.uint8)); labels.append(0)\n    for _ in range(N_class1):\n        imgs.append(rng.integers(0, 255, (32, 32, 3), dtype=np.uint8)); labels.append(1)\n    for _ in range(N_class2):\n        imgs.append(rng.integers(0, 255, (32, 32, 3), dtype=np.uint8)); labels.append(2)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n    )\n\n    clf.fit(\n        df,\n        img_sizes=[32],\n        epochs=3,\n        batch_size=16,\n        n_folds=3,\n        num_workers=0,\n        use_weighted_sampler=True,\n        class_weights_in_loss=True,\n        val_metric=\"f1_macro\",\n    )\n\n    test_df = df[df['label'].isin([1, 2])].head(10)\n    if len(test_df) > 0:\n        preds = clf.predict(test_df, return_proba=False, tta=False)\n        print(f\"✓ Test 8 passed. Imbalanced handling works. Predictions for rare classes: {preds}\")\n    else:\n        print(f\"✓ Test 8 passed. Imbalanced handling works.\")\n\ntest_8_imbalanced()\n\n# ==================== ТЕСТ 9: EfficientNet с разными TTA стратегиями ====================\nprint(\"\\nTEST 9: EfficientNet with various TTA strategies\")\ndef test_9_efficientnet_tta():\n    rng = np.random.default_rng(9)\n    N = 25\n    imgs = [rng.integers(0, 255, (128, 128, 3), dtype=np.uint8) for _ in range(N)]\n    labels = rng.integers(0, 3, N)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"efficientnet_b0\",\n        pretrained=False,\n        tta_hflip=True,\n        tta_scales=[112, 128, 144],\n        tta_crop_size=112,\n    )\n\n    clf.fit(\n        df,\n        img_sizes=[128],\n        epochs=2,\n        batch_size=5,\n        n_folds=2,\n        num_workers=0,\n        ema_decay=0.999,\n    )\n\n    preds_no_tta = clf.predict(df[:5], return_proba=True, tta=False)\n    preds_with_tta = clf.predict(df[:5], return_proba=True, tta=True)\n\n    print(f\"✓ Test 9 passed. EfficientNet+TTA works.\")\n    print(f\"  No TTA shape: {preds_no_tta.shape}, With TTA shape: {preds_with_tta.shape}\")\n\ntest_9_efficientnet_tta()\n\n# ==================== ТЕСТ 10: ConvNeXt с валидацией на отдельном датасете ====================\nprint(\"\\nTEST 10: ConvNeXt with train/val split\")\ndef test_10_convnext_split():\n    rng = np.random.default_rng(10)\n    N = 80\n    imgs = [rng.integers(0, 255, (96, 96, 3), dtype=np.uint8) for _ in range(N)]\n    labels = rng.choice(['alpha', 'beta', 'gamma', 'delta'], N)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n    df_train, df_test = train_test_split(df, test_size=0.25, stratify=df['label'], random_state=10)\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"convnext_tiny\",\n        pretrained=False,\n        drop_path_rate=0.2,\n    )\n\n    clf.fit(\n        df_train,\n        img_sizes=[96],\n        epochs=3,\n        batch_size=8,\n        n_folds=3,\n        num_workers=0,\n        warmup_epochs=1.0,\n        grad_clip=1.0,\n        val_metric=\"accuracy\",\n    )\n\n    test_proba = clf.predict(df_test, return_proba=True, tta=True)\n    test_preds = clf.predict(df_test, return_proba=False, tta=False)\n    acc = accuracy_score(df_test['label'].values, [clf.classes_[i] for i in test_preds])\n    print(f\"✓ Test 10 passed. ConvNeXt works. Test accuracy: {acc:.2f}\")\n\ntest_10_convnext_split()\n\n# ==================== ТЕСТ 11: Граничный случай - 1 пример на класс ====================\nprint(\"\\nTEST 11: Edge case - 1 sample per class\")\ndef test_11_one_sample_per_class():\n    rng = np.random.default_rng(11)\n    imgs = [rng.integers(0, 255, (32, 32, 3), dtype=np.uint8) for _ in range(6)]\n    labels = [0, 0, 1, 1, 2, 2]\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n    )\n\n    try:\n        clf.fit(\n            df,\n            img_sizes=[32],\n            epochs=1,\n            batch_size=2,\n            n_folds=2,\n            num_workers=0,\n            stratify=True,\n            mixup_alpha=0.0,\n            cutmix_alpha=0.0,\n        )\n        preds = clf.predict(df, return_proba=False, tta=False)\n        print(f\"✓ Test 11 passed. Ultra-small dataset works. Predictions: {preds}\")\n    except Exception as e:\n        print(f\"✗ Test 11 failed with error: {e}\")\n\ntest_11_one_sample_per_class()\n\n# ==================== ТЕСТ 12: Swin Transformer с разными входными размерами ====================\nprint(\"\\nTEST 12: Swin Transformer with different input sizes\")\ndef test_12_swin():\n    rng = np.random.default_rng(12)\n    N = 30\n    imgs = [rng.integers(0, 255, (224, 224, 3), dtype=np.uint8) for _ in range(N)]\n    labels = rng.integers(0, 4, N)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"swin_tiny_patch4_window7_224\",\n        pretrained=False,\n        model_img_size=224,\n        drop_path_rate=0.1,\n    )\n\n    clf.fit(\n        df,\n        img_sizes=[224],\n        epochs=2,\n        batch_size=4,\n        n_folds=2,\n        num_workers=0,\n        layer_decay=0.8,\n    )\n\n    emb = clf.get_embeddings(df[:10])\n    print(f\"✓ Test 12 passed. Swin Transformer works. Embeddings shape: {emb.shape}\")\n\ntest_12_swin()\n\n# ==================== ТЕСТ 13: Multilabel с одним активным классом (граничный случай) ====================\nprint(\"\\nTEST 13: Multilabel edge case - single active class\")\ndef test_13_multilabel_edge():\n    rng = np.random.default_rng(13)\n    N = 40\n    imgs = [rng.integers(0, 255, (64, 64, 3), dtype=np.uint8) for _ in range(N)]\n    labels = []\n    for i in range(N):\n        if i < 30:\n            labels.append(rng.choice(['A', 'B', 'C', 'D', 'E']))\n        else:\n            n = rng.integers(2, 4)\n            labs = rng.choice(['A', 'B', 'C', 'D', 'E'], n, replace=False)\n            labels.append(\" \".join(sorted(labs)))\n    df = pd.DataFrame({\"img\": imgs, \"tags\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"tags\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n        multilabel=True,\n        label_sep=\" \",\n    )\n    clf.loss_name = \"bce\"\n\n    clf.fit(\n        df,\n        img_sizes=[64],\n        epochs=3,\n        batch_size=8,\n        n_folds=3,\n        num_workers=0,\n        optimize_thresholds=True,\n    )\n    proba = clf.predict(df[:5], tta=False)\n    print(f\"✓ Test 13 passed. Multilabel with mostly single labels works.\")\n    print(f\"  Probabilities shape: {proba.shape}, Thresholds: {clf.thresholds_}\")\n\ntest_13_multilabel_edge()\n\n# ==================== ТЕСТ 14: Holdout (test_size) мультикласс ====================\nprint(\"\\nTEST 14: Holdout (test_size) multiclass split\")\ndef test_14_holdout_multiclass():\n    rng = np.random.default_rng(14)\n    N = 120\n    imgs = [rng.integers(0, 255, (48, 48, 3), dtype=np.uint8) for _ in range(N)]\n    labels = rng.integers(0, 3, N)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n    )\n\n    clf.fit(\n        df,\n        test_size=0.3,\n        stratify=True,\n        img_sizes=[48],\n        epochs=2,\n        batch_size=16,\n        num_workers=0,\n    )\n\n    mask = (clf.oof_fold_ >= 0)\n    print(f\"Holdout val size: {mask.sum()} of {len(mask)}\")\n    preds = clf.predict(df[:8], return_proba=False, tta=False)\n    print(f\"✓ Test 14 passed. Holdout multiclass works. Predictions: {preds}\")\n\ntest_14_holdout_multiclass()\n\n# ==================== ТЕСТ 15: Holdout (test_size) мультилейбл ====================\nprint(\"\\nTEST 15: Holdout (test_size) multilabel split\")\ndef test_15_holdout_multilabel():\n    rng = np.random.default_rng(15)\n    N, C = 60, 5\n    def mk_img():\n        return rng.integers(0, 255, (64, 64, 3), dtype=np.uint8)\n    imgs = [mk_img() for _ in range(N)]\n    tags = []\n    for _ in range(N):\n        k = rng.integers(1, 4)\n        labs = sorted(rng.choice(C, size=k, replace=False).tolist())\n        tags.append(\" \".join([f\"t{i}\" for i in labs]))\n    df = pd.DataFrame({\"img\": imgs, \"tags\": tags})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"tags\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n        multilabel=True,\n        label_sep=\" \",\n    )\n    clf.loss_name = \"asl\"\n\n    clf.fit(\n        df,\n        test_size=0.25,\n        img_sizes=[64],\n        epochs=2,\n        batch_size=8,\n        num_workers=0,\n        val_metric=\"map_macro\",\n        optimize_thresholds=True,\n    )\n\n    mask = (clf.oof_fold_ >= 0)\n    print(f\"Holdout val size (multilabel): {mask.sum()} of {len(mask)}\")\n    proba = clf.predict(df[:6], tta=False)\n    print(f\"✓ Test 15 passed. Holdout multilabel works. Proba shape: {proba.shape}\")\n\ntest_15_holdout_multilabel()\n\n# ==================== ТЕСТ 16: Аккумуляция градиентов (grad_accum_steps) ====================\nprint(\"\\nTEST 16: Gradient Accumulation\")\ndef test_16_grad_accum():\n    rng = np.random.default_rng(16)\n    N = 64\n    imgs = [rng.integers(0, 255, (64, 64, 3), dtype=np.uint8) for _ in range(N)]\n    labels = rng.integers(0, 4, N)\n    df = pd.DataFrame({\"img\": imgs, \"label\": labels})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"label\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n    )\n\n    # Малый батч + накопление = эффективный большой батч\n    clf.fit(\n        df,\n        img_sizes=[64],\n        epochs=2,\n        batch_size=4,        # микробатч\n        grad_accum_steps=4,  # 4 шага => экв. батч = 16\n        n_folds=2,\n        num_workers=0,\n        mixup_alpha=0.0,\n        cutmix_alpha=0.0,\n    )\n\n    preds = clf.predict(df[:8], return_proba=False, tta=False)\n    print(f\"✓ Test 16 passed. Grad accumulation works. Predictions: {preds}\")\n\ntest_16_grad_accum()\n\n# ==================== ТЕСТ 17: Multilabel BCE pos_weight (явно) ====================\nprint(\"\\nTEST 17: Multilabel BCE with explicit pos_weight\")\ndef test_17_bce_posweight():\n    rng = np.random.default_rng(17)\n    N, C = 80, 4\n    imgs = [rng.integers(0, 255, (72, 72, 3), dtype=np.uint8) for _ in range(N)]\n    # Делаем редкий класс t3\n    tags = []\n    for i in range(N):\n        labs = []\n        if rng.random() < 0.8:\n            labs.append(\"t0\")\n        if rng.random() < 0.5:\n            labs.append(\"t1\")\n        if rng.random() < 0.3:\n            labs.append(\"t2\")\n        if rng.random() < 0.05:\n            labs.append(\"t3\")  # редкий класс\n        if not labs:\n            labs = [\"t0\"]\n        tags.append(\" \".join(sorted(set(labs))))\n    df = pd.DataFrame({\"img\": imgs, \"tags\": tags})\n\n    clf = ImageClassificationPipeline(\n        target_column_name=\"tags\",\n        image_column_name=\"img\",\n        model_name=\"resnet18\",\n        pretrained=False,\n        multilabel=True,\n        label_sep=\" \",\n    )\n    clf.loss_name = \"bce\"\n\n    # Явно подаём pos_weight, например, усиливая редкий последний класс\n    bce_pw = [1.0, 1.5, 2.0, 10.0]\n\n    clf.fit(\n        df,\n        img_sizes=[72],\n        epochs=2,\n        batch_size=8,\n        n_folds=2,\n        num_workers=0,\n        val_metric=\"map_macro\",\n        optimize_thresholds=True,\n        bce_pos_weight=bce_pw,\n    )\n\n    proba = clf.predict(df[:6], tta=False)\n    print(f\"✓ Test 17 passed. Explicit BCE pos_weight works. Proba shape: {proba.shape}\")\n\ntest_17_bce_posweight()\n\n# ==================== ФИНАЛЬНАЯ СТАТИСТИКА ====================\nprint(\"\\n\" + \"=\"*60)\nprint(\"ALL CRASH TESTS (WITH HOLDOUT, ACCUM, POS_WEIGHT) COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*60)\nprint(\"The pipeline is robust and handles:\")\nprint(\"✓ Tiny datasets (N=6-10)\")\nprint(\"✓ Various image formats (numpy arrays, file paths)\")\nprint(\"✓ Progressive resize and strong augmentations\")\nprint(\"✓ Multiple architectures (ResNet, EfficientNet, ConvNeXt, ViT, Swin)\")\nprint(\"✓ Multiclass and multilabel classification\")\nprint(\"✓ Imbalanced datasets (sampler + class weights; pos_weight for multilabel)\")\nprint(\"✓ GroupKFold for preventing data leakage\")\nprint(\"✓ Advanced optimizations (SAM, LLRD, EMA)\")\nprint(\"✓ Mixup/Cutmix with smart disabling\")\nprint(\"✓ Multiple TTA strategies\")\nprint(\"✓ Edge cases (1-2 samples per class)\")\nprint(\"✓ Different loss functions (CE, BCE, Focal, ASL)\")\nprint(\"✓ Holdout mode via test_size for fast iteration\")\nprint(\"✓ Gradient accumulation (grad_accum_steps)\")\nprint(\"✓ Explicit pos_weight for multilabel BCE\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 1.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.GradScaler\", category=FutureWarning)\n\nimport numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nrng = np.random.default_rng(42)\n\ndef make_blob(h, w, color):\n    img = np.zeros((h, w, 3), dtype=np.uint8)\n    cy, cx = rng.integers(h//4, 3*h//4), rng.integers(w//4, 3*w//4)\n    r = rng.integers(min(h,w)//8, min(h,w)//4)\n    import cv2\n    cv2.circle(img, (cx, cy), int(r), tuple(map(int, color)), -1)\n    img += rng.integers(0, 10, size=img.shape, dtype=np.uint8)\n    return img\n\nN, H, W = 90, 96, 96\nlabels = rng.integers(0, 3, size=N)\npalette = [(200, 50, 50), (50, 200, 50), (50, 50, 200)]\nimgs = [make_blob(H, W, palette[int(y)]) for y in labels]\ndf = pd.DataFrame({\"image\": imgs, \"label\": labels})\ndf_tr, df_va = train_test_split(df, test_size=0.25, stratify=df[\"label\"], random_state=42)\n\nclf = ImageClassificationPipeline(\n    target_column_name=\"label\",\n    image_column_name=\"image\",\n    model_name=\"resnet18\",\n    pretrained=True,\n    drop_path_rate=0.1,\n    multilabel=False,\n    amp=True,\n)\n\nclf.fit(\n    df_tr,\n    img_sizes=[96, 128],\n    stage_epochs=[3, 3],\n    aug_strength=\"medium\",\n    epochs=6,\n    batch_size=16,\n    num_workers=0,            # безопасно для ноутбуков/Colab\n    ema_decay=0.9999,\n    lr=2e-3,\n    weight_decay=1e-4,\n    warmup_epochs=0.5,\n    grad_clip=1.0,\n    mixup_alpha=0.5,\n    cutmix_alpha=0.8,\n    label_smoothing=0.05,\n    disable_mix_last_n_epochs=1,\n    n_folds=3,\n    val_metric=\"f1_macro\",\n)\n\nproba = clf.predict(df_va, return_proba=True, tta=True)\npreds = clf.predict(df_va, return_proba=False, tta=False)\nprint(\"VAL F1-macro:\", f1_score(df_va[\"label\"].values, preds, average=\"macro\"))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 2.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.GradScaler\", category=FutureWarning)\n\nimport numpy as np, pandas as pd\n\nrng = np.random.default_rng(13)\nN, H, W, C = 80, 96, 96, 5\n\ndef make_multi(h, w, active_ids):\n    img = rng.integers(0, 50, size=(h, w, 3), dtype=np.uint8)\n    import cv2\n    colors = [(200,0,0),(0,200,0),(0,0,200),(200,200,0),(200,0,200)]\n    for i in active_ids:\n        if i % 2 == 0:\n            y = rng.integers(10, h-10)\n            cv2.line(img, (10, y), (w-10, y), colors[i], 3)\n        else:\n            x = rng.integers(10, w-10)\n            cv2.line(img, (x, 10), (x, h-10), colors[i], 3)\n    return img\n\ntags_all = [f\"t{i}\" for i in range(C)]\nimgs, tag_strings = [], []\nfor _ in range(N):\n    k = rng.integers(1, 4)\n    labs = sorted(rng.choice(C, size=k, replace=False).tolist())\n    imgs.append(make_multi(H, W, labs))\n    tag_strings.append(\" \".join([f\"t{i}\" for i in labs]))\n\ndf = pd.DataFrame({\"image\": imgs, \"tags\": tag_strings})\n\nclf_ml = ImageClassificationPipeline(\n    target_column_name=\"tags\",\n    image_column_name=\"image\",\n    model_name=\"resnet18\",\n    pretrained=True,\n    multilabel=True,\n    label_sep=\" \",\n    amp=True,\n)\nclf_ml.loss_name = \"asl\"  # \"bce\" | \"asl\" | \"focal\"\n\nclf_ml.fit(\n    df,\n    img_sizes=[96, 128],\n    stage_epochs=[3, 3],\n    aug_strength=\"medium\",\n    epochs=6,\n    batch_size=16,\n    num_workers=0,\n    ema_decay=0.9999,\n    lr=1e-3,\n    weight_decay=1e-4,\n    val_metric=\"map_macro\",\n    optimize_thresholds=True,\n    n_folds=3,\n)\n\nproba = clf_ml.predict(df, tta=False)\nprint(\"Tuned thresholds:\", clf_ml.thresholds_)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 3.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.GradScaler\", category=FutureWarning)\n\nimport numpy as np, pandas as pd\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LogisticRegression\n\nrng = np.random.default_rng(7)\nN, H, W, KCLS = 90, 224, 224, 4\n\ndef make_shape(h, w, cls):\n    img = np.zeros((h, w, 3), dtype=np.uint8)\n    import cv2\n    c = [(220,70,70),(70,220,70),(70,70,220),(220,220,70)][cls]\n    if cls == 0:\n        cv2.circle(img, (w//2, h//2), min(h,w)//4, c, -1)\n    elif cls == 1:\n        cv2.rectangle(img, (w//4,h//4), (3*w//4,3*h//4), c, -1)\n    elif cls == 2:\n        pts = np.array([[w//2,h//4],[w//4,3*h//4],[3*w//4,3*h//4]], np.int32)\n        cv2.fillPoly(img, [pts], c)\n    else:\n        cv2.line(img, (w//4,h//2), (3*w//4,h//2), c, 8)\n        cv2.line(img, (w//2,h//4), (w//2,3*h//4), c, 8)\n    img += rng.integers(0, 15, size=img.shape, dtype=np.uint8)\n    return img\n\nlabels = rng.integers(0, KCLS, size=N)\ngroups = rng.integers(0, 15, size=N)\nimgs = [make_shape(H, W, int(y)) for y in labels]\ndf = pd.DataFrame({\"image\": imgs, \"label\": labels, \"group_id\": groups})\n\nclf_vit = ImageClassificationPipeline(\n    target_column_name=\"label\",\n    image_column_name=\"image\",\n    model_name=\"vit_tiny_patch16_224\",\n    pretrained=True,\n    multilabel=False,\n    model_img_size=224,       # фиксированный вход для ViT\n    tta_hflip=True,           # параметры TTA — это часть инференса\n    tta_scales=[224],\n    tta_crop_size=None,\n    amp=True,\n    drop_path_rate=0.2,\n)\n\nclf_vit.fit(\n    df,\n    img_sizes=[224],\n    stage_epochs=[6],\n    aug_strength=\"medium\",\n    epochs=6,\n    batch_size=16,\n    num_workers=0,\n    ema_decay=0.9999,\n    lr=1e-3,\n    weight_decay=0.02,\n    layer_decay=0.8,        # LLRD\n    n_folds=3,\n    group_column=\"group_id\",\n    val_metric=\"f1_macro\",\n)\n\nproba = clf_vit.predict(df, return_proba=True, tta=True)\n\n# Эмбеддинги + логистическая регрессия\nemb = clf_vit.get_embeddings(df)\nlr = LogisticRegression(max_iter=300, n_jobs=-1)\nlr.fit(emb, labels)\npred_lr = lr.predict(emb)\nprint(\"F1-macro (логрег на эмбеддингах):\", f1_score(labels, pred_lr, average=\"macro\"))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования 4.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", message=\"`torch.cuda.amp.GradScaler\", category=FutureWarning)\n\nimport numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# 1) Синтетика (4 класса, картинки 224x224)\nrng = np.random.default_rng(21)\nN, H, W, KCLS = 300, 224, 224, 4\n\ndef make_shape(h, w, cls):\n    import cv2\n    img = np.zeros((h, w, 3), dtype=np.uint8)\n    c = [(220,70,70),(70,220,70),(70,70,220),(220,220,70)][cls]\n    if cls == 0:\n        cv2.circle(img, (w//2, h//2), min(h,w)//4, c, -1)\n    elif cls == 1:\n        cv2.rectangle(img, (w//4,h//4), (3*w//4,3*h//4), c, -1)\n    elif cls == 2:\n        pts = np.array([[w//2,h//4],[w//4,3*h//4],[3*w//4,3*h//4]], np.int32)\n        cv2.fillPoly(img, [pts], c)\n    else:\n        cv2.line(img, (w//4,h//2), (3*w//4,h//2), c, 8)\n        cv2.line(img, (w//2,h//4), (w//2,3*h//4), c, 8)\n    # немного шума\n    img += rng.integers(0, 10, size=img.shape, dtype=np.uint8)\n    return img\n\nlabels = rng.integers(0, KCLS, size=N)\nimgs = [make_shape(H, W, int(y)) for y in labels]\ndf = pd.DataFrame({\"image\": imgs, \"label\": labels})\n\n# Отдельный тестовый сплит (валидация на настоящем тесте)\ndf_train, df_test = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=2025)\n\n# 2) Пайплайн (инициализация только параметрами данных/модели/инференса)\nclf = ImageClassificationPipeline(\n    target_column_name=\"label\",\n    image_column_name=\"image\",\n    model_name=\"vit_tiny_patch16_224\",\n    pretrained=True,\n    multilabel=False,\n    model_img_size=224,      # фиксированный вход для ViT\n    tta_hflip=True,\n    tta_scales=[224],\n    tta_crop_size=None,\n    amp=True,\n    drop_path_rate=0.2,\n)\n\n# 4) Обучаем без K-fold: holdout через test_size (например, 10% на валидацию внутри fit)\n#    Плюс grad_accum_steps (аккумуляция градиентов) для экономии VRAM.\nclf.fit(\n    df_train,\n    test_size=0.1,            # holdout внутри обучающей выборки\n    img_sizes=[224],          # один размер (без прогрессивного ресайза)\n    epochs=5,\n    batch_size=8,             # микробатч\n    grad_accum_steps=2,       # => эффективный батч ~ 16\n    num_workers=0,\n    ema_decay=0.9995,\n    lr=8e-4,\n    weight_decay=0.02,\n    warmup_epochs=0.5,\n    grad_clip=1.0,\n    layer_decay=0.8,          # LLRD для ViT (опционально)\n    val_metric=\"f1_macro\",\n)\n\n# 5) Валидация на внешнем тесте (df_test)\nproba_test = clf.predict(df_test, return_proba=True, tta=True)\npreds_test = proba_test.argmax(1)\ny_true = df_test[\"label\"].values\nprint(\"Test Accuracy:\", accuracy_score(y_true, preds_test))\nprint(\"Test F1-macro:\", f1_score(y_true, preds_test, average=\"macro\"))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение YOLO для детекции или подсчёта объектов на картинке.","metadata":{}},{"cell_type":"code","source":"# yolo_detection_pipeline.py\n# Обновлённый пайплайн:\n# - авто-выбор устройства (GPU '0' / мульти-GPU '0,1,...' / CPU),\n# - тюнинг подсчёта: plain (conf/iou/max_det) ИЛИ area-gated (cs/cb/area_thr + iou/max_det),\n# - Ridge-калибровка по резидуалу (K-fold CV, стандартизация),\n# - устойчивый инференс при мульти-GPU (тюнинг/инференс на первой карте),\n# - финальная валидация RMSE/MAE для задачи подсчёта.\n#\n# Требования:\n# pip install -U ultralytics opencv-python pandas numpy scikit-learn tqdm\n\n!pip install ultralytics\n\nimport os\nimport cv2\nimport json\nimport shutil\nimport tempfile\nimport warnings\nimport random\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom ultralytics import YOLO\nfrom ultralytics.utils.downloads import attempt_download_asset\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import KFold\n\n\nclass YOLODetectionPipeline:\n    \"\"\"\n    YOLO-пайплайн из pandas DataFrame с опциональным подбором гиперпараметров подсчёта\n    (plain или area-gated) и калибровкой линейной моделью (Ridge по резидуалу).\n\n    Входные данные (DataFrame):\n      - image_path (str): путь к изображению (jpg/png)\n      - boxes_col: GT-боксы в YOLO-нормировке [0..1] (списки/массивы/строка)\n\n    Основной сценарий:\n      - fit(): обучает YOLO; (опц.) тюнинг порогов (plain/area-gated + iou/max_det);\n               (опц.) калибрует Ridge; (опц.) валидирует RMSE/MAE.\n      - predict(): возвращает детекции (boxes_json + count). Калибровка НЕ применяется.\n      - predict_counts(): возвращает числовой подсчёт; применяет лучшие пороги/калибровку, если включены.\n\n    Управление временем тюнинга:\n      - tune_val_subsample: подвыборка валидации (int — кол-во, float — доля 0..1).\n      - tune_max_combinations: ограничивает число проверяемых комбо (случайно из сетки).\n    \"\"\"\n\n    def __init__(self,\n                 model_ckpt: str = \"yolov8n.pt\",\n                 data_root: str | None = None,\n                 image_col: str = \"image_path\",\n                 boxes_col: str = \"boxes\",\n                 class_names: list[str] | None = None,\n                 use_symlinks: bool = True,\n                 verbose: bool = True,\n                 # переключатели\n                 enable_tuning: bool = True,\n                 enable_ridge: bool = True,\n                 validate_count: bool = True,\n                 # режим тюнинга plain vs area-gated\n                 enable_area_gate: bool = True,        # True → тюним (conf_small, conf_big, area_thr) + (iou, max_det)\n                 enable_tta_flip: bool = False,        # True → TTA flip при plain-подсчёте\n                 # управление временем тюнинга\n                 tune_val_subsample: int | float | None = None,  # int=кол-во; float=доля [0..1]\n                 tune_max_combinations: int | None = 100,\n                 random_state: int = 42,\n                 # сетки для plain-тюнинга\n                 tune_conf_grid = (0.20, 0.25, 0.30, 0.35),\n                 tune_iou_grid  = (0.55, 0.60),\n                 tune_max_det_grid = (300, 600),\n                 # сетки для area-gated (под мелкие объекты из ваших стат)\n                 tune_conf_small_grid = (0.10, 0.12, 0.14, 0.18),\n                 tune_conf_big_grid   = (0.30, 0.40, 0.50),\n                 tune_area_thr_grid   = (0.0008, 0.0010, 0.0012, 0.0015),\n                 gate_conf_base: float = 0.07,  # базовый conf для извлечения кандидатов при area-gated\n                 # сетка для Ridge\n                 ridge_alpha_grid = (0.3, 1.0, 3.0),\n                 # пороги нормированной площади для фич Ridge/plain\n                 small_thr: float = 0.0010,\n                 big_thr: float   = 0.003):\n        self.model_ckpt = model_ckpt\n        self.image_col = image_col\n        self.boxes_col = boxes_col\n        self.class_names = class_names or [\"obj\"]\n        self.use_symlinks = use_symlinks\n        self.verbose = verbose\n\n        self.enable_tuning = enable_tuning\n        self.enable_ridge = enable_ridge\n        self.validate_count = validate_count\n        self.enable_area_gate = enable_area_gate\n        self.enable_tta_flip = enable_tta_flip\n\n        self.tune_val_subsample = tune_val_subsample\n        self.tune_max_combinations = tune_max_combinations\n        self.random_state = random_state\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n        # рабочая папка\n        self._tmpdir_owned = False\n        if data_root is None:\n            self.data_root = tempfile.mkdtemp(prefix=\"yolo_ds_\")\n            self._tmpdir_owned = True\n        else:\n            self.data_root = os.path.abspath(data_root)\n            os.makedirs(self.data_root, exist_ok=True)\n\n        self.dataset_yaml = os.path.join(self.data_root, \"dataset.yaml\")\n        self.model_path = None\n        self._model = None\n        self._device = None  # строка устройства, использованная при fit()\n\n        # сетки и пороги\n        self.tune_conf_grid = tuple(float(x) for x in tune_conf_grid)\n        self.tune_iou_grid  = tuple(float(x) for x in tune_iou_grid)\n        self.tune_max_det_grid = tuple(int(x) for x in tune_max_det_grid)\n\n        self.tune_conf_small_grid = tuple(float(x) for x in tune_conf_small_grid)\n        self.tune_conf_big_grid   = tuple(float(x) for x in tune_conf_big_grid)\n        self.tune_area_thr_grid   = tuple(float(x) for x in tune_area_thr_grid)\n        self.gate_conf_base = float(gate_conf_base)\n\n        self.ridge_alpha_grid = tuple(float(x) for x in ridge_alpha_grid)\n        self.small_thr = float(small_thr)\n        self.big_thr   = float(big_thr)\n\n        # сохранённые результаты тюнинга/калибровки\n        self.calib_ = dict(\n            # plain режим:\n            best_conf=None, best_iou=None, best_max_det=None,\n            # area-gated:\n            gate_conf_small=None, gate_conf_big=None, gate_area_thr=None, gate_conf_base=None,\n            # Ridge:\n            ridge_alpha=None, ridge_model=None, ridge_mu=None, ridge_sd=None,\n            # общий:\n            imgsz=None\n        )\n\n    # -------------------- device helpers --------------------\n    @staticmethod\n    def _resolve_device(device: str | int | None) -> str:\n        \"\"\"\n        Выбор устройства для обучения:\n          - None / \"auto\": \"0,1,...,N-1\" при наличии CUDA, иначе \"cpu\"\n          - иначе вернуть строку как есть (например, \"0\" или \"cpu\")\n        \"\"\"\n        if device is None or str(device).lower() == \"auto\":\n            if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n                n = torch.cuda.device_count()\n                return \",\".join(str(i) for i in range(n))\n            return \"cpu\"\n        return str(device)\n\n    def _infer_device(self) -> str:\n        \"\"\"\n        Устройство для инференса/тюнинга:\n          - если тренировались на '0,1,...' → берём первую карту '0'\n          - если тренировались на 'k' → её же\n          - иначе авто: '0' при наличии CUDA, 'cpu' без GPU\n        \"\"\"\n        if getattr(self, \"_device\", None):\n            if isinstance(self._device, str) and \",\" in self._device:\n                return self._device.split(\",\")[0]\n            return self._device\n        return \"0\" if (torch.cuda.is_available() and torch.cuda.device_count() > 0) else \"cpu\"\n\n    # -------------------- helpers: разметка → YOLO-тxt --------------------\n    @staticmethod\n    def _is_nan_like(x):\n        if x is None: return True\n        if isinstance(x, float) and np.isnan(x): return True\n        if isinstance(x, str) and x.strip()==\"\": return True\n        return False\n\n    def _parse_boxes(self, row):\n        boxes_raw = row[self.boxes_col] if self.boxes_col in row else None\n        if self._is_nan_like(boxes_raw): return []\n        out = []\n        if isinstance(boxes_raw, (list, tuple, np.ndarray)):\n            for it in boxes_raw:\n                vals = list(map(float, it))\n                if len(vals) >= 4:\n                    x,y,w,h = vals[:4]\n                    if 0 <= x <= 1 and 0 <= y <= 1 and 0 < w <= 1 and 0 < h <= 1:\n                        out.append((0, x,y,w,h))\n        elif isinstance(boxes_raw, str):\n            lines = [ln.strip() for ln in boxes_raw.strip().splitlines() if ln.strip()]\n            for ln in lines:\n                parts = ln.split()\n                vals = list(map(float, parts))\n                if len(vals) == 4:\n                    x,y,w,h = vals\n                    out.append((0, x,y,w,h))\n                elif len(vals) >= 5:\n                    cls,x,y,w,h = int(vals[0]), *vals[1:5]\n                    out.append((cls, float(x),float(y),float(w),float(h)))\n        return out\n\n    def _link_or_copy(self, src, dst):\n        os.makedirs(os.path.dirname(dst), exist_ok=True)\n        if self.use_symlinks:\n            try:\n                if os.path.lexists(dst): os.remove(dst)\n                os.symlink(os.path.abspath(src), dst)\n                return\n            except Exception:\n                pass\n        shutil.copy2(src, dst)\n\n    def _write_label_file(self, label_path, boxes):\n        os.makedirs(os.path.dirname(label_path), exist_ok=True)\n        with open(label_path, \"w\", encoding=\"utf-8\") as f:\n            for cls, x,y,w,h in boxes:\n                f.write(f\"{int(cls)} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n\n    def _materialize(self, train_df, val_df, train_split=\"train\", val_split=\"val\"):\n        for split_name, df in [(train_split, train_df), (val_split, val_df)]:\n            img_dir = os.path.join(self.data_root, \"images\", split_name)\n            lbl_dir = os.path.join(self.data_root, \"labels\", split_name)\n            os.makedirs(img_dir, exist_ok=True); os.makedirs(lbl_dir, exist_ok=True)\n            it = df.iterrows()\n            if self.verbose: it = tqdm(it, total=len(df), desc=f\"[build] {split_name}\")\n            for _, row in it:\n                src = row[self.image_col]\n                if not os.path.exists(src):\n                    raise FileNotFoundError(f\"Image not found: {src}\")\n                fname = os.path.basename(src)\n                stem, _ = os.path.splitext(fname)\n                self._link_or_copy(src, os.path.join(img_dir, fname))\n                self._write_label_file(os.path.join(lbl_dir, stem + \".txt\"), self._parse_boxes(row))\n\n        with open(self.dataset_yaml, \"w\", encoding=\"utf-8\") as f:\n            f.write(f\"path: {self.data_root}\\ntrain: images/{train_split}\\nval: images/{val_split}\\nnames:\\n\")\n            for i, name in enumerate(self.class_names):\n                f.write(f\"  {i}: {name}\\n\")\n\n    # -------------------- fit: train + (tune/ridge/validate) --------------------\n    def fit(self,\n            train_df: pd.DataFrame,\n            val_df: pd.DataFrame | None = None,\n            test_size: float = 0.2,\n            epochs: int = 50,\n            imgsz: int = 640,\n            batch: int = 16,\n            device: str | int | None = \"auto\",\n            workers: int = 4,\n            patience: int = 50,\n            optimizer: str = \"auto\",\n            augment: bool = True,\n            seed: int = 42,\n            close_mosaic: int | None = 10,\n            cos_lr: bool = True,\n            rect: bool = False,\n            iou: float = 0.7,\n            **extra_train_kwargs):\n\n        # выбрать устройство для тренировки и сохранить\n        self._device = self._resolve_device(device)\n        if self.verbose:\n            print(f\"[device] training device='{self._device}'\")\n\n        np.random.seed(seed); random.seed(seed)\n\n        # если val_df не задан — делаем простую стратификацию по бинам count\n        if val_df is None:\n            tmp = train_df.copy()\n            counts = [len(self._parse_boxes(r)) for _, r in tmp.iterrows()]\n            tmp[\"_bins\"] = np.clip((np.array(counts)//5).astype(int), 0, 50)\n            val_mask = tmp.groupby(\"_bins\", group_keys=False).apply(\n                lambda g: g.sample(frac=test_size, random_state=seed)).index\n            val_df = train_df.loc[val_mask]\n            train_df = train_df.drop(index=val_mask)\n            train_df = train_df.reset_index(drop=True); val_df = val_df.reset_index(drop=True)\n\n        self._materialize(train_df, val_df)\n\n        # загрузить/скачать чекпоинт\n        if not os.path.exists(self.model_ckpt) and self.model_ckpt.endswith(\".pt\"):\n            try:\n                if self.verbose: print(f\"Checkpoint '{self.model_ckpt}' not found. Attempting to download...\")\n                attempt_download_asset(self.model_ckpt)\n            except Exception as e:\n                raise FileNotFoundError(f\"Failed to download '{self.model_ckpt}'. Error: {e}\")\n\n        # train args\n        train_args = {\n            'data': self.dataset_yaml, 'epochs': epochs, 'imgsz': imgsz, 'batch': batch,\n            'device': self._device, 'workers': workers, 'patience': patience, 'optimizer': optimizer,\n            'augment': augment, 'seed': seed, 'close_mosaic': close_mosaic, 'cos_lr': cos_lr,\n            'rect': rect, 'iou': iou, 'verbose': self.verbose\n        }\n        train_args.update(extra_train_kwargs)\n\n        # обучение\n        model = YOLO(self.model_ckpt)\n        model.train(**train_args)\n\n        # ГАРАНТИРОВАННО берём лучший чекпоинт\n        best_path = None\n        if hasattr(model, \"trainer\") and getattr(model.trainer, \"best\", None):\n            best_path = str(model.trainer.best)     # .../runs/detect/exp/weights/best.pt\n        elif getattr(model, \"ckpt_path\", None):\n            best_path = str(model.ckpt_path)\n        else:\n            best_path = self.model_ckpt\n        self.model_path = best_path\n        if self.verbose:\n            print(f\"[fit] best model: {self.model_path}\")\n\n        # тюнинг/калибровка/валидация\n        try:\n            self._tune_and_or_calibrate(val_df, imgsz=imgsz)\n            if self.validate_count:\n                self._validate_counting(val_df)\n        except Exception as e:\n            if self.verbose:\n                print(f\"[post-fit] skipped tuning/calibration/validation due to: {e}\")\n\n        return self.model_path\n\n    # -------------------- инференс-хелперы --------------------\n    def _ensure_model(self):\n        if self._model is None:\n            path = self.model_path or self.model_ckpt\n            self._model = YOLO(path)\n\n    @torch.no_grad()\n    def _raw_counts(self, paths, imgsz, conf, iou, max_det):\n        \"\"\"Plain len(detections). Если enable_tta_flip=True — усреднение с augment=True.\"\"\"\n        self._ensure_model()\n        out = []\n        dev = self._infer_device()\n        if not self.enable_tta_flip:\n            for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[counts]\"):\n                batch = paths[i:i+64]\n                res = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                          max_det=max_det, device=dev, verbose=False)\n                for r in res:\n                    out.append(int(len(r.boxes) if (r.boxes is not None) else 0))\n        else:\n            for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[counts-tta]\"):\n                batch = paths[i:i+64]\n                r1 = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                         max_det=max_det, device=dev, verbose=False)\n                r2 = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                         max_det=max_det, device=dev, verbose=False, augment=True)\n                for a, b in zip(r1, r2):\n                    n1 = int(len(a.boxes) if (a.boxes is not None) else 0)\n                    n2 = int(len(b.boxes) if (b.boxes is not None) else 0)\n                    out.append(0.5 * (n1 + n2))\n        return np.array(out, dtype=float)\n\n    @torch.no_grad()\n    def _yolo_feats(self, paths, imgsz, conf, iou, max_det):\n        \"\"\"Фичи для Ridge: [n, conf_sum, conf_mean, conf_max, area_mean, frac_small, frac_mid, frac_big].\"\"\"\n        self._ensure_model()\n        rows = []\n        dev = self._infer_device()\n        for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[feats]\"):\n            batch = paths[i:i+64]\n            res = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                      max_det=max_det, device=dev, verbose=False)\n            for r in res:\n                if r.boxes is None or len(r.boxes) == 0:\n                    rows.append(dict(n=0, conf_sum=0, conf_mean=0, conf_max=0,\n                                     area_mean=0, frac_small=0, frac_mid=0, frac_big=0))\n                    continue\n                confs = r.boxes.conf.cpu().numpy()\n                xywhn = r.boxes.xywhn.cpu().numpy()\n                areas = (xywhn[:, 2] * xywhn[:, 3]).clip(0, 1)\n                rows.append(dict(\n                    n=len(confs),\n                    conf_sum=float(confs.sum()),\n                    conf_mean=float(confs.mean()),\n                    conf_max=float(confs.max()),\n                    area_mean=float(areas.mean()),\n                    frac_small=float((areas < self.small_thr).mean()),\n                    frac_mid=float(((areas >= self.small_thr) & (areas <= self.big_thr)).mean()),\n                    frac_big=float((areas > self.big_thr).mean())\n                ))\n        return pd.DataFrame(rows).to_numpy()\n\n    @torch.no_grad()\n    def _detect_conf_area(self, paths, imgsz, conf_base, iou, max_det):\n        \"\"\"Возвращает список массивов Nx2 [conf, area] при базовом пороге conf_base (для area-gated).\"\"\"\n        self._ensure_model()\n        dev = self._infer_device()\n        out = []\n        for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[boxes]\"):\n            batch = paths[i:i+64]\n            res = self._model.predict(batch, imgsz=imgsz, conf=conf_base, iou=iou,\n                                      max_det=max_det, device=dev, verbose=False)\n            for r in res:\n                if r.boxes is None or len(r.boxes) == 0:\n                    out.append(np.empty((0, 2), dtype=np.float32))\n                    continue\n                confs = r.boxes.conf.cpu().numpy()\n                xywhn = r.boxes.xywhn.cpu().numpy()\n                areas = (xywhn[:, 2] * xywhn[:, 3]).clip(0, 1)\n                out.append(np.stack([confs, areas], axis=1))\n        return out\n\n    @staticmethod\n    def _count_with_area_gate(conf_area_list, conf_small, conf_big, area_thr):\n        \"\"\"Подсчёт с двупороговой фильтрацией по площади.\"\"\"\n        counts = []\n        for ca in conf_area_list:\n            if ca.size == 0:\n                counts.append(0); continue\n            conf = ca[:, 0]; area = ca[:, 1]\n            small_mask = (area < area_thr)  & (conf >= conf_small)\n            big_mask   = (area >= area_thr) & (conf >= conf_big)\n            counts.append(int(small_mask.sum() + big_mask.sum()))\n        return np.array(counts, dtype=float)\n\n    # -------------------- подвыборка валидации --------------------\n    def _subset_val(self, val_df: pd.DataFrame) -> pd.DataFrame:\n        if self.tune_val_subsample is None:\n            return val_df\n        n = len(val_df)\n        if isinstance(self.tune_val_subsample, float):\n            k = max(1, int(round(n * self.tune_val_subsample)))\n        else:\n            k = int(self.tune_val_subsample)\n        k = min(k, n)\n        return val_df.sample(n=k, random_state=self.random_state).reset_index(drop=True)\n\n    # -------------------- Ridge по резидуалу с K-fold CV --------------------\n    def _fit_ridge_cv_on_residual(self, X: np.ndarray, y_true: np.ndarray, y_plain: np.ndarray):\n        alphas = self.ridge_alpha_grid\n        k = min(5, len(y_true)) if len(y_true) >= 3 else 2\n        kf = KFold(n_splits=k, shuffle=True, random_state=self.random_state)\n\n        # стандартизация фич\n        mu = X.mean(axis=0)\n        sd = X.std(axis=0); sd[sd == 0] = 1.0\n        Xs = (X - mu) / sd\n        r = y_true - y_plain\n\n        best_alpha, best_cv = None, 1e9\n        for a in alphas:\n            cv_scores = []\n            for tr, va in kf.split(Xs):\n                m = Ridge(alpha=float(a)).fit(Xs[tr], r[tr])\n                pr = m.predict(Xs[va])\n                cv_scores.append(mean_squared_error(r[va], pr, squared=False))\n            cv_rmse = float(np.mean(cv_scores))\n            if cv_rmse < best_cv:\n                best_alpha, best_cv = float(a), cv_rmse\n\n        # финальная подгонка на всех\n        model = Ridge(alpha=best_alpha).fit(Xs, r)\n        return dict(model=model, alpha=best_alpha, mu=mu, sd=sd, cv_rmse=best_cv)\n\n    # -------------------- тюнинг и/или калибровка --------------------\n    def _tune_and_or_calibrate(self, val_df: pd.DataFrame, imgsz: int):\n        val_sub = self._subset_val(val_df)\n        paths = val_sub[self.image_col].tolist()\n        y_true = np.array([len(self._parse_boxes(r)) for _, r in val_sub.iterrows()], dtype=float)\n\n        # 1) Тюнинг plain или area-gated\n        if self.enable_tuning:\n            if self.enable_area_gate:\n                # Подготовим базовый порог для сбора кандидатов, согласованный с минимальным cs\n                min_cs = min(self.tune_conf_small_grid) if len(self.tune_conf_small_grid) else 0.10\n                base_collect = max(0.03, min(self.gate_conf_base, min_cs - 0.02))\n                if self.verbose:\n                    print(f\"[tune-gate] base_collect={base_collect:.3f} (min_cs={min_cs:.3f})\")\n\n                # 1) Предрасчёт списков [conf, area] для всех (iou, max_det)\n                iou_grid = tuple(self.tune_iou_grid)\n                md_grid  = tuple(self.tune_max_det_grid)\n                conf_area_by_key = {}\n                total_prepasses = len(iou_grid) * len(md_grid)\n                if self.verbose:\n                    print(f\"[tune-gate] precomputing boxes for {total_prepasses} (iou,max_det) pairs...\")\n                for iou_ in iou_grid:\n                    for md_ in md_grid:\n                        conf_area_by_key[(iou_, md_)] = self._detect_conf_area(\n                            paths, imgsz=imgsz,\n                            conf_base=float(base_collect),\n                            iou=float(iou_), max_det=int(md_)\n                        )\n\n                # 2) Подбор cs/cb/area_thr + iou/max_det\n                all_combos = []\n                for iou_ in iou_grid:\n                    for md_ in md_grid:\n                        for cs in self.tune_conf_small_grid:\n                            # эффективная база (ниже cs)\n                            base_eff = max(0.03, min(float(self.gate_conf_base), float(cs) - 0.02))\n                            for cb in self.tune_conf_big_grid:\n                                for at in self.tune_area_thr_grid:\n                                    all_combos.append((float(iou_), int(md_), float(cs), float(cb), float(at), float(base_eff)))\n\n                random.shuffle(all_combos)\n                full_space = len(all_combos)\n                if self.tune_max_combinations is not None:\n                    all_combos = all_combos[:int(self.tune_max_combinations)]\n                if self.verbose:\n                    print(f\"[tune-gate] search combos: {len(all_combos)} (cap), full={full_space}\")\n\n                best = dict(rmse=1e9, iou=None, max_det=None, cs=None, cb=None, at=None, base=None)\n                for (iou_, md_, cs, cb, at, base_eff) in all_combos:\n                    conf_area = conf_area_by_key[(iou_, md_)]\n                    y_pred = self._count_with_area_gate(conf_area, cs, cb, at)\n                    rmse = mean_squared_error(y_true, y_pred, squared=False)\n                    if rmse < best[\"rmse\"]:\n                        best.update(dict(rmse=rmse, iou=iou_, max_det=md_,\n                                         cs=cs, cb=cb, at=at, base=base_eff))\n\n                if self.verbose:\n                    print(f\"[tune-gate] best: cs={best['cs']:.3f}, cb={best['cb']:.3f}, \"\n                          f\"area_thr={best['at']:.4f}, iou={best['iou']}, max_det={best['max_det']}  RMSE={best['rmse']:.3f}\")\n\n                self.calib_.update(dict(\n                    best_conf=None, best_iou=float(best['iou']), best_max_det=int(best['max_det']),\n                    gate_conf_small=float(best['cs']), gate_conf_big=float(best['cb']),\n                    gate_area_thr=float(best['at']), gate_conf_base=float(best['base'])\n                ))\n            else:\n                combos = [(float(c), float(i), int(m))\n                          for i in self.tune_iou_grid\n                          for m in self.tune_max_det_grid\n                          for c in self.tune_conf_grid]\n                random.shuffle(combos)\n                full_space = len(combos)\n                if self.tune_max_combinations is not None:\n                    combos = combos[:int(self.tune_max_combinations)]\n                if self.verbose:\n                    print(f\"[tune-plain] search combos: {len(combos)} (cap), full={full_space}\")\n\n                best = dict(rmse=1e9, conf=None, iou=None, max_det=None)\n                for conf, iou_v, max_det in combos:\n                    y_pred = self._raw_counts(paths, imgsz=imgsz, conf=conf, iou=iou_v, max_det=max_det)\n                    rmse = mean_squared_error(y_true, y_pred, squared=False)\n                    if rmse < best[\"rmse\"]:\n                        best.update(dict(rmse=rmse, conf=conf, iou=iou_v, max_det=max_det))\n                if self.verbose:\n                    print(f\"[tune] best plain count: conf={best['conf']}, iou={best['iou']}, max_det={best['max_det']}  RMSE={best['rmse']:.3f}\")\n\n                self.calib_.update(dict(\n                    best_conf=best['conf'], best_iou=best['iou'], best_max_det=best['max_det'],\n                    gate_conf_small=None, gate_conf_big=None, gate_area_thr=None, gate_conf_base=None\n                ))\n        else:\n            # без тюнинга — дефолты для plain\n            self.calib_.update(dict(\n                best_conf=0.25, best_iou=0.5, best_max_det=1000,\n                gate_conf_small=None, gate_conf_big=None, gate_area_thr=None, gate_conf_base=None\n            ))\n\n        # 2) Калибровка Ridge (по резидуалу, с CV)\n        ridge_model, ridge_alpha = None, None\n        if self.enable_ridge:\n            # строим y_plain на том же сабсете val_sub\n            if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n                iou_use = float(self.calib_[\"best_iou\"])\n                md_use  = int(self.calib_[\"best_max_det\"])\n                conf_area = self._detect_conf_area(paths, imgsz=imgsz,\n                                                   conf_base=float(self.calib_[\"gate_conf_base\"]),\n                                                   iou=iou_use, max_det=md_use)\n                y_plain = self._count_with_area_gate(conf_area,\n                                                     float(self.calib_[\"gate_conf_small\"]),\n                                                     float(self.calib_[\"gate_conf_big\"]),\n                                                     float(self.calib_[\"gate_area_thr\"]))\n                X = self._yolo_feats(paths, imgsz=imgsz,\n                                     conf=float(self.calib_[\"gate_conf_base\"]),\n                                     iou=iou_use, max_det=md_use)\n            else:\n                conf_use = float(self.calib_[\"best_conf\"] if self.calib_.get(\"best_conf\") is not None else 0.25)\n                iou_use  = float(self.calib_[\"best_iou\"]  if self.calib_.get(\"best_iou\")  is not None else 0.5)\n                md_use   = int(self.calib_[\"best_max_det\"] if self.calib_.get(\"best_max_det\") is not None else 1000)\n                y_plain  = self._raw_counts(paths, imgsz=imgsz, conf=conf_use, iou=iou_use, max_det=md_use)\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=conf_use, iou=iou_use, max_det=md_use)\n\n            pack = self._fit_ridge_cv_on_residual(X, y_true, y_plain)\n            ridge_model, ridge_alpha = pack[\"model\"], pack[\"alpha\"]\n            if self.verbose:\n                print(f\"[calib] Ridge(residual) alpha={ridge_alpha}  CV-RMSE(resid)={pack['cv_rmse']:.3f}\")\n\n            # сохраним стандартизацию\n            self.calib_.update(dict(\n                ridge_alpha=ridge_alpha, ridge_model=ridge_model,\n                ridge_mu=pack[\"mu\"], ridge_sd=pack[\"sd\"], imgsz=imgsz\n            ))\n        else:\n            self.calib_.update(dict(ridge_alpha=None, ridge_model=None, imgsz=imgsz))\n\n    # -------------------- финальная валидация подсчёта --------------------\n    def _validate_counting(self, val_df: pd.DataFrame):\n        paths = val_df[self.image_col].tolist()\n        y_true = np.array([len(self._parse_boxes(r)) for _, r in val_df.iterrows()], dtype=float)\n\n        imgsz = self.calib_['imgsz'] or 640\n\n        # plain/gate\n        if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n            iou_use = float(self.calib_[\"best_iou\"])\n            md_use  = int(self.calib_[\"best_max_det\"])\n            conf_area = self._detect_conf_area(paths, imgsz=imgsz, conf_base=self.calib_[\"gate_conf_base\"],\n                                               iou=iou_use, max_det=md_use)\n            y_plain = self._count_with_area_gate(conf_area,\n                                                 self.calib_[\"gate_conf_small\"],\n                                                 self.calib_[\"gate_conf_big\"],\n                                                 self.calib_[\"gate_area_thr\"])\n        else:\n            conf  = self.calib_['best_conf'] if self.enable_tuning else 0.25\n            iou_v   = self.calib_['best_iou']  if self.enable_tuning else 0.5\n            max_det = self.calib_['best_max_det'] if self.enable_tuning else 1000\n            y_plain = self._raw_counts(paths, imgsz, conf, iou_v, max_det)\n\n        rmse_plain = mean_squared_error(y_true, y_plain, squared=False)\n        mae_plain  = mean_absolute_error(y_true, y_plain)\n\n        # calibrated\n        if self.enable_ridge and self.calib_['ridge_model'] is not None:\n            if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=float(self.calib_[\"gate_conf_base\"]),\n                                     iou=float(self.calib_[\"best_iou\"]), max_det=int(self.calib_[\"best_max_det\"]))\n            else:\n                X = self._yolo_feats(paths, imgsz=imgsz,\n                                     conf=(self.calib_[\"best_conf\"] if self.calib_[\"best_conf\"] is not None else 0.25),\n                                     iou=(self.calib_[\"best_iou\"] if self.calib_[\"best_iou\"] is not None else 0.5),\n                                     max_det=(self.calib_[\"best_max_det\"] if self.calib_[\"best_max_det\"] is not None else 1000))\n            mu = self.calib_.get(\"ridge_mu\"); sd = self.calib_.get(\"ridge_sd\")\n            Xs = (X - mu) / sd\n            resid = self.calib_['ridge_model'].predict(Xs)\n            y_cal = np.clip(y_plain + resid, 0, None)\n            rmse_cal = mean_squared_error(y_true, y_cal, squared=False)\n            mae_cal  = mean_absolute_error(y_true, y_cal)\n            print(f\"[val-count] plain: RMSE={rmse_plain:.3f}, MAE={mae_plain:.3f}  |  calibrated: RMSE={rmse_cal:.3f}, MAE={mae_cal:.3f}\")\n        else:\n            print(f\"[val-count] plain: RMSE={rmse_plain:.3f}, MAE={mae_plain:.3f}  |  calibrated: (disabled)\")\n\n    # -------------------- публичный инференс: детекции --------------------\n    @torch.no_grad()\n    def predict(self, df: pd.DataFrame,\n                conf: float = 0.25, iou: float = 0.6,\n                imgsz: int = 640, device: str | int | None = \"auto\",\n                max_det: int = 300, agnostic_nms: bool = False) -> pd.DataFrame:\n        \"\"\"Детекции (калибровка НЕ используется).\"\"\"\n        assert self.image_col in df.columns\n        if self._model is None:\n            self._model = YOLO(self.model_path or self.model_ckpt)\n\n        dev = self._resolve_device(device if device is not None else self._infer_device())\n\n        paths = df[self.image_col].tolist()\n        preds = []\n        for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[predict]\"):\n            batch = paths[i:i+64]\n            res = self._model(batch, conf=conf, iou=iou, imgsz=imgsz,\n                              device=dev, verbose=False, max_det=max_det,\n                              agnostic_nms=agnostic_nms)\n            for r in res:\n                boxes = []\n                if r.boxes is not None and len(r.boxes):\n                    xywhn = r.boxes.xywhn.cpu().numpy()\n                    confv = r.boxes.conf.cpu().numpy()\n                    clsv  = r.boxes.cls.cpu().numpy().astype(int)\n                    for (x,y,w,h), c, k in zip(xywhn, confv, clsv):\n                        boxes.append({\"cls\": int(k), \"conf\": float(c),\n                                      \"x\": float(x), \"y\": float(y), \"w\": float(w), \"h\": float(h)})\n                preds.append({\n                    self.image_col: r.path,\n                    \"count\": len(boxes),\n                    \"boxes_json\": json.dumps(boxes, ensure_ascii=False)\n                })\n        return pd.DataFrame(preds)\n\n    # -------------------- публичный инференс: подсчёт --------------------\n    @torch.no_grad()\n    def predict_counts(self, df: pd.DataFrame,\n                       imgsz: int | None = None,\n                       conf: float | None = None,\n                       iou: float | None = None,\n                       max_det: int | None = None,\n                       device: str | int | None = \"auto\",\n                       clamp_nonneg: bool = True,\n                       do_round: bool = False) -> pd.DataFrame:\n        \"\"\"\n        Подсчёт объектов.\n          - Если enable_ridge=True и калибратор обучен → y = y_plain + Ridge(residual).\n          - Иначе → plain len(dets).\n          - Если enable_area_gate=True и тюнинг выполнен → двупороговая фильтрация (conf_small/conf_big) по area.\n        \"\"\"\n        assert self.image_col in df.columns\n        if self._model is None:\n            self._model = YOLO(self.model_path or self.model_ckpt)\n\n        dev = self._resolve_device(device if device is not None else self._infer_device())\n        imgsz = imgsz or self.calib_['imgsz'] or 640\n        paths = df[self.image_col].tolist()\n\n        # area-gated путь\n        if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n            conf_base = float(self.calib_[\"gate_conf_base\"])\n            iou_use   = float(self.calib_[\"best_iou\"])\n            max_det_use = int(self.calib_[\"best_max_det\"])\n            conf_area = self._detect_conf_area(paths, imgsz=imgsz, conf_base=conf_base, iou=iou_use, max_det=max_det_use)\n            cs, cb, at = float(self.calib_[\"gate_conf_small\"]), float(self.calib_[\"gate_conf_big\"]), float(self.calib_[\"gate_area_thr\"])\n            y_plain = self._count_with_area_gate(conf_area, cs, cb, at)\n\n            if self.enable_ridge and self.calib_.get(\"ridge_model\") is not None:\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=conf_base, iou=iou_use, max_det=max_det_use)\n                # стандартизация и предсказание резидуала\n                mu = self.calib_.get(\"ridge_mu\"); sd = self.calib_.get(\"ridge_sd\")\n                Xs = (X - mu) / sd\n                resid = self.calib_['ridge_model'].predict(Xs)\n                y = y_plain + resid\n            else:\n                y = y_plain\n\n        else:\n            # обычный plain путь\n            if self.enable_tuning and self.calib_.get(\"best_conf\") is not None:\n                conf_def, iou_def, max_det_def = self.calib_['best_conf'], self.calib_['best_iou'], self.calib_['best_max_det']\n            else:\n                conf_def, iou_def, max_det_def = 0.25, 0.5, 1000\n\n            use_conf  = conf    if conf    is not None else conf_def\n            use_iou   = iou     if iou     is not None else iou_def\n            use_maxdet= max_det if max_det is not None else max_det_def\n\n            if self.enable_ridge and self.calib_.get(\"ridge_model\") is not None:\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=use_conf, iou=use_iou, max_det=use_maxdet)\n                mu = self.calib_.get(\"ridge_mu\"); sd = self.calib_.get(\"ridge_sd\")\n                Xs = (X - mu) / sd\n                resid = self.calib_['ridge_model'].predict(Xs)\n                # базовый plain-счёт для сложения с резидуалом:\n                y_plain = self._raw_counts(paths, imgsz=imgsz, conf=use_conf, iou=use_iou, max_det=use_maxdet)\n                y = y_plain + resid\n            else:\n                y = self._raw_counts(paths, imgsz=imgsz, conf=use_conf, iou=use_iou, max_det=use_maxdet)\n\n        if clamp_nonneg: y = np.clip(y, 0, None)\n        if do_round:     y = np.rint(y)\n\n        out = df[[self.image_col]].copy()\n        out[\"label\"] = y\n        return out\n\n    # -------------------- housekeeping --------------------\n    def cleanup(self):\n        if self._tmpdir_owned and os.path.isdir(self.data_root):\n            shutil.rmtree(self.data_root, ignore_errors=True)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования. (Специализирован для подсчёта объектов)","metadata":{}},{"cell_type":"code","source":"pipe = YOLODetectionPipeline(\n    model_ckpt=\"yolov8l.pt\",\n    image_col=\"image_path\",\n    boxes_col=\"label\",\n    class_names=[\"obj\"],\n    verbose=True,\n    use_symlinks=True,\n\n    enable_tuning=True,\n    enable_ridge=True,\n    validate_count=True,\n\n    enable_area_gate=True,\n    enable_tta_flip=False,\n\n    tune_val_subsample=None,\n    tune_max_combinations=500,\n\n    tune_conf_grid=(0.18, 0.21, 0.24, 0.27, 0.30, 0.33, 0.36, 0.39, 0.42),\n    tune_iou_grid=(0.4, 0.45, 0.50, 0.55, 0.60, 0.65),\n    tune_max_det_grid=(50, 100, 200, 300, 600),\n    tune_conf_small_grid=(0.08, 0.10, 0.12, 0.14, 0.16, 0.18, 0.20),\n    tune_conf_big_grid=(0.28, 0.32, 0.36, 0.40, 0.45, 0.50, 0.55),\n    tune_area_thr_grid=(0.0006, 0.0008, 0.0010, 0.0012, 0.0015, 0.0018),\n    ridge_alpha_grid=(0.01, 0.03, 0.05, 0.075, 0.1, 0.3, 0.6, 1.0, 2.0),\n\n    gate_conf_base=0.07,\n    small_thr=0.0010,\n    big_thr=0.003,\n\n    random_state=42\n)\n\npipe.fit(\n    train_df=train_df,\n    val_df=val_df,\n    epochs=60,\n    imgsz=640,\n    batch=32,\n    device='0,1',\n    workers=4,\n    lr0=0.01,\n    cos_lr=True,\n    rect=True,\n    iou=0.5\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}