{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import os\n# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# !pip install evaluate wav2clip\n\n# # core/utils.py\n# import os\n# import random\n# import numpy as np\n# import torch\n# from PIL import Image\n\n# def set_seed(seed: int = 42):\n#     random.seed(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed_all(seed)\n\n# def to_pil(x):\n#     if isinstance(x, Image.Image): return x.convert(\"RGB\")\n#     if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n#     if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n#     raise ValueError(\"Expected path/np.ndarray/PIL.Image\")\n\n# def load_audio(path: str, target_sr: int) -> np.ndarray:\n#     try:\n#         import torchaudio\n#     except Exception as e:\n#         raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n#     waveform, sr = torchaudio.load(path)\n#     if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n#     if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n#     return waveform.squeeze(0).numpy().astype(np.float32)\n\n# def safe_load(component_cls, checkpoint: str, cache_dir: str = \"./model_cache\",\n#               local_files_only: bool = None, **kwargs):\n#     if local_files_only is None:\n#         local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n#     name = getattr(component_cls, \"__name__\", \"\")\n#     if \"Tokenizer\" in name:\n#         kwargs.setdefault(\"use_fast\", True)\n#     return component_cls.from_pretrained(\n#         checkpoint, cache_dir=cache_dir, local_files_only=local_files_only, **kwargs\n#     )\n\n# # core/types.py\n# import torch\n\n# def preferred_device():\n#     if torch.cuda.is_available():\n#         return torch.device(\"cuda\")\n#     if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n#         return torch.device(\"mps\")\n#     return torch.device(\"cpu\")\n\n# # core/device.py\n# from typing import Any, Dict, List, Protocol, Literal\n# import torch\n\n# Modality = Literal[\"text\", \"image\", \"audio\"]\n\n# class Processor(Protocol):\n#     modality: Modality\n#     def prepare_batch(self, raw_items: List[Any]) -> Dict[str, torch.Tensor]:\n#         ...\n\n# class Encoder(Protocol):\n#     modality: Modality\n#     embed_dim: int\n#     def encode_batch(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n#         ...\n\n# class ItemAggregator(Protocol):\n#     def aggregate(self, embs: torch.Tensor, counts: List[int], max_items: int, how: str) -> torch.Tensor:\n#         ...\n\n# class Fusion(Protocol):\n#     def __call__(self, zs: Dict[Modality, torch.Tensor]) -> torch.Tensor:\n#         ...\n\n# # data/tokenization.py\n# from functools import lru_cache\n# from typing import Dict, List, Optional\n# import numpy as np\n# import torch\n\n# class BatchTokenizer:\n#     def __init__(self, tokenizer, max_length=512, cache_size=10000, batch_size=256, padding_strategy=\"max_length\"):\n#         assert padding_strategy in {\"max_length\", \"dynamic\"}\n#         self.tok = tokenizer\n#         self.max_length = max_length\n#         self.batch_size = batch_size\n#         self.padding_strategy = padding_strategy\n#         self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n#         self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n#         if self.is_fast:\n#             print(\"✓ Используется Fast Tokenizer\")\n\n#     def _tokenize_single(self, text: str) -> Dict[str, np.ndarray]:\n#         res = self.tok(text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n#         return {k: v.squeeze(0).cpu().numpy() for k, v in res.items()}\n\n#     def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n#         if self.padding_strategy == \"dynamic\":\n#             res = self.tok(texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n#             return {k: (v.long() if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else v) for k, v in res.items()}\n#         if use_cache and len(texts) < 100:\n#             items = [self._cache(t) for t in texts]\n#             keys = items[0].keys()\n#             out = {}\n#             for k in keys:\n#                 dtype = torch.long if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else torch.float32\n#                 out[k] = torch.tensor(np.stack([it[k] for it in items]), dtype=dtype)\n#             return out\n#         res = self.tok(texts, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n#         return {k: (v.long() if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else v) for k, v in res.items()}\n\n#     def tokenize_dataset_lazy(self, texts: List[str], batch_size: Optional[int] = None):\n#         b = batch_size or self.batch_size\n#         for i in range(0, len(texts), b):\n#             yield self.tokenize_batch(texts[i:i+b], use_cache=False)\n\n#     def clear_cache(self):\n#         self._cache.cache_clear()\n\n# # data/dataset.py\n# import gc\n# from typing import Any, Dict, List, Optional, Callable\n# import numpy as np\n# import pandas as pd\n# import torch\n# from torch.utils.data import Dataset\n\n# class MultiModalDataset(Dataset):\n#     \"\"\"\n#     Универсальный датасет для классификации и регрессии.\n#     Поддерживает:\n#       - text pretokenize: BatchTokenizer или кастомные функции (single/batched)\n#       - dynamic/max паддинг (dynamic отключает предтокенизацию)\n#       - мульти-изображения и мульти-аудио на сэмпл\n#     \"\"\"\n#     def __init__(\n#         self,\n#         df: pd.DataFrame,\n#         target_col: Optional[str],                 # для регрессии — колонка с np.ndarray; для классификации — метка\n#         task: str = \"regression\",                  # \"regression\" | \"classification\"\n#         label2id: Optional[Dict[Any, int]] = None, # для классификации\n#         text_columns: Optional[List[str]] = None,\n#         image_columns: Optional[List[str]] = None,\n#         audio_columns: Optional[List[str]] = None,\n#         text_tokenizer=None,                       # BatchTokenizer\n#         text_tokenizer_fn: Optional[Callable] = None,\n#         text_tokenizer_fn_batched: Optional[Callable] = None,\n#         special_tokens: Optional[Dict[str, Any]] = None,\n#         pretokenize: bool = False,\n#         pretokenize_batch_size: int = 256,\n#         tokenizer_returns_tensors: bool = False,\n#         deduplicate_texts: bool = True\n#     ):\n#         super().__init__()\n#         self.df = df.reset_index(drop=True)\n#         self.target_col = target_col\n#         self.task = task.lower()\n#         self.label2id = label2id or {}\n#         self.text_columns = text_columns or []\n#         self.image_columns = image_columns or []\n#         self.audio_columns = audio_columns or []\n\n#         self.batch_tokenizer = text_tokenizer\n#         self.text_tokenizer_fn = text_tokenizer_fn\n#         self.text_tokenizer_fn_batched = text_tokenizer_fn_batched\n\n#         self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n#         self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n#         self._N = len(self.df)\n\n#         # labels\n#         if self.task == \"classification\":\n#             if self.target_col is None:\n#                 y = np.zeros(self._N, dtype=np.int64)\n#             else:\n#                 y = self.df[self.target_col].map(self.label2id).fillna(0).astype(int).values\n#             self._labels = torch.tensor(y, dtype=torch.long)\n#         else:\n#             self._labels = self._prepare_labels_regression(self.df, self.target_col)\n\n#         # lists for images/audios\n#         self._image_lists = self._collect_multi_values(self.df, self.image_columns) if self.image_columns else None\n#         self._audio_lists = self._collect_multi_values(self.df, self.audio_columns) if self.audio_columns else None\n\n#         self._tok_bank: Optional[Dict[str, torch.Tensor]] = None\n#         self._has_text = bool(self.text_columns)\n\n#         # dynamic padding -> disable pretokenization\n#         if pretokenize and self.batch_tokenizer is not None and getattr(self.batch_tokenizer, \"padding_strategy\", \"max_length\") == \"dynamic\":\n#             print(\"⚠ Предтокенизация отключена: выбран dynamic-паддинг для текста.\")\n#             pretokenize = False\n\n#         if self._has_text and pretokenize:\n#             if self.batch_tokenizer is not None and self.text_tokenizer_fn is None and self.text_tokenizer_fn_batched is None:\n#                 self._pretokenize_with_batch_tokenizer(pretokenize_batch_size)\n#             else:\n#                 self._pretokenize_with_custom_fn(pretokenize_batch_size, deduplicate_texts)\n\n#     # labels for regression: [N, K]\n#     def _prepare_labels_regression(self, df: pd.DataFrame, target_col: Optional[str]) -> torch.Tensor:\n#         if target_col is None or target_col not in df.columns:\n#             return torch.zeros((len(df), 1), dtype=torch.float32)\n#         labels_list = []\n#         for i in range(len(df)):\n#             v = df.iloc[i][target_col]\n#             if isinstance(v, (list, tuple, np.ndarray)):\n#                 arr = np.asarray(v, dtype=np.float32)\n#             else:\n#                 try:\n#                     arr = np.asarray([float(v)], dtype=np.float32)\n#                 except Exception:\n#                     arr = np.asarray([0.0], dtype=np.float32)\n#             labels_list.append(arr)\n#         K = max(a.shape[0] for a in labels_list) if labels_list else 1\n#         out = np.zeros((len(labels_list), K), dtype=np.float32)\n#         for i, a in enumerate(labels_list):\n#             out[i, :a.shape[0]] = a\n#         return torch.tensor(out, dtype=torch.float32)\n\n#     def _join_text(self, row: pd.Series) -> str:\n#         sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n#         return sep.join([(\"\" if pd.isna(row[c]) else str(row[c])) for c in self.text_columns])\n\n#     @staticmethod\n#     def _as_list(v):\n#         if v is None or (isinstance(v, float) and np.isnan(v)):\n#             return []\n#         if isinstance(v, (list, tuple)):\n#             return list(v)\n#         return [v]\n\n#     def _collect_multi_values(self, df: pd.DataFrame, columns: List[str]) -> List[List[Any]]:\n#         out = []\n#         as_list = self._as_list\n#         for _, row in df.iterrows():\n#             lst: List[Any] = []\n#             for c in columns:\n#                 if c in row:\n#                     lst.extend([x for x in as_list(row[c]) if x is not None])\n#             out.append(lst)\n#         return out\n\n#     def _pretokenize_with_batch_tokenizer(self, batch_size: int):\n#         print(\"Предтокенизация с BatchTokenizer...\")\n#         texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n#         banks: Dict[str, List[torch.Tensor]] = {}\n#         for start in range(0, self._N, batch_size):\n#             batch = texts[start:start + batch_size]\n#             tok = self.batch_tokenizer.tokenize_batch(batch, use_cache=False)\n#             for k in tok:\n#                 if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\"):\n#                     tok[k] = tok[k].long()\n#                 else:\n#                     tok[k] = tok[k].to(torch.float32)\n#             for k, v in tok.items():\n#                 banks.setdefault(k, []).append(v)\n#         self._tok_bank = {k: torch.cat(v_parts, dim=0).contiguous() for k, v_parts in banks.items()}\n#         shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n#         print(f\"✓ Предтокенизация завершена: {self._N} образцов | keys={list(self._tok_bank.keys())}, shapes={shapes}\")\n\n#     def _pretokenize_with_custom_fn(self, batch_size: int, deduplicate_texts: bool):\n#         cols = list(self.text_columns)\n#         if self.text_tokenizer_fn_batched is not None:\n#             print(\"Предтокенизация кастомной batched-функцией...\")\n#             first_end = min(self._N, max(8, batch_size))\n#             batch_data = []\n#             for i in range(first_end):\n#                 row = self.df.iloc[i]\n#                 d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n#                 batch_data.append(d)\n#             first_tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n#             if not isinstance(first_tok, dict):\n#                 raise ValueError(\"text_tokenizer_fn_batched должна возвращать dict тензоров [B, L]\")\n#             bank: Dict[str, torch.Tensor] = {}\n#             for k, t in first_tok.items():\n#                 if not torch.is_tensor(t): t = torch.tensor(t)\n#                 bank[k] = torch.empty((self._N, t.size(1)), dtype=t.dtype)\n#                 bank[k][:first_end] = t[:first_end]\n#             for start in range(first_end, self._N, batch_size):\n#                 end = min(self._N, start + batch_size)\n#                 batch_data = []\n#                 for i in range(start, end):\n#                     row = self.df.iloc[i]\n#                     d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n#                     batch_data.append(d)\n#                 tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n#                 for k, t in tok.items():\n#                     if not torch.is_tensor(t): t = torch.tensor(t)\n#                     bank[k][start:end] = t\n#             self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n#             shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n#             print(f\"✓ Предтокенизация кастомной batched-функцией завершена: shapes={shapes}\")\n#             return\n\n#         print(\"Предтокенизация кастомной single-функцией...\")\n#         col_arrays = [self.df[c].astype(str).where(~self.df[c].isna(), other=\"\").tolist() for c in cols]\n#         first_td = {c: col_arrays[i][0] for i, c in enumerate(cols)}\n#         first_tok = self.text_tokenizer_fn(first_td, self.special_tokens)\n#         if not isinstance(first_tok, dict):\n#             raise ValueError(\"custom text_tokenizer_fn должна возвращать dict тензоров\")\n#         for k, t in first_tok.items():\n#             if not torch.is_tensor(t): first_tok[k] = torch.tensor(t)\n#         bank: Dict[str, torch.Tensor] = {k: torch.empty((self._N, *t.shape), dtype=t.dtype) for k, t in first_tok.items()}\n#         for k, t in first_tok.items():\n#             bank[k][0].copy_(t)\n\n#         cache: Dict[tuple, Dict[str, torch.Tensor]] = {}\n#         if deduplicate_texts:\n#             key0 = tuple(first_td.get(c, \"\") for c in cols)\n#             cache[key0] = {k: v.clone() for k, v in first_tok.items()}\n\n#         for i in range(1, self._N):\n#             td = {c: col_arrays[j][i] for j, c in enumerate(cols)}\n#             if deduplicate_texts:\n#                 key = tuple(td.get(c, \"\") for c in cols)\n#                 tok = cache.get(key)\n#                 if tok is None:\n#                     tok = self.text_tokenizer_fn(td, self.special_tokens)\n#                     for k, t in tok.items():\n#                         if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n#                     cache[key] = {k: v.clone() for k, v in tok.items()}\n#             else:\n#                 tok = self.text_tokenizer_fn(td, self.special_tokens)\n#                 for k, t in tok.items():\n#                     if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n#             for k, t in tok.items():\n#                 bank[k][i].copy_(t)\n\n#         self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n#         shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n#         print(f\"✓ Предтокенизация кастомной single-функцией завершена: shapes={shapes}\")\n\n#     def __len__(self):\n#         return self._N\n\n#     def __getitem__(self, idx: int) -> Dict[str, Any]:\n#         item: Dict[str, Any] = {}\n#         item[\"labels\"] = self._labels[idx]  # [K] float32 для регрессии, int64 для классификации\n#         if self.text_columns:\n#             if self._tok_bank is not None:\n#                 item[\"text_tokens\"] = {k: v[idx] for k, v in self._tok_bank.items()}\n#             else:\n#                 item[\"text\"] = self._join_text(self.df.iloc[idx])\n#         if self._image_lists is not None:\n#             item[\"images\"] = self._image_lists[idx]\n#         if self._audio_lists is not None:\n#             item[\"audios\"] = self._audio_lists[idx]\n#         return item\n\n#     def get_cache_stats(self):\n#         has = self._tok_bank is not None\n#         sizes = {k: tuple(v.shape) for k, v in (self._tok_bank or {}).items()}\n#         return {\"has_pretokenized\": has, \"shapes\": sizes, \"N\": self._N}\n\n#     def clear_cache(self):\n#         self._tok_bank = None\n#         gc.collect()\n#         if torch.cuda.is_available():\n#             torch.cuda.empty_cache()\n\n# # data/collate.py\n# from typing import Any, Dict, List, Optional\n# import numpy as np\n# import torch\n# # from mmkit.core.utils import to_pil, load_audio\n\n# class MultiModalCollator:\n#     \"\"\"\n#     Готовит backend_inputs для модели.\n#     - Если в item есть \"text_tokens\": стакает их.\n#     - Иначе использует processors[\"text\"] для батчевой токенизации.\n#     - Изображения/аудио: разворачивает в плоский список + считает counts.\n#     \"\"\"\n#     def __init__(self, processors: Dict[str, Any], task: str = \"regression\", audio_sr_fallback: int = 16000):\n#         self.processors = processors\n#         self.task = task.lower()\n#         self.audio_sr_fallback = audio_sr_fallback\n\n#     def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n#         labels = [b.get(\"labels\") for b in batch]\n#         if labels and isinstance(labels[0], torch.Tensor):\n#             labels = torch.stack(labels)\n#         else:\n#             if self.task == \"classification\":\n#                 labels = torch.tensor(labels, dtype=torch.long)\n#             else:\n#                 labels = torch.tensor(labels, dtype=torch.float32)\n\n#         out = {\"labels\": labels, \"backend_inputs\": {}}\n#         # text\n#         if \"text\" in self.processors or any((\"text_tokens\" in b) for b in batch):\n#             if \"text_tokens\" in batch[0]:\n#                 t0 = batch[0][\"text_tokens\"]\n#                 text_inputs = {}\n#                 for key in t0.keys():\n#                     if torch.is_tensor(t0[key]):\n#                         text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n#                     else:\n#                         dtype = torch.long if key in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else torch.float32\n#                         text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n#             else:\n#                 texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n#                 text_inputs = self.processors[\"text\"].prepare_batch(texts)\n#             out[\"backend_inputs\"][\"text_inputs\"] = text_inputs\n\n#         # images\n#         if \"image\" in self.processors:\n#             flat_images, counts = [], []\n#             for b in batch:\n#                 lst = b.get(\"images\", []) or []\n#                 lst = [to_pil(x) for x in lst if x is not None]\n#                 counts.append(len(lst))\n#                 flat_images.extend(lst)\n#             if len(flat_images) > 0:\n#                 out[\"backend_inputs\"][\"image_inputs\"] = self.processors[\"image\"].prepare_batch(flat_images)\n#             else:\n#                 out[\"backend_inputs\"][\"image_inputs\"] = {\"pixel_values\": None}\n#             out[\"backend_inputs\"][\"image_counts\"] = torch.tensor(counts, dtype=torch.long)\n\n#         # audio\n#         if \"audio\" in self.processors:\n#             flat, counts = [], []\n#             for b in batch:\n#                 lst = b.get(\"audios\", []) or []\n#                 counts.append(len(lst))\n#                 for a in lst:\n#                     if isinstance(a, str):\n#                         sr = getattr(self.processors[\"audio\"], \"sr\", self.audio_sr_fallback)\n#                         flat.append(load_audio(a, sr))\n#                     else:\n#                         arr = np.asarray(a, dtype=np.float32)\n#                         if arr.ndim>1: arr = np.squeeze(arr)\n#                         if arr.ndim>1: arr = arr.reshape(-1)\n#                         flat.append(arr)\n#             out[\"backend_inputs\"][\"audio_counts\"] = torch.tensor(counts, dtype=torch.long)\n#             out[\"backend_inputs\"][\"audio_inputs\"] = self.processors[\"audio\"].prepare_batch(flat) if len(flat)>0 else {\"input_values\": None, \"input_features\": None, \"raw_audios\": []}\n\n#         out[\"backend_inputs\"][\"batch_size\"] = len(batch)\n#         return out\n\n# # processors/text.py\n# # from mmkit.data.tokenization import BatchTokenizer\n\n# class TextProcessor:\n#     modality = \"text\"\n#     def __init__(self, tokenizer, max_length=512, padding=\"max_length\", cache_size=10000, batch_size=256):\n#         self.bt = BatchTokenizer(tokenizer, max_length=max_length, cache_size=cache_size, batch_size=batch_size, padding_strategy=padding)\n#     def prepare_batch(self, texts):\n#         return self.bt.tokenize_batch(texts, use_cache=True)\n#     def clear_cache(self):\n#         self.bt.clear_cache()\n\n# # processors/image.py\n# class ImageProcessor:\n#     modality = \"image\"\n#     def __init__(self, hf_processor):\n#         self.proc = hf_processor\n#     def prepare_batch(self, images):\n#         if len(images)==0: return {\"pixel_values\": None}\n#         x = self.proc(images=images, return_tensors=\"pt\")\n#         return {\"pixel_values\": x[\"pixel_values\"]}\n\n# # processors/audio.py\n# class ClapAudioProcessor:\n#     modality=\"audio\"\n#     def __init__(self, hf_processor, sr=48000):\n#         self.proc = hf_processor\n#         self.sr = sr\n#     def prepare_batch(self, raw_list):\n#         x = self.proc(audios=raw_list, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n#         return {\"input_features\": x[\"input_features\"]}\n\n# class Wav2VecAudioProcessor:\n#     modality=\"audio\"\n#     def __init__(self, hf_processor, sr=16000):\n#         self.proc = hf_processor\n#         self.sr = sr\n#     def prepare_batch(self, raw_list):\n#         x = self.proc(raw_list, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n#         return {\"input_values\": x[\"input_values\"]}\n\n# class Wav2ClipAudioProcessor:\n#     modality=\"audio\"\n#     def __init__(self, sr=16000):\n#         self.sr = sr\n#     def prepare_batch(self, raw_list):\n#         return {\"raw_audios\": raw_list}  # список np.ndarray; энкодер сам обработает\n\n# # encoders/base.py\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class BaseEncoder(nn.Module):\n#     modality: str = \"text\"\n#     embed_dim: int = 0\n\n#     def _normalize(self, x: torch.Tensor) -> torch.Tensor:\n#         if x.dim() > 2:\n#             x = x.view(x.size(0), -1)\n#         return F.normalize(x, dim=-1, eps=1e-12)\n\n# # encoders/text_auto.py\n# import torch\n# from transformers import AutoModel\n# # from mmkit.core.utils import safe_load\n# # from .base import BaseEncoder\n\n# class AutoTextEncoder(BaseEncoder):\n#     modality=\"text\"\n#     def __init__(self, checkpoint=\"bert-base-multilingual-cased\", cache_dir=\"./model_cache\"):\n#         super().__init__()\n#         self.model = safe_load(AutoModel, checkpoint, cache_dir)\n#         self.embed_dim = self.model.config.hidden_size\n\n#     @torch.no_grad()\n#     def encode_batch(self, inputs):\n#         out = self.model(**inputs)\n#         z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state.mean(dim=1)\n#         return self._normalize(z)\n\n# # encoders/text_clip.py\n# import torch\n# from transformers import CLIPModel\n# # from mmkit.core.utils import safe_load\n# # from .base import BaseEncoder\n\n# class CLIPTextEncoder(BaseEncoder):\n#     modality=\"text\"\n#     def __init__(self, checkpoint=\"openai/clip-vit-base-patch32\", cache_dir=\"./model_cache\"):\n#         super().__init__()\n#         self.model = safe_load(CLIPModel, checkpoint, cache_dir)\n#         self.embed_dim = self.model.config.projection_dim\n\n#     @torch.no_grad()\n#     def encode_batch(self, inputs):\n#         z = self.model.get_text_features(**inputs)\n#         return self._normalize(z)\n\n# # encoders/image_auto.py\n# import torch\n# from transformers import AutoModel\n# # from mmkit.core.utils import safe_load\n# # from .base import BaseEncoder\n\n# class AutoImageEncoder(BaseEncoder):\n#     modality=\"image\"\n#     def __init__(self, checkpoint=\"google/vit-base-patch16-224\", cache_dir=\"./model_cache\"):\n#         super().__init__()\n#         self.model = safe_load(AutoModel, checkpoint, cache_dir)\n#         self.embed_dim = self.model.config.hidden_size\n\n#     @torch.no_grad()\n#     def encode_batch(self, inputs):\n#         pv = inputs.get(\"pixel_values\", None)\n#         if pv is None: \n#             return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n#         out = self.model(pixel_values=pv)\n#         z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state[:, 0]\n#         return self._normalize(z)\n\n# # encoders/image_clip.py\n# import torch\n# from transformers import CLIPModel\n# # from mmkit.core.utils import safe_load\n# # from .base import BaseEncoder\n\n# class CLIPImageEncoder(BaseEncoder):\n#     modality=\"image\"\n#     def __init__(self, checkpoint=\"openai/clip-vit-base-patch32\", cache_dir=\"./model_cache\"):\n#         super().__init__()\n#         self.model = safe_load(CLIPModel, checkpoint, cache_dir)\n#         self.embed_dim = self.model.config.projection_dim\n\n#     @torch.no_grad()\n#     def encode_batch(self, inputs):\n#         pv = inputs.get(\"pixel_values\", None)\n#         if pv is None:\n#             return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n#         z = self.model.get_image_features(pixel_values=pv)\n#         return self._normalize(z)\n\n# # encoders/audio_clap.py\n# import torch\n# from transformers import ClapModel\n# # from mmkit.core.utils import safe_load\n# # from .base import BaseEncoder\n\n# class CLAPAudioEncoder(BaseEncoder):\n#     modality=\"audio\"\n#     def __init__(self, checkpoint=\"laion/clap-htsat-unfused\", cache_dir=\"./model_cache\"):\n#         super().__init__()\n#         self.model = safe_load(ClapModel, checkpoint, cache_dir)\n#         self.embed_dim = getattr(self.model.config, \"projection_dim\", 512)\n\n#     @torch.no_grad()\n#     def encode_batch(self, inputs):\n#         feats = inputs.get(\"input_features\")\n#         if feats is None:\n#             return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n#         z = self.model.get_audio_features(input_features=feats.float())\n#         return self._normalize(z.float())\n\n# # encoders/audio_wav2vec.py\n# import torch\n# from transformers import AutoModel\n# # from mmkit.core.utils import safe_load\n# # from .base import BaseEncoder\n\n# class Wav2Vec2Encoder(BaseEncoder):\n#     modality=\"audio\"\n#     def __init__(self, checkpoint=\"facebook/wav2vec2-base-960h\", cache_dir=\"./model_cache\"):\n#         super().__init__()\n#         self.model = safe_load(AutoModel, checkpoint, cache_dir)\n#         self.embed_dim = getattr(self.model.config, \"hidden_size\", 768)\n\n#     @torch.no_grad()\n#     def encode_batch(self, inputs):\n#         iv = inputs.get(\"input_values\")\n#         if iv is None:\n#             return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n#         out = self.model(input_values=iv.float())\n#         z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state.mean(dim=1)\n#         return self._normalize(z.float())\n\n# # encoders/audio_wav2clip.py\n# import numpy as np\n# import torch\n# # from .base import BaseEncoder\n\n# class Wav2ClipEncoder(BaseEncoder):\n#     modality=\"audio\"\n#     def __init__(self):\n#         super().__init__()\n#         try:\n#             import wav2clip as w2c\n#         except Exception as e:\n#             raise RuntimeError(\"Требуется wav2clip: pip install wav2clip\") from e\n#         self.w2c = w2c\n#         model = None\n#         if hasattr(w2c, \"get_model\"):\n#             model = w2c.get_model()\n#         elif hasattr(w2c, \"model\"):\n#             m = w2c.model\n#             model = m() if callable(m) else m\n#         else:\n#             raise RuntimeError(\"wav2clip не содержит get_model()/model\")\n#         self.model = model\n#         self.embed_dim = 512\n\n#     @torch.no_grad()\n#     def encode_batch(self, inputs):\n#         raws = inputs.get(\"raw_audios\", [])\n#         if len(raws)==0:\n#             return torch.zeros(0, self.embed_dim)\n#         embs = []\n#         for arr in raws:\n#             a = np.asarray(arr, dtype=np.float32)\n#             if a.ndim>1: a = np.squeeze(a)\n#             if a.ndim>1: a = a.reshape(-1)\n#             if a.size < 512:\n#                 a = np.pad(a, (0, 512 - a.size), mode=\"constant\")\n#             try:\n#                 e = self.w2c.embed_audio(a, self.model)\n#                 e = np.asarray(e)\n#             except Exception:\n#                 x = torch.from_numpy(a).float().unsqueeze(0)\n#                 y = self.model(x)\n#                 if isinstance(y, (tuple, list)):\n#                     y = y[0]\n#                 if torch.is_tensor(y):\n#                     if y.dim()==2 and y.size(0)==1: y=y.squeeze(0)\n#                     e = y.detach().cpu().numpy()\n#                 else:\n#                     e = np.asarray(y)\n#             if e.ndim>1: e = e.reshape(-1)\n#             embs.append(torch.as_tensor(e, dtype=torch.float32))\n#         z = torch.stack(embs, dim=0)\n#         z = torch.nn.functional.normalize(z, dim=-1, eps=1e-12)\n#         return z\n\n# # aggregation/item_pool.py\n# import torch\n# import torch.nn.functional as F\n\n# class ItemAggregator:\n#     def aggregate(self, embs: torch.Tensor, counts, max_k: int, how: str):\n#         if embs is None or (torch.is_tensor(embs) and embs.numel()==0):\n#             D = 0 if embs is None else getattr(embs, \"size\", lambda *_: 0)(-1)\n#             out_dim = D*max_k if how==\"concat\" else D\n#             return torch.zeros((len(counts), out_dim), device=embs.device if torch.is_tensor(embs) else \"cpu\", dtype=torch.float32)\n#         if embs.dim()==1: embs = embs.unsqueeze(0)\n#         if embs.dim()>2: embs = embs.view(embs.size(0), -1)\n#         N, D = embs.size()\n#         out_dim = D*max_k if how==\"concat\" else D\n#         out = torch.zeros((len(counts), out_dim), device=embs.device, dtype=embs.dtype)\n#         off = 0\n#         for i, c in enumerate(counts):\n#             if c<=0 or off>=N: continue\n#             take = min(c, N-off)\n#             sample = embs[off:off+take]\n#             off += take\n#             if how==\"concat\":\n#                 sample = sample[:max_k]\n#                 if sample.size(0)<max_k:\n#                     pad = torch.zeros((max_k - sample.size(0), D), device=embs.device, dtype=embs.dtype)\n#                     sample = torch.cat([sample, pad], dim=0)\n#                 out[i] = sample.reshape(-1)\n#             else:\n#                 out[i] = sample.mean(dim=0)\n#         return F.normalize(out, dim=-1, eps=1e-12)\n\n# # aggregation/fusion.py\n# import torch\n\n# class ConcatFusion:\n#     def __call__(self, zs: dict) -> torch.Tensor:\n#         feats = [zs[k] for k in [\"image\",\"text\",\"audio\"] if k in zs]\n#         return torch.cat(feats, dim=-1)\n\n# class MeanFusion:\n#     def __call__(self, zs: dict) -> torch.Tensor:\n#         feats = [zs[k] for k in [\"image\",\"text\",\"audio\"] if k in zs]\n#         return torch.stack(feats, dim=0).mean(dim=0)\n\n# # heads/regression.py\n# import torch.nn as nn\n\n# class RegressionHead(nn.Module):\n#     def __init__(self, in_dim, out_dim, hidden=256, dropout=0.1):\n#         super().__init__()\n#         self.net = nn.Sequential(\n#             nn.LayerNorm(in_dim),\n#             nn.Linear(in_dim, hidden),\n#             nn.GELU(),\n#             nn.Dropout(dropout),\n#             nn.Linear(hidden, out_dim)\n#         )\n#     def forward(self, x):\n#         return self.net(x)\n\n# # heads/classification.py\n# import torch.nn as nn\n\n# class ClassificationHead(nn.Module):\n#     def __init__(self, in_dim, num_labels, hidden=512, dropout=0.1):\n#         super().__init__()\n#         self.net = nn.Sequential(\n#             nn.LayerNorm(in_dim),\n#             nn.Linear(in_dim, hidden),\n#             nn.GELU(),\n#             nn.Dropout(dropout),\n#             nn.Linear(hidden, num_labels)\n#         )\n#     def forward(self, x): return self.net(x)\n\n# # model/multimodal.py\n# from typing import Dict\n# import torch\n# import torch.nn as nn\n# # from mmkit.aggregation.item_pool import ItemAggregator\n\n# class MultiModalModel(nn.Module):\n#     \"\"\"\n#     Модель: dict(encoders) + aggregator (для image/audio множественных) + fusion + head.\n#     \"\"\"\n#     def __init__(self, encoders: Dict[str, nn.Module], aggregator: ItemAggregator, fusion, head,\n#                  image_cfg=None, audio_cfg=None):\n#         super().__init__()\n#         self.enc = nn.ModuleDict(encoders)\n#         self.agg = aggregator\n#         self.fusion = fusion\n#         self.head = head\n#         self.image_cfg = image_cfg or {\"max_items\":1, \"how\":\"concat\"}\n#         self.audio_cfg = audio_cfg or {\"max_items\":1, \"how\":\"concat\"}\n\n#     def _fwd_features(self, backend_inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n#         zs = {}\n#         if \"text\" in self.enc and \"text_inputs\" in backend_inputs:\n#             zs[\"text\"] = self.enc[\"text\"].encode_batch(backend_inputs[\"text_inputs\"])\n#         if \"image\" in self.enc and \"image_inputs\" in backend_inputs:\n#             flat = self.enc[\"image\"].encode_batch(backend_inputs[\"image_inputs\"])\n#             counts = backend_inputs.get(\"image_counts\", torch.tensor([0]*backend_inputs.get(\"batch_size\", flat.size(0)))).tolist()\n#             zs[\"image\"] = self.agg.aggregate(flat, counts, self.image_cfg[\"max_items\"], self.image_cfg[\"how\"])\n#         if \"audio\" in self.enc and \"audio_inputs\" in backend_inputs:\n#             flat = self.enc[\"audio\"].encode_batch(backend_inputs[\"audio_inputs\"])\n#             counts = backend_inputs.get(\"audio_counts\", torch.tensor([0]*backend_inputs.get(\"batch_size\", flat.size(0)))).tolist()\n#             zs[\"audio\"] = self.agg.aggregate(flat, counts, self.audio_cfg[\"max_items\"], self.audio_cfg[\"how\"])\n#         return zs\n\n#     def forward(self, backend_inputs: Dict[str, torch.Tensor], labels: torch.Tensor = None):\n#         zs = self._fwd_features(backend_inputs)\n#         if not zs:\n#             raise ValueError(\"Энкодеры не вернули эмбеддинги\")\n#         fused = self.fusion(zs)\n#         logits = self.head(fused)\n#         return {\"logits\": logits}\n\n#     @torch.no_grad()\n#     def get_embeddings(self, backend_inputs: Dict[str, torch.Tensor], return_per_modality: bool = False):\n#         zs = self._fwd_features(backend_inputs)\n#         fused = self.fusion(zs)\n#         return (fused, zs) if return_per_modality else fused\n\n# # train/trainer_hf.py\n# from typing import Optional\n# import torch\n# import torch.nn.functional as F\n# from transformers import Trainer\n\n# class MSETrainer(Trainer):\n#     def compute_loss(\n#         self,\n#         model,\n#         inputs,\n#         return_outputs: bool = False,\n#         num_items_in_batch: Optional[int] = None,\n#         **kwargs\n#     ):\n#         labels = inputs.pop(\"labels\").to(torch.float32)\n#         backend_inputs = inputs.pop(\"backend_inputs\", None)\n#         if backend_inputs is None:\n#             # fallback: все остальное считаем backend_inputs\n#             backend_inputs = inputs\n\n#         out = model(backend_inputs=backend_inputs)  # передаем строго backend_inputs\n#         logits = out[\"logits\"]\n#         preds = logits if logits.dim() == 1 else (logits.squeeze(-1) if logits.size(-1) == 1 else logits)\n#         labels = labels.view_as(preds)\n#         loss = F.mse_loss(preds, labels)\n#         return (loss, out) if return_outputs else loss\n\n# class WeightedCETrainer(Trainer):\n#     def __init__(self, *args, class_weights=None, **kwargs):\n#         super().__init__(*args, **kwargs)\n#         self.class_weights = (\n#             torch.as_tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n#         )\n\n#     def compute_loss(\n#         self,\n#         model,\n#         inputs,\n#         return_outputs: bool = False,\n#         num_items_in_batch: Optional[int] = None,\n#         **kwargs\n#     ):\n#         labels = inputs.pop(\"labels\")\n#         backend_inputs = inputs.pop(\"backend_inputs\", None)\n#         if backend_inputs is None:\n#             backend_inputs = inputs\n\n#         out = model(backend_inputs=backend_inputs)  # только backend_inputs\n#         logits = out[\"logits\"]\n\n#         weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n#         loss = F.cross_entropy(logits, labels.long(), weight=weight)\n#         return (loss, out) if return_outputs else loss\n\n# # train/callbacks.py\n# from transformers.trainer_callback import TrainerCallback\n# from tqdm.auto import tqdm\n\n# class PbarConsoleLogger(TrainerCallback):\n#     def __init__(self, pbar):\n#         self.pbar = pbar\n#         self.last_logs = {}\n#         self.last_train_loss = None\n#         self.printed_eval_steps = set()\n#         self.tqdm = tqdm\n\n#     def _step(self, state) -> int:\n#         return int(state.global_step or 0)\n\n#     def _fmt_postfix(self):\n#         parts = []\n#         if 'loss' in self.last_logs:\n#             parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n#         if 'eval_loss' in self.last_logs:\n#             parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n#         for k, v in self.last_logs.items():\n#             if k.startswith('eval_') and k not in (\n#                 'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n#             ):\n#                 parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n#         return \" | \".join(parts)\n\n#     def on_step_end(self, args, state, control, **kwargs):\n#         n = min(self._step(state), self.pbar.total)\n#         if n > self.pbar.n:\n#             self.pbar.update(n - self.pbar.n)\n#         if self.last_logs:\n#             self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n#         self.pbar.refresh()\n\n#     def on_log(self, args, state, control, logs=None, **kwargs):\n#         if not logs:\n#             return\n#         for k, v in logs.items():\n#             if isinstance(v, (int, float)):\n#                 self.last_logs[k] = float(v)\n#         if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n#             self.last_train_loss = float(logs['loss'])\n\n#         self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n#         self.pbar.refresh()\n\n#         if any(k.startswith('eval_') for k in logs.keys()):\n#             step = self._step(state)\n#             if step in self.printed_eval_steps:\n#                 return\n#             self.printed_eval_steps.add(step)\n#             train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n#             val_loss = logs.get('eval_loss', None)\n#             val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n#             exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n#             extra_parts = []\n#             for k, v in logs.items():\n#                 if k.startswith('eval_') and k not in exclude:\n#                     metric_name = k.replace('eval_', '')\n#                     extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n#             line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n#             if extra_parts:\n#                 line += \", \" + \", \".join(extra_parts)\n#             self.tqdm.write(line)\n\n#     def on_train_end(self, args, state, control, **kwargs):\n#         n = min(self._step(state), self.pbar.total)\n#         if n > self.pbar.n:\n#             self.pbar.update(n - self.pbar.n)\n#         self.pbar.refresh()\n\n# # train/metrics.py\n# import numpy as np\n\n# def build_regression_metrics(name: str):\n#     name = name.lower()\n#     if name not in (\"rmse\", \"mae\", \"r2\"):\n#         raise ValueError('metric_name для регрессии должен быть \"rmse\", \"mae\" или \"r2\"')\n\n#     def compute(p):\n#         preds = p.predictions\n#         y = p.label_ids\n#         preds = preds.squeeze(-1) if preds.ndim == 2 and preds.shape[-1] == 1 else preds\n#         y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n#         axis = 1 if preds.ndim == 2 else None\n#         if name == \"rmse\":\n#             err = preds - y\n#             mse = np.mean(err**2, axis=axis)\n#             rmse = np.sqrt(mse)\n#             return {\"rmse\": float(np.mean(rmse))}\n#         elif name == \"mae\":\n#             mae = np.mean(np.abs(preds - y), axis=axis)\n#             return {\"mae\": float(np.mean(mae))}\n#         else:\n#             y_mean = np.mean(y, axis=axis, keepdims=True) if preds.ndim == 2 else np.mean(y)\n#             ss_res = np.sum((y - preds) ** 2, axis=axis)\n#             ss_tot = np.sum((y - y_mean) ** 2, axis=axis)\n#             r2 = 1.0 - (ss_res / (ss_tot + 1e-12))\n#             return {\"r2\": float(np.mean(r2))}\n#     return compute\n\n# # train/recipes.py\n# from transformers import CLIPTokenizer, CLIPImageProcessor, AutoTokenizer, AutoImageProcessor, ClapProcessor, AutoProcessor\n# # from mmkit.processors.text import TextProcessor\n# # from mmkit.processors.image import ImageProcessor\n# # from mmkit.processors.audio import ClapAudioProcessor, Wav2VecAudioProcessor, Wav2ClipAudioProcessor\n\n# # from mmkit.encoders.text_clip import CLIPTextEncoder\n# # from mmkit.encoders.text_auto import AutoTextEncoder\n# # from mmkit.encoders.image_clip import CLIPImageEncoder\n# # from mmkit.encoders.image_auto import AutoImageEncoder\n# # from mmkit.encoders.audio_clap import CLAPAudioEncoder\n# # from mmkit.encoders.audio_wav2vec import Wav2Vec2Encoder\n# # from mmkit.encoders.audio_wav2clip import Wav2ClipEncoder\n\n# # from mmkit.aggregation.item_pool import ItemAggregator\n# # from mmkit.aggregation.fusion import ConcatFusion, MeanFusion\n# # from mmkit.heads.regression import RegressionHead\n# # from mmkit.model.multimodal import MultiModalModel\n\n# def build_auto_regression(modalities,\n#                           fusion=\"concat\",\n#                           # text config\n#                           text_model_type=\"clip\", text_checkpoint=\"openai/clip-vit-base-patch32\", text_max_length=77, text_padding=\"max_length\",\n#                           # image config\n#                           image_model_type=\"clip\", image_checkpoint=\"openai/clip-vit-base-patch32\", max_images=1, image_agg=\"concat\",\n#                           # audio config\n#                           audio_model_type=\"clap\", audio_checkpoint=\"laion/clap-htsat-unfused\", audio_sr=48000, max_audios=1, audio_agg=\"concat\",\n#                           # head config\n#                           out_dim=1, hidden=256, dropout=0.1,\n#                           cache_dir=\"./model_cache\"):\n#     encoders = {}\n#     processors = {}\n#     # text\n#     if \"text\" in modalities:\n#         if text_model_type == \"clip\":\n#             tok = CLIPTokenizer.from_pretrained(text_checkpoint, cache_dir=cache_dir)\n#             processors[\"text\"] = TextProcessor(tok, max_length=text_max_length, padding=text_padding)\n#             encoders[\"text\"] = CLIPTextEncoder(text_checkpoint, cache_dir=cache_dir)\n#             text_dim = encoders[\"text\"].embed_dim\n#         else:\n#             tok = AutoTokenizer.from_pretrained(text_checkpoint, cache_dir=cache_dir)\n#             processors[\"text\"] = TextProcessor(tok, max_length=text_max_length, padding=text_padding)\n#             encoders[\"text\"] = AutoTextEncoder(text_checkpoint, cache_dir=cache_dir)\n#             text_dim = encoders[\"text\"].embed_dim\n#     # image\n#     image_cfg = None\n#     if \"image\" in modalities:\n#         if image_model_type == \"clip\":\n#             ip = CLIPImageProcessor.from_pretrained(image_checkpoint, cache_dir=cache_dir)\n#             processors[\"image\"] = ImageProcessor(ip)\n#             encoders[\"image\"] = CLIPImageEncoder(image_checkpoint, cache_dir=cache_dir)\n#         else:\n#             ip = AutoImageProcessor.from_pretrained(image_checkpoint, cache_dir=cache_dir)\n#             processors[\"image\"] = ImageProcessor(ip)\n#             encoders[\"image\"] = AutoImageEncoder(image_checkpoint, cache_dir=cache_dir)\n#         image_dim = encoders[\"image\"].embed_dim\n#         image_cfg = {\"max_items\": max_images, \"how\": image_agg}\n#     # audio\n#     audio_cfg = None\n#     if \"audio\" in modalities:\n#         if audio_model_type == \"clap\":\n#             ap = ClapProcessor.from_pretrained(audio_checkpoint, cache_dir=cache_dir)\n#             processors[\"audio\"] = ClapAudioProcessor(ap, sr=audio_sr)\n#             encoders[\"audio\"] = CLAPAudioEncoder(audio_checkpoint, cache_dir=cache_dir)\n#         elif audio_model_type == \"wav2vec\":\n#             ap = AutoProcessor.from_pretrained(audio_checkpoint, cache_dir=cache_dir)\n#             processors[\"audio\"] = Wav2VecAudioProcessor(ap, sr=audio_sr)\n#             encoders[\"audio\"] = Wav2Vec2Encoder(audio_checkpoint, cache_dir=cache_dir)\n#         else:  # wav2clip\n#             processors[\"audio\"] = Wav2ClipAudioProcessor(sr=audio_sr)\n#             encoders[\"audio\"] = Wav2ClipEncoder()\n#         audio_dim = encoders[\"audio\"].embed_dim\n#         audio_cfg = {\"max_items\": max_audios, \"how\": audio_agg}\n\n#     agg = ItemAggregator()\n#     fusion_mod = ConcatFusion() if fusion==\"concat\" else MeanFusion()\n#     # compute in_dim\n#     dims = []\n#     for m in [\"image\",\"text\",\"audio\"]:\n#         if m in encoders:\n#             dims.append(encoders[m].embed_dim)\n#     if fusion==\"concat\":\n#         in_dim = sum(dims)\n#     else:\n#         assert len(set(dims))==1, \"Для fusion=mean требуется одинаковая размерность эмбеддингов\"\n#         in_dim = dims[0]\n#     head = RegressionHead(in_dim, out_dim=out_dim, hidden=hidden, dropout=dropout)\n#     model = MultiModalModel(encoders, agg, fusion_mod, head, image_cfg=image_cfg, audio_cfg=audio_cfg)\n#     return processors, model\n\n# # pipelines/regression.py\n# import math\n# import gc\n# from typing import Any, Dict, List, Optional\n# import numpy as np\n# import pandas as pd\n# import torch\n# from torch.utils.data import DataLoader\n# from tqdm.auto import tqdm\n\n# from transformers import TrainingArguments\n# # from mmkit.core.utils import set_seed\n# # from mmkit.data.dataset import MultiModalDataset\n# # from mmkit.data.collate import MultiModalCollator\n# # from mmkit.train.trainer_hf import MSETrainer\n# # from mmkit.train.callbacks import PbarConsoleLogger\n# # from mmkit.train.metrics import build_regression_metrics\n# # from mmkit.train.recipes import build_auto_regression\n\n# class MultiModalRegressionPipeline:\n#     \"\"\"\n#     Высокоуровневый пайплайн мультимодальной регрессии (multi-target).\n#     Поддерживает: text/image/audio, dynamic/max padding, предтокенизацию, чанки, RMSE/MAE/R2.\n#     \"\"\"\n#     def __init__(self,\n#                  modalities: List[str],\n#                  target_column_names: List[str],\n#                  text_columns: Optional[List[str]] = None,\n#                  image_columns: Optional[List[str]] = None,\n#                  audio_columns: Optional[List[str]] = None,\n#                  # recipe params\n#                  backend: str = \"auto\",\n#                  clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n#                  clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n#                  text_model_config: Optional[Dict[str, Any]] = None,\n#                  image_model_config: Optional[Dict[str, Any]] = None,\n#                  audio_model_config: Optional[Dict[str, Any]] = None,\n#                  fusion: str = \"concat\",\n#                  # tokenizer\n#                  text_padding: str = \"max_length\",\n#                  text_max_length: int = 77,\n#                  use_batch_tokenizer: bool = True,\n#                  pretokenize_data: bool = True,\n#                  pretokenize_batch_size: int = 256,\n#                  tokenizer_cache_size: int = 10000,\n#                  max_pretokenize_samples: int = 100000,\n#                  local_cache_dir: str = \"./model_cache\"):\n#         self.modalities = sorted(list(set(modalities)))\n#         self.target_column_names = list(target_column_names)\n#         self.text_columns = text_columns or []\n#         self.image_columns = image_columns or []\n#         self.audio_columns = audio_columns or []\n#         self.backend_name = backend.lower()\n#         self.clip_checkpoint = clip_checkpoint\n#         self.clap_checkpoint = clap_checkpoint\n#         self.text_model_config = text_model_config\n#         self.image_model_config = image_model_config\n#         self.audio_model_config = audio_model_config\n#         self.fusion = fusion\n\n#         self.text_padding = text_padding\n#         self.text_max_length = text_max_length\n#         self.use_batch_tokenizer = use_batch_tokenizer\n#         self.pretokenize_data = pretokenize_data\n#         self.pretokenize_batch_size = pretokenize_batch_size\n#         self.tokenizer_cache_size = tokenizer_cache_size\n#         self.max_pretokenize_samples = max_pretokenize_samples\n#         self.local_cache_dir = local_cache_dir\n\n#         self._target_vec_col = \"__target_vector__\"\n#         self.processors = {}\n#         self.model = None\n#         self.trainer = None\n\n#     def _attach_target_vector(self, df: pd.DataFrame, fill_zeros: bool = False) -> pd.DataFrame:\n#         df_c = df.copy()\n#         K = len(self.target_column_names)\n#         if fill_zeros:\n#             df_c[self._target_vec_col] = [np.zeros(K, dtype=np.float32) for _ in range(len(df_c))]\n#         else:\n#             def _row_to_vec(row):\n#                 vals = [row[c] for c in self.target_column_names]\n#                 return np.asarray(vals, dtype=np.float32)\n#             df_c[self._target_vec_col] = df_c.apply(_row_to_vec, axis=1)\n#         return df_c\n\n#     def _validate_modalities(self, df: pd.DataFrame):\n#         if \"text\" in self.modalities:\n#             assert self.text_columns, \"Вы выбрали text, но text_columns пустой\"\n#             missing = [c for c in self.text_columns if c not in df.columns]\n#             if missing: raise ValueError(f\"Нет текстовых колонок: {missing}\")\n#         if \"image\" in self.modalities:\n#             assert self.image_columns, \"Вы выбрали image, но image_columns пуст\"\n#             missing = [c for c in self.image_columns if c not in df.columns]\n#             if missing: raise ValueError(f\"Нет колонок изображений: {missing}\")\n#         if \"audio\" in self.modalities:\n#             assert self.audio_columns, \"Вы выбрали audio, но audio_columns пуст\"\n#             missing = [c for c in self.audio_columns if c not in df.columns]\n#             if missing: raise ValueError(f\"Нет колонок аудио: {missing}\")\n\n#     def _validate_targets(self, df: pd.DataFrame):\n#         miss = [c for c in self.target_column_names if c not in df.columns]\n#         if miss:\n#             raise ValueError(f\"В DataFrame отсутствуют целевые колонки: {miss}\")\n\n#     def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n#         df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n#         n_eval = int(math.ceil(len(df) * test_size))\n#         return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n#     def _build_model(self, num_outputs: int, image_cfg: Dict[str, Any], audio_cfg: Dict[str, Any]):\n#         # Авто-рецепт: CLIP для text+image, CLAP для audio (или явные конфиги)\n#         if self.backend_name == \"auto\":\n#             # auto defaults\n#             txt_type = \"clip\" if \"image\" in self.modalities else (\"auto\" if \"text\" in self.modalities else None)\n#             img_type = \"clip\" if \"image\" in self.modalities else None\n#             aud_type = \"clap\" if \"audio\" in self.modalities else None\n\n#             txt_cp = self.clip_checkpoint if txt_type==\"clip\" else (self.text_model_config[\"checkpoint\"] if self.text_model_config else \"bert-base-multilingual-cased\")\n#             img_cp = self.clip_checkpoint if img_type==\"clip\" else (self.image_model_config[\"checkpoint\"] if self.image_model_config else \"google/vit-base-patch16-224\")\n#             aud_cp = self.clap_checkpoint if aud_type==\"clap\" else (self.audio_model_config[\"checkpoint\"] if self.audio_model_config else \"facebook/wav2vec2-base-960h\")\n\n#             if aud_type is None and \"audio\" in self.modalities and self.audio_model_config:\n#                 aud_type = self.audio_model_config.get(\"model_type\",\"clap\")\n\n#             processors, model = build_auto_regression(\n#                 modalities=self.modalities,\n#                 fusion=self.fusion,\n#                 text_model_type=\"clip\" if txt_type==\"clip\" else \"auto\",\n#                 text_checkpoint=txt_cp,\n#                 text_max_length=self.text_max_length,\n#                 text_padding=self.text_padding,\n#                 image_model_type=\"clip\" if img_type==\"clip\" else \"auto\",\n#                 image_checkpoint=img_cp,\n#                 max_images=image_cfg.get(\"max_images\", 1),\n#                 image_agg=image_cfg.get(\"image_agg\", \"concat\"),\n#                 audio_model_type=aud_type if aud_type else \"clap\",\n#                 audio_checkpoint=aud_cp,\n#                 audio_sr=audio_cfg.get(\"sr\", 48000),\n#                 max_audios=audio_cfg.get(\"max_audios\", 1),\n#                 audio_agg=audio_cfg.get(\"audio_agg\", \"concat\"),\n#                 out_dim=num_outputs,\n#                 hidden=256,\n#                 dropout=0.1,\n#                 cache_dir=self.local_cache_dir\n#             )\n#             return processors, model\n\n#         # Иначе — можно сделать фабрику из переданных конфигов (опущено для краткости)\n#         raise NotImplementedError(\"Ручная сборка (не auto) не реализована в этой версии\")\n\n#     def fit(\n#         self,\n#         train_data: pd.DataFrame,\n#         epochs: int = 3,\n#         test_size: float = 0.2,\n#         test_data: Optional[pd.DataFrame] = None,\n#         per_device_train_batch_size: int = 16,\n#         gradient_accumulation_steps: int = 1,\n#         learning_rate: float = 2e-4,\n#         metric_name: str = \"rmse\",       # \"rmse\" | \"mae\" | \"r2\"\n#         fp16: bool = True,\n#         logging_steps: int = 50,\n#         eval_steps: int = 200,\n#         output_dir: str = \"./result_reg\",\n#         seed: int = 42,\n#         gradient_checkpointing: bool = False\n#     ):\n#         import os\n#         from transformers import TrainingArguments\n    \n#         # 1) Проверки и подготовка\n#         self._validate_modalities(train_data)\n#         self._validate_targets(train_data)\n#         set_seed(seed)\n    \n#         df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n#         if test_data is not None:\n#             self._validate_targets(test_data)\n    \n#         # Преобразуем таргеты в одну вектор-колонку\n#         df_train_ext = self._attach_target_vector(df_train, fill_zeros=False)\n#         df_eval_ext  = self._attach_target_vector(df_eval,  fill_zeros=False)\n    \n#         # 2) Модель и процессоры (auto-рецепт)\n#         image_cfg = {\"max_images\": 1, \"image_agg\": \"concat\"}\n#         audio_cfg = {\"sr\": 48000, \"max_audios\": 1, \"audio_agg\": \"concat\"}\n    \n#         self.processors, self.model = self._build_model(\n#             num_outputs=len(self.target_column_names),\n#             image_cfg=image_cfg,\n#             audio_cfg=audio_cfg\n#         )\n    \n#         if gradient_checkpointing:\n#             try:\n#                 self.model.gradient_checkpointing_enable()\n#             except Exception:\n#                 pass\n    \n#         # 3) Datasets (без чанков)\n#         has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n#         pretokenize_train = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n#                              and len(df_train_ext) <= self.max_pretokenize_samples)\n#         pretokenize_eval  = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n#                              and len(df_eval_ext)  <= self.max_pretokenize_samples)\n    \n#         ds_train = MultiModalDataset(\n#             df=df_train_ext,\n#             target_col=\"__target_vector__\",\n#             task=\"regression\",\n#             text_columns=self.text_columns,\n#             image_columns=self.image_columns,\n#             audio_columns=self.audio_columns,\n#             text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_train) else None,\n#             pretokenize=pretokenize_train,\n#             pretokenize_batch_size=self.pretokenize_batch_size\n#         )\n#         ds_eval = MultiModalDataset(\n#             df=df_eval_ext,\n#             target_col=\"__target_vector__\",\n#             task=\"regression\",\n#             text_columns=self.text_columns,\n#             image_columns=self.image_columns,\n#             audio_columns=self.audio_columns,\n#             text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_eval) else None,\n#             pretokenize=pretokenize_eval,\n#             pretokenize_batch_size=self.pretokenize_batch_size\n#         )\n    \n#         collate = MultiModalCollator(self.processors, task=\"regression\")\n#         compute_metrics = build_regression_metrics(metric_name)\n    \n#         # 4) Аргументы обучения (eval_strategy)\n#         args = TrainingArguments(\n#             output_dir=output_dir,\n#             num_train_epochs=epochs,\n#             per_device_train_batch_size=per_device_train_batch_size,\n#             per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n#             gradient_accumulation_steps=gradient_accumulation_steps,\n#             eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n#             learning_rate=learning_rate,\n#             warmup_ratio=0.1,\n#             lr_scheduler_type=\"cosine\",\n#             weight_decay=0.0,\n#             eval_strategy=\"steps\",\n#             eval_steps=eval_steps,\n#             save_strategy=\"steps\",\n#             save_steps=eval_steps,\n#             load_best_model_at_end=True,\n#             metric_for_best_model=f\"eval_{metric_name}\",\n#             save_total_limit=1,\n#             logging_strategy=\"steps\",\n#             logging_steps=logging_steps,\n#             report_to=\"none\",\n#             fp16=fp16 and torch.cuda.is_available(),\n#             dataloader_num_workers=min(4, os.cpu_count() or 4),\n#             seed=seed,\n#             remove_unused_columns=False,\n#             gradient_checkpointing=gradient_checkpointing,\n#             dataloader_pin_memory=True,\n#             ddp_find_unused_parameters=False,\n#             disable_tqdm=False\n#         )\n    \n#         # 5) Trainer и старт обучения\n#         self.trainer = MSETrainer(\n#             model=self.model,\n#             args=args,\n#             train_dataset=ds_train,\n#             eval_dataset=ds_eval,\n#             data_collator=collate,\n#             compute_metrics=compute_metrics\n#         )\n    \n#         try:\n#             from transformers.trainer_callback import PrinterCallback\n#             self.trainer.remove_callback(PrinterCallback)\n#         except Exception:\n#             pass\n    \n#         self.trainer.train()\n    \n#         if has_bt:\n#             try: self.processors[\"text\"].clear_cache()\n#             except Exception: pass\n    \n#         return self\n\n#     def predict(self, df: pd.DataFrame, batch_size: Optional[int] = None) -> np.ndarray:\n#         if self.trainer is None:\n#             raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n#         df_c = self._attach_target_vector(df, fill_zeros=True)\n#         has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n\n#         ds = MultiModalDataset(\n#             df=df_c,\n#             target_col=\"__target_vector__\",\n#             task=\"regression\",\n#             text_columns=self.text_columns,\n#             image_columns=self.image_columns,\n#             audio_columns=self.audio_columns,\n#             text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n#             pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n#             pretokenize_batch_size=self.pretokenize_batch_size\n#         )\n#         collate = MultiModalCollator(self.processors, task=\"regression\")\n\n#         if batch_size:\n#             original_bs = self.trainer.args.per_device_eval_batch_size\n#             self.trainer.args.per_device_eval_batch_size = batch_size\n\n#         effective_batch = batch_size or self.trainer.args.per_device_eval_batch_size\n#         num_batches = (len(df_c) + effective_batch - 1) // effective_batch\n#         print(f\"Running predictions (batch_size={effective_batch}, num_batches={num_batches})...\")\n\n#         original_disable_tqdm = self.trainer.args.disable_tqdm\n#         self.trainer.args.disable_tqdm = False\n\n#         preds = self.trainer.predict(test_dataset=ds, metric_key_prefix=\"predict\")\n\n#         self.trainer.args.disable_tqdm = original_disable_tqdm\n#         if batch_size:\n#             self.trainer.args.per_device_eval_batch_size = original_bs\n\n#         ds.clear_cache()\n#         y = preds.predictions\n#         y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n#         return y\n\n#     def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n#         if self.trainer is None or self.model is None:\n#             raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n#         device = next(self.trainer.model.parameters()).device\n#         self.model.to(device).eval()\n\n#         df_c = self._attach_target_vector(df, fill_zeros=True)\n#         has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n\n#         ds = MultiModalDataset(\n#             df=df_c,\n#             target_col=\"__target_vector__\",\n#             task=\"regression\",\n#             text_columns=self.text_columns,\n#             image_columns=self.image_columns,\n#             audio_columns=self.audio_columns,\n#             text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n#             pretokenize=False\n#         )\n#         collate = MultiModalCollator(self.processors, task=\"regression\")\n#         loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n\n#         fused_list = []\n#         per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n#         num_batches = (len(df_c) + batch_size - 1) // batch_size\n#         print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n#         with torch.no_grad():\n#             for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n#                 bi = batch[\"backend_inputs\"]\n#                 def move_to_device(obj):\n#                     if torch.is_tensor(obj): return obj.to(device)\n#                     if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n#                     if isinstance(obj, list): return [move_to_device(v) for v in obj]\n#                     return obj\n#                 bi = move_to_device(bi)\n#                 fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n#                 fused_list.append(fused.cpu().numpy())\n#                 if return_per_modality:\n#                     for m in per_mod_lists.keys():\n#                         if m in per:\n#                             per_mod_lists[m].append(per[m].cpu().numpy())\n\n#         fused_arr = np.vstack(fused_list)\n#         if not return_per_modality:\n#             print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n#             return fused_arr\n#         per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n#         print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n#         for m, arr in per_mod.items():\n#             print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n#         return fused_arr, per_mod\n\n# # pipelines/classification.py\n# import os\n# import math\n# import gc\n# from typing import Any, Dict, List, Optional, Tuple\n\n# import numpy as np\n# import pandas as pd\n# import torch\n# from torch.utils.data import DataLoader\n# from tqdm.auto import tqdm\n# from transformers import TrainingArguments\n\n# # from mmkit.core.utils import set_seed\n# # from mmkit.data.dataset import MultiModalDataset\n# # from mmkit.data.collate import MultiModalCollator\n\n# # from mmkit.train.trainer_hf import WeightedCETrainer\n# # from mmkit.train.callbacks import PbarConsoleLogger\n\n# # from mmkit.aggregation.item_pool import ItemAggregator\n# # from mmkit.aggregation.fusion import ConcatFusion, MeanFusion\n# # from mmkit.heads.classification import ClassificationHead\n# # from mmkit.model.multimodal import MultiModalModel\n\n# # Процессоры\n# from transformers import (\n#     CLIPTokenizer,\n#     CLIPImageProcessor,\n#     AutoTokenizer,\n#     AutoImageProcessor,\n#     ClapProcessor,\n#     AutoProcessor,\n# )\n\n# # from mmkit.processors.text import TextProcessor\n# # from mmkit.processors.image import ImageProcessor\n# # from mmkit.processors.audio import (\n# #     ClapAudioProcessor,\n# #     Wav2VecAudioProcessor,\n# #     Wav2ClipAudioProcessor,\n# # )\n\n# # # Энкодеры\n# # from mmkit.encoders.text_clip import CLIPTextEncoder\n# # from mmkit.encoders.text_auto import AutoTextEncoder\n# # from mmkit.encoders.image_clip import CLIPImageEncoder\n# # from mmkit.encoders.image_auto import AutoImageEncoder\n# # from mmkit.encoders.audio_clap import CLAPAudioEncoder\n# # from mmkit.encoders.audio_wav2vec import Wav2Vec2Encoder\n# # from mmkit.encoders.audio_wav2clip import Wav2ClipEncoder\n\n\n# class MultiModalClassificationPipeline:\n#     \"\"\"\n#     Высокоуровневый пайплайн мультимодальной классификации (single/multi-class).\n#     Поддерживает text/image/audio, dynamic/max padding, предтокенизацию, чанковую тренировку,\n#     accuracy/f1 метрики, class weights, predict и извлечение эмбеддингов.\n\n#     По умолчанию (backend=\"auto\"):\n#       - если есть image: CLIP для text и image;\n#       - если есть audio: CLAP (или wav2vec/wav2clip по желанию — см. параметры),\n#       - для чистого текста: Auto (BERT) по умолчанию.\n\n#     Параметры:\n#       - modalities: список модальностей среди [\"text\",\"image\",\"audio\"]\n#       - target_column_name: имя колонки с метками (строки/инты)\n#       - num_labels: число классов (если None — возьмётся из train_data по unique)\n#       - text_columns, image_columns, audio_columns: колонки данных\n#       - fusion: \"concat\" | \"mean\" (для mean размеры эмбеддингов должны совпадать)\n#       - text_padding: \"max_length\" | \"dynamic\"\n#       - text_max_length: длина токенов\n#       - pretokenize_data: предтокенизация датасета (выключится при dynamic padding)\n#       - tokenizer_cache_size: LRU-кэш токенизатора\n#       - max_pretokenize_samples: лимит на предтокенизацию в чанке\n#       - local_cache_dir: кэш HF\n\n#       - audio_backend: \"clap\" | \"wav2vec\" | \"wav2clip\" (для audio)\n#       - audio_sr: sampling rate для аудио-процессоров\n#       - max_images_per_sample, image_agg: агрегация нескольких картинок (\"concat\"/\"mean\")\n#       - max_audios_per_sample, audio_agg: агрегация для аудио\n#     \"\"\"\n#     def __init__(\n#         self,\n#         modalities: List[str],\n#         target_column_name: str,\n#         num_labels: Optional[int] = None,\n#         text_columns: Optional[List[str]] = None,\n#         image_columns: Optional[List[str]] = None,\n#         audio_columns: Optional[List[str]] = None,\n\n#         # backend/модели\n#         backend: str = \"auto\",\n#         clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n#         clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n#         text_model_config: Optional[Dict[str, Any]] = None,\n#         image_model_config: Optional[Dict[str, Any]] = None,\n#         audio_model_config: Optional[Dict[str, Any]] = None,\n\n#         fusion: str = \"concat\",\n\n#         # токенизация текста\n#         text_padding: str = \"max_length\",\n#         text_max_length: int = 77,\n#         use_batch_tokenizer: bool = True,\n#         pretokenize_data: bool = True,\n#         pretokenize_batch_size: int = 256,\n#         tokenizer_cache_size: int = 10000,\n#         max_pretokenize_samples: int = 100000,\n#         local_cache_dir: str = \"./model_cache\",\n\n#         # изображение/аудио агрегация\n#         max_images_per_sample: int = 1,\n#         image_agg: str = \"concat\",\n#         audio_backend: str = \"clap\",  # \"clap\" | \"wav2vec\" | \"wav2clip\"\n#         audio_sr: int = 48000,\n#         max_audios_per_sample: int = 1,\n#         audio_agg: str = \"concat\",\n#     ):\n#         self.modalities = sorted(list(set(modalities)))\n#         self.target_column_name = target_column_name\n#         self.num_labels = num_labels\n\n#         self.text_columns = text_columns or []\n#         self.image_columns = image_columns or []\n#         self.audio_columns = audio_columns or []\n\n#         self.backend_name = backend.lower()\n#         self.clip_checkpoint = clip_checkpoint\n#         self.clap_checkpoint = clap_checkpoint\n#         self.text_model_config = text_model_config\n#         self.image_model_config = image_model_config\n#         self.audio_model_config = audio_model_config\n\n#         self.fusion = fusion\n#         self.text_padding = text_padding\n#         self.text_max_length = text_max_length\n#         self.use_batch_tokenizer = use_batch_tokenizer\n#         self.pretokenize_data = pretokenize_data\n#         self.pretokenize_batch_size = pretokenize_batch_size\n#         self.tokenizer_cache_size = tokenizer_cache_size\n#         self.max_pretokenize_samples = max_pretokenize_samples\n#         self.local_cache_dir = local_cache_dir\n\n#         self.max_images_per_sample = int(max_images_per_sample)\n#         self.image_agg = image_agg\n#         self.audio_backend = audio_backend\n#         self.audio_sr = int(audio_sr)\n#         self.max_audios_per_sample = int(max_audios_per_sample)\n#         self.audio_agg = audio_agg\n\n#         # Внутренние поля\n#         self.label2id: Optional[Dict[Any, int]] = None\n#         self.id2label: Optional[Dict[int, str]] = None\n#         self.processors: Dict[str, Any] = {}\n#         self.model: Optional[MultiModalModel] = None\n#         self.trainer: Optional[WeightedCETrainer] = None\n\n#     # --------------------------\n#     # Валидации и подсобки\n#     # --------------------------\n#     def _validate_modalities(self, df: pd.DataFrame):\n#         if \"text\" in self.modalities:\n#             assert self.text_columns, \"Вы выбрали text, но text_columns пустой\"\n#             missing = [c for c in self.text_columns if c not in df.columns]\n#             if missing:\n#                 raise ValueError(f\"Нет текстовых колонок: {missing}\")\n#         if \"image\" in self.modalities:\n#             assert self.image_columns, \"Вы выбрали image, но image_columns пуст\"\n#             missing = [c for c in self.image_columns if c not in df.columns]\n#             if missing:\n#                 raise ValueError(f\"Нет колонок изображений: {missing}\")\n#         if \"audio\" in self.modalities:\n#             assert self.audio_columns, \"Вы выбрали audio, но audio_columns пуст\"\n#             missing = [c for c in self.audio_columns if c not in df.columns]\n#             if missing:\n#                 raise ValueError(f\"Нет колонок аудио: {missing}\")\n\n#     def _ensure_label_mapping(self, df_train: pd.DataFrame):\n#         classes = sorted(df_train[self.target_column_name].unique().tolist())\n#         if self.num_labels is None:\n#             self.num_labels = len(classes)\n#         elif self.num_labels != len(classes):\n#             print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n#         self.label2id = {c: i for i, c in enumerate(classes)}\n#         self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n#     def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n#         df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n#         n_eval = int(math.ceil(len(df) * test_size))\n#         return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n#     def _ensure_label_column(self, df: pd.DataFrame) -> pd.DataFrame:\n#         df_c = df.copy()\n#         if self.target_column_name not in df_c.columns:\n#             # Если нет метки — подставим первую известную\n#             fake_label = next(iter(self.label2id.keys()))\n#             df_c[self.target_column_name] = [fake_label] * len(df_c)\n#         return df_c\n\n#     # --------------------------\n#     # Модель и процессоры (auto)\n#     # --------------------------\n#     def _build_model_and_processors(self) -> Tuple[Dict[str, Any], MultiModalModel]:\n#         encoders = {}\n#         processors = {}\n\n#         # text\n#         if \"text\" in self.modalities:\n#             # Если есть image — используем CLIP-текст, иначе Auto (BERT)\n#             use_clip_text = (\"image\" in self.modalities)\n#             if use_clip_text:\n#                 tok = CLIPTokenizer.from_pretrained(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n#                 processors[\"text\"] = TextProcessor(\n#                     tok, max_length=self.text_max_length, padding=self.text_padding,\n#                     cache_size=self.tokenizer_cache_size, batch_size=self.pretokenize_batch_size\n#                 )\n#                 encoders[\"text\"] = CLIPTextEncoder(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n#             else:\n#                 tok = AutoTokenizer.from_pretrained(\n#                     (self.text_model_config or {}).get(\"checkpoint\", \"bert-base-multilingual-cased\"),\n#                     cache_dir=self.local_cache_dir\n#                 )\n#                 processors[\"text\"] = TextProcessor(\n#                     tok, max_length=self.text_max_length, padding=self.text_padding,\n#                     cache_size=self.tokenizer_cache_size, batch_size=self.pretokenize_batch_size\n#                 )\n#                 encoders[\"text\"] = AutoTextEncoder(\n#                     (self.text_model_config or {}).get(\"checkpoint\", \"bert-base-multilingual-cased\"),\n#                     cache_dir=self.local_cache_dir\n#                 )\n\n#         # image\n#         image_cfg = None\n#         if \"image\" in self.modalities:\n#             if (self.image_model_config or {}).get(\"model_type\", \"clip\") == \"clip\":\n#                 ip = CLIPImageProcessor.from_pretrained(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n#                 processors[\"image\"] = ImageProcessor(ip)\n#                 encoders[\"image\"] = CLIPImageEncoder(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n#             else:\n#                 cp = (self.image_model_config or {}).get(\"checkpoint\", \"google/vit-base-patch16-224\")\n#                 ip = AutoImageProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n#                 processors[\"image\"] = ImageProcessor(ip)\n#                 encoders[\"image\"] = AutoImageEncoder(cp, cache_dir=self.local_cache_dir)\n#             image_cfg = {\"max_items\": self.max_images_per_sample, \"how\": self.image_agg}\n\n#         # audio\n#         audio_cfg = None\n#         if \"audio\" in self.modalities:\n#             ab = (self.audio_model_config or {}).get(\"model_type\", self.audio_backend)\n#             if ab == \"clap\":\n#                 cp = (self.audio_model_config or {}).get(\"checkpoint\", self.clap_checkpoint)\n#                 ap = ClapProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n#                 processors[\"audio\"] = ClapAudioProcessor(ap, sr=self.audio_sr)\n#                 encoders[\"audio\"] = CLAPAudioEncoder(cp, cache_dir=self.local_cache_dir)\n#             elif ab == \"wav2vec\":\n#                 cp = (self.audio_model_config or {}).get(\"checkpoint\", \"facebook/wav2vec2-base-960h\")\n#                 ap = AutoProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n#                 processors[\"audio\"] = Wav2VecAudioProcessor(ap, sr=self.audio_sr)\n#                 encoders[\"audio\"] = Wav2Vec2Encoder(cp, cache_dir=self.local_cache_dir)\n#             else:  # wav2clip\n#                 processors[\"audio\"] = Wav2ClipAudioProcessor(sr=self.audio_sr)\n#                 encoders[\"audio\"] = Wav2ClipEncoder()\n#             audio_cfg = {\"max_items\": self.max_audios_per_sample, \"how\": self.audio_agg}\n\n#         # Fusion + Head\n#         agg = ItemAggregator()\n#         fusion_mod = ConcatFusion() if self.fusion == \"concat\" else MeanFusion()\n#         dims = [encoders[m].embed_dim for m in [\"image\", \"text\", \"audio\"] if m in encoders]\n#         if self.fusion == \"concat\":\n#             in_dim = sum(dims)\n#         else:\n#             assert len(set(dims)) == 1, \"Для fusion='mean' размеры эмбеддингов всех модальностей должны совпадать\"\n#             in_dim = dims[0]\n#         head = ClassificationHead(in_dim=in_dim, num_labels=self.num_labels, hidden=512, dropout=0.1)\n#         model = MultiModalModel(encoders, agg, fusion_mod, head, image_cfg=image_cfg, audio_cfg=audio_cfg)\n#         return processors, model\n\n#     # --------------------------\n#     # Метрики\n#     # --------------------------\n#     def _build_compute_metrics(self, metric_name: str):\n#         metric_name = metric_name.lower()\n#         if metric_name == \"f1\":\n#             try:\n#                 import evaluate\n#                 f1_metric = evaluate.load(\"f1\")\n#                 def compute(p):\n#                     preds = p.predictions.argmax(-1)\n#                     return f1_metric.compute(predictions=preds, references=p.label_ids, average=\"weighted\")\n#                 return compute\n#             except Exception:\n#                 print(\"evaluate не доступен, использую accuracy\")\n#                 metric_name = \"accuracy\"\n\n#         if metric_name == \"accuracy\":\n#             try:\n#                 import evaluate\n#                 acc = evaluate.load(\"accuracy\")\n#                 def compute(p):\n#                     preds = p.predictions.argmax(-1)\n#                     return acc.compute(predictions=preds, references=p.label_ids)\n#                 return compute\n#             except Exception:\n#                 def compute(p):\n#                     preds = p.predictions.argmax(-1)\n#                     y = p.label_ids\n#                     return {\"accuracy\": float(np.mean(preds == y))}\n#                 return compute\n\n#         raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n\n#     # --------------------------\n#     # Обучение\n#     # --------------------------\n#     def fit(\n#         self,\n#         train_data: pd.DataFrame,\n#         epochs: int = 3,\n#         test_size: float = 0.2,\n#         test_data: Optional[pd.DataFrame] = None,\n#         per_device_train_batch_size: int = 16,\n#         gradient_accumulation_steps: int = 1,\n#         learning_rate: float = 2e-4,\n#         metric_name: str = \"accuracy\",   # \"accuracy\" | \"f1\"\n#         fp16: bool = True,\n#         logging_steps: int = 50,\n#         eval_steps: int = 200,\n#         output_dir: str = \"./result_cls\",\n#         seed: int = 42,\n#         gradient_checkpointing: bool = False,\n#         class_weights: Optional[np.ndarray] = None,  # если None — авто-расчёт по train_data\n#     ):\n#         import os\n#         from transformers import TrainingArguments\n    \n#         # 1) Проверки и подготовка\n#         self._validate_modalities(train_data)\n#         set_seed(seed)\n    \n#         # train/val split\n#         df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n    \n#         # labels mapping\n#         self._ensure_label_mapping(df_train)\n    \n#         # class weights\n#         if class_weights is None:\n#             y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n#             counts = np.bincount(y_train_all, minlength=self.num_labels)\n#             n_all = counts.sum()\n#             cw = np.zeros(self.num_labels, dtype=np.float32)\n#             nz = counts > 0\n#             cw[nz] = n_all / (self.num_labels * counts[nz].astype(np.float32))\n#             class_weights = cw\n    \n#         # 2) Модель и процессоры (auto)\n#         self.processors, self.model = self._build_model_and_processors()\n#         if gradient_checkpointing:\n#             try:\n#                 self.model.gradient_checkpointing_enable()  # если поддерживается\n#             except Exception:\n#                 pass\n    \n#         # 3) Datasets (без чанков)\n#         has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n#         pretokenize_train = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n#                              and len(df_train) <= self.max_pretokenize_samples)\n#         pretokenize_eval  = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n#                              and len(df_eval)  <= self.max_pretokenize_samples)\n    \n#         ds_train = MultiModalDataset(\n#             df=df_train,\n#             target_col=self.target_column_name,\n#             task=\"classification\",\n#             label2id=self.label2id,\n#             text_columns=self.text_columns,\n#             image_columns=self.image_columns,\n#             audio_columns=self.audio_columns,\n#             text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_train) else None,\n#             pretokenize=pretokenize_train,\n#             pretokenize_batch_size=self.pretokenize_batch_size\n#         )\n#         ds_eval = MultiModalDataset(\n#             df=df_eval,\n#             target_col=self.target_column_name,\n#             task=\"classification\",\n#             label2id=self.label2id,\n#             text_columns=self.text_columns,\n#             image_columns=self.image_columns,\n#             audio_columns=self.audio_columns,\n#             text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_eval) else None,\n#             pretokenize=pretokenize_eval,\n#             pretokenize_batch_size=self.pretokenize_batch_size\n#         )\n    \n#         collate = MultiModalCollator(self.processors, task=\"classification\")\n#         compute_metrics = self._build_compute_metrics(metric_name)\n\n#         args = TrainingArguments(\n#             output_dir=output_dir,\n#             num_train_epochs=epochs,\n#             per_device_train_batch_size=per_device_train_batch_size,\n#             per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n#             gradient_accumulation_steps=gradient_accumulation_steps,\n#             eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n#             learning_rate=learning_rate,\n#             warmup_ratio=0.1,\n#             lr_scheduler_type=\"cosine\",\n#             weight_decay=0.01,\n#             eval_strategy=\"steps\",\n#             eval_steps=eval_steps,\n#             save_strategy=\"steps\",\n#             save_steps=eval_steps,\n#             load_best_model_at_end=True,\n#             metric_for_best_model=f\"eval_{metric_name}\",\n#             save_total_limit=1,\n#             logging_strategy=\"steps\",\n#             logging_steps=logging_steps,\n#             report_to=\"none\",\n#             fp16=fp16 and torch.cuda.is_available(),\n#             dataloader_num_workers=min(4, os.cpu_count() or 4),\n#             seed=seed,\n#             remove_unused_columns=False,\n#             gradient_checkpointing=gradient_checkpointing,\n#             dataloader_pin_memory=True,\n#             ddp_find_unused_parameters=False,\n#             disable_tqdm=False\n#         )\n    \n#         # 5) Trainer и старт обучения\n#         self.trainer = WeightedCETrainer(\n#             model=self.model,\n#             args=args,\n#             train_dataset=ds_train,\n#             eval_dataset=ds_eval,\n#             data_collator=collate,\n#             compute_metrics=compute_metrics,\n#             class_weights=class_weights\n#         )\n    \n#         # (необязательно) убрать стандартный принтер-коллбек\n#         try:\n#             from transformers.trainer_callback import PrinterCallback\n#             self.trainer.remove_callback(PrinterCallback)\n#         except Exception:\n#             pass\n    \n#         self.trainer.train()\n    \n#         # очистить кэш токенизатора (если был)\n#         if has_bt:\n#             try: self.processors[\"text\"].clear_cache()\n#             except Exception: pass\n    \n#         return self\n\n#     # --------------------------\n#     # Предсказания\n#     # --------------------------\n#     def predict(\n#         self,\n#         df: pd.DataFrame,\n#         return_label_str: bool = False,\n#         return_proba: bool = False,\n#         batch_size: Optional[int] = None\n#     ) -> np.ndarray:\n#         if self.trainer is None:\n#             raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n#         df_c = self._ensure_label_column(df)\n\n#         has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n#         ds = MultiModalDataset(\n#             df=df_c,\n#             target_col=self.target_column_name,\n#             task=\"classification\",\n#             label2id=self.label2id,\n#             text_columns=self.text_columns,\n#             image_columns=self.image_columns,\n#             audio_columns=self.audio_columns,\n#             text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n#             pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n#             pretokenize_batch_size=self.pretokenize_batch_size\n#         )\n#         collate = MultiModalCollator(self.processors, task=\"classification\")\n\n#         # override bs\n#         if batch_size:\n#             original_bs = self.trainer.args.per_device_eval_batch_size\n#             self.trainer.args.per_device_eval_batch_size = batch_size\n\n#         effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n#         num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n#         print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n#         original_disable_tqdm = self.trainer.args.disable_tqdm\n#         self.trainer.args.disable_tqdm = False\n\n#         preds = self.trainer.predict(test_dataset=ds, metric_key_prefix=\"predict\")\n\n#         self.trainer.args.disable_tqdm = original_disable_tqdm\n#         if batch_size:\n#             self.trainer.args.per_device_eval_batch_size = original_bs\n\n#         ds.clear_cache()\n\n#         logits = preds.predictions\n#         if return_proba:\n#             exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n#             probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n#             return probabilities\n\n#         y_pred = np.argmax(logits, axis=-1)\n#         if return_label_str:\n#             return np.array([self.id2label[int(i)] for i in y_pred])\n#         return y_pred\n\n#     # --------------------------\n#     # Эмбеддинги\n#     # --------------------------\n#     def get_embeddings(\n#         self,\n#         df: pd.DataFrame,\n#         batch_size: int = 32,\n#         return_per_modality: bool = False\n#     ):\n#         if self.trainer is None or self.model is None:\n#             raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n#         device = next(self.trainer.model.parameters()).device\n#         self.model.to(device).eval()\n\n#         df_c = self._ensure_label_column(df)\n\n#         has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n#         ds = MultiModalDataset(\n#             df=df_c,\n#             target_col=self.target_column_name,\n#             task=\"classification\",\n#             label2id=self.label2id,\n#             text_columns=self.text_columns,\n#             image_columns=self.image_columns,\n#             audio_columns=self.audio_columns,\n#             text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n#             pretokenize=False\n#         )\n#         collate = MultiModalCollator(self.processors, task=\"classification\")\n#         loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n\n#         fused_list = []\n#         per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n#         num_batches = (len(df_c) + batch_size - 1) // batch_size\n#         print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n#         with torch.no_grad():\n#             for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n#                 bi = batch[\"backend_inputs\"]\n\n#                 def move_to_device(obj):\n#                     if torch.is_tensor(obj): return obj.to(device)\n#                     if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n#                     if isinstance(obj, list): return [move_to_device(v) for v in obj]\n#                     return obj\n\n#                 bi = move_to_device(bi)\n#                 fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n#                 fused_list.append(fused.cpu().numpy())\n#                 if return_per_modality:\n#                     for m in per_mod_lists.keys():\n#                         if m in per:\n#                             per_mod_lists[m].append(per[m].cpu().numpy())\n\n#         fused_arr = np.vstack(fused_list)\n#         if not return_per_modality:\n#             print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n#             return fused_arr\n\n#         per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n#         print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n#         for m, arr in per_mod.items():\n#             print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n\n#         return fused_arr, per_mod\n\n# # ensembles/stacking.py\n# from typing import List, Optional, Literal, Tuple, Union\n# import numpy as np\n\n# from sklearn.linear_model import LogisticRegression, Ridge, LinearRegression\n# from sklearn.multioutput import MultiOutputRegressor\n# from sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Task = Literal[\"classification\", \"regression\"]\n# MetaType = Literal[\"auto\", \"logistic\", \"ridge\", \"linear\"]\n\n\n# def _softmax(x: np.ndarray, axis: int = 1) -> np.ndarray:\n#     x = x - np.max(x, axis=axis, keepdims=True)\n#     e = np.exp(x)\n#     return e / np.sum(e, axis=axis, keepdims=True)\n\n\n# def _check_shapes(outputs_list: List[np.ndarray]) -> Tuple[int, Tuple[int, ...]]:\n#     if len(outputs_list) == 0:\n#         raise ValueError(\"Пустой список выходов базовых моделей\")\n#     n = outputs_list[0].shape[0]\n#     for i, a in enumerate(outputs_list):\n#         if a.ndim != 2:\n#             raise ValueError(f\"Ожидался массив 2D для модели #{i}, получено shape={a.shape}\")\n#         if a.shape[0] != n:\n#             raise ValueError(f\"Несогласованная размерность N: outputs[0].shape[0]={n} != outputs[{i}].shape[0]={a.shape[0]}\")\n#     return n, outputs_list[0].shape\n\n\n# class StackingMetaLearner:\n#     \"\"\"\n#     Стакинг-ансамбль с обучением только мета-модели (склейка выходов базовых моделей):\n#       - classification: LogisticRegression по признакам = concat(логиты|вероятности) базовых моделей\n#       - regression: Ridge (по умолчанию) или LinearRegression по concat предсказаний базовых моделей\n\n#     Теперь поддерживает текстовые метки классов (y как список/массив строк).\n#     Метки автоматически кодируются во внутренние индексы через LabelEncoder.\n\n#     Как использовать:\n#       - classification:\n#           outputs_list = [logits_model1, logits_model2, ...], каждый [N, C]\n#           y = [N] (int или str — можно строки-классы)\n#           meta.fit(outputs_list, y)\n#           meta.predict(outputs_list_new, return_labels=True) -> строковые метки\n#       - regression:\n#           outputs_list = [preds_model1, preds_model2, ...], каждый [N, K]\n#           y = [N] или [N, K]\n#     \"\"\"\n#     def __init__(\n#         self,\n#         task: Task = \"classification\",\n#         meta: MetaType = \"auto\",\n#         use_proba: bool = True,\n#         standardize: bool = False,\n#         ridge_alpha: float = 1.0,\n#         logistic_C: float = 1.0,\n#         logistic_penalty: Union[str, None] = \"l2\",\n#         class_weight: Optional[Union[str, dict]] = None,  # dict: можно передавать маппинг по строковым классам\n#         fit_intercept: bool = True,\n#         random_state: Optional[int] = None\n#     ):\n#         self.task = task\n#         self.meta = meta\n#         self.use_proba = use_proba\n#         self.standardize = standardize\n#         self.ridge_alpha = ridge_alpha\n#         self.logistic_C = logistic_C\n#         self.logistic_penalty = logistic_penalty\n#         self.class_weight = class_weight\n#         self.fit_intercept = fit_intercept\n#         self.random_state = random_state\n\n#         self.model = None\n#         self.scaler: Optional[StandardScaler] = None\n\n#         # Информация о форме признаков/классов\n#         self.n_models_: Optional[int] = None\n#         self.n_classes_: Optional[int] = None\n#         self.n_targets_: Optional[int] = None  # для регрессии\n\n#         # Для классификации: энкодер и классы\n#         self.label_encoder_: Optional[LabelEncoder] = None\n#         self.classes_: Optional[np.ndarray] = None\n\n#         # Внутреннее преобразование весов классов (после энкодинга)\n#         self._class_weight_enc: Optional[Union[str, dict]] = None\n\n#     # ------------- Вспомогательное -------------\n#     def _stack_features_classification(self, logits_list: List[np.ndarray]) -> np.ndarray:\n#         \"\"\"\n#         Превращает список логитов/вероятностей в матрицу признаков X.\n#         - Если use_proba=True: применяем softmax к каждому [N,C] и конкатенируем -> [N, M*C]\n#         - Иначе: конкатенируем логиты -> [N, M*C]\n#         \"\"\"\n#         n, (N, C) = _check_shapes(logits_list)\n#         self.n_models_ = len(logits_list)\n#         self.n_classes_ = C\n#         parts = [(_softmax(z) if self.use_proba else z) for z in logits_list]\n#         X = np.hstack(parts).astype(np.float64, copy=False)\n#         return X\n\n#     def _stack_features_regression(self, preds_list: List[np.ndarray]) -> np.ndarray:\n#         \"\"\"\n#         Превращает список предсказаний регрессии в матрицу признаков X.\n#         Каждый элемент: [N, K_i] (обычно K_i совпадает у всех моделей), конкатенация -> [N, sum(K_i)]\n#         \"\"\"\n#         n, _ = _check_shapes(preds_list)\n#         self.n_models_ = len(preds_list)\n#         # Воспримем K по первой модели\n#         K = preds_list[0].shape[1]\n#         self.n_targets_ = K\n#         X = np.hstack(preds_list).astype(np.float64, copy=False)\n#         return X\n\n#     def _build_X(self, outputs_list: List[np.ndarray]) -> np.ndarray:\n#         if self.task == \"classification\":\n#             return self._stack_features_classification(outputs_list)\n#         else:\n#             return self._stack_features_regression(outputs_list)\n\n#     def _init_model(self, y_encoded: np.ndarray):\n#         if self.task == \"classification\":\n#             if self.meta in (\"auto\", \"logistic\"):\n#                 solver = \"lbfgs\" if (self.logistic_penalty in (\"l2\", None, \"none\")) else \"saga\"\n#                 penalty = None if self.logistic_penalty in (None, \"none\") else self.logistic_penalty\n#                 self.model = LogisticRegression(\n#                     C=self.logistic_C,\n#                     penalty=penalty,\n#                     class_weight=self._class_weight_enc,  # уже преобразованные веса\n#                     fit_intercept=self.fit_intercept,\n#                     multi_class=\"auto\",\n#                     solver=solver,\n#                     max_iter=2000,\n#                     random_state=self.random_state\n#                 )\n#             else:\n#                 raise ValueError(\"Для classification поддерживается только meta='logistic' (или 'auto').\")\n#         else:\n#             # regression\n#             if self.meta in (\"auto\", \"ridge\"):\n#                 base = Ridge(alpha=self.ridge_alpha, fit_intercept=self.fit_intercept, random_state=self.random_state)\n#             elif self.meta == \"linear\":\n#                 base = LinearRegression(fit_intercept=self.fit_intercept)\n#             else:\n#                 raise ValueError(\"Для regression meta должно быть 'ridge'|'linear'|'auto'.\")\n\n#             # Мультивыходная регрессия: завернём в MultiOutputRegressor при K>1\n#             if y_encoded.ndim == 2 and y_encoded.shape[1] > 1:\n#                 self.model = MultiOutputRegressor(base)\n#             else:\n#                 self.model = base\n\n#     def _encode_class_weights_if_needed(self):\n#         \"\"\"\n#         Преобразует словарь весов классов из оригинальных меток в индексы,\n#         если была использована LabelEncoder.\n#         \"\"\"\n#         if not isinstance(self.class_weight, dict):\n#             self._class_weight_enc = self.class_weight  # 'balanced' или None\n#             return\n#         cw = {}\n#         for k, v in self.class_weight.items():\n#             if self.label_encoder_ is not None and not isinstance(k, (int, np.integer)):\n#                 idx = int(self.label_encoder_.transform([k])[0])\n#             else:\n#                 idx = int(k)\n#             cw[idx] = float(v)\n#         self._class_weight_enc = cw\n\n#     # ------------- Публичный API -------------\n#     def fit(self, outputs_list: List[np.ndarray], y: np.ndarray):\n#         \"\"\"\n#         Обучение только мета-модели.\n#         - classification: outputs_list = [logits_i: [N, C]], y: [N] (int или str — допускаются строковые метки)\n#         - regression:     outputs_list = [preds_i:  [N, K]], y: [N] или [N, K]\n#         \"\"\"\n#         if len(outputs_list) == 0:\n#             raise ValueError(\"outputs_list пуст\")\n\n#         X = self._build_X(outputs_list)\n\n#         y = np.asarray(y)\n#         if X.shape[0] != y.shape[0]:\n#             raise ValueError(f\"N samples mismatch: X={X.shape[0]} vs y={y.shape[0]}\")\n\n#         # Классификация: допускаем строковые метки\n#         if self.task == \"classification\":\n#             if y.ndim != 1:\n#                 raise ValueError(\"Для классификации y должен быть одномерным массивом меток.\")\n\n#             # Определяем, нужно ли кодировать метки\n#             if y.dtype.kind in (\"U\", \"S\", \"O\"):  # строки/объекты\n#                 self.label_encoder_ = LabelEncoder()\n#                 y_encoded = self.label_encoder_.fit_transform(y)\n#                 self.classes_ = self.label_encoder_.classes_\n#             else:\n#                 # уже числовые\n#                 self.label_encoder_ = None\n#                 y_encoded = y.astype(int, copy=False)\n#                 # classes_ можно получить после fit у модели, но сохраним явные уникальные\n#                 self.classes_ = np.unique(y_encoded)\n\n#             # Стандартизация (опционально)\n#             if self.standardize:\n#                 self.scaler = StandardScaler(with_mean=True, with_std=True)\n#                 X = self.scaler.fit_transform(X)\n#             else:\n#                 self.scaler = None\n\n#             # Веса классов: преобразуем словарь по необходимости\n#             self._encode_class_weights_if_needed()\n\n#             # Инициализация и обучение мета-модели\n#             self._init_model(y_encoded)\n#             self.model.fit(X, y_encoded.astype(int))\n\n#         else:\n#             # Регрессия\n#             if self.standardize:\n#                 self.scaler = StandardScaler(with_mean=True, with_std=True)\n#                 X = self.scaler.fit_transform(X)\n#             else:\n#                 self.scaler = None\n\n#             y_encoded = y  # для регрессии кодирование не нужно\n#             self._init_model(y_encoded)\n#             self.model.fit(X, y_encoded)\n\n#         return self\n\n#     def _transform_X(self, outputs_list: List[np.ndarray]) -> np.ndarray:\n#         X = self._build_X(outputs_list)\n#         if self.scaler is not None:\n#             X = self.scaler.transform(X)\n#         return X\n\n#     def predict(self, outputs_list: List[np.ndarray], return_labels: bool = False) -> np.ndarray:\n#         \"\"\"\n#         - classification: возврат индексов классов [N] по умолчанию; если return_labels=True — строковые метки\n#         - regression:     возврат предсказаний [N] или [N, K]\n#         \"\"\"\n#         if self.model is None:\n#             raise RuntimeError(\"Мета-модель не обучена. Вызовите fit().\")\n#         X = self._transform_X(outputs_list)\n#         y_pred = self.model.predict(X)\n\n#         if self.task == \"classification\" and return_labels:\n#             if self.label_encoder_ is not None:\n#                 return self.label_encoder_.inverse_transform(y_pred.astype(int))\n#             else:\n#                 # метки изначально были числовые — вернуть как есть\n#                 return y_pred\n\n#         return y_pred\n\n#     def predict_labels(self, outputs_list: List[np.ndarray]) -> np.ndarray:\n#         \"\"\"\n#         Удобный синоним для классификации: сразу вернуть строковые метки классов [N].\n#         Если метки изначально были числовые — вернёт числа.\n#         \"\"\"\n#         return self.predict(outputs_list, return_labels=True)\n\n#     def predict_proba(self, outputs_list: List[np.ndarray]) -> np.ndarray:\n#         \"\"\"\n#         Только для classification: возврат вероятностей [N, C].\n#         \"\"\"\n#         if self.task != \"classification\":\n#             raise RuntimeError(\"predict_proba доступен только для classification.\")\n#         if self.model is None:\n#             raise RuntimeError(\"Мета-модель не обучена. Вызовите fit().\")\n#         X = self._transform_X(outputs_list)\n#         if hasattr(self.model, \"predict_proba\"):\n#             return self.model.predict_proba(X)\n#         # fallback (не должен понадобиться для LogisticRegression)\n#         logits = self.model.decision_function(X)\n#         return _softmax(logits)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n!pip install evaluate wav2clip\n\n# core/utils.py\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom PIL import Image\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef to_pil(x):\n    if isinstance(x, Image.Image): return x.convert(\"RGB\")\n    if isinstance(x, str): return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray): return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Expected path/np.ndarray/PIL.Image\")\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)\n    if waveform.size(0) > 1: waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr: waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\ndef safe_load(component_cls, checkpoint: str, cache_dir: str = \"./model_cache\",\n              local_files_only: bool = None, **kwargs):\n    if local_files_only is None:\n        local_files_only = os.environ.get(\"HF_HUB_OFFLINE\", \"0\") == \"1\"\n    name = getattr(component_cls, \"__name__\", \"\")\n    if \"Tokenizer\" in name:\n        kwargs.setdefault(\"use_fast\", True)\n    return component_cls.from_pretrained(\n        checkpoint, cache_dir=cache_dir, local_files_only=local_files_only, **kwargs\n    )\n\n# core/types.py\nimport torch\n\ndef preferred_device():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n\n# core/device.py\nfrom typing import Any, Dict, List, Protocol, Literal\nimport torch\n\nModality = Literal[\"text\", \"image\", \"audio\"]\n\nclass Processor(Protocol):\n    modality: Modality\n    def prepare_batch(self, raw_items: List[Any]) -> Dict[str, torch.Tensor]:\n        ...\n\nclass Encoder(Protocol):\n    modality: Modality\n    embed_dim: int\n    def encode_batch(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n        ...\n\nclass ItemAggregator(Protocol):\n    def aggregate(self, embs: torch.Tensor, counts: List[int], max_items: int, how: str) -> torch.Tensor:\n        ...\n\nclass Fusion(Protocol):\n    def __call__(self, zs: Dict[Modality, torch.Tensor]) -> torch.Tensor:\n        ...\n\n# data/augmentation.py\nimport os\nimport uuid\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, List, Optional, Callable\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\ndef augment_data_table(\n    df: pd.DataFrame,\n    save_dir: str,\n    image_columns: Optional[List[str]] = None,\n    text_columns: Optional[List[str]] = None,\n    audio_columns: Optional[List[str]] = None,\n    image_augment_fn: Optional[List[Callable[[Image.Image], Image.Image]]] = None,\n    text_augment_fn: Optional[List[Callable[[str], str]]] = None,\n    audio_augment_fn: Optional[List[Callable[..., np.ndarray]]] = None,\n    id_col: Optional[str] = None,\n    audio_sr: int = 16000,\n) -> pd.DataFrame:\n    \"\"\"\n    Делает простую аугментацию данных.\n    - Выполняет аугментацию ТОЛЬКО для тех модальностей, для которых передан список функций.\n    - Для каждой функции модальности создаёт ОДИН вариант датасета (без перемножения между модальностями).\n    - Картинки/аудио сохраняются в save_dir, в таблице подставляются пути к новым файлам.\n    - Возвращает конкатенацию всех вариантов с колонкой __aug.\n    \"\"\"\n    image_columns = image_columns or []\n    text_columns  = text_columns or []\n    audio_columns = audio_columns or []\n\n    os.makedirs(save_dir, exist_ok=True)\n    run_tag = uuid.uuid4().hex[:8]\n\n    def _row_id(i: int) -> str:\n        if id_col and id_col in df.columns:\n            return str(df.iloc[i][id_col])\n        return f\"row{i}\"\n\n    def _to_pil(v: Any) -> Image.Image:\n        if isinstance(v, Image.Image): return v.convert(\"RGB\")\n        if isinstance(v, str):         return Image.open(v).convert(\"RGB\")\n        if isinstance(v, np.ndarray):\n            if v.dtype != np.uint8: v = np.clip(v, 0, 255).astype(np.uint8)\n            return Image.fromarray(v)\n        raise TypeError(\"Unsupported image type\")\n\n    def _save_img(im: Image.Image, out_dir: str, name: str) -> str:\n        os.makedirs(out_dir, exist_ok=True)\n        path = os.path.join(out_dir, f\"{name}.png\")\n        im.save(path)\n        return path\n\n    def _read_audio(x: Any) -> tuple[np.ndarray, int]:\n        if isinstance(x, str):\n            try:\n                import soundfile as sf\n                y, sr = sf.read(x, always_2d=False)\n            except Exception:\n                from scipy.io import wavfile\n                sr, y = wavfile.read(x)\n            y = np.asarray(y)\n            if y.ndim > 1: y = y.mean(axis=-1)\n            y = (y.astype(np.float32) / (32768.0 if y.dtype.kind in \"iu\" else 1.0))\n            return np.clip(y, -1.0, 1.0), sr\n        elif isinstance(x, np.ndarray):\n            y = x\n            if y.ndim > 1: y = y.mean(axis=-1)\n            y = y.astype(np.float32, copy=False)\n            if y.dtype.kind in \"iu\": y = y / 32768.0\n            return np.clip(y, -1.0, 1.0), audio_sr\n        raise TypeError(\"Unsupported audio type\")\n\n    def _save_wav(y: np.ndarray, sr: int, out_dir: str, name: str) -> str:\n        os.makedirs(out_dir, exist_ok=True)\n        path = os.path.join(out_dir, f\"{name}.wav\")\n        try:\n            import soundfile as sf\n            sf.write(path, y.astype(np.float32, copy=False), sr, subtype=\"PCM_16\")\n        except Exception:\n            from scipy.io import wavfile\n            wavfile.write(path, sr, (np.clip(y, -1, 1) * 32767).astype(np.int16))\n        return path\n\n    out_parts = []\n\n    # ТОЛЬКО если переданы функции и есть столбцы изображений\n    if image_columns and image_augment_fn:\n        for k, fn in tqdm(list(enumerate(image_augment_fn)), desc='Аугментация картинок'):\n            df_aug = df.copy(deep=True)\n            for col_i, col in enumerate(image_columns):\n                out_dir = os.path.join(save_dir, \"images\", col, f\"aug_image_{k}_{run_tag}\")\n                df_aug[col] = [\n                    _save_img(fn(_to_pil(v)), out_dir, f\"{_row_id(i)}\") if v is not None else v\n                    for i, v in tqdm(list(enumerate(df[col].tolist())),\n                                     desc=f'Столбец {col_i+1}/{len(image_columns)}', leave=False)\n                ]\n            df_aug[\"__aug\"] = f\"image_{k}\"\n            out_parts.append(df_aug)\n\n    # ТОЛЬКО если переданы функции и есть текстовые столбцы\n    if text_columns and text_augment_fn:\n        for k, fn in tqdm(list(enumerate(text_augment_fn)), desc='Аугментация текстов'):\n            df_aug = df.copy(deep=True)\n            for col_i, col in enumerate(text_columns):\n                df_aug[col] = [\n                    fn(v) if isinstance(v, str) else v\n                    for v in tqdm(df[col].tolist(),\n                                  desc=f'Столбец {col_i+1}/{len(text_columns)}', leave=False)\n                ]\n            df_aug[\"__aug\"] = f\"text_{k}\"\n            out_parts.append(df_aug)\n\n    # ТОЛЬКО если переданы функции и есть аудио столбцы\n    if audio_columns and audio_augment_fn:\n        for k, fn in tqdm(list(enumerate(audio_augment_fn)), desc='Аугментация звуков'):\n            df_aug = df.copy(deep=True)\n            for col_i, col in enumerate(audio_columns):\n                out_dir = os.path.join(save_dir, \"audio\", col, f\"aug_audio_{k}_{run_tag}\")\n                new_vals = []\n                for i, v in tqdm(list(enumerate(df[col].tolist())),\n                                 desc=f'Столбец {col_i+1}/{len(audio_columns)}', leave=False):\n                    if v is None:\n                        new_vals.append(v); continue\n                    y, sr = _read_audio(v)\n                    try:    y2 = fn(y, sr)\n                    except TypeError: y2 = fn(y)\n                    path = _save_wav(np.asarray(y2, dtype=np.float32), sr, out_dir, f\"{_row_id(i)}\")\n                    new_vals.append(path)\n                df_aug[col] = new_vals\n            df_aug[\"__aug\"] = f\"audio_{k}\"\n            out_parts.append(df_aug)\n\n    if not out_parts:\n        raise ValueError(\"Не переданы функции аугментации ни для одной модальности — нечего аугментировать.\")\n\n    return pd.concat(out_parts, ignore_index=True)\n\n# data/tokenization.py\nfrom functools import lru_cache\nfrom typing import Dict, List, Optional\nimport numpy as np\nimport torch\n\nclass BatchTokenizer:\n    def __init__(self, tokenizer, max_length=512, cache_size=10000, batch_size=256, padding_strategy=\"max_length\"):\n        assert padding_strategy in {\"max_length\", \"dynamic\"}\n        self.tok = tokenizer\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.padding_strategy = padding_strategy\n        self._cache = lru_cache(maxsize=cache_size)(self._tokenize_single)\n        self.is_fast = hasattr(tokenizer, \"is_fast\") and tokenizer.is_fast\n        if self.is_fast:\n            print(\"✓ Используется Fast Tokenizer\")\n\n    def _tokenize_single(self, text: str) -> Dict[str, np.ndarray]:\n        res = self.tok(text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        return {k: v.squeeze(0).cpu().numpy() for k, v in res.items()}\n\n    def tokenize_batch(self, texts: List[str], use_cache: bool = True) -> Dict[str, torch.Tensor]:\n        if self.padding_strategy == \"dynamic\":\n            res = self.tok(texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n            return {k: (v.long() if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else v) for k, v in res.items()}\n        if use_cache and len(texts) < 100:\n            items = [self._cache(t) for t in texts]\n            keys = items[0].keys()\n            out = {}\n            for k in keys:\n                dtype = torch.long if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else torch.float32\n                out[k] = torch.tensor(np.stack([it[k] for it in items]), dtype=dtype)\n            return out\n        res = self.tok(texts, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        return {k: (v.long() if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else v) for k, v in res.items()}\n\n    def tokenize_dataset_lazy(self, texts: List[str], batch_size: Optional[int] = None):\n        b = batch_size or self.batch_size\n        for i in range(0, len(texts), b):\n            yield self.tokenize_batch(texts[i:i+b], use_cache=False)\n\n    def clear_cache(self):\n        self._cache.cache_clear()\n\n# data/dataset.py\nimport gc\nfrom typing import Any, Dict, List, Optional, Callable\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\n\nclass MultiModalDataset(Dataset):\n    \"\"\"\n    Универсальный датасет для классификации и регрессии.\n    Поддерживает:\n      - text pretokenize: BatchTokenizer или кастомные функции (single/batched)\n      - dynamic/max паддинг (dynamic отключает предтокенизацию)\n      - мульти-изображения и мульти-аудио на сэмпл\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: Optional[str],                 # для регрессии — колонка с np.ndarray; для классификации — метка\n        task: str = \"regression\",                  # \"regression\" | \"classification\"\n        label2id: Optional[Dict[Any, int]] = None, # для классификации\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        text_tokenizer=None,                       # BatchTokenizer\n        text_tokenizer_fn: Optional[Callable] = None,\n        text_tokenizer_fn_batched: Optional[Callable] = None,\n        special_tokens: Optional[Dict[str, Any]] = None,\n        pretokenize: bool = False,\n        pretokenize_batch_size: int = 256,\n        tokenizer_returns_tensors: bool = False,\n        deduplicate_texts: bool = True\n    ):\n        super().__init__()\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.task = task.lower()\n        self.label2id = label2id or {}\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.batch_tokenizer = text_tokenizer\n        self.text_tokenizer_fn = text_tokenizer_fn\n        self.text_tokenizer_fn_batched = text_tokenizer_fn_batched\n\n        self.special_tokens = special_tokens or {\"sep\": \" [SEP] \"}\n        self.tokenizer_returns_tensors = tokenizer_returns_tensors\n\n        self._N = len(self.df)\n\n        # labels\n        if self.task == \"classification\":\n            if self.target_col is None:\n                y = np.zeros(self._N, dtype=np.int64)\n            else:\n                y = self.df[self.target_col].map(self.label2id).fillna(0).astype(int).values\n            self._labels = torch.tensor(y, dtype=torch.long)\n        else:\n            self._labels = self._prepare_labels_regression(self.df, self.target_col)\n\n        # lists for images/audios\n        self._image_lists = self._collect_multi_values(self.df, self.image_columns) if self.image_columns else None\n        self._audio_lists = self._collect_multi_values(self.df, self.audio_columns) if self.audio_columns else None\n\n        self._tok_bank: Optional[Dict[str, torch.Tensor]] = None\n        self._has_text = bool(self.text_columns)\n\n        # dynamic padding -> disable pretokenization\n        if pretokenize and self.batch_tokenizer is not None and getattr(self.batch_tokenizer, \"padding_strategy\", \"max_length\") == \"dynamic\":\n            print(\"⚠ Предтокенизация отключена: выбран dynamic-паддинг для текста.\")\n            pretokenize = False\n\n        if self._has_text and pretokenize:\n            if self.batch_tokenizer is not None and self.text_tokenizer_fn is None and self.text_tokenizer_fn_batched is None:\n                self._pretokenize_with_batch_tokenizer(pretokenize_batch_size)\n            else:\n                self._pretokenize_with_custom_fn(pretokenize_batch_size, deduplicate_texts)\n\n    # labels for regression: [N, K]\n    def _prepare_labels_regression(self, df: pd.DataFrame, target_col: Optional[str]) -> torch.Tensor:\n        if target_col is None or target_col not in df.columns:\n            return torch.zeros((len(df), 1), dtype=torch.float32)\n        labels_list = []\n        for i in range(len(df)):\n            v = df.iloc[i][target_col]\n            if isinstance(v, (list, tuple, np.ndarray)):\n                arr = np.asarray(v, dtype=np.float32)\n            else:\n                try:\n                    arr = np.asarray([float(v)], dtype=np.float32)\n                except Exception:\n                    arr = np.asarray([0.0], dtype=np.float32)\n            labels_list.append(arr)\n        K = max(a.shape[0] for a in labels_list) if labels_list else 1\n        out = np.zeros((len(labels_list), K), dtype=np.float32)\n        for i, a in enumerate(labels_list):\n            out[i, :a.shape[0]] = a\n        return torch.tensor(out, dtype=torch.float32)\n\n    def _join_text(self, row: pd.Series) -> str:\n        sep = self.special_tokens.get(\"sep\", \" [SEP] \")\n        return sep.join([(\"\" if pd.isna(row[c]) else str(row[c])) for c in self.text_columns])\n\n    @staticmethod\n    def _as_list(v):\n        if v is None or (isinstance(v, float) and np.isnan(v)):\n            return []\n        if isinstance(v, (list, tuple)):\n            return list(v)\n        return [v]\n\n    def _collect_multi_values(self, df: pd.DataFrame, columns: List[str]) -> List[List[Any]]:\n        out = []\n        as_list = self._as_list\n        for _, row in df.iterrows():\n            lst: List[Any] = []\n            for c in columns:\n                if c in row:\n                    lst.extend([x for x in as_list(row[c]) if x is not None])\n            out.append(lst)\n        return out\n\n    def _pretokenize_with_batch_tokenizer(self, batch_size: int):\n        print(\"Предтокенизация с BatchTokenizer...\")\n        texts = [self._join_text(self.df.iloc[i]) for i in range(self._N)]\n        banks: Dict[str, List[torch.Tensor]] = {}\n        for start in range(0, self._N, batch_size):\n            batch = texts[start:start + batch_size]\n            tok = self.batch_tokenizer.tokenize_batch(batch, use_cache=False)\n            for k in tok:\n                if k in (\"input_ids\",\"attention_mask\",\"token_type_ids\"):\n                    tok[k] = tok[k].long()\n                else:\n                    tok[k] = tok[k].to(torch.float32)\n            for k, v in tok.items():\n                banks.setdefault(k, []).append(v)\n        self._tok_bank = {k: torch.cat(v_parts, dim=0).contiguous() for k, v_parts in banks.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"✓ Предтокенизация завершена: {self._N} образцов | keys={list(self._tok_bank.keys())}, shapes={shapes}\")\n\n    def _pretokenize_with_custom_fn(self, batch_size: int, deduplicate_texts: bool):\n        cols = list(self.text_columns)\n        if self.text_tokenizer_fn_batched is not None:\n            print(\"Предтокенизация кастомной batched-функцией...\")\n            first_end = min(self._N, max(8, batch_size))\n            batch_data = []\n            for i in range(first_end):\n                row = self.df.iloc[i]\n                d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                batch_data.append(d)\n            first_tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n            if not isinstance(first_tok, dict):\n                raise ValueError(\"text_tokenizer_fn_batched должна возвращать dict тензоров [B, L]\")\n            bank: Dict[str, torch.Tensor] = {}\n            for k, t in first_tok.items():\n                if not torch.is_tensor(t): t = torch.tensor(t)\n                bank[k] = torch.empty((self._N, t.size(1)), dtype=t.dtype)\n                bank[k][:first_end] = t[:first_end]\n            for start in range(first_end, self._N, batch_size):\n                end = min(self._N, start + batch_size)\n                batch_data = []\n                for i in range(start, end):\n                    row = self.df.iloc[i]\n                    d = {c: (\"\" if pd.isna(row[c]) else str(row[c])) for c in cols}\n                    batch_data.append(d)\n                tok = self.text_tokenizer_fn_batched(batch_data, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): t = torch.tensor(t)\n                    bank[k][start:end] = t\n            self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n            shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n            print(f\"✓ Предтокенизация кастомной batched-функцией завершена: shapes={shapes}\")\n            return\n\n        print(\"Предтокенизация кастомной single-функцией...\")\n        col_arrays = [self.df[c].astype(str).where(~self.df[c].isna(), other=\"\").tolist() for c in cols]\n        first_td = {c: col_arrays[i][0] for i, c in enumerate(cols)}\n        first_tok = self.text_tokenizer_fn(first_td, self.special_tokens)\n        if not isinstance(first_tok, dict):\n            raise ValueError(\"custom text_tokenizer_fn должна возвращать dict тензоров\")\n        for k, t in first_tok.items():\n            if not torch.is_tensor(t): first_tok[k] = torch.tensor(t)\n        bank: Dict[str, torch.Tensor] = {k: torch.empty((self._N, *t.shape), dtype=t.dtype) for k, t in first_tok.items()}\n        for k, t in first_tok.items():\n            bank[k][0].copy_(t)\n\n        cache: Dict[tuple, Dict[str, torch.Tensor]] = {}\n        if deduplicate_texts:\n            key0 = tuple(first_td.get(c, \"\") for c in cols)\n            cache[key0] = {k: v.clone() for k, v in first_tok.items()}\n\n        for i in range(1, self._N):\n            td = {c: col_arrays[j][i] for j, c in enumerate(cols)}\n            if deduplicate_texts:\n                key = tuple(td.get(c, \"\") for c in cols)\n                tok = cache.get(key)\n                if tok is None:\n                    tok = self.text_tokenizer_fn(td, self.special_tokens)\n                    for k, t in tok.items():\n                        if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n                    cache[key] = {k: v.clone() for k, v in tok.items()}\n            else:\n                tok = self.text_tokenizer_fn(td, self.special_tokens)\n                for k, t in tok.items():\n                    if not torch.is_tensor(t): tok[k] = torch.tensor(t)\n            for k, t in tok.items():\n                bank[k][i].copy_(t)\n\n        self._tok_bank = {k: v.contiguous() for k, v in bank.items()}\n        shapes = {k: tuple(v.shape) for k, v in self._tok_bank.items()}\n        print(f\"✓ Предтокенизация кастомной single-функцией завершена: shapes={shapes}\")\n\n    def __len__(self):\n        return self._N\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        item: Dict[str, Any] = {}\n        item[\"labels\"] = self._labels[idx]  # [K] float32 для регрессии, int64 для классификации\n        if self.text_columns:\n            if self._tok_bank is not None:\n                item[\"text_tokens\"] = {k: v[idx] for k, v in self._tok_bank.items()}\n            else:\n                item[\"text\"] = self._join_text(self.df.iloc[idx])\n        if self._image_lists is not None:\n            item[\"images\"] = self._image_lists[idx]\n        if self._audio_lists is not None:\n            item[\"audios\"] = self._audio_lists[idx]\n        return item\n\n    def get_cache_stats(self):\n        has = self._tok_bank is not None\n        sizes = {k: tuple(v.shape) for k, v in (self._tok_bank or {}).items()}\n        return {\"has_pretokenized\": has, \"shapes\": sizes, \"N\": self._N}\n\n    def clear_cache(self):\n        self._tok_bank = None\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n# data/collate.py\nfrom typing import Any, Dict, List, Optional\nimport numpy as np\nimport torch\n# from mmkit.core.utils import to_pil, load_audio\n\nclass MultiModalCollator:\n    \"\"\"\n    Готовит backend_inputs для модели.\n    - Если в item есть \"text_tokens\": стакает их.\n    - Иначе использует processors[\"text\"] для батчевой токенизации.\n    - Изображения/аудио: разворачивает в плоский список + считает counts.\n    \"\"\"\n    def __init__(self, processors: Dict[str, Any], task: str = \"regression\", audio_sr_fallback: int = 16000):\n        self.processors = processors\n        self.task = task.lower()\n        self.audio_sr_fallback = audio_sr_fallback\n\n    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        labels = [b.get(\"labels\") for b in batch]\n        if labels and isinstance(labels[0], torch.Tensor):\n            labels = torch.stack(labels)\n        else:\n            if self.task == \"classification\":\n                labels = torch.tensor(labels, dtype=torch.long)\n            else:\n                labels = torch.tensor(labels, dtype=torch.float32)\n\n        out = {\"labels\": labels, \"backend_inputs\": {}}\n        # text\n        if \"text\" in self.processors or any((\"text_tokens\" in b) for b in batch):\n            if \"text_tokens\" in batch[0]:\n                t0 = batch[0][\"text_tokens\"]\n                text_inputs = {}\n                for key in t0.keys():\n                    if torch.is_tensor(t0[key]):\n                        text_inputs[key] = torch.stack([b[\"text_tokens\"][key] for b in batch])\n                    else:\n                        dtype = torch.long if key in (\"input_ids\",\"attention_mask\",\"token_type_ids\") else torch.float32\n                        text_inputs[key] = torch.tensor([b[\"text_tokens\"][key] for b in batch], dtype=dtype)\n            else:\n                texts = [b.get(\"text\", \"\") or \" \" for b in batch]\n                text_inputs = self.processors[\"text\"].prepare_batch(texts)\n            out[\"backend_inputs\"][\"text_inputs\"] = text_inputs\n\n        # images\n        if \"image\" in self.processors:\n            flat_images, counts = [], []\n            for b in batch:\n                lst = b.get(\"images\", []) or []\n                lst = [to_pil(x) for x in lst if x is not None]\n                counts.append(len(lst))\n                flat_images.extend(lst)\n            if len(flat_images) > 0:\n                out[\"backend_inputs\"][\"image_inputs\"] = self.processors[\"image\"].prepare_batch(flat_images)\n            else:\n                out[\"backend_inputs\"][\"image_inputs\"] = {\"pixel_values\": None}\n            out[\"backend_inputs\"][\"image_counts\"] = torch.tensor(counts, dtype=torch.long)\n\n        # audio\n        if \"audio\" in self.processors:\n            flat, counts = [], []\n            for b in batch:\n                lst = b.get(\"audios\", []) or []\n                counts.append(len(lst))\n                for a in lst:\n                    if isinstance(a, str):\n                        sr = getattr(self.processors[\"audio\"], \"sr\", self.audio_sr_fallback)\n                        flat.append(load_audio(a, sr))\n                    else:\n                        arr = np.asarray(a, dtype=np.float32)\n                        if arr.ndim>1: arr = np.squeeze(arr)\n                        if arr.ndim>1: arr = arr.reshape(-1)\n                        flat.append(arr)\n            out[\"backend_inputs\"][\"audio_counts\"] = torch.tensor(counts, dtype=torch.long)\n            out[\"backend_inputs\"][\"audio_inputs\"] = self.processors[\"audio\"].prepare_batch(flat) if len(flat)>0 else {\"input_values\": None, \"input_features\": None, \"raw_audios\": []}\n\n        out[\"backend_inputs\"][\"batch_size\"] = len(batch)\n        return out\n\n# processors/text.py\n# from mmkit.data.tokenization import BatchTokenizer\n\nclass TextProcessor:\n    modality = \"text\"\n    def __init__(self, tokenizer, max_length=512, padding=\"max_length\", cache_size=10000, batch_size=256):\n        self.bt = BatchTokenizer(tokenizer, max_length=max_length, cache_size=cache_size, batch_size=batch_size, padding_strategy=padding)\n    def prepare_batch(self, texts):\n        return self.bt.tokenize_batch(texts, use_cache=True)\n    def clear_cache(self):\n        self.bt.clear_cache()\n\n# processors/image.py\nclass ImageProcessor:\n    modality = \"image\"\n    def __init__(self, hf_processor):\n        self.proc = hf_processor\n    def prepare_batch(self, images):\n        if len(images)==0: return {\"pixel_values\": None}\n        x = self.proc(images=images, return_tensors=\"pt\")\n        return {\"pixel_values\": x[\"pixel_values\"]}\n\n# processors/audio.py\nclass ClapAudioProcessor:\n    modality=\"audio\"\n    def __init__(self, hf_processor, sr=48000):\n        self.proc = hf_processor\n        self.sr = sr\n    def prepare_batch(self, raw_list):\n        x = self.proc(audios=raw_list, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n        return {\"input_features\": x[\"input_features\"]}\n\nclass Wav2VecAudioProcessor:\n    modality=\"audio\"\n    def __init__(self, hf_processor, sr=16000):\n        self.proc = hf_processor\n        self.sr = sr\n    def prepare_batch(self, raw_list):\n        x = self.proc(raw_list, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n        return {\"input_values\": x[\"input_values\"]}\n\nclass Wav2ClipAudioProcessor:\n    modality=\"audio\"\n    def __init__(self, sr=16000):\n        self.sr = sr\n    def prepare_batch(self, raw_list):\n        return {\"raw_audios\": raw_list}  # список np.ndarray; энкодер сам обработает\n\n# encoders/base.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BaseEncoder(nn.Module):\n    modality: str = \"text\"\n    embed_dim: int = 0\n\n    def _normalize(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() > 2:\n            x = x.view(x.size(0), -1)\n        return F.normalize(x, dim=-1, eps=1e-12)\n\n# encoders/text_auto.py\nimport torch\nfrom transformers import AutoModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass AutoTextEncoder(BaseEncoder):\n    modality=\"text\"\n    def __init__(self, checkpoint=\"bert-base-multilingual-cased\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(AutoModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.hidden_size\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        out = self.model(**inputs)\n        z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state.mean(dim=1)\n        return self._normalize(z)\n\n# encoders/text_clip.py\nimport torch\nfrom transformers import CLIPModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass CLIPTextEncoder(BaseEncoder):\n    modality=\"text\"\n    def __init__(self, checkpoint=\"openai/clip-vit-base-patch32\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(CLIPModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.projection_dim\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        z = self.model.get_text_features(**inputs)\n        return self._normalize(z)\n\n# encoders/image_auto.py\nimport torch\nfrom transformers import AutoModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass AutoImageEncoder(BaseEncoder):\n    modality=\"image\"\n    def __init__(self, checkpoint=\"google/vit-base-patch16-224\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(AutoModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.hidden_size\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        pv = inputs.get(\"pixel_values\", None)\n        if pv is None: \n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        out = self.model(pixel_values=pv)\n        z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state[:, 0]\n        return self._normalize(z)\n\n# encoders/image_clip.py\nimport torch\nfrom transformers import CLIPModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass CLIPImageEncoder(BaseEncoder):\n    modality=\"image\"\n    def __init__(self, checkpoint=\"openai/clip-vit-base-patch32\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(CLIPModel, checkpoint, cache_dir)\n        self.embed_dim = self.model.config.projection_dim\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        pv = inputs.get(\"pixel_values\", None)\n        if pv is None:\n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        z = self.model.get_image_features(pixel_values=pv)\n        return self._normalize(z)\n\n# encoders/audio_clap.py\nimport torch\nfrom transformers import ClapModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass CLAPAudioEncoder(BaseEncoder):\n    modality=\"audio\"\n    def __init__(self, checkpoint=\"laion/clap-htsat-unfused\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(ClapModel, checkpoint, cache_dir)\n        self.embed_dim = getattr(self.model.config, \"projection_dim\", 512)\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        feats = inputs.get(\"input_features\")\n        if feats is None:\n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        z = self.model.get_audio_features(input_features=feats.float())\n        return self._normalize(z.float())\n\n# encoders/audio_wav2vec.py\nimport torch\nfrom transformers import AutoModel\n# from mmkit.core.utils import safe_load\n# from .base import BaseEncoder\n\nclass Wav2Vec2Encoder(BaseEncoder):\n    modality=\"audio\"\n    def __init__(self, checkpoint=\"facebook/wav2vec2-base-960h\", cache_dir=\"./model_cache\"):\n        super().__init__()\n        self.model = safe_load(AutoModel, checkpoint, cache_dir)\n        self.embed_dim = getattr(self.model.config, \"hidden_size\", 768)\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        iv = inputs.get(\"input_values\")\n        if iv is None:\n            return torch.zeros(0, self.embed_dim, device=next(self.model.parameters()).device)\n        out = self.model(input_values=iv.float())\n        z = out.pooler_output if getattr(out, \"pooler_output\", None) is not None else out.last_hidden_state.mean(dim=1)\n        return self._normalize(z.float())\n\n# encoders/audio_wav2clip.py\nimport numpy as np\nimport torch\n# from .base import BaseEncoder\n\nclass Wav2ClipEncoder(BaseEncoder):\n    modality=\"audio\"\n    def __init__(self):\n        super().__init__()\n        try:\n            import wav2clip as w2c\n        except Exception as e:\n            raise RuntimeError(\"Требуется wav2clip: pip install wav2clip\") from e\n        self.w2c = w2c\n        model = None\n        if hasattr(w2c, \"get_model\"):\n            model = w2c.get_model()\n        elif hasattr(w2c, \"model\"):\n            m = w2c.model\n            model = m() if callable(m) else m\n        else:\n            raise RuntimeError(\"wav2clip не содержит get_model()/model\")\n        self.model = model\n        self.embed_dim = 512\n\n    @torch.no_grad()\n    def encode_batch(self, inputs):\n        raws = inputs.get(\"raw_audios\", [])\n        if len(raws)==0:\n            return torch.zeros(0, self.embed_dim)\n        embs = []\n        for arr in raws:\n            a = np.asarray(arr, dtype=np.float32)\n            if a.ndim>1: a = np.squeeze(a)\n            if a.ndim>1: a = a.reshape(-1)\n            if a.size < 512:\n                a = np.pad(a, (0, 512 - a.size), mode=\"constant\")\n            try:\n                e = self.w2c.embed_audio(a, self.model)\n                e = np.asarray(e)\n            except Exception:\n                x = torch.from_numpy(a).float().unsqueeze(0)\n                y = self.model(x)\n                if isinstance(y, (tuple, list)):\n                    y = y[0]\n                if torch.is_tensor(y):\n                    if y.dim()==2 and y.size(0)==1: y=y.squeeze(0)\n                    e = y.detach().cpu().numpy()\n                else:\n                    e = np.asarray(y)\n            if e.ndim>1: e = e.reshape(-1)\n            embs.append(torch.as_tensor(e, dtype=torch.float32))\n        z = torch.stack(embs, dim=0)\n        z = torch.nn.functional.normalize(z, dim=-1, eps=1e-12)\n        return z\n\n# aggregation/item_pool.py\nimport torch\nimport torch.nn.functional as F\n\nclass ItemAggregator:\n    def aggregate(self, embs: torch.Tensor, counts, max_k: int, how: str):\n        if embs is None or (torch.is_tensor(embs) and embs.numel()==0):\n            D = 0 if embs is None else getattr(embs, \"size\", lambda *_: 0)(-1)\n            out_dim = D*max_k if how==\"concat\" else D\n            return torch.zeros((len(counts), out_dim), device=embs.device if torch.is_tensor(embs) else \"cpu\", dtype=torch.float32)\n        if embs.dim()==1: embs = embs.unsqueeze(0)\n        if embs.dim()>2: embs = embs.view(embs.size(0), -1)\n        N, D = embs.size()\n        out_dim = D*max_k if how==\"concat\" else D\n        out = torch.zeros((len(counts), out_dim), device=embs.device, dtype=embs.dtype)\n        off = 0\n        for i, c in enumerate(counts):\n            if c<=0 or off>=N: continue\n            take = min(c, N-off)\n            sample = embs[off:off+take]\n            off += take\n            if how==\"concat\":\n                sample = sample[:max_k]\n                if sample.size(0)<max_k:\n                    pad = torch.zeros((max_k - sample.size(0), D), device=embs.device, dtype=embs.dtype)\n                    sample = torch.cat([sample, pad], dim=0)\n                out[i] = sample.reshape(-1)\n            else:\n                out[i] = sample.mean(dim=0)\n        return F.normalize(out, dim=-1, eps=1e-12)\n\n# aggregation/fusion.py\nimport torch\n\nclass ConcatFusion:\n    def __call__(self, zs: dict) -> torch.Tensor:\n        feats = [zs[k] for k in [\"image\",\"text\",\"audio\"] if k in zs]\n        return torch.cat(feats, dim=-1)\n\nclass MeanFusion:\n    def __call__(self, zs: dict) -> torch.Tensor:\n        feats = [zs[k] for k in [\"image\",\"text\",\"audio\"] if k in zs]\n        return torch.stack(feats, dim=0).mean(dim=0)\n\n# heads/regression.py\nimport torch.nn as nn\n\nclass RegressionHead(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden=256, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, out_dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n# heads/classification.py\nimport torch.nn as nn\n\nclass ClassificationHead(nn.Module):\n    def __init__(self, in_dim, num_labels, hidden=512, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n    def forward(self, x): return self.net(x)\n\n# model/multimodal.py\nfrom typing import Dict\nimport torch\nimport torch.nn as nn\n# from mmkit.aggregation.item_pool import ItemAggregator\n\nclass MultiModalModel(nn.Module):\n    \"\"\"\n    Модель: dict(encoders) + aggregator (для image/audio множественных) + fusion + head.\n    \"\"\"\n    def __init__(self, encoders: Dict[str, nn.Module], aggregator: ItemAggregator, fusion, head,\n                 image_cfg=None, audio_cfg=None):\n        super().__init__()\n        self.enc = nn.ModuleDict(encoders)\n        self.agg = aggregator\n        self.fusion = fusion\n        self.head = head\n        self.image_cfg = image_cfg or {\"max_items\":1, \"how\":\"concat\"}\n        self.audio_cfg = audio_cfg or {\"max_items\":1, \"how\":\"concat\"}\n\n    def _fwd_features(self, backend_inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        zs = {}\n        if \"text\" in self.enc and \"text_inputs\" in backend_inputs:\n            zs[\"text\"] = self.enc[\"text\"].encode_batch(backend_inputs[\"text_inputs\"])\n        if \"image\" in self.enc and \"image_inputs\" in backend_inputs:\n            flat = self.enc[\"image\"].encode_batch(backend_inputs[\"image_inputs\"])\n            counts = backend_inputs.get(\"image_counts\", torch.tensor([0]*backend_inputs.get(\"batch_size\", flat.size(0)))).tolist()\n            zs[\"image\"] = self.agg.aggregate(flat, counts, self.image_cfg[\"max_items\"], self.image_cfg[\"how\"])\n        if \"audio\" in self.enc and \"audio_inputs\" in backend_inputs:\n            flat = self.enc[\"audio\"].encode_batch(backend_inputs[\"audio_inputs\"])\n            counts = backend_inputs.get(\"audio_counts\", torch.tensor([0]*backend_inputs.get(\"batch_size\", flat.size(0)))).tolist()\n            zs[\"audio\"] = self.agg.aggregate(flat, counts, self.audio_cfg[\"max_items\"], self.audio_cfg[\"how\"])\n        return zs\n\n    def forward(self, backend_inputs: Dict[str, torch.Tensor], labels: torch.Tensor = None):\n        zs = self._fwd_features(backend_inputs)\n        if not zs:\n            raise ValueError(\"Энкодеры не вернули эмбеддинги\")\n        fused = self.fusion(zs)\n        logits = self.head(fused)\n        return {\"logits\": logits}\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, torch.Tensor], return_per_modality: bool = False):\n        zs = self._fwd_features(backend_inputs)\n        fused = self.fusion(zs)\n        return (fused, zs) if return_per_modality else fused\n\n# train/trainer_hf.py\nfrom typing import Optional\nimport torch\nimport torch.nn.functional as F\nfrom transformers import Trainer\n\nclass MSETrainer(Trainer):\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n        **kwargs\n    ):\n        labels = inputs.pop(\"labels\").to(torch.float32)\n        backend_inputs = inputs.pop(\"backend_inputs\", None)\n        if backend_inputs is None:\n            # fallback: все остальное считаем backend_inputs\n            backend_inputs = inputs\n\n        out = model(backend_inputs=backend_inputs)  # передаем строго backend_inputs\n        logits = out[\"logits\"]\n        preds = logits if logits.dim() == 1 else (logits.squeeze(-1) if logits.size(-1) == 1 else logits)\n        labels = labels.view_as(preds)\n        loss = F.mse_loss(preds, labels)\n        return (loss, out) if return_outputs else loss\n\nclass WeightedCETrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = (\n            torch.as_tensor(class_weights, dtype=torch.float32) if class_weights is not None else None\n        )\n\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: Optional[int] = None,\n        **kwargs\n    ):\n        labels = inputs.pop(\"labels\")\n        backend_inputs = inputs.pop(\"backend_inputs\", None)\n        if backend_inputs is None:\n            backend_inputs = inputs\n\n        out = model(backend_inputs=backend_inputs)  # только backend_inputs\n        logits = out[\"logits\"]\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = F.cross_entropy(logits, labels.long(), weight=weight)\n        return (loss, out) if return_outputs else loss\n\n# train/callbacks.py\nfrom transformers.trainer_callback import TrainerCallback\nfrom tqdm.auto import tqdm\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n        self.tqdm = tqdm\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            self.tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n# train/metrics.py\nimport numpy as np\n\ndef build_regression_metrics(name: str):\n    name = name.lower()\n    if name not in (\"rmse\", \"mae\", \"r2\"):\n        raise ValueError('metric_name для регрессии должен быть \"rmse\", \"mae\" или \"r2\"')\n\n    def compute(p):\n        preds = p.predictions\n        y = p.label_ids\n        preds = preds.squeeze(-1) if preds.ndim == 2 and preds.shape[-1] == 1 else preds\n        y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n        axis = 1 if preds.ndim == 2 else None\n        if name == \"rmse\":\n            err = preds - y\n            mse = np.mean(err**2, axis=axis)\n            rmse = np.sqrt(mse)\n            return {\"rmse\": float(np.mean(rmse))}\n        elif name == \"mae\":\n            mae = np.mean(np.abs(preds - y), axis=axis)\n            return {\"mae\": float(np.mean(mae))}\n        else:\n            y_mean = np.mean(y, axis=axis, keepdims=True) if preds.ndim == 2 else np.mean(y)\n            ss_res = np.sum((y - preds) ** 2, axis=axis)\n            ss_tot = np.sum((y - y_mean) ** 2, axis=axis)\n            r2 = 1.0 - (ss_res / (ss_tot + 1e-12))\n            return {\"r2\": float(np.mean(r2))}\n    return compute\n\n# train/recipes.py\nfrom transformers import CLIPTokenizer, CLIPImageProcessor, AutoTokenizer, AutoImageProcessor, ClapProcessor, AutoProcessor\n# from mmkit.processors.text import TextProcessor\n# from mmkit.processors.image import ImageProcessor\n# from mmkit.processors.audio import ClapAudioProcessor, Wav2VecAudioProcessor, Wav2ClipAudioProcessor\n\n# from mmkit.encoders.text_clip import CLIPTextEncoder\n# from mmkit.encoders.text_auto import AutoTextEncoder\n# from mmkit.encoders.image_clip import CLIPImageEncoder\n# from mmkit.encoders.image_auto import AutoImageEncoder\n# from mmkit.encoders.audio_clap import CLAPAudioEncoder\n# from mmkit.encoders.audio_wav2vec import Wav2Vec2Encoder\n# from mmkit.encoders.audio_wav2clip import Wav2ClipEncoder\n\n# from mmkit.aggregation.item_pool import ItemAggregator\n# from mmkit.aggregation.fusion import ConcatFusion, MeanFusion\n# from mmkit.heads.regression import RegressionHead\n# from mmkit.model.multimodal import MultiModalModel\n\ndef build_auto_regression(modalities,\n                          fusion=\"concat\",\n                          # text config\n                          text_model_type=\"clip\", text_checkpoint=\"openai/clip-vit-base-patch32\", text_max_length=77, text_padding=\"max_length\",\n                          # image config\n                          image_model_type=\"clip\", image_checkpoint=\"openai/clip-vit-base-patch32\", max_images=1, image_agg=\"concat\",\n                          # audio config\n                          audio_model_type=\"clap\", audio_checkpoint=\"laion/clap-htsat-unfused\", audio_sr=48000, max_audios=1, audio_agg=\"concat\",\n                          # head config\n                          out_dim=1, hidden=256, dropout=0.1,\n                          cache_dir=\"./model_cache\"):\n    encoders = {}\n    processors = {}\n    # text\n    if \"text\" in modalities:\n        if text_model_type == \"clip\":\n            tok = CLIPTokenizer.from_pretrained(text_checkpoint, cache_dir=cache_dir)\n            processors[\"text\"] = TextProcessor(tok, max_length=text_max_length, padding=text_padding)\n            encoders[\"text\"] = CLIPTextEncoder(text_checkpoint, cache_dir=cache_dir)\n            text_dim = encoders[\"text\"].embed_dim\n        else:\n            tok = AutoTokenizer.from_pretrained(text_checkpoint, cache_dir=cache_dir)\n            processors[\"text\"] = TextProcessor(tok, max_length=text_max_length, padding=text_padding)\n            encoders[\"text\"] = AutoTextEncoder(text_checkpoint, cache_dir=cache_dir)\n            text_dim = encoders[\"text\"].embed_dim\n    # image\n    image_cfg = None\n    if \"image\" in modalities:\n        if image_model_type == \"clip\":\n            ip = CLIPImageProcessor.from_pretrained(image_checkpoint, cache_dir=cache_dir)\n            processors[\"image\"] = ImageProcessor(ip)\n            encoders[\"image\"] = CLIPImageEncoder(image_checkpoint, cache_dir=cache_dir)\n        else:\n            ip = AutoImageProcessor.from_pretrained(image_checkpoint, cache_dir=cache_dir)\n            processors[\"image\"] = ImageProcessor(ip)\n            encoders[\"image\"] = AutoImageEncoder(image_checkpoint, cache_dir=cache_dir)\n        image_dim = encoders[\"image\"].embed_dim\n        image_cfg = {\"max_items\": max_images, \"how\": image_agg}\n    # audio\n    audio_cfg = None\n    if \"audio\" in modalities:\n        if audio_model_type == \"clap\":\n            ap = ClapProcessor.from_pretrained(audio_checkpoint, cache_dir=cache_dir)\n            processors[\"audio\"] = ClapAudioProcessor(ap, sr=audio_sr)\n            encoders[\"audio\"] = CLAPAudioEncoder(audio_checkpoint, cache_dir=cache_dir)\n        elif audio_model_type == \"wav2vec\":\n            ap = AutoProcessor.from_pretrained(audio_checkpoint, cache_dir=cache_dir)\n            processors[\"audio\"] = Wav2VecAudioProcessor(ap, sr=audio_sr)\n            encoders[\"audio\"] = Wav2Vec2Encoder(audio_checkpoint, cache_dir=cache_dir)\n        else:  # wav2clip\n            processors[\"audio\"] = Wav2ClipAudioProcessor(sr=audio_sr)\n            encoders[\"audio\"] = Wav2ClipEncoder()\n        audio_dim = encoders[\"audio\"].embed_dim\n        audio_cfg = {\"max_items\": max_audios, \"how\": audio_agg}\n\n    agg = ItemAggregator()\n    fusion_mod = ConcatFusion() if fusion==\"concat\" else MeanFusion()\n    # compute in_dim\n    dims = []\n    for m in [\"image\",\"text\",\"audio\"]:\n        if m in encoders:\n            dims.append(encoders[m].embed_dim)\n    if fusion==\"concat\":\n        in_dim = sum(dims)\n    else:\n        assert len(set(dims))==1, \"Для fusion=mean требуется одинаковая размерность эмбеддингов\"\n        in_dim = dims[0]\n    head = RegressionHead(in_dim, out_dim=out_dim, hidden=hidden, dropout=dropout)\n    model = MultiModalModel(encoders, agg, fusion_mod, head, image_cfg=image_cfg, audio_cfg=audio_cfg)\n    return processors, model\n\n# pipelines/regression.py\nimport math\nimport gc\nfrom typing import Any, Dict, List, Optional\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nfrom transformers import TrainingArguments\n# from mmkit.core.utils import set_seed\n# from mmkit.data.dataset import MultiModalDataset\n# from mmkit.data.collate import MultiModalCollator\n# from mmkit.train.trainer_hf import MSETrainer\n# from mmkit.train.callbacks import PbarConsoleLogger\n# from mmkit.train.metrics import build_regression_metrics\n# from mmkit.train.recipes import build_auto_regression\n\nclass MultiModalRegressionPipeline:\n    \"\"\"\n    Высокоуровневый пайплайн мультимодальной регрессии (multi-target).\n    Поддерживает: text/image/audio, dynamic/max padding, предтокенизацию, чанки, RMSE/MAE/R2.\n    \"\"\"\n    def __init__(self,\n                 modalities: List[str],\n                 target_column_names: List[str],\n                 text_columns: Optional[List[str]] = None,\n                 image_columns: Optional[List[str]] = None,\n                 audio_columns: Optional[List[str]] = None,\n                 # recipe params\n                 backend: str = \"auto\",\n                 clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n                 clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n                 text_model_config: Optional[Dict[str, Any]] = None,\n                 image_model_config: Optional[Dict[str, Any]] = None,\n                 audio_model_config: Optional[Dict[str, Any]] = None,\n                 fusion: str = \"concat\",\n                 # tokenizer\n                 text_padding: str = \"max_length\",\n                 text_max_length: int = 77,\n                 use_batch_tokenizer: bool = True,\n                 pretokenize_data: bool = True,\n                 pretokenize_batch_size: int = 256,\n                 tokenizer_cache_size: int = 10000,\n                 max_pretokenize_samples: int = 100000,\n                 local_cache_dir: str = \"./model_cache\"):\n        self.modalities = sorted(list(set(modalities)))\n        self.target_column_names = list(target_column_names)\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n        self.fusion = fusion\n\n        self.text_padding = text_padding\n        self.text_max_length = text_max_length\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n\n        self._target_vec_col = \"__target_vector__\"\n        self.processors = {}\n        self.model = None\n        self.trainer = None\n\n    def _attach_target_vector(self, df: pd.DataFrame, fill_zeros: bool = False) -> pd.DataFrame:\n        df_c = df.copy()\n        K = len(self.target_column_names)\n        if fill_zeros:\n            df_c[self._target_vec_col] = [np.zeros(K, dtype=np.float32) for _ in range(len(df_c))]\n        else:\n            def _row_to_vec(row):\n                vals = [row[c] for c in self.target_column_names]\n                return np.asarray(vals, dtype=np.float32)\n            df_c[self._target_vec_col] = df_c.apply(_row_to_vec, axis=1)\n        return df_c\n\n    def _validate_modalities(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            assert self.text_columns, \"Вы выбрали text, но text_columns пустой\"\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing: raise ValueError(f\"Нет текстовых колонок: {missing}\")\n        if \"image\" in self.modalities:\n            assert self.image_columns, \"Вы выбрали image, но image_columns пуст\"\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing: raise ValueError(f\"Нет колонок изображений: {missing}\")\n        if \"audio\" in self.modalities:\n            assert self.audio_columns, \"Вы выбрали audio, но audio_columns пуст\"\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing: raise ValueError(f\"Нет колонок аудио: {missing}\")\n\n    def _validate_targets(self, df: pd.DataFrame):\n        miss = [c for c in self.target_column_names if c not in df.columns]\n        if miss:\n            raise ValueError(f\"В DataFrame отсутствуют целевые колонки: {miss}\")\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _build_model(self, num_outputs: int, image_cfg: Dict[str, Any], audio_cfg: Dict[str, Any]):\n        # Авто-рецепт: CLIP для text+image, CLAP для audio (или явные конфиги)\n        if self.backend_name == \"auto\":\n            # auto defaults\n            txt_type = \"clip\" if \"image\" in self.modalities else (\"auto\" if \"text\" in self.modalities else None)\n            img_type = \"clip\" if \"image\" in self.modalities else None\n            aud_type = \"clap\" if \"audio\" in self.modalities else None\n\n            txt_cp = self.clip_checkpoint if txt_type==\"clip\" else (self.text_model_config[\"checkpoint\"] if self.text_model_config else \"bert-base-multilingual-cased\")\n            img_cp = self.clip_checkpoint if img_type==\"clip\" else (self.image_model_config[\"checkpoint\"] if self.image_model_config else \"google/vit-base-patch16-224\")\n            aud_cp = self.clap_checkpoint if aud_type==\"clap\" else (self.audio_model_config[\"checkpoint\"] if self.audio_model_config else \"facebook/wav2vec2-base-960h\")\n\n            if aud_type is None and \"audio\" in self.modalities and self.audio_model_config:\n                aud_type = self.audio_model_config.get(\"model_type\",\"clap\")\n\n            processors, model = build_auto_regression(\n                modalities=self.modalities,\n                fusion=self.fusion,\n                text_model_type=\"clip\" if txt_type==\"clip\" else \"auto\",\n                text_checkpoint=txt_cp,\n                text_max_length=self.text_max_length,\n                text_padding=self.text_padding,\n                image_model_type=\"clip\" if img_type==\"clip\" else \"auto\",\n                image_checkpoint=img_cp,\n                max_images=image_cfg.get(\"max_images\", 1),\n                image_agg=image_cfg.get(\"image_agg\", \"concat\"),\n                audio_model_type=aud_type if aud_type else \"clap\",\n                audio_checkpoint=aud_cp,\n                audio_sr=audio_cfg.get(\"sr\", 48000),\n                max_audios=audio_cfg.get(\"max_audios\", 1),\n                audio_agg=audio_cfg.get(\"audio_agg\", \"concat\"),\n                out_dim=num_outputs,\n                hidden=256,\n                dropout=0.1,\n                cache_dir=self.local_cache_dir\n            )\n            return processors, model\n\n        # Иначе — можно сделать фабрику из переданных конфигов (опущено для краткости)\n        raise NotImplementedError(\"Ручная сборка (не auto) не реализована в этой версии\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"rmse\",       # \"rmse\" | \"mae\" | \"r2\"\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result_reg\",\n        seed: int = 42,\n        gradient_checkpointing: bool = False\n    ):\n        import os\n        from transformers import TrainingArguments\n    \n        # 1) Проверки и подготовка\n        self._validate_modalities(train_data)\n        self._validate_targets(train_data)\n        set_seed(seed)\n    \n        df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n        if test_data is not None:\n            self._validate_targets(test_data)\n    \n        # Преобразуем таргеты в одну вектор-колонку\n        df_train_ext = self._attach_target_vector(df_train, fill_zeros=False)\n        df_eval_ext  = self._attach_target_vector(df_eval,  fill_zeros=False)\n    \n        # 2) Модель и процессоры (auto-рецепт)\n        image_cfg = {\"max_images\": 1, \"image_agg\": \"concat\"}\n        audio_cfg = {\"sr\": 48000, \"max_audios\": 1, \"audio_agg\": \"concat\"}\n    \n        self.processors, self.model = self._build_model(\n            num_outputs=len(self.target_column_names),\n            image_cfg=image_cfg,\n            audio_cfg=audio_cfg\n        )\n    \n        if gradient_checkpointing:\n            try:\n                self.model.gradient_checkpointing_enable()\n            except Exception:\n                pass\n    \n        # 3) Datasets (без чанков)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        pretokenize_train = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_train_ext) <= self.max_pretokenize_samples)\n        pretokenize_eval  = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_eval_ext)  <= self.max_pretokenize_samples)\n    \n        ds_train = MultiModalDataset(\n            df=df_train_ext,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_train) else None,\n            pretokenize=pretokenize_train,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        ds_eval = MultiModalDataset(\n            df=df_eval_ext,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_eval) else None,\n            pretokenize=pretokenize_eval,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n    \n        collate = MultiModalCollator(self.processors, task=\"regression\")\n        compute_metrics = build_regression_metrics(metric_name)\n    \n        # 4) Аргументы обучения (eval_strategy)\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.0,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=False\n        )\n    \n        # 5) Trainer и старт обучения\n        self.trainer = MSETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train,\n            eval_dataset=ds_eval,\n            data_collator=collate,\n            compute_metrics=compute_metrics\n        )\n    \n        try:\n            from transformers.trainer_callback import PrinterCallback\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n    \n        self.trainer.train()\n    \n        if has_bt:\n            try: self.processors[\"text\"].clear_cache()\n            except Exception: pass\n    \n        return self\n\n    def predict(self, df: pd.DataFrame, batch_size: Optional[int] = None) -> np.ndarray:\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        collate = MultiModalCollator(self.processors, task=\"regression\")\n\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch - 1) // effective_batch\n        print(f\"Running predictions (batch_size={effective_batch}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds, metric_key_prefix=\"predict\")\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        ds.clear_cache()\n        y = preds.predictions\n        y = y.squeeze(-1) if y.ndim == 2 and y.shape[-1] == 1 else y\n        return y\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        device = next(self.trainer.model.parameters()).device\n        self.model.to(device).eval()\n\n        df_c = self._attach_target_vector(df, fill_zeros=True)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=\"__target_vector__\",\n            task=\"regression\",\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=False\n        )\n        collate = MultiModalCollator(self.processors, task=\"regression\")\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = batch[\"backend_inputs\"]\n                def move_to_device(obj):\n                    if torch.is_tensor(obj): return obj.to(device)\n                    if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n                    if isinstance(obj, list): return [move_to_device(v) for v in obj]\n                    return obj\n                bi = move_to_device(bi)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n        return fused_arr, per_mod\n\n# pipelines/classification.py\nimport os\nimport math\nimport gc\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import TrainingArguments\n\n# from mmkit.core.utils import set_seed\n# from mmkit.data.dataset import MultiModalDataset\n# from mmkit.data.collate import MultiModalCollator\n\n# from mmkit.train.trainer_hf import WeightedCETrainer\n# from mmkit.train.callbacks import PbarConsoleLogger\n\n# from mmkit.aggregation.item_pool import ItemAggregator\n# from mmkit.aggregation.fusion import ConcatFusion, MeanFusion\n# from mmkit.heads.classification import ClassificationHead\n# from mmkit.model.multimodal import MultiModalModel\n\n# Процессоры\nfrom transformers import (\n    CLIPTokenizer,\n    CLIPImageProcessor,\n    AutoTokenizer,\n    AutoImageProcessor,\n    ClapProcessor,\n    AutoProcessor,\n)\n\n# from mmkit.processors.text import TextProcessor\n# from mmkit.processors.image import ImageProcessor\n# from mmkit.processors.audio import (\n#     ClapAudioProcessor,\n#     Wav2VecAudioProcessor,\n#     Wav2ClipAudioProcessor,\n# )\n\n# # Энкодеры\n# from mmkit.encoders.text_clip import CLIPTextEncoder\n# from mmkit.encoders.text_auto import AutoTextEncoder\n# from mmkit.encoders.image_clip import CLIPImageEncoder\n# from mmkit.encoders.image_auto import AutoImageEncoder\n# from mmkit.encoders.audio_clap import CLAPAudioEncoder\n# from mmkit.encoders.audio_wav2vec import Wav2Vec2Encoder\n# from mmkit.encoders.audio_wav2clip import Wav2ClipEncoder\n\n\nclass MultiModalClassificationPipeline:\n    \"\"\"\n    Высокоуровневый пайплайн мультимодальной классификации (single/multi-class).\n    Поддерживает text/image/audio, dynamic/max padding, предтокенизацию, чанковую тренировку,\n    accuracy/f1 метрики, class weights, predict и извлечение эмбеддингов.\n\n    По умолчанию (backend=\"auto\"):\n      - если есть image: CLIP для text и image;\n      - если есть audio: CLAP (или wav2vec/wav2clip по желанию — см. параметры),\n      - для чистого текста: Auto (BERT) по умолчанию.\n\n    Параметры:\n      - modalities: список модальностей среди [\"text\",\"image\",\"audio\"]\n      - target_column_name: имя колонки с метками (строки/инты)\n      - num_labels: число классов (если None — возьмётся из train_data по unique)\n      - text_columns, image_columns, audio_columns: колонки данных\n      - fusion: \"concat\" | \"mean\" (для mean размеры эмбеддингов должны совпадать)\n      - text_padding: \"max_length\" | \"dynamic\"\n      - text_max_length: длина токенов\n      - pretokenize_data: предтокенизация датасета (выключится при dynamic padding)\n      - tokenizer_cache_size: LRU-кэш токенизатора\n      - max_pretokenize_samples: лимит на предтокенизацию в чанке\n      - local_cache_dir: кэш HF\n\n      - audio_backend: \"clap\" | \"wav2vec\" | \"wav2clip\" (для audio)\n      - audio_sr: sampling rate для аудио-процессоров\n      - max_images_per_sample, image_agg: агрегация нескольких картинок (\"concat\"/\"mean\")\n      - max_audios_per_sample, audio_agg: агрегация для аудио\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        target_column_name: str,\n        num_labels: Optional[int] = None,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n\n        # backend/модели\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        text_model_config: Optional[Dict[str, Any]] = None,\n        image_model_config: Optional[Dict[str, Any]] = None,\n        audio_model_config: Optional[Dict[str, Any]] = None,\n\n        fusion: str = \"concat\",\n\n        # токенизация текста\n        text_padding: str = \"max_length\",\n        text_max_length: int = 77,\n        use_batch_tokenizer: bool = True,\n        pretokenize_data: bool = True,\n        pretokenize_batch_size: int = 256,\n        tokenizer_cache_size: int = 10000,\n        max_pretokenize_samples: int = 100000,\n        local_cache_dir: str = \"./model_cache\",\n\n        # изображение/аудио агрегация\n        max_images_per_sample: int = 1,\n        image_agg: str = \"concat\",\n        audio_backend: str = \"clap\",  # \"clap\" | \"wav2vec\" | \"wav2clip\"\n        audio_sr: int = 48000,\n        max_audios_per_sample: int = 1,\n        audio_agg: str = \"concat\",\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.target_column_name = target_column_name\n        self.num_labels = num_labels\n\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n\n        self.backend_name = backend.lower()\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.text_model_config = text_model_config\n        self.image_model_config = image_model_config\n        self.audio_model_config = audio_model_config\n\n        self.fusion = fusion\n        self.text_padding = text_padding\n        self.text_max_length = text_max_length\n        self.use_batch_tokenizer = use_batch_tokenizer\n        self.pretokenize_data = pretokenize_data\n        self.pretokenize_batch_size = pretokenize_batch_size\n        self.tokenizer_cache_size = tokenizer_cache_size\n        self.max_pretokenize_samples = max_pretokenize_samples\n        self.local_cache_dir = local_cache_dir\n\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.image_agg = image_agg\n        self.audio_backend = audio_backend\n        self.audio_sr = int(audio_sr)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n        self.audio_agg = audio_agg\n\n        # Внутренние поля\n        self.label2id: Optional[Dict[Any, int]] = None\n        self.id2label: Optional[Dict[int, str]] = None\n        self.processors: Dict[str, Any] = {}\n        self.model: Optional[MultiModalModel] = None\n        self.trainer: Optional[WeightedCETrainer] = None\n\n    # --------------------------\n    # Валидации и подсобки\n    # --------------------------\n    def _validate_modalities(self, df: pd.DataFrame):\n        if \"text\" in self.modalities:\n            assert self.text_columns, \"Вы выбрали text, но text_columns пустой\"\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"Нет текстовых колонок: {missing}\")\n        if \"image\" in self.modalities:\n            assert self.image_columns, \"Вы выбрали image, но image_columns пуст\"\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"Нет колонок изображений: {missing}\")\n        if \"audio\" in self.modalities:\n            assert self.audio_columns, \"Вы выбрали audio, но audio_columns пуст\"\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"Нет колонок аудио: {missing}\")\n\n    def _ensure_label_mapping(self, df_train: pd.DataFrame):\n        classes = sorted(df_train[self.target_column_name].unique().tolist())\n        if self.num_labels is None:\n            self.num_labels = len(classes)\n        elif self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _ensure_label_column(self, df: pd.DataFrame) -> pd.DataFrame:\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            # Если нет метки — подставим первую известную\n            fake_label = next(iter(self.label2id.keys()))\n            df_c[self.target_column_name] = [fake_label] * len(df_c)\n        return df_c\n\n    # --------------------------\n    # Модель и процессоры (auto)\n    # --------------------------\n    def _build_model_and_processors(self) -> Tuple[Dict[str, Any], MultiModalModel]:\n        encoders = {}\n        processors = {}\n\n        # text\n        if \"text\" in self.modalities:\n            # Если есть image — используем CLIP-текст, иначе Auto (BERT)\n            use_clip_text = (\"image\" in self.modalities)\n            if use_clip_text:\n                tok = CLIPTokenizer.from_pretrained(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n                processors[\"text\"] = TextProcessor(\n                    tok, max_length=self.text_max_length, padding=self.text_padding,\n                    cache_size=self.tokenizer_cache_size, batch_size=self.pretokenize_batch_size\n                )\n                encoders[\"text\"] = CLIPTextEncoder(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n            else:\n                tok = AutoTokenizer.from_pretrained(\n                    (self.text_model_config or {}).get(\"checkpoint\", \"bert-base-multilingual-cased\"),\n                    cache_dir=self.local_cache_dir\n                )\n                processors[\"text\"] = TextProcessor(\n                    tok, max_length=self.text_max_length, padding=self.text_padding,\n                    cache_size=self.tokenizer_cache_size, batch_size=self.pretokenize_batch_size\n                )\n                encoders[\"text\"] = AutoTextEncoder(\n                    (self.text_model_config or {}).get(\"checkpoint\", \"bert-base-multilingual-cased\"),\n                    cache_dir=self.local_cache_dir\n                )\n\n        # image\n        image_cfg = None\n        if \"image\" in self.modalities:\n            if (self.image_model_config or {}).get(\"model_type\", \"clip\") == \"clip\":\n                ip = CLIPImageProcessor.from_pretrained(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n                processors[\"image\"] = ImageProcessor(ip)\n                encoders[\"image\"] = CLIPImageEncoder(self.clip_checkpoint, cache_dir=self.local_cache_dir)\n            else:\n                cp = (self.image_model_config or {}).get(\"checkpoint\", \"google/vit-base-patch16-224\")\n                ip = AutoImageProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n                processors[\"image\"] = ImageProcessor(ip)\n                encoders[\"image\"] = AutoImageEncoder(cp, cache_dir=self.local_cache_dir)\n            image_cfg = {\"max_items\": self.max_images_per_sample, \"how\": self.image_agg}\n\n        # audio\n        audio_cfg = None\n        if \"audio\" in self.modalities:\n            ab = (self.audio_model_config or {}).get(\"model_type\", self.audio_backend)\n            if ab == \"clap\":\n                cp = (self.audio_model_config or {}).get(\"checkpoint\", self.clap_checkpoint)\n                ap = ClapProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n                processors[\"audio\"] = ClapAudioProcessor(ap, sr=self.audio_sr)\n                encoders[\"audio\"] = CLAPAudioEncoder(cp, cache_dir=self.local_cache_dir)\n            elif ab == \"wav2vec\":\n                cp = (self.audio_model_config or {}).get(\"checkpoint\", \"facebook/wav2vec2-base-960h\")\n                ap = AutoProcessor.from_pretrained(cp, cache_dir=self.local_cache_dir)\n                processors[\"audio\"] = Wav2VecAudioProcessor(ap, sr=self.audio_sr)\n                encoders[\"audio\"] = Wav2Vec2Encoder(cp, cache_dir=self.local_cache_dir)\n            else:  # wav2clip\n                processors[\"audio\"] = Wav2ClipAudioProcessor(sr=self.audio_sr)\n                encoders[\"audio\"] = Wav2ClipEncoder()\n            audio_cfg = {\"max_items\": self.max_audios_per_sample, \"how\": self.audio_agg}\n\n        # Fusion + Head\n        agg = ItemAggregator()\n        fusion_mod = ConcatFusion() if self.fusion == \"concat\" else MeanFusion()\n        dims = [encoders[m].embed_dim for m in [\"image\", \"text\", \"audio\"] if m in encoders]\n        if self.fusion == \"concat\":\n            in_dim = sum(dims)\n        else:\n            assert len(set(dims)) == 1, \"Для fusion='mean' размеры эмбеддингов всех модальностей должны совпадать\"\n            in_dim = dims[0]\n        head = ClassificationHead(in_dim=in_dim, num_labels=self.num_labels, hidden=512, dropout=0.1)\n        model = MultiModalModel(encoders, agg, fusion_mod, head, image_cfg=image_cfg, audio_cfg=audio_cfg)\n        return processors, model\n\n    # --------------------------\n    # Метрики\n    # --------------------------\n    def _build_compute_metrics(self, metric_name: str):\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            try:\n                import evaluate\n                f1_metric = evaluate.load(\"f1\")\n                def compute(p):\n                    preds = p.predictions.argmax(-1)\n                    return f1_metric.compute(predictions=preds, references=p.label_ids, average=\"macro\")\n                return compute\n            except Exception:\n                print(\"evaluate не доступен, использую accuracy\")\n                metric_name = \"accuracy\"\n\n        if metric_name == \"accuracy\":\n            try:\n                import evaluate\n                acc = evaluate.load(\"accuracy\")\n                def compute(p):\n                    preds = p.predictions.argmax(-1)\n                    return acc.compute(predictions=preds, references=p.label_ids)\n                return compute\n            except Exception:\n                def compute(p):\n                    preds = p.predictions.argmax(-1)\n                    y = p.label_ids\n                    return {\"accuracy\": float(np.mean(preds == y))}\n                return compute\n\n        raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n\n    # --------------------------\n    # Обучение\n    # --------------------------\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        test_data: Optional[pd.DataFrame] = None,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"accuracy\",   # \"accuracy\" | \"f1\"\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result_cls\",\n        seed: int = 42,\n        gradient_checkpointing: bool = False,\n        class_weights: Optional[np.ndarray] = None,  # если None — авто-расчёт по train_data\n    ):\n        import os\n        from transformers import TrainingArguments\n    \n        # 1) Проверки и подготовка\n        self._validate_modalities(train_data)\n        set_seed(seed)\n    \n        # train/val split\n        df_train, df_eval = (train_data, test_data) if test_data is not None else self._split(train_data, test_size, seed)\n    \n        # labels mapping\n        self._ensure_label_mapping(df_train)\n    \n        # class weights\n        if class_weights is None:\n            y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n            counts = np.bincount(y_train_all, minlength=self.num_labels)\n            n_all = counts.sum()\n            cw = np.zeros(self.num_labels, dtype=np.float32)\n            nz = counts > 0\n            cw[nz] = n_all / (self.num_labels * counts[nz].astype(np.float32))\n            class_weights = cw\n    \n        # 2) Модель и процессоры (auto)\n        self.processors, self.model = self._build_model_and_processors()\n        if gradient_checkpointing:\n            try:\n                self.model.gradient_checkpointing_enable()  # если поддерживается\n            except Exception:\n                pass\n    \n        # 3) Datasets (без чанков)\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        pretokenize_train = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_train) <= self.max_pretokenize_samples)\n        pretokenize_eval  = (self.pretokenize_data and has_bt and self.text_padding != \"dynamic\"\n                             and len(df_eval)  <= self.max_pretokenize_samples)\n    \n        ds_train = MultiModalDataset(\n            df=df_train,\n            target_col=self.target_column_name,\n            task=\"classification\",\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_train) else None,\n            pretokenize=pretokenize_train,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        ds_eval = MultiModalDataset(\n            df=df_eval,\n            target_col=self.target_column_name,\n            task=\"classification\",\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if (has_bt and pretokenize_eval) else None,\n            pretokenize=pretokenize_eval,\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n    \n        collate = MultiModalCollator(self.processors, task=\"classification\")\n        compute_metrics = self._build_compute_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=min(4, os.cpu_count() or 4),\n            seed=seed,\n            remove_unused_columns=False,\n            gradient_checkpointing=gradient_checkpointing,\n            dataloader_pin_memory=True,\n            ddp_find_unused_parameters=False,\n            disable_tqdm=False\n        )\n    \n        # 5) Trainer и старт обучения\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train,\n            eval_dataset=ds_eval,\n            data_collator=collate,\n            compute_metrics=compute_metrics,\n            class_weights=class_weights\n        )\n    \n        # (необязательно) убрать стандартный принтер-коллбек\n        try:\n            from transformers.trainer_callback import PrinterCallback\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n    \n        self.trainer.train()\n    \n        # очистить кэш токенизатора (если был)\n        if has_bt:\n            try: self.processors[\"text\"].clear_cache()\n            except Exception: pass\n    \n        return self\n\n    # --------------------------\n    # Предсказания\n    # --------------------------\n    def predict(\n        self,\n        df: pd.DataFrame,\n        return_label_str: bool = False,\n        return_proba: bool = False,\n        batch_size: Optional[int] = None\n    ) -> np.ndarray:\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        df_c = self._ensure_label_column(df)\n\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            task=\"classification\",\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=(self.pretokenize_data and has_bt and len(df_c) < 10000 and self.text_padding != \"dynamic\"),\n            pretokenize_batch_size=self.pretokenize_batch_size\n        )\n        collate = MultiModalCollator(self.processors, task=\"classification\")\n\n        # override bs\n        if batch_size:\n            original_bs = self.trainer.args.per_device_eval_batch_size\n            self.trainer.args.per_device_eval_batch_size = batch_size\n\n        effective_batch_size = batch_size or self.trainer.args.per_device_eval_batch_size\n        num_batches = (len(df_c) + effective_batch_size - 1) // effective_batch_size\n        print(f\"Running predictions (batch_size={effective_batch_size}, num_batches={num_batches})...\")\n\n        original_disable_tqdm = self.trainer.args.disable_tqdm\n        self.trainer.args.disable_tqdm = False\n\n        preds = self.trainer.predict(test_dataset=ds, metric_key_prefix=\"predict\")\n\n        self.trainer.args.disable_tqdm = original_disable_tqdm\n        if batch_size:\n            self.trainer.args.per_device_eval_batch_size = original_bs\n\n        ds.clear_cache()\n\n        logits = preds.predictions\n        if return_proba:\n            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n            return probabilities\n\n        y_pred = np.argmax(logits, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    # --------------------------\n    # Эмбеддинги\n    # --------------------------\n    def get_embeddings(\n        self,\n        df: pd.DataFrame,\n        batch_size: int = 32,\n        return_per_modality: bool = False\n    ):\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        device = next(self.trainer.model.parameters()).device\n        self.model.to(device).eval()\n\n        df_c = self._ensure_label_column(df)\n\n        has_bt = \"text\" in self.processors and hasattr(self.processors[\"text\"], \"bt\")\n        ds = MultiModalDataset(\n            df=df_c,\n            target_col=self.target_column_name,\n            task=\"classification\",\n            label2id=self.label2id,\n            text_columns=self.text_columns,\n            image_columns=self.image_columns,\n            audio_columns=self.audio_columns,\n            text_tokenizer=self.processors[\"text\"].bt if has_bt else None,\n            pretokenize=False\n        )\n        collate = MultiModalCollator(self.processors, task=\"classification\")\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        num_batches = (len(df_c) + batch_size - 1) // batch_size\n        print(f\"Extracting embeddings (batch_size={batch_size}, num_batches={num_batches})...\")\n\n        with torch.no_grad():\n            for batch in tqdm(loader, total=num_batches, desc=\"Extracting embeddings\", unit=\"batch\", leave=True):\n                bi = batch[\"backend_inputs\"]\n\n                def move_to_device(obj):\n                    if torch.is_tensor(obj): return obj.to(device)\n                    if isinstance(obj, dict): return {k: move_to_device(v) for k, v in obj.items()}\n                    if isinstance(obj, list): return [move_to_device(v) for v in obj]\n                    return obj\n\n                bi = move_to_device(bi)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            print(f\"✓ Embeddings shape: {fused_arr.shape}\")\n            return fused_arr\n\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        print(f\"✓ Fused embeddings shape: {fused_arr.shape}\")\n        for m, arr in per_mod.items():\n            print(f\"✓ {m.capitalize()} embeddings shape: {arr.shape}\")\n\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ultralytics\n\nimport os\nimport cv2\nimport json\nimport shutil\nimport tempfile\nimport warnings\nimport random\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom ultralytics import YOLO\nfrom ultralytics.utils.downloads import attempt_download_asset\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import KFold\n\n\nclass YOLODetectionPipeline:\n    \"\"\"\n    YOLO-пайплайн из pandas DataFrame с опциональным подбором гиперпараметров подсчёта\n    (plain или area-gated) и калибровкой линейной моделью (Ridge по резидуалу).\n\n    Входные данные (DataFrame):\n      - image_path (str): путь к изображению (jpg/png)\n      - boxes_col: GT-боксы в YOLO-нормировке [0..1] (списки/массивы/строка)\n\n    Основной сценарий:\n      - fit(): обучает YOLO; (опц.) тюнинг порогов (plain/area-gated + iou/max_det);\n               (опц.) калибрует Ridge; (опц.) валидирует RMSE/MAE.\n      - predict(): возвращает детекции (boxes_json + count). Калибровка НЕ применяется.\n      - predict_counts(): возвращает числовой подсчёт; применяет лучшие пороги/калибровку, если включены.\n\n    Управление временем тюнинга:\n      - tune_val_subsample: подвыборка валидации (int — кол-во, float — доля 0..1).\n      - tune_max_combinations: ограничивает число проверяемых комбо (случайно из сетки).\n    \"\"\"\n\n    def __init__(self,\n                 model_ckpt: str = \"yolov8n.pt\",\n                 data_root: str | None = None,\n                 image_col: str = \"image_path\",\n                 boxes_col: str = \"boxes\",\n                 class_names: list[str] | None = None,\n                 use_symlinks: bool = True,\n                 verbose: bool = True,\n                 # переключатели\n                 enable_tuning: bool = True,\n                 enable_ridge: bool = True,\n                 validate_count: bool = True,\n                 # режим тюнинга plain vs area-gated\n                 enable_area_gate: bool = True,        # True → тюним (conf_small, conf_big, area_thr) + (iou, max_det)\n                 enable_tta_flip: bool = False,        # True → TTA flip при plain-подсчёте\n                 # управление временем тюнинга\n                 tune_val_subsample: int | float | None = None,  # int=кол-во; float=доля [0..1]\n                 tune_max_combinations: int | None = 100,\n                 random_state: int = 42,\n                 # сетки для plain-тюнинга\n                 tune_conf_grid = (0.20, 0.25, 0.30, 0.35),\n                 tune_iou_grid  = (0.55, 0.60),\n                 tune_max_det_grid = (300, 600),\n                 # сетки для area-gated (под мелкие объекты из ваших стат)\n                 tune_conf_small_grid = (0.10, 0.12, 0.14, 0.18),\n                 tune_conf_big_grid   = (0.30, 0.40, 0.50),\n                 tune_area_thr_grid   = (0.0008, 0.0010, 0.0012, 0.0015),\n                 gate_conf_base: float = 0.07,  # базовый conf для извлечения кандидатов при area-gated\n                 # сетка для Ridge\n                 ridge_alpha_grid = (0.3, 1.0, 3.0),\n                 # пороги нормированной площади для фич Ridge/plain\n                 small_thr: float = 0.0010,\n                 big_thr: float   = 0.003):\n        self.model_ckpt = model_ckpt\n        self.image_col = image_col\n        self.boxes_col = boxes_col\n        self.class_names = class_names or [\"obj\"]\n        self.use_symlinks = use_symlinks\n        self.verbose = verbose\n\n        self.enable_tuning = enable_tuning\n        self.enable_ridge = enable_ridge\n        self.validate_count = validate_count\n        self.enable_area_gate = enable_area_gate\n        self.enable_tta_flip = enable_tta_flip\n\n        self.tune_val_subsample = tune_val_subsample\n        self.tune_max_combinations = tune_max_combinations\n        self.random_state = random_state\n        random.seed(random_state)\n        np.random.seed(random_state)\n\n        # рабочая папка\n        self._tmpdir_owned = False\n        if data_root is None:\n            self.data_root = tempfile.mkdtemp(prefix=\"yolo_ds_\")\n            self._tmpdir_owned = True\n        else:\n            self.data_root = os.path.abspath(data_root)\n            os.makedirs(self.data_root, exist_ok=True)\n\n        self.dataset_yaml = os.path.join(self.data_root, \"dataset.yaml\")\n        self.model_path = None\n        self._model = None\n        self._device = None  # строка устройства, использованная при fit()\n\n        # сетки и пороги\n        self.tune_conf_grid = tuple(float(x) for x in tune_conf_grid)\n        self.tune_iou_grid  = tuple(float(x) for x in tune_iou_grid)\n        self.tune_max_det_grid = tuple(int(x) for x in tune_max_det_grid)\n\n        self.tune_conf_small_grid = tuple(float(x) for x in tune_conf_small_grid)\n        self.tune_conf_big_grid   = tuple(float(x) for x in tune_conf_big_grid)\n        self.tune_area_thr_grid   = tuple(float(x) for x in tune_area_thr_grid)\n        self.gate_conf_base = float(gate_conf_base)\n\n        self.ridge_alpha_grid = tuple(float(x) for x in ridge_alpha_grid)\n        self.small_thr = float(small_thr)\n        self.big_thr   = float(big_thr)\n\n        # сохранённые результаты тюнинга/калибровки\n        self.calib_ = dict(\n            # plain режим:\n            best_conf=None, best_iou=None, best_max_det=None,\n            # area-gated:\n            gate_conf_small=None, gate_conf_big=None, gate_area_thr=None, gate_conf_base=None,\n            # Ridge:\n            ridge_alpha=None, ridge_model=None, ridge_mu=None, ridge_sd=None,\n            # общий:\n            imgsz=None\n        )\n\n    # -------------------- device helpers --------------------\n    @staticmethod\n    def _resolve_device(device: str | int | None) -> str:\n        \"\"\"\n        Выбор устройства для обучения:\n          - None / \"auto\": \"0,1,...,N-1\" при наличии CUDA, иначе \"cpu\"\n          - иначе вернуть строку как есть (например, \"0\" или \"cpu\")\n        \"\"\"\n        if device is None or str(device).lower() == \"auto\":\n            if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n                n = torch.cuda.device_count()\n                return \",\".join(str(i) for i in range(n))\n            return \"cpu\"\n        return str(device)\n\n    def _infer_device(self) -> str:\n        \"\"\"\n        Устройство для инференса/тюнинга:\n          - если тренировались на '0,1,...' → берём первую карту '0'\n          - если тренировались на 'k' → её же\n          - иначе авто: '0' при наличии CUDA, 'cpu' без GPU\n        \"\"\"\n        if getattr(self, \"_device\", None):\n            if isinstance(self._device, str) and \",\" in self._device:\n                return self._device.split(\",\")[0]\n            return self._device\n        return \"0\" if (torch.cuda.is_available() and torch.cuda.device_count() > 0) else \"cpu\"\n\n    # -------------------- helpers: разметка → YOLO-тxt --------------------\n    @staticmethod\n    def _is_nan_like(x):\n        if x is None: return True\n        if isinstance(x, float) and np.isnan(x): return True\n        if isinstance(x, str) and x.strip()==\"\": return True\n        return False\n\n    def _parse_boxes(self, row):\n        boxes_raw = row[self.boxes_col] if self.boxes_col in row else None\n        if self._is_nan_like(boxes_raw): return []\n        out = []\n        if isinstance(boxes_raw, (list, tuple, np.ndarray)):\n            for it in boxes_raw:\n                vals = list(map(float, it))\n                if len(vals) >= 4:\n                    x,y,w,h = vals[:4]\n                    if 0 <= x <= 1 and 0 <= y <= 1 and 0 < w <= 1 and 0 < h <= 1:\n                        out.append((0, x,y,w,h))\n        elif isinstance(boxes_raw, str):\n            lines = [ln.strip() for ln in boxes_raw.strip().splitlines() if ln.strip()]\n            for ln in lines:\n                parts = ln.split()\n                vals = list(map(float, parts))\n                if len(vals) == 4:\n                    x,y,w,h = vals\n                    out.append((0, x,y,w,h))\n                elif len(vals) >= 5:\n                    cls,x,y,w,h = int(vals[0]), *vals[1:5]\n                    out.append((cls, float(x),float(y),float(w),float(h)))\n        return out\n\n    def _link_or_copy(self, src, dst):\n        os.makedirs(os.path.dirname(dst), exist_ok=True)\n        if self.use_symlinks:\n            try:\n                if os.path.lexists(dst): os.remove(dst)\n                os.symlink(os.path.abspath(src), dst)\n                return\n            except Exception:\n                pass\n        shutil.copy2(src, dst)\n\n    def _write_label_file(self, label_path, boxes):\n        os.makedirs(os.path.dirname(label_path), exist_ok=True)\n        with open(label_path, \"w\", encoding=\"utf-8\") as f:\n            for cls, x,y,w,h in boxes:\n                f.write(f\"{int(cls)} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\\n\")\n\n    def _materialize(self, train_df, val_df, train_split=\"train\", val_split=\"val\"):\n        for split_name, df in [(train_split, train_df), (val_split, val_df)]:\n            img_dir = os.path.join(self.data_root, \"images\", split_name)\n            lbl_dir = os.path.join(self.data_root, \"labels\", split_name)\n            os.makedirs(img_dir, exist_ok=True); os.makedirs(lbl_dir, exist_ok=True)\n            it = df.iterrows()\n            if self.verbose: it = tqdm(it, total=len(df), desc=f\"[build] {split_name}\")\n            for _, row in it:\n                src = row[self.image_col]\n                if not os.path.exists(src):\n                    raise FileNotFoundError(f\"Image not found: {src}\")\n                fname = os.path.basename(src)\n                stem, _ = os.path.splitext(fname)\n                self._link_or_copy(src, os.path.join(img_dir, fname))\n                self._write_label_file(os.path.join(lbl_dir, stem + \".txt\"), self._parse_boxes(row))\n\n        with open(self.dataset_yaml, \"w\", encoding=\"utf-8\") as f:\n            f.write(f\"path: {self.data_root}\\ntrain: images/{train_split}\\nval: images/{val_split}\\nnames:\\n\")\n            for i, name in enumerate(self.class_names):\n                f.write(f\"  {i}: {name}\\n\")\n\n    # -------------------- fit: train + (tune/ridge/validate) --------------------\n    def fit(self,\n            train_df: pd.DataFrame,\n            val_df: pd.DataFrame | None = None,\n            test_size: float = 0.2,\n            epochs: int = 50,\n            imgsz: int = 640,\n            batch: int = 16,\n            device: str | int | None = \"auto\",\n            workers: int = 4,\n            patience: int = 50,\n            optimizer: str = \"auto\",\n            augment: bool = True,\n            seed: int = 42,\n            close_mosaic: int | None = 10,\n            cos_lr: bool = True,\n            rect: bool = False,\n            iou: float = 0.7,\n            **extra_train_kwargs):\n\n        # выбрать устройство для тренировки и сохранить\n        self._device = self._resolve_device(device)\n        if self.verbose:\n            print(f\"[device] training device='{self._device}'\")\n\n        np.random.seed(seed); random.seed(seed)\n\n        # если val_df не задан — делаем простую стратификацию по бинам count\n        if val_df is None:\n            tmp = train_df.copy()\n            counts = [len(self._parse_boxes(r)) for _, r in tmp.iterrows()]\n            tmp[\"_bins\"] = np.clip((np.array(counts)//5).astype(int), 0, 50)\n            val_mask = tmp.groupby(\"_bins\", group_keys=False).apply(\n                lambda g: g.sample(frac=test_size, random_state=seed)).index\n            val_df = train_df.loc[val_mask]\n            train_df = train_df.drop(index=val_mask)\n            train_df = train_df.reset_index(drop=True); val_df = val_df.reset_index(drop=True)\n\n        self._materialize(train_df, val_df)\n\n        # загрузить/скачать чекпоинт\n        if not os.path.exists(self.model_ckpt) and self.model_ckpt.endswith(\".pt\"):\n            try:\n                if self.verbose: print(f\"Checkpoint '{self.model_ckpt}' not found. Attempting to download...\")\n                attempt_download_asset(self.model_ckpt)\n            except Exception as e:\n                raise FileNotFoundError(f\"Failed to download '{self.model_ckpt}'. Error: {e}\")\n\n        # train args\n        train_args = {\n            'data': self.dataset_yaml, 'epochs': epochs, 'imgsz': imgsz, 'batch': batch,\n            'device': self._device, 'workers': workers, 'patience': patience, 'optimizer': optimizer,\n            'augment': augment, 'seed': seed, 'close_mosaic': close_mosaic, 'cos_lr': cos_lr,\n            'rect': rect, 'iou': iou, 'verbose': self.verbose\n        }\n        train_args.update(extra_train_kwargs)\n\n        # обучение\n        model = YOLO(self.model_ckpt)\n        model.train(**train_args)\n\n        # ГАРАНТИРОВАННО берём лучший чекпоинт\n        best_path = None\n        if hasattr(model, \"trainer\") and getattr(model.trainer, \"best\", None):\n            best_path = str(model.trainer.best)     # .../runs/detect/exp/weights/best.pt\n        elif getattr(model, \"ckpt_path\", None):\n            best_path = str(model.ckpt_path)\n        else:\n            best_path = self.model_ckpt\n        self.model_path = best_path\n        if self.verbose:\n            print(f\"[fit] best model: {self.model_path}\")\n\n        # тюнинг/калибровка/валидация\n        try:\n            self._tune_and_or_calibrate(val_df, imgsz=imgsz)\n            if self.validate_count:\n                self._validate_counting(val_df)\n        except Exception as e:\n            if self.verbose:\n                print(f\"[post-fit] skipped tuning/calibration/validation due to: {e}\")\n\n        return self.model_path\n\n    # -------------------- инференс-хелперы --------------------\n    def _ensure_model(self):\n        if self._model is None:\n            path = self.model_path or self.model_ckpt\n            self._model = YOLO(path)\n\n    @torch.no_grad()\n    def _raw_counts(self, paths, imgsz, conf, iou, max_det):\n        \"\"\"Plain len(detections). Если enable_tta_flip=True — усреднение с augment=True.\"\"\"\n        self._ensure_model()\n        out = []\n        dev = self._infer_device()\n        if not self.enable_tta_flip:\n            for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[counts]\"):\n                batch = paths[i:i+64]\n                res = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                          max_det=max_det, device=dev, verbose=False)\n                for r in res:\n                    out.append(int(len(r.boxes) if (r.boxes is not None) else 0))\n        else:\n            for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[counts-tta]\"):\n                batch = paths[i:i+64]\n                r1 = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                         max_det=max_det, device=dev, verbose=False)\n                r2 = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                         max_det=max_det, device=dev, verbose=False, augment=True)\n                for a, b in zip(r1, r2):\n                    n1 = int(len(a.boxes) if (a.boxes is not None) else 0)\n                    n2 = int(len(b.boxes) if (b.boxes is not None) else 0)\n                    out.append(0.5 * (n1 + n2))\n        return np.array(out, dtype=float)\n\n    @torch.no_grad()\n    def _yolo_feats(self, paths, imgsz, conf, iou, max_det):\n        \"\"\"Фичи для Ridge: [n, conf_sum, conf_mean, conf_max, area_mean, frac_small, frac_mid, frac_big].\"\"\"\n        self._ensure_model()\n        rows = []\n        dev = self._infer_device()\n        for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[feats]\"):\n            batch = paths[i:i+64]\n            res = self._model.predict(batch, imgsz=imgsz, conf=conf, iou=iou,\n                                      max_det=max_det, device=dev, verbose=False)\n            for r in res:\n                if r.boxes is None or len(r.boxes) == 0:\n                    rows.append(dict(n=0, conf_sum=0, conf_mean=0, conf_max=0,\n                                     area_mean=0, frac_small=0, frac_mid=0, frac_big=0))\n                    continue\n                confs = r.boxes.conf.cpu().numpy()\n                xywhn = r.boxes.xywhn.cpu().numpy()\n                areas = (xywhn[:, 2] * xywhn[:, 3]).clip(0, 1)\n                rows.append(dict(\n                    n=len(confs),\n                    conf_sum=float(confs.sum()),\n                    conf_mean=float(confs.mean()),\n                    conf_max=float(confs.max()),\n                    area_mean=float(areas.mean()),\n                    frac_small=float((areas < self.small_thr).mean()),\n                    frac_mid=float(((areas >= self.small_thr) & (areas <= self.big_thr)).mean()),\n                    frac_big=float((areas > self.big_thr).mean())\n                ))\n        return pd.DataFrame(rows).to_numpy()\n\n    @torch.no_grad()\n    def _detect_conf_area(self, paths, imgsz, conf_base, iou, max_det):\n        \"\"\"Возвращает список массивов Nx2 [conf, area] при базовом пороге conf_base (для area-gated).\"\"\"\n        self._ensure_model()\n        dev = self._infer_device()\n        out = []\n        for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[boxes]\"):\n            batch = paths[i:i+64]\n            res = self._model.predict(batch, imgsz=imgsz, conf=conf_base, iou=iou,\n                                      max_det=max_det, device=dev, verbose=False)\n            for r in res:\n                if r.boxes is None or len(r.boxes) == 0:\n                    out.append(np.empty((0, 2), dtype=np.float32))\n                    continue\n                confs = r.boxes.conf.cpu().numpy()\n                xywhn = r.boxes.xywhn.cpu().numpy()\n                areas = (xywhn[:, 2] * xywhn[:, 3]).clip(0, 1)\n                out.append(np.stack([confs, areas], axis=1))\n        return out\n\n    @staticmethod\n    def _count_with_area_gate(conf_area_list, conf_small, conf_big, area_thr):\n        \"\"\"Подсчёт с двупороговой фильтрацией по площади.\"\"\"\n        counts = []\n        for ca in conf_area_list:\n            if ca.size == 0:\n                counts.append(0); continue\n            conf = ca[:, 0]; area = ca[:, 1]\n            small_mask = (area < area_thr)  & (conf >= conf_small)\n            big_mask   = (area >= area_thr) & (conf >= conf_big)\n            counts.append(int(small_mask.sum() + big_mask.sum()))\n        return np.array(counts, dtype=float)\n\n    # -------------------- подвыборка валидации --------------------\n    def _subset_val(self, val_df: pd.DataFrame) -> pd.DataFrame:\n        if self.tune_val_subsample is None:\n            return val_df\n        n = len(val_df)\n        if isinstance(self.tune_val_subsample, float):\n            k = max(1, int(round(n * self.tune_val_subsample)))\n        else:\n            k = int(self.tune_val_subsample)\n        k = min(k, n)\n        return val_df.sample(n=k, random_state=self.random_state).reset_index(drop=True)\n\n    # -------------------- Ridge по резидуалу с K-fold CV --------------------\n    def _fit_ridge_cv_on_residual(self, X: np.ndarray, y_true: np.ndarray, y_plain: np.ndarray):\n        alphas = self.ridge_alpha_grid\n        k = min(5, len(y_true)) if len(y_true) >= 3 else 2\n        kf = KFold(n_splits=k, shuffle=True, random_state=self.random_state)\n\n        # стандартизация фич\n        mu = X.mean(axis=0)\n        sd = X.std(axis=0); sd[sd == 0] = 1.0\n        Xs = (X - mu) / sd\n        r = y_true - y_plain\n\n        best_alpha, best_cv = None, 1e9\n        for a in alphas:\n            cv_scores = []\n            for tr, va in kf.split(Xs):\n                m = Ridge(alpha=float(a)).fit(Xs[tr], r[tr])\n                pr = m.predict(Xs[va])\n                cv_scores.append(mean_squared_error(r[va], pr, squared=False))\n            cv_rmse = float(np.mean(cv_scores))\n            if cv_rmse < best_cv:\n                best_alpha, best_cv = float(a), cv_rmse\n\n        # финальная подгонка на всех\n        model = Ridge(alpha=best_alpha).fit(Xs, r)\n        return dict(model=model, alpha=best_alpha, mu=mu, sd=sd, cv_rmse=best_cv)\n\n    # -------------------- тюнинг и/или калибровка --------------------\n    def _tune_and_or_calibrate(self, val_df: pd.DataFrame, imgsz: int):\n        val_sub = self._subset_val(val_df)\n        paths = val_sub[self.image_col].tolist()\n        y_true = np.array([len(self._parse_boxes(r)) for _, r in val_sub.iterrows()], dtype=float)\n\n        # 1) Тюнинг plain или area-gated\n        if self.enable_tuning:\n            if self.enable_area_gate:\n                # Подготовим базовый порог для сбора кандидатов, согласованный с минимальным cs\n                min_cs = min(self.tune_conf_small_grid) if len(self.tune_conf_small_grid) else 0.10\n                base_collect = max(0.03, min(self.gate_conf_base, min_cs - 0.02))\n                if self.verbose:\n                    print(f\"[tune-gate] base_collect={base_collect:.3f} (min_cs={min_cs:.3f})\")\n\n                # 1) Предрасчёт списков [conf, area] для всех (iou, max_det)\n                iou_grid = tuple(self.tune_iou_grid)\n                md_grid  = tuple(self.tune_max_det_grid)\n                conf_area_by_key = {}\n                total_prepasses = len(iou_grid) * len(md_grid)\n                if self.verbose:\n                    print(f\"[tune-gate] precomputing boxes for {total_prepasses} (iou,max_det) pairs...\")\n                for iou_ in iou_grid:\n                    for md_ in md_grid:\n                        conf_area_by_key[(iou_, md_)] = self._detect_conf_area(\n                            paths, imgsz=imgsz,\n                            conf_base=float(base_collect),\n                            iou=float(iou_), max_det=int(md_)\n                        )\n\n                # 2) Подбор cs/cb/area_thr + iou/max_det\n                all_combos = []\n                for iou_ in iou_grid:\n                    for md_ in md_grid:\n                        for cs in self.tune_conf_small_grid:\n                            # эффективная база (ниже cs)\n                            base_eff = max(0.03, min(float(self.gate_conf_base), float(cs) - 0.02))\n                            for cb in self.tune_conf_big_grid:\n                                for at in self.tune_area_thr_grid:\n                                    all_combos.append((float(iou_), int(md_), float(cs), float(cb), float(at), float(base_eff)))\n\n                random.shuffle(all_combos)\n                full_space = len(all_combos)\n                if self.tune_max_combinations is not None:\n                    all_combos = all_combos[:int(self.tune_max_combinations)]\n                if self.verbose:\n                    print(f\"[tune-gate] search combos: {len(all_combos)} (cap), full={full_space}\")\n\n                best = dict(rmse=1e9, iou=None, max_det=None, cs=None, cb=None, at=None, base=None)\n                for (iou_, md_, cs, cb, at, base_eff) in all_combos:\n                    conf_area = conf_area_by_key[(iou_, md_)]\n                    y_pred = self._count_with_area_gate(conf_area, cs, cb, at)\n                    rmse = mean_squared_error(y_true, y_pred, squared=False)\n                    if rmse < best[\"rmse\"]:\n                        best.update(dict(rmse=rmse, iou=iou_, max_det=md_,\n                                         cs=cs, cb=cb, at=at, base=base_eff))\n\n                if self.verbose:\n                    print(f\"[tune-gate] best: cs={best['cs']:.3f}, cb={best['cb']:.3f}, \"\n                          f\"area_thr={best['at']:.4f}, iou={best['iou']}, max_det={best['max_det']}  RMSE={best['rmse']:.3f}\")\n\n                self.calib_.update(dict(\n                    best_conf=None, best_iou=float(best['iou']), best_max_det=int(best['max_det']),\n                    gate_conf_small=float(best['cs']), gate_conf_big=float(best['cb']),\n                    gate_area_thr=float(best['at']), gate_conf_base=float(best['base'])\n                ))\n            else:\n                combos = [(float(c), float(i), int(m))\n                          for i in self.tune_iou_grid\n                          for m in self.tune_max_det_grid\n                          for c in self.tune_conf_grid]\n                random.shuffle(combos)\n                full_space = len(combos)\n                if self.tune_max_combinations is not None:\n                    combos = combos[:int(self.tune_max_combinations)]\n                if self.verbose:\n                    print(f\"[tune-plain] search combos: {len(combos)} (cap), full={full_space}\")\n\n                best = dict(rmse=1e9, conf=None, iou=None, max_det=None)\n                for conf, iou_v, max_det in combos:\n                    y_pred = self._raw_counts(paths, imgsz=imgsz, conf=conf, iou=iou_v, max_det=max_det)\n                    rmse = mean_squared_error(y_true, y_pred, squared=False)\n                    if rmse < best[\"rmse\"]:\n                        best.update(dict(rmse=rmse, conf=conf, iou=iou_v, max_det=max_det))\n                if self.verbose:\n                    print(f\"[tune] best plain count: conf={best['conf']}, iou={best['iou']}, max_det={best['max_det']}  RMSE={best['rmse']:.3f}\")\n\n                self.calib_.update(dict(\n                    best_conf=best['conf'], best_iou=best['iou'], best_max_det=best['max_det'],\n                    gate_conf_small=None, gate_conf_big=None, gate_area_thr=None, gate_conf_base=None\n                ))\n        else:\n            # без тюнинга — дефолты для plain\n            self.calib_.update(dict(\n                best_conf=0.25, best_iou=0.5, best_max_det=1000,\n                gate_conf_small=None, gate_conf_big=None, gate_area_thr=None, gate_conf_base=None\n            ))\n\n        # 2) Калибровка Ridge (по резидуалу, с CV)\n        ridge_model, ridge_alpha = None, None\n        if self.enable_ridge:\n            # строим y_plain на том же сабсете val_sub\n            if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n                iou_use = float(self.calib_[\"best_iou\"])\n                md_use  = int(self.calib_[\"best_max_det\"])\n                conf_area = self._detect_conf_area(paths, imgsz=imgsz,\n                                                   conf_base=float(self.calib_[\"gate_conf_base\"]),\n                                                   iou=iou_use, max_det=md_use)\n                y_plain = self._count_with_area_gate(conf_area,\n                                                     float(self.calib_[\"gate_conf_small\"]),\n                                                     float(self.calib_[\"gate_conf_big\"]),\n                                                     float(self.calib_[\"gate_area_thr\"]))\n                X = self._yolo_feats(paths, imgsz=imgsz,\n                                     conf=float(self.calib_[\"gate_conf_base\"]),\n                                     iou=iou_use, max_det=md_use)\n            else:\n                conf_use = float(self.calib_[\"best_conf\"] if self.calib_.get(\"best_conf\") is not None else 0.25)\n                iou_use  = float(self.calib_[\"best_iou\"]  if self.calib_.get(\"best_iou\")  is not None else 0.5)\n                md_use   = int(self.calib_[\"best_max_det\"] if self.calib_.get(\"best_max_det\") is not None else 1000)\n                y_plain  = self._raw_counts(paths, imgsz=imgsz, conf=conf_use, iou=iou_use, max_det=md_use)\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=conf_use, iou=iou_use, max_det=md_use)\n\n            pack = self._fit_ridge_cv_on_residual(X, y_true, y_plain)\n            ridge_model, ridge_alpha = pack[\"model\"], pack[\"alpha\"]\n            if self.verbose:\n                print(f\"[calib] Ridge(residual) alpha={ridge_alpha}  CV-RMSE(resid)={pack['cv_rmse']:.3f}\")\n\n            # сохраним стандартизацию\n            self.calib_.update(dict(\n                ridge_alpha=ridge_alpha, ridge_model=ridge_model,\n                ridge_mu=pack[\"mu\"], ridge_sd=pack[\"sd\"], imgsz=imgsz\n            ))\n        else:\n            self.calib_.update(dict(ridge_alpha=None, ridge_model=None, imgsz=imgsz))\n\n    # -------------------- финальная валидация подсчёта --------------------\n    def _validate_counting(self, val_df: pd.DataFrame):\n        paths = val_df[self.image_col].tolist()\n        y_true = np.array([len(self._parse_boxes(r)) for _, r in val_df.iterrows()], dtype=float)\n\n        imgsz = self.calib_['imgsz'] or 640\n\n        # plain/gate\n        if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n            iou_use = float(self.calib_[\"best_iou\"])\n            md_use  = int(self.calib_[\"best_max_det\"])\n            conf_area = self._detect_conf_area(paths, imgsz=imgsz, conf_base=self.calib_[\"gate_conf_base\"],\n                                               iou=iou_use, max_det=md_use)\n            y_plain = self._count_with_area_gate(conf_area,\n                                                 self.calib_[\"gate_conf_small\"],\n                                                 self.calib_[\"gate_conf_big\"],\n                                                 self.calib_[\"gate_area_thr\"])\n        else:\n            conf  = self.calib_['best_conf'] if self.enable_tuning else 0.25\n            iou_v   = self.calib_['best_iou']  if self.enable_tuning else 0.5\n            max_det = self.calib_['best_max_det'] if self.enable_tuning else 1000\n            y_plain = self._raw_counts(paths, imgsz, conf, iou_v, max_det)\n\n        rmse_plain = mean_squared_error(y_true, y_plain, squared=False)\n        mae_plain  = mean_absolute_error(y_true, y_plain)\n\n        # calibrated\n        if self.enable_ridge and self.calib_['ridge_model'] is not None:\n            if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=float(self.calib_[\"gate_conf_base\"]),\n                                     iou=float(self.calib_[\"best_iou\"]), max_det=int(self.calib_[\"best_max_det\"]))\n            else:\n                X = self._yolo_feats(paths, imgsz=imgsz,\n                                     conf=(self.calib_[\"best_conf\"] if self.calib_[\"best_conf\"] is not None else 0.25),\n                                     iou=(self.calib_[\"best_iou\"] if self.calib_[\"best_iou\"] is not None else 0.5),\n                                     max_det=(self.calib_[\"best_max_det\"] if self.calib_[\"best_max_det\"] is not None else 1000))\n            mu = self.calib_.get(\"ridge_mu\"); sd = self.calib_.get(\"ridge_sd\")\n            Xs = (X - mu) / sd\n            resid = self.calib_['ridge_model'].predict(Xs)\n            y_cal = np.clip(y_plain + resid, 0, None)\n            rmse_cal = mean_squared_error(y_true, y_cal, squared=False)\n            mae_cal  = mean_absolute_error(y_true, y_cal)\n            print(f\"[val-count] plain: RMSE={rmse_plain:.3f}, MAE={mae_plain:.3f}  |  calibrated: RMSE={rmse_cal:.3f}, MAE={mae_cal:.3f}\")\n        else:\n            print(f\"[val-count] plain: RMSE={rmse_plain:.3f}, MAE={mae_plain:.3f}  |  calibrated: (disabled)\")\n\n    # -------------------- публичный инференс: детекции --------------------\n    @torch.no_grad()\n    def predict(self, df: pd.DataFrame,\n                conf: float = 0.25, iou: float = 0.6,\n                imgsz: int = 640, device: str | int | None = \"auto\",\n                max_det: int = 300, agnostic_nms: bool = False) -> pd.DataFrame:\n        \"\"\"Детекции (калибровка НЕ используется).\"\"\"\n        assert self.image_col in df.columns\n        if self._model is None:\n            self._model = YOLO(self.model_path or self.model_ckpt)\n\n        dev = self._resolve_device(device if device is not None else self._infer_device())\n\n        paths = df[self.image_col].tolist()\n        preds = []\n        for i in tqdm(range(0, len(paths), 64), disable=not self.verbose, desc=\"[predict]\"):\n            batch = paths[i:i+64]\n            res = self._model(batch, conf=conf, iou=iou, imgsz=imgsz,\n                              device=dev, verbose=False, max_det=max_det,\n                              agnostic_nms=agnostic_nms)\n            for r in res:\n                boxes = []\n                if r.boxes is not None and len(r.boxes):\n                    xywhn = r.boxes.xywhn.cpu().numpy()\n                    confv = r.boxes.conf.cpu().numpy()\n                    clsv  = r.boxes.cls.cpu().numpy().astype(int)\n                    for (x,y,w,h), c, k in zip(xywhn, confv, clsv):\n                        boxes.append({\"cls\": int(k), \"conf\": float(c),\n                                      \"x\": float(x), \"y\": float(y), \"w\": float(w), \"h\": float(h)})\n                preds.append({\n                    self.image_col: r.path,\n                    \"count\": len(boxes),\n                    \"boxes_json\": json.dumps(boxes, ensure_ascii=False)\n                })\n        return pd.DataFrame(preds)\n\n    # -------------------- публичный инференс: подсчёт --------------------\n    @torch.no_grad()\n    def predict_counts(self, df: pd.DataFrame,\n                       imgsz: int | None = None,\n                       conf: float | None = None,\n                       iou: float | None = None,\n                       max_det: int | None = None,\n                       device: str | int | None = \"auto\",\n                       clamp_nonneg: bool = True,\n                       do_round: bool = False) -> pd.DataFrame:\n        \"\"\"\n        Подсчёт объектов.\n          - Если enable_ridge=True и калибратор обучен → y = y_plain + Ridge(residual).\n          - Иначе → plain len(dets).\n          - Если enable_area_gate=True и тюнинг выполнен → двупороговая фильтрация (conf_small/conf_big) по area.\n        \"\"\"\n        assert self.image_col in df.columns\n        if self._model is None:\n            self._model = YOLO(self.model_path or self.model_ckpt)\n\n        dev = self._resolve_device(device if device is not None else self._infer_device())\n        imgsz = imgsz or self.calib_['imgsz'] or 640\n        paths = df[self.image_col].tolist()\n\n        # area-gated путь\n        if self.enable_tuning and self.enable_area_gate and self.calib_.get(\"gate_conf_small\") is not None:\n            conf_base = float(self.calib_[\"gate_conf_base\"])\n            iou_use   = float(self.calib_[\"best_iou\"])\n            max_det_use = int(self.calib_[\"best_max_det\"])\n            conf_area = self._detect_conf_area(paths, imgsz=imgsz, conf_base=conf_base, iou=iou_use, max_det=max_det_use)\n            cs, cb, at = float(self.calib_[\"gate_conf_small\"]), float(self.calib_[\"gate_conf_big\"]), float(self.calib_[\"gate_area_thr\"])\n            y_plain = self._count_with_area_gate(conf_area, cs, cb, at)\n\n            if self.enable_ridge and self.calib_.get(\"ridge_model\") is not None:\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=conf_base, iou=iou_use, max_det=max_det_use)\n                # стандартизация и предсказание резидуала\n                mu = self.calib_.get(\"ridge_mu\"); sd = self.calib_.get(\"ridge_sd\")\n                Xs = (X - mu) / sd\n                resid = self.calib_['ridge_model'].predict(Xs)\n                y = y_plain + resid\n            else:\n                y = y_plain\n\n        else:\n            # обычный plain путь\n            if self.enable_tuning and self.calib_.get(\"best_conf\") is not None:\n                conf_def, iou_def, max_det_def = self.calib_['best_conf'], self.calib_['best_iou'], self.calib_['best_max_det']\n            else:\n                conf_def, iou_def, max_det_def = 0.25, 0.5, 1000\n\n            use_conf  = conf    if conf    is not None else conf_def\n            use_iou   = iou     if iou     is not None else iou_def\n            use_maxdet= max_det if max_det is not None else max_det_def\n\n            if self.enable_ridge and self.calib_.get(\"ridge_model\") is not None:\n                X = self._yolo_feats(paths, imgsz=imgsz, conf=use_conf, iou=use_iou, max_det=use_maxdet)\n                mu = self.calib_.get(\"ridge_mu\"); sd = self.calib_.get(\"ridge_sd\")\n                Xs = (X - mu) / sd\n                resid = self.calib_['ridge_model'].predict(Xs)\n                # базовый plain-счёт для сложения с резидуалом:\n                y_plain = self._raw_counts(paths, imgsz=imgsz, conf=use_conf, iou=use_iou, max_det=use_maxdet)\n                y = y_plain + resid\n            else:\n                y = self._raw_counts(paths, imgsz=imgsz, conf=use_conf, iou=use_iou, max_det=use_maxdet)\n\n        if clamp_nonneg: y = np.clip(y, 0, None)\n        if do_round:     y = np.rint(y)\n\n        out = df[[self.image_col]].copy()\n        out[\"label\"] = y\n        return out\n\n    # -------------------- housekeeping --------------------\n    def cleanup(self):\n        if self._tmpdir_owned and os.path.isdir(self.data_root):\n            shutil.rmtree(self.data_root, ignore_errors=True)\n\n# pipe = YOLODetectionPipeline(\n#     model_ckpt=\"yolov8l.pt\",\n#     image_col=\"image_path\",\n#     boxes_col=\"label\",\n#     class_names=[\"obj\"],\n#     verbose=True,\n#     use_symlinks=True,\n\n#     enable_tuning=True,\n#     enable_ridge=True,\n#     validate_count=True,\n\n#     enable_area_gate=True,\n#     enable_tta_flip=False,\n\n#     tune_val_subsample=None,\n#     tune_max_combinations=500,\n\n#     tune_conf_grid=(0.18, 0.21, 0.24, 0.27, 0.30, 0.33, 0.36, 0.39, 0.42),\n#     tune_iou_grid=(0.4, 0.45, 0.50, 0.55, 0.60, 0.65),\n#     tune_max_det_grid=(50, 100, 200, 300, 600),\n#     tune_conf_small_grid=(0.08, 0.10, 0.12, 0.14, 0.16, 0.18, 0.20),\n#     tune_conf_big_grid=(0.28, 0.32, 0.36, 0.40, 0.45, 0.50, 0.55),\n#     tune_area_thr_grid=(0.0006, 0.0008, 0.0010, 0.0012, 0.0015, 0.0018),\n#     ridge_alpha_grid=(0.01, 0.03, 0.05, 0.075, 0.1, 0.3, 0.6, 1.0, 2.0),\n\n#     gate_conf_base=0.07,\n#     small_thr=0.0010,\n#     big_thr=0.003,\n\n#     random_state=42\n# )\n\n# pipe.fit(\n#     train_df=train_df,\n#     val_df=val_df,\n#     epochs=60,\n#     imgsz=640,\n#     batch=32,\n#     device='0,1',\n#     workers=4,\n#     lr0=0.01,\n#     cos_lr=True,\n#     rect=True,\n#     iou=0.5\n# )\n\n# pipeline = YOLODetectionPipeline(\n#     model_ckpt=\"yolov10n.pt\",\n#     image_col=\"image_path\",\n#     boxes_col=\"label\",\n#     enable_tuning=True,\n#     enable_ridge=True,\n#     validate_count=True,\n#     tune_val_subsample=None,  # вся валидация для подбора параметров для подсчёта объектов\n#     tune_max_combinations=100,  ######################### 50\n#     random_state=42,\n#     verbose=True\n# )\n# pipeline.fit(\n#     train_df=train_df,\n#     val_df=val_df,\n#     lr0=0.01,\n#     epochs=70,  ######################### 40\n#     imgsz=640,\n#     batch=32,\n#     iou=0.5,\n#     rect=True,\n#     device='0,1'\n# )","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_params = []\nhead_params = []\n\nfor name, param in model.named_parameters():\n    if 'classifier' in name or 'head' in name or 'projection' in name:\n        head_params.append(param)\n    else:\n        encoder_params.append(param)\n\noptimizer_grouped_parameters = [\n    {'params': encoder_params, 'lr': 3e-5},\n    {'params': head_params, 'lr': 6e-5}\n]\n\noptimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n\ntrainer = Trainer(\n    ...,\n    optimizers=(optimizer, None)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}