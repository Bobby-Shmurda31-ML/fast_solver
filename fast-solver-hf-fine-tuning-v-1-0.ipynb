{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Дообучение encoder'а для классификации последовательностей.","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nimport evaluate\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\n\nclass WeightedCETrainer(Trainer):\n    \"\"\"\n    Кастомный Trainer с поддержкой взвешенной CrossEntropy.\n    Приоритет источников весов:\n      1) class_weights (готовый тензор весов)\n      2) train_labels (массив меток train-сплита)\n      3) train_data_df + target_column_name (DataFrame и имя колонки меток)\n    Формула: weight_i = N / (K * n_i), где K — число классов, n_i — кол-во примеров класса i.\n    Отсутствующие классы получают вес 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        train_data_df=None,\n        target_column_name=None,\n        num_labels=None,\n        train_labels=None,\n        class_weights=None,\n        **kwargs\n    ):\n        # Убираем future warning: tokenizer -> processing_class (если передан)\n        processing = kwargs.pop(\"tokenizer\", None)\n        if processing is not None and \"processing_class\" not in kwargs:\n            kwargs[\"processing_class\"] = processing\n\n        super().__init__(*args, **kwargs)\n\n        self.num_labels = num_labels or getattr(self.model.config, \"num_labels\", None)\n\n        if class_weights is not None:\n            # Готовые веса\n            w = torch.as_tensor(class_weights, dtype=torch.float32)\n        else:\n            # Считаем веса из train_labels или train_data_df\n            labels_arr = None\n            if train_labels is not None:\n                labels_arr = np.asarray(train_labels)\n            elif train_data_df is not None and target_column_name is not None:\n                labels_arr = np.asarray(train_data_df[target_column_name].values)\n\n            w = None\n            if labels_arr is not None and self.num_labels is not None:\n                if not np.issubdtype(labels_arr.dtype, np.integer):\n                    _, labels_arr = np.unique(labels_arr, return_inverse=True)\n                labels_arr = labels_arr.astype(int)\n                counts = np.bincount(labels_arr, minlength=self.num_labels)\n                n_samples = counts.sum()\n\n                weights = np.zeros(self.num_labels, dtype=np.float32)\n                nonzero = counts > 0\n                weights[nonzero] = n_samples / (self.num_labels * counts[nonzero].astype(np.float32))\n                w = torch.tensor(weights, dtype=torch.float32)\n\n        self.class_weights = w  # может быть None\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss_fct = torch.nn.CrossEntropyLoss(weight=weight)\n        loss = loss_fct(logits, labels.long())\n\n        return (loss, outputs) if return_outputs else loss\n\nclass SequenceClassification:\n    \"\"\"\n    Pipeline, благодаря которому можно быстро и удобно загружать encoder из transformers и дообучать его для\n    классификации последовательностей. Проблема дисбаланса классов решается (взвешенная CrossEntropy).\n\n    Необходимые импорты:\n    import pandas as pd\n    import numpy as np\n    import torch\n    from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n    import evaluate\n    from datasets import Dataset\n    from tqdm.auto import tqdm\n    \"\"\"\n\n    def __init__(self, checkpoint, num_labels, target_column_name, texts_columns_names):\n        \"\"\"\n        :param checkpoint: Название предобученной модели из Hugging Face Hub (например, 'bert-base-uncased').\n        :param num_labels: Количество классов в задаче классификации.\n        :param target_column_name: Имя столбца с целевой переменной.\n        :param texts_columns_names: Список с именами текстовых столбцов, которые нужно объединить и использовать.\n        \"\"\"\n\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n        self.target_column_name = target_column_name\n        self.texts_columns_names = texts_columns_names\n        self.num_labels = num_labels\n\n        # Маппинги меток (заполняются в fit при необходимости)\n        self.label2id = None\n        self.id2label = None\n\n        self.trainer = None\n        self.compute_metrics = None\n\n    def _prepare_dataset(self, df):\n        \"\"\"Внутренний метод для подготовки датасета.\"\"\"\n        df_copy = df.copy()\n\n        # Используем специальный SEP-токен токенайзера (если есть), чтобы не склеивать строки буквальным \"[SEP]\"\n        sep_tok = self.tokenizer.sep_token if self.tokenizer.sep_token is not None else \"[SEP]\"\n        df_copy['text'] = df_copy[self.texts_columns_names].fillna('').agg(sep_tok.join, axis=1)\n\n        # Применяем mapping меток к id, если он уже построен\n        if self.label2id is not None:\n            df_copy[self.target_column_name] = df_copy[self.target_column_name].map(self.label2id).astype(int)\n\n        dataset = Dataset.from_pandas(df_copy[[self.target_column_name, 'text']])\n\n        def tokenize_function(examples):\n            return self.tokenizer(\n                examples['text'],\n                truncation=True,\n                padding='max_length',\n                max_length=self.model.config.max_position_embeddings\n            )\n\n        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n        tokenized_dataset = tokenized_dataset.rename_column(self.target_column_name, 'labels')\n\n        return tokenized_dataset\n\n    def _setup_compute_metrics(self, metric_name):\n        \"\"\"Внутренний метод для настройки функции подсчета метрик.\"\"\"\n        if metric_name == 'f1':\n            metric = evaluate.load('f1')\n            def compute_f1(pred):\n                if isinstance(pred, (tuple, list)):\n                    logits, labels = pred\n                else:\n                    logits, labels = pred.predictions, pred.label_ids\n                predictions = np.argmax(logits, axis=-1)\n                return metric.compute(predictions=predictions, references=labels, average='weighted')\n            self.compute_metrics = compute_f1\n\n        elif metric_name == 'accuracy':\n            metric = evaluate.load('accuracy')\n            def compute_accuracy(pred):\n                if isinstance(pred, (tuple, list)):\n                    logits, labels = pred\n                else:\n                    logits, labels = pred.predictions, pred.label_ids\n                predictions = np.argmax(logits, axis=-1)\n                return metric.compute(predictions=predictions, references=labels)\n            self.compute_metrics = compute_accuracy\n        else:\n            raise ValueError('Параметр \"metric_name\" может быть только \"f1\" или \"accuracy\".')\n\n    def fit(self, train_data, epochs=3, test_size=0.2, per_device_train_batch_size=32, \n            gradient_accumulation_steps=1, learning_rate=2e-5, metric_name='f1', fp16=True,\n            logging_steps=50, eval_steps=100, output_dir='./result'):\n        \"\"\"\n        Дообучает модель на предоставленных данных.\n        \"\"\"\n\n        # Если метки не 0..K-1 (или строковые), построим mapping по train_data\n        classes = sorted(train_data[self.target_column_name].unique().tolist())\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n        # Сохраняем в конфиг модели\n        self.model.config.label2id = {str(k): int(v) for k, v in self.label2id.items()}\n        self.model.config.id2label = {int(k): str(v) for k, v in self.id2label.items()}\n\n        # Готовим датасет и делим его на train/test\n        full_dataset = self._prepare_dataset(train_data)\n        data = full_dataset.train_test_split(test_size=test_size)\n\n        # Считаем веса классов по train-сплиту (после split)\n        train_labels = np.array(data['train']['labels'])\n        counts = np.bincount(train_labels, minlength=self.num_labels)\n        n_samples = counts.sum()\n        class_weights = np.zeros(self.num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        class_weights[nonzero] = n_samples / (self.num_labels * counts[nonzero].astype(np.float32))\n\n        self._setup_compute_metrics(metric_name=metric_name.lower())\n\n        training_arguments = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type='cosine',\n            weight_decay=0.01,\n            eval_strategy='steps',\n            eval_steps=eval_steps,\n            save_strategy='steps',\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=metric_name,\n            save_total_limit=1,\n            logging_strategy='steps',\n            logging_steps=logging_steps,\n            report_to='none',\n            fp16=fp16 and torch.cuda.is_available()\n        )\n\n        # Используем WeightedCETrainer и передаем готовые class_weights\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=training_arguments,\n            train_dataset=data['train'],\n            eval_dataset=data['test'],\n            tokenizer=self.tokenizer,\n            compute_metrics=self.compute_metrics,\n            num_labels=self.num_labels,\n            train_labels=train_labels,\n            class_weights=class_weights\n        )\n\n        self.trainer.train()\n\n        return self\n\n    def predict(self, df):\n        \"\"\"\n        Делает предсказания для новых данных.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель еще не обучена. Вызовите метод .fit() перед предсказанием.\")\n\n        df_copy = df.copy()\n        df_copy[self.target_column_name] = 0  # фиктивный таргет\n        predict_dataset = self._prepare_dataset(df_copy)\n        predictions = self.trainer.predict(predict_dataset)\n\n        return np.argmax(predictions.predictions, axis=-1)\n\n    def get_embeddings(self, df, batch_size=32):\n        \"\"\"\n        Извлекает эмбеддинги [CLS] токена для каждой строки текста.\n        Этот метод идеально подходит для создания признаков для других моделей (CatBoost, LightGBM).\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель должна быть обучена. Вызовите .fit() перед извлечением эмбеддингов.\")\n\n        device = self.trainer.model.device\n        self.trainer.model.eval()\n\n        df_copy = df.copy()\n        sep_tok = self.tokenizer.sep_token if self.tokenizer.sep_token is not None else \"[SEP]\"\n        df_copy['text'] = df_copy[self.texts_columns_names].fillna('').agg(sep_tok.join, axis=1)\n        texts = df_copy['text'].tolist()\n\n        all_embeddings = []\n\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Извлечение эмбеддингов\"):\n            batch_texts = texts[i:i + batch_size]\n\n            inputs = self.tokenizer(\n                batch_texts, \n                truncation=True,\n                padding='max_length',\n                max_length=self.model.config.max_position_embeddings,\n                return_tensors=\"pt\"\n            ).to(device)\n\n            with torch.no_grad():\n                base_model = getattr(self.trainer.model, self.trainer.model.base_model_prefix)\n                outputs = base_model(**inputs)\n\n            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n            all_embeddings.append(cls_embeddings.cpu().numpy())\n\n        return np.vstack(all_embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"# создание данных\ntrain_data = pd.DataFrame({\n    'name': ['Федя', 'Одиссей', 'Маргулан', 'Андриус'],\n    'user_description': ['Я люблю отдыхать.', 'Ненавижу работать дома!', 'Я ГЕЙМЕР.', 'Осуждаю курение.'],\n    'target': [1, 1, 1, 0]\n})\nsubmission_data = pd.DataFrame({\n    'name': ['Женя', 'Людмила'],\n    'user_description': ['Нет амбиций.', 'Кумирница Арнольда Шварцнеггера.',]\n})\n\n# создание и обучение модели\nmodel = SequenceClassification(\n    checkpoint='DeepPavlov/rubert-base-cased',\n    num_labels=2,\n    target_column_name='target',\n    texts_columns_names=['name', 'user_description']\n)\nmodel.fit(\n    train_data,\n    epochs=3,\n    test_size=0.1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-5,\n    metric_name='f1'\n)\n\n# прогнозирование и получение эмбеддингов\nlabels = model.predict(submission_data)\nprint(labels)\nembeddings = model.get_embeddings(submission_data, batch_size=2)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение encoder'а для регрессии на основе последовательностей.","metadata":{}},{"cell_type":"code","source":"class SequenceRegression:\n    \"\"\"\n    Pipeline, благодаря которому можно быстро и удобно загружать encoder из transformers и дообучать его для\n    задачи регрессии (предсказания непрерывного значения).\n    \"\"\"\n\n    def __init__(self, checkpoint, target_column_name, texts_columns_names):\n        \"\"\"\n        :param checkpoint: Название предобученной модели из Hugging Face Hub.\n        :param target_column_name: Имя столбца с целевой переменной (непрерывным значением).\n        :param texts_columns_names: Список с именами текстовых столбцов для объединения.\n        \"\"\"\n\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n        self.target_column_name = target_column_name\n        self.texts_columns_names = texts_columns_names\n\n        self.trainer = None\n        self.compute_metrics = None\n\n    def _prepare_dataset(self, df):\n        \"\"\"Внутренний метод для подготовки датасета.\"\"\"\n\n        df_copy = df.copy()\n        df_copy[self.target_column_name] = np.log1p(df_copy[self.target_column_name])\n        df_copy['text'] = df_copy[self.texts_columns_names].fillna('').agg('[SEP]'.join, axis=1)\n        df_copy[self.target_column_name] = df_copy[self.target_column_name].astype(np.float32)\n        \n        dataset = Dataset.from_pandas(df_copy[[self.target_column_name, 'text']])\n\n        def tokenize_function(examples):\n            return self.tokenizer(\n                examples['text'], \n                truncation=True,\n                padding='max_length',\n                max_length=self.model.config.max_position_embeddings\n            )\n\n        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n        tokenized_dataset = tokenized_dataset.rename_column(self.target_column_name, 'labels')\n        return tokenized_dataset\n\n    def _setup_compute_metrics(self, metric_name):\n        \"\"\"Внутренний метод для настройки функции подсчета метрик для регрессии.\"\"\"\n\n        if metric_name == 'mape':\n            metric = evaluate.load(\"mape\")\n            def compute_mape(pred):\n                logits, labels = pred\n                predictions = logits.flatten()\n\n                # исключаем нулевые значения из labels, чтобы избежать деления на ноль\n                mask = labels != 0\n                if not np.any(mask): return {'mape': 0.0}\n                \n                filtered_predictions = predictions[mask]\n                filtered_labels = labels[mask]\n                return metric.compute(predictions=filtered_predictions, references=filtered_labels)\n            self.compute_metrics = compute_mape\n        else:\n            try:\n                metric = evaluate.load(metric_name)\n\n                def compute_generic_metric(pred):\n                    logits, labels = pred\n                    predictions = logits.flatten()\n                    return metric.compute(predictions=predictions, references=labels)\n\n                self.compute_metrics = compute_generic_metric\n            except FileNotFoundError:\n                raise ValueError(f'Метрика \"{metric_name}\" не найдена.')\n\n    def fit(self, train_data, epochs=3, test_size=0.2, per_device_train_batch_size=32, \n            gradient_accumulation_steps=1, learning_rate=2e-5, metric_name='mse', fp16=True,\n            logging_steps=50, eval_steps=100, output_dir='./result'):\n        \"\"\"Дообучает модель на предоставленных данных для задачи регрессии.\"\"\"\n\n        full_dataset = self._prepare_dataset(train_data)\n        data = full_dataset.train_test_split(test_size=test_size)\n        self._setup_compute_metrics(metric_name=metric_name.lower())\n\n        training_arguments = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type='cosine',\n            weight_decay=0.01,\n            eval_strategy='steps',\n            eval_steps=eval_steps,\n            save_strategy='steps',\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=metric_name,\n            save_total_limit=1,\n            logging_strategy='steps',\n            logging_steps=logging_steps,\n            report_to='none',\n            fp16=fp16\n        )\n\n        self.trainer = Trainer(\n            model=self.model, args=training_arguments, train_dataset=data['train'],\n            eval_dataset=data['test'], tokenizer=self.tokenizer, compute_metrics=self.compute_metrics\n        )\n\n        self.trainer.train()\n\n        return self\n\n    def predict(self, df):\n        \"\"\"\n        Делает предсказания для новых данных.\n        \"\"\"\n\n        if self.trainer is None:\n            raise RuntimeError(\"Модель еще не обучена. Вызовите метод .fit() перед предсказанием.\")\n\n        df_copy = df.copy()\n        df_copy[self.target_column_name] = 0.0  # фиктивный таргет\n        predict_dataset = self._prepare_dataset(df_copy)\n        predictions = self.trainer.predict(predict_dataset)\n        predictions =  np.expm1(predictions.predictions.flatten())\n        return predictions\n\n    def get_embeddings(self, df, batch_size=32):\n        \"\"\"\n        Извлекает эмбеддинги [CLS] токена для каждой строки текста.\n        Этот метод идеально подходит для создания признаков для других моделей (CatBoost, LightGBM).\n\n        :param df: pd.DataFrame с текстами для обработки.\n        :param batch_size: Размер батча для обработки. Подбирайте для оптимальной скорости.\n        :return: np.array размером (n_samples, hidden_size) с эмбеддингами.\n        \"\"\"\n\n        if self.trainer is None:\n            raise RuntimeError(\"Модель должна быть обучена. Вызовите .fit() перед извлечением эмбеддингов.\")\n        \n        model = self.trainer.model\n        device = next(model.parameters()).device\n        model.eval()\n        \n        df_copy = df.copy()\n        df_copy['text'] = df_copy[self.texts_columns_names].fillna('').agg('[SEP]'.join, axis=1)\n        texts = df_copy['text'].tolist()\n        \n        max_len = min(getattr(self.tokenizer, \"model_max_length\", 512),\n                      getattr(self.model.config, \"max_position_embeddings\", 512))\n        \n        all_embeddings = []\n        \n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i + batch_size]\n        \n            inputs = self.tokenizer(\n                batch_texts,\n                truncation=True,\n                padding='max_length',\n                max_length=max_len,\n                return_tensors='pt'\n            ).to(device)\n        \n            with torch.no_grad():\n                outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n        \n            # Берём последнюю скрытую матрицу и [CLS]\n            last_hidden = outputs.hidden_states[-1] if outputs.hidden_states is not None else outputs.last_hidden_state\n            cls_embeddings = last_hidden[:, 0, :]\n            all_embeddings.append(cls_embeddings.cpu().numpy())\n        \n        return np.vstack(all_embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"# создание данных\ntrain_data = pd.DataFrame({\n    'name': ['Федя', 'Одиссей', 'Маргулан', 'Андриус'],\n    'user_description': ['Я люблю отдыхать.', 'Ненавижу работать дома!', 'Я киберспортсмен мира.', 'Осуждаю курение.'],\n    'target': [10000, 40000, 1000000, 55000]\n})\nsubmission_data = pd.DataFrame({\n    'name': ['Женя', 'Людмила'],\n    'user_description': ['Нет амбиций.', 'Кумирница Арнольда Шварцнеггера.',]\n})\n\n# создание и обучение модели\nmodel = SequenceRegression(\n    checkpoint='DeepPavlov/rubert-base-cased',\n    target_column_name='target',\n    texts_columns_names=['name', 'user_description']\n)\nmodel.fit(\n    train_data,\n    epochs=3,\n    test_size=0.1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-5,\n    metric_name='mape'\n    # fp16=True  # если используется GPU\n)\n\n# прогнозирование и получение эмбеддингов\nlabels = model.predict(submission_data)  # аргумента batch_size нет\nprint(labels)\nembeddings = model.get_embeddings(submission_data, batch_size=2)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение encoder'а для классификации токенов.","metadata":{}},{"cell_type":"code","source":"!pip install evaluate seqeval\nimport numpy as np\nimport torch\nimport evaluate\nfrom transformers import (\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForTokenClassification,\n)\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\nclass WeightedTokenCETrainer(Trainer):\n    \"\"\"\n    Кастомный Trainer для токен-классификации с взвешенной CrossEntropy.\n    Веса считаются с учетом числа токенов класса (после разметки и чанкинга):\n      weight_i = N / (K * n_i),\n    где K — число классов, n_i — число токенов класса i, N — сумма всех n_i.\n    Отсутствующие классы получают вес 0. Метки -100 игнорируются в потере.\n    \"\"\"\n\n    def __init__(self, *args, class_weights=None, **kwargs):\n        # Тихо переводим устаревший аргумент tokenizer в processing_class\n        processing = kwargs.pop(\"tokenizer\", None)\n        if processing is not None and \"processing_class\" not in kwargs:\n            kwargs[\"processing_class\"] = processing\n\n        super().__init__(*args, **kwargs)\n\n        self.class_weights = None\n        if class_weights is not None:\n            self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32)\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n        # logits: (batch, seq_len, num_labels) -> (batch*seq_len, num_labels)\n        # labels: (batch, seq_len) -> (batch*seq_len)\n        loss_fct = torch.nn.CrossEntropyLoss(\n            weight=(self.class_weights.to(logits.device) if self.class_weights is not None else None),\n            ignore_index=-100,\n        )\n        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n\nclass TokenClassification:\n    \"\"\"\n    Необходимые импорты:\n    import numpy as np\n    import torch\n    import evaluate\n    from transformers import (\n        AutoModelForTokenClassification,\n        AutoTokenizer,\n        Trainer,\n        TrainingArguments,\n        DataCollatorForTokenClassification,\n    )\n    import pandas as pd\n    from datasets import Dataset\n    from sklearn.model_selection import train_test_split\n    from tqdm.auto import tqdm\n    \"\"\"\n    \n    def __init__(self, checkpoint, label2id, tokens_column_name, tags_column_name):\n        self.id2label = {v: k for k, v in label2id.items()}\n        self.label2id = label2id\n        \n        self.model = AutoModelForTokenClassification.from_pretrained(\n            checkpoint,\n            num_labels=len(self.id2label),\n            id2label=self.id2label,\n            label2id=self.label2id,\n            ignore_mismatched_sizes=True\n        )\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.tokens_column_name = tokens_column_name\n        self.tags_column_name = tags_column_name\n        self.trainer = None\n        self.compute_metrics = None\n        self.data_collator = DataCollatorForTokenClassification(tokenizer=self.tokenizer)\n\n    def _align_labels_with_tokens(self, labels, word_ids):\n        new_labels = []\n        current_word = None\n        \n        for word_id in word_ids:\n            if word_id != current_word:\n                current_word = word_id\n                label = -100 if word_id is None else labels[word_id]\n                new_labels.append(label)\n            elif word_id is None:\n                new_labels.append(-100)\n            else:\n                new_labels.append(-100)\n        \n        return new_labels\n\n    def _prepare_dataset_with_sliding_window(self, df, max_length, stride):\n        df = df.copy()\n        \n        all_chunked_input_ids = []\n        all_chunked_attention_masks = []\n        all_chunked_labels = []\n\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Нарезка данных на чанки\"):\n            tokens = row[self.tokens_column_name]\n            labels = row[self.tags_column_name]\n            \n            tokenized_chunks = self.tokenizer(\n                [tokens],\n                is_split_into_words=True,\n                return_overflowing_tokens=True,\n                max_length=max_length,\n                stride=stride,\n                truncation=True\n            )\n            \n            tokenized_chunks.pop(\"overflow_to_sample_mapping\", None)\n            \n            num_chunks = len(tokenized_chunks['input_ids'])\n            for i in range(num_chunks):\n                chunk_word_ids = self.tokenizer.word_ids(batch_index=i) if hasattr(self.tokenizer, \"word_ids\") else tokenized_chunks.word_ids(batch_index=i)\n                # На случай, если метод доступен только у BatchEncoding:\n                if chunk_word_ids is None:\n                    chunk_word_ids = tokenized_chunks.word_ids(batch_index=i)\n                aligned_labels = self._align_labels_with_tokens(labels, chunk_word_ids)\n                \n                all_chunked_input_ids.append(tokenized_chunks['input_ids'][i])\n                all_chunked_attention_masks.append(tokenized_chunks['attention_mask'][i])\n                all_chunked_labels.append(aligned_labels)\n\n        return Dataset.from_dict({\n            'input_ids': all_chunked_input_ids,\n            'attention_mask': all_chunked_attention_masks,\n            'labels': all_chunked_labels\n        })\n\n    def _setup_compute_metrics(self):\n        metric = evaluate.load(\"seqeval\")\n        \n        def compute_seqeval_metrics(p):\n            # Поддержка EvalPrediction и tuple\n            if isinstance(p, (tuple, list)):\n                predictions, labels = p\n            else:\n                predictions, labels = p.predictions, p.label_ids\n\n            predictions = np.argmax(predictions, axis=2)\n            \n            true_predictions = [\n                [self.id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            \n            true_labels = [\n                [self.id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n            \n            results = metric.compute(predictions=true_predictions, references=true_labels)\n            \n            return {\n                \"precision\": results.get(\"overall_precision\", 0.0),\n                \"recall\": results.get(\"overall_recall\", 0.0),\n                \"f1\": results.get(\"overall_f1\", 0.0),\n                \"accuracy\": results.get(\"overall_accuracy\", 0.0),\n            }\n            \n        self.compute_metrics = compute_seqeval_metrics\n\n    def fit(self, train_data, epochs=3, per_device_train_batch_size=16, gradient_accumulation_steps=1,\n            test_size=0.2, learning_rate=2e-5, fp16=True, stride=128, logging_steps=50,\n            eval_steps=100, output_dir='./result'):\n        \n        max_length = self.model.config.max_position_embeddings\n        \n        train_data_prepared = train_data.copy()\n        # Маппинг строковых тегов -> id при необходимости\n        if isinstance(train_data_prepared[self.tags_column_name].iloc[0][0], str):\n            train_data_prepared[self.tags_column_name] = train_data_prepared[self.tags_column_name].apply(\n                lambda tags: [self.label2id[tag] for tag in tags]\n            )\n        \n        # Сплит по документам, затем чанкуем отдельно\n        train_df, eval_df = train_test_split(train_data_prepared, test_size=test_size, random_state=42)\n        train_dataset = self._prepare_dataset_with_sliding_window(train_df, max_length, stride)\n        eval_dataset = self._prepare_dataset_with_sliding_window(eval_df, max_length, stride)\n\n        # Считаем веса классов по токенам train_dataset (игнорируя -100)\n        labels_list = train_dataset[\"labels\"]  # список списков\n        flat = np.concatenate([np.asarray(x, dtype=np.int64) for x in labels_list]) if len(labels_list) > 0 else np.array([], dtype=np.int64)\n        flat = flat[flat >= 0]  # убираем -100\n        num_labels = len(self.id2label)\n        counts = np.bincount(flat, minlength=num_labels) if flat.size > 0 else np.zeros(num_labels, dtype=np.int64)\n        n_samples = counts.sum()\n        class_weights = np.zeros(num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        if n_samples > 0:\n            class_weights[nonzero] = n_samples / (num_labels * counts[nonzero].astype(np.float32))\n        # Отсутствующие классы останутся с весом 0\n\n        self._setup_compute_metrics()\n\n        training_arguments = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type='cosine',\n            weight_decay=0.01,\n            eval_strategy='steps',\n            eval_steps=eval_steps,\n            save_strategy='steps',\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model='f1',\n            save_total_limit=1,\n            logging_strategy='steps',\n            logging_steps=logging_steps,\n            report_to='none',\n            fp16=fp16\n        )\n\n        # Используем взвешенную CrossEntropy через кастомный Trainer\n        self.trainer = WeightedTokenCETrainer(\n            model=self.model,\n            args=training_arguments,\n            data_collator=self.data_collator,\n            compute_metrics=self.compute_metrics,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=self.tokenizer,\n            class_weights=class_weights\n        )\n        \n        self.trainer.train()\n        \n        return self\n\n    def _predict_single_document(self, tokens, stride):\n        max_length = self.model.config.max_position_embeddings\n        \n        tokenized_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride, \n            truncation=True,\n        )\n        \n        tokenized_inputs.pop(\"overflow_to_sample_mapping\", None)\n        chunk_dataset = Dataset.from_dict(tokenized_inputs)\n        \n        outputs = self.trainer.predict(chunk_dataset)\n        predictions = np.argmax(outputs.predictions, axis=2)\n\n        num_original_words = len(tokens)\n        final_predictions = np.full(num_original_words, -1, dtype=np.int32)\n\n        for i, chunk_preds in enumerate(predictions):\n            chunk_word_ids = tokenized_inputs.word_ids(batch_index=i)\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if word_id is not None and final_predictions[word_id] == -1:\n                    final_predictions[word_id] = chunk_preds[token_pos]\n\n        return [self.id2label.get(pid, 'O') for pid in final_predictions]\n\n    def predict(self, df, stride=128):\n        all_final_labels = []\n        \n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Предсказание (sliding window)\"):\n            original_tokens = row[self.tokens_column_name]\n            if not original_tokens:\n                all_final_labels.append([])\n                continue\n            \n            document_labels = self._predict_single_document(original_tokens, stride)\n            all_final_labels.append(document_labels)\n            \n        return all_final_labels\n\n    def _get_embeddings_single_document(self, tokens, stride, device):\n        max_length = self.model.config.max_position_embeddings\n        num_original_words = len(tokens)\n\n        chunk_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        chunk_inputs.pop(\"overflow_to_sample_mapping\")\n        \n        with torch.no_grad():\n            base_model = getattr(self.trainer.model, self.trainer.model.base_model_prefix)\n            outputs = base_model(**chunk_inputs)\n        \n        chunk_embeddings = outputs.last_hidden_state\n        \n        hidden_size = self.model.config.hidden_size\n        final_word_embeddings = torch.zeros(num_original_words, hidden_size, device=device)\n        word_counts = torch.zeros(num_original_words, device=device)\n\n        for i in range(len(chunk_embeddings)):\n            chunk_embeds = chunk_embeddings[i]\n            chunk_word_ids = chunk_inputs.word_ids(batch_index=i)\n\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if word_id is not None:\n                    final_word_embeddings[word_id] += chunk_embeds[token_pos]\n                    if token_pos == 0 or chunk_word_ids[token_pos - 1] != word_id:\n                        word_counts[word_id] += 1\n        \n        average_embeddings = final_word_embeddings / (word_counts.unsqueeze(1) + 1e-8)\n        \n        return average_embeddings.cpu().numpy()\n\n    def get_embeddings(self, df, stride=128):\n        self.trainer.model.eval()\n        device = self.trainer.model.device\n        all_final_embeddings = []\n\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Генерация эмбеддингов (sliding window)\"):\n            original_tokens = row[self.tokens_column_name]\n            if not original_tokens:\n                all_final_embeddings.append(np.array([]))\n                continue\n            \n            document_embeddings = self._get_embeddings_single_document(original_tokens, stride, device)\n            all_final_embeddings.append(document_embeddings)\n            \n        return all_final_embeddings","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nПример использования.","metadata":{}},{"cell_type":"code","source":"# Для классификации токенов нам нужны списки токенов и списки тегов\ntrain_data = pd.DataFrame({\n    'tokens': [\n        ['Федор', 'Достоевский', 'родился', 'в', 'Москве', '.'],\n        ['Анна', 'Керн', 'была', 'музой', 'Пушкина', '.'],\n        ['Компания', 'Яндекс', 'представила', 'Алису', '.'],\n        ['Илон', 'Маск', 'основал', 'SpaceX', 'и', 'Tesla', '.']\n    ],\n    'ner_tags': [\n        ['B-PER', 'I-PER', 'O', 'O', 'B-LOC', 'O'],\n        ['B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O'],\n        ['O', 'B-ORG', 'O', 'B-PER', 'O'],\n        ['B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'B-ORG', 'O']\n    ]\n})\n\n# Для предсказания нам нужны только токены\nsubmission_data = pd.DataFrame({\n    'tokens': [\n        ['Лев', 'Толстой', 'написал', 'роман', '\"', 'Война', 'и', 'мир', '\"', '.'],\n        ['Сергей', 'Королев', 'работал', 'в', 'РКК', '\"', 'Энергия', '\"', '.']\n    ]\n})\n\n# 2. Создание и обучение модели\n# Создаем маппинг из тегов в ID\ntags = train_data['ner_tags'].explode().unique()\nlabel2id = {tag: i for i, tag in enumerate(tags)}\n\nmodel = TokenClassification(\n    checkpoint='DeepPavlov/rubert-base-cased',\n    label2id=label2id,\n    tokens_column_name='tokens',\n    tags_column_name='ner_tags'\n)\n\nmodel.fit(\n    train_data,\n    epochs=3,\n    test_size=0.25,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-5,\n    fp16=False\n)\n\n# 3. Прогнозирование и получение эмбеддингов\nlabels = model.predict(submission_data)\nprint(labels)\nembeddings = model.get_embeddings(submission_data)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение ViT'а и AST'а для классификации звуков.","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\nimport pandas as pd\nimport numpy as np\nimport torch\nimport librosa\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioClassification, Trainer, TrainingArguments, DataCollatorWithPadding\nimport evaluate\nfrom datasets import Dataset, Audio, ClassLabel\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nclass WeightedCETrainer(Trainer):\n    \"\"\"\n    Кастомный Trainer с автоматическим вычислением весов классов.\n    weight_i = N / (K * n_i), где:\n      N — число обучающих примеров,\n      K — число классов,\n      n_i — число примеров класса i.\n    Отсутствующие классы получают вес 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        train_data_df=None,\n        target_column_name=None,\n        num_labels=None,\n        **kwargs\n    ):\n        # тихо переводим устаревший аргумент tokenizer в processing_class,\n        # чтобы убрать FutureWarning и ничего не менять в вызывающем коде\n        processing = kwargs.pop(\"tokenizer\", None)\n        if processing is not None and \"processing_class\" not in kwargs:\n            kwargs[\"processing_class\"] = processing\n\n        super().__init__(*args, **kwargs)\n\n        self.num_labels = num_labels or getattr(self.model.config, \"num_labels\", None)\n        self.class_weights = None\n\n        if train_data_df is not None and target_column_name is not None and self.num_labels is not None:\n            labels = np.asarray(train_data_df[target_column_name].values)\n\n            # на всякий: если метки не целочисленные — факторизуем\n            if not np.issubdtype(labels.dtype, np.integer):\n                _, labels = np.unique(labels, return_inverse=True)\n\n            labels = labels.astype(int)\n            counts = np.bincount(labels, minlength=self.num_labels)\n            n_samples = counts.sum()\n\n            weights = np.zeros(self.num_labels, dtype=np.float32)\n            nonzero = counts > 0\n            weights[nonzero] = n_samples / (self.num_labels * counts[nonzero].astype(np.float32))\n\n            self.class_weights = torch.tensor(weights, dtype=torch.float32)\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss_fct = torch.nn.CrossEntropyLoss(weight=weight)\n        loss = loss_fct(logits, labels.long())\n\n        return (loss, outputs) if return_outputs else loss\n\nclass AudioClassification:\n    \"\"\"\n    Pipeline, благодаря которому можно быстро и удобно загружать аудио-энкодер из transformers и дообучать его для\n    классификации аудиофайлов. Проблема дисбаланса классов частично решается с помощью метрики F1-weighted и\n    взвешенной функции потерь (WeightedCETrainer). Добавлена возможность аугментации данных \"на лету\".\n    \"\"\"\n\n    def __init__(self, checkpoint, num_labels, target_column_name, audio_path_column_name, audio_freq=16000, use_augmentation=False):\n        \"\"\"\n        :param checkpoint: Название предобученной модели из Hugging Face Hub (например, 'MIT/ast-finetuned-audioset-10-10-0.4593').\n        :param num_labels: Количество классов в задаче классификации.\n        :param target_column_name: Имя столбца с целевой переменной.\n        :param audio_path_column_name: Имя столбца с путями к аудиофайлам.\n        :param audio_freq: Частота дискретизации, к которой будут приведены все аудио. По умолчанию 16000.\n        :param use_augmentation: Использовать ли аугментацию данных при обучении. По умолчанию False.\n        \"\"\"\n        self.feature_extractor = AutoFeatureExtractor.from_pretrained(checkpoint)\n        self.model = AutoModelForAudioClassification.from_pretrained(\n            checkpoint,\n            num_labels=num_labels,\n            ignore_mismatched_sizes=True  # Позволяет заменить \"голову\" классификатора\n        )\n        self.target_column_name = target_column_name\n        self.audio_path_column_name = audio_path_column_name\n        self.audio_freq = audio_freq\n        self.num_labels = num_labels\n        self.use_augmentation = use_augmentation\n\n        self.trainer = None  # будет создан после обучения\n        self.compute_metrics = None  # будет создана после выбора метрики\n\n    def _apply_augmentations(self, samples, sample_rate):\n        \"\"\"\n        Внутренний метод для применения серии аугментаций к одному аудиофайлу.\n        Каждая аугментация применяется с вероятностью 20%.\n        \"\"\"\n        # 1. AddGaussianNoise\n        if np.random.rand() < 0.2:\n            noise_amplitude = np.random.uniform(0.001, 0.015)\n            noise = np.random.randn(len(samples))\n            samples = samples + noise_amplitude * noise\n\n        # 2. TimeStretch\n        if np.random.rand() < 0.2:\n            rate = np.random.uniform(0.8, 1.25)\n            samples = librosa.effects.time_stretch(y=samples, rate=rate)\n\n        # 3. PitchShift\n        if np.random.rand() < 0.2:\n            n_steps = np.random.uniform(-4, 4)\n            samples = librosa.effects.pitch_shift(y=samples, sr=sample_rate, n_steps=n_steps)\n\n        # 4. Shift (циклический сдвиг)\n        if np.random.rand() < 0.2:\n            shift_fraction = np.random.uniform(-0.5, 0.5)\n            shift_samples = int(len(samples) * shift_fraction)\n            samples = np.roll(samples, shift_samples)\n\n        return samples\n\n    def _prepare_dataset(self, df, augment=False):\n        \"\"\"Внутренний метод для подготовки датасета.\"\"\"\n        df_copy = df.copy()\n\n        # Создаем Dataset из pandas DataFrame\n        dataset = Dataset.from_pandas(df_copy[[self.target_column_name, self.audio_path_column_name]])\n\n        # Используем магию datasets: указываем, что колонка - это аудио,\n        # и оно будет автоматически загружено и передискретизировано\n        dataset = dataset.cast_column(self.audio_path_column_name, Audio(sampling_rate=self.audio_freq))\n\n        def preprocess_function(examples):\n            # Извлекаем аудиоданные (уже загруженные и передискретизированные)\n            audio_arrays = [x[\"array\"] for x in examples[self.audio_path_column_name]]\n\n            # Применяем аугментацию, если флаг установлен\n            if augment:\n                audio_arrays = [self._apply_augmentations(samples=audio, sample_rate=self.audio_freq) for audio in audio_arrays]\n\n            # ВАЖНО: не возвращаем тензоры здесь — паддинг сделает коллатор\n            inputs = self.feature_extractor(\n                audio_arrays,\n                sampling_rate=self.audio_freq\n            )\n            return inputs\n\n        # Применяем предобработку\n        processed_dataset = dataset.map(\n            preprocess_function,\n            remove_columns=[self.audio_path_column_name],\n            batched=True\n        )\n        processed_dataset = processed_dataset.rename_column(self.target_column_name, 'labels')\n\n        return processed_dataset\n\n    def _setup_compute_metrics(self, metric_name):\n        \"\"\"Внутренний метод для настройки функции подсчета метрик.\"\"\"\n        if metric_name == 'f1':\n            metric = evaluate.load('f1')\n            def compute_f1(pred):\n                if isinstance(pred, (tuple, list)):\n                    logits, labels = pred\n                else:\n                    logits, labels = pred.predictions, pred.label_ids\n                predictions = np.argmax(logits, axis=-1)\n                return metric.compute(predictions=predictions, references=labels, average='weighted')\n            self.compute_metrics = compute_f1\n\n        elif metric_name == 'accuracy':\n            metric = evaluate.load('accuracy')\n            def compute_accuracy(pred):\n                if isinstance(pred, (tuple, list)):\n                    logits, labels = pred\n                else:\n                    logits, labels = pred.predictions, pred.label_ids\n                predictions = np.argmax(logits, axis=-1)\n                return metric.compute(predictions=predictions, references=labels)\n            self.compute_metrics = compute_accuracy\n        else:\n            raise ValueError('Параметр \"metric_name\" может быть только \"f1\" или \"accuracy\".')\n\n    def fit(self, train_data, epochs=3, test_size=0.2, per_device_train_batch_size=16,\n            gradient_accumulation_steps=1, learning_rate=2e-5, metric_name='f1', fp16=True,\n            logging_steps=50, eval_steps=100, output_dir='./result'):\n        \"\"\"\n        Дообучает модель на предоставленных данных.\n        \"\"\"\n        # Разделяем DataFrame на train и test до создания Dataset'ов.\n        # Это необходимо, чтобы применять аугментацию только к обучающей выборке.\n        train_df, test_df = train_test_split(\n            train_data,\n            test_size=test_size,\n            stratify=train_data[self.target_column_name],\n            random_state=42 # для воспроизводимости\n        )\n\n        train_dataset = self._prepare_dataset(train_df, augment=self.use_augmentation)\n        eval_dataset = self._prepare_dataset(test_df, augment=False) # Аугментация на тесте не нужна\n\n        # Приводим колонку с метками к ClassLabel для обеих выборок\n        label_casting = ClassLabel(num_classes=self.num_labels)\n        train_dataset = train_dataset.cast_column(\"labels\", label_casting)\n        eval_dataset = eval_dataset.cast_column(\"labels\", label_casting)\n\n        self._setup_compute_metrics(metric_name=metric_name.lower())\n\n        training_arguments = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type='cosine',\n            weight_decay=0.01,\n            eval_strategy='steps',\n            save_strategy='steps',\n            load_best_model_at_end=True,\n            metric_for_best_model=metric_name,\n            save_total_limit=1,\n            logging_strategy='steps',\n            logging_steps=logging_steps,\n            eval_steps=eval_steps,\n            save_steps=eval_steps,\n            report_to='none',\n            fp16=fp16 and torch.cuda.is_available()\n        )\n\n        # Явный коллейтор для аудио: паддинг по input_values\n        data_collator = DataCollatorWithPadding(tokenizer=self.feature_extractor, padding=True)\n\n        # Используем WeightedCETrainer вместо стандартного Trainer\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=training_arguments,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            processing_class=self.feature_extractor,  # вместо tokenizer\n            compute_metrics=self.compute_metrics,\n            data_collator=data_collator,\n            # Доп. аргументы для вычисления весов классов:\n            train_data_df=train_df,\n            target_column_name=self.target_column_name,\n            num_labels=self.num_labels,\n        )\n\n        self.trainer.train()\n\n        return self\n\n    def predict(self, df):\n        \"\"\"\n        Делает предсказания для новых данных.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель еще не обучена. Вызовите метод .fit() перед предсказанием.\")\n\n        df_copy = df.copy()\n        df_copy[self.target_column_name] = 0  # фиктивный таргет\n        # При предсказании аугментация всегда выключена\n        predict_dataset = self._prepare_dataset(df_copy, augment=False)\n\n        predictions = self.trainer.predict(predict_dataset)\n        return np.argmax(predictions.predictions, axis=-1)\n\n    def get_embeddings(self, df, batch_size=32):\n        \"\"\"\n        Извлекает эмбеддинги для каждого аудиофайла.\n        Этот метод идеально подходит для создания признаков для других моделей (CatBoost, LightGBM).\n\n        :param df: pd.DataFrame с путями к аудиофайлам.\n        :param batch_size: Размер батча для обработки. Подбирайте для оптимальной скорости.\n        :return: np.array размером (n_samples, hidden_size) с эмбеддингами.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель должна быть обучена. Вызовите .fit() перед извлечением эмбеддингов.\")\n\n        device = self.trainer.model.device\n        self.trainer.model.eval()\n\n        paths = df[self.audio_path_column_name].tolist()\n        all_embeddings = []\n\n        for i in tqdm(range(0, len(paths), batch_size), desc=\"Извлечение эмбеддингов\"):\n            batch_paths = paths[i:i + batch_size]\n            batch_audio = [librosa.load(p, sr=self.audio_freq)[0] for p in batch_paths]\n\n            inputs = self.feature_extractor(\n                batch_audio,\n                sampling_rate=self.audio_freq,\n                return_tensors=\"pt\"\n            ).to(device)\n\n            with torch.no_grad():\n                # Получаем базовую модель без классификационной \"головы\"\n                base_model_prefix = self.trainer.model.base_model_prefix\n                base_model = getattr(self.trainer.model, base_model_prefix)\n                outputs = base_model(**inputs)\n\n            # Для AST (и ViT) эмбеддинг всего инпута - это скрытое состояние [CLS] токена\n            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n            all_embeddings.append(cls_embeddings.cpu().numpy())\n\n        return np.vstack(all_embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport soundfile as sf\nimport pandas as pd\n\nTEMP_AUDIO_DIR = 'temp_audio'\n\nif os.path.exists(TEMP_AUDIO_DIR):\n    shutil.rmtree(TEMP_AUDIO_DIR)\nos.makedirs(TEMP_AUDIO_DIR)\n\nSAMPLE_RATE = 16000\nDURATION = 2\nN_SAMPLES = SAMPLE_RATE * DURATION\nAMPLITUDE = np.iinfo(np.int16).max * 0.3\nt = np.linspace(0., DURATION, N_SAMPLES)\n\ndata_config = [\n    {'label': 0, 'count': 5, 'base_freq': 400},\n    {'label': 1, 'count': 7, 'base_freq': 1000},\n    {'label': 2, 'count': 6, 'base_freq': 250},\n]\n\nall_files_data = []\n\nfor item in data_config:\n    label = item['label']\n    count = item['count']\n    base_freq = item['base_freq']\n\n    for i in range(1, count + 1):\n        freq_variation = np.random.randint(-50, 50)\n        current_freq = base_freq + freq_variation\n        \n        audio_data = (AMPLITUDE * np.sin(2. * np.pi * current_freq * t)).astype(np.int16)\n        \n        file_path = os.path.join(TEMP_AUDIO_DIR, f\"{label}_{i}.wav\")\n        \n        sf.write(file_path, audio_data, SAMPLE_RATE)\n        \n        all_files_data.append({'path': file_path, 'label': label})\n\nfinal_df = pd.DataFrame(all_files_data).sample(frac=1)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Подготовка данных в формате pd.DataFrame\n# В реальном сценарии данные загружаются из CSV файла.\ntrain_data = final_df[:12]\nsubmission_data = final_df[12:]\n\n# 2. Инициализация и обучение модели\nmodel = AudioClassification(\n    checkpoint='MIT/ast-finetuned-audioset-10-10-0.4593',\n    num_labels=3,\n    target_column_name='label',\n    audio_path_column_name='path',\n    use_augmentation=True\n)\n\nmodel.fit(\n    train_data,\n    epochs=1,\n    test_size=0.25,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    learning_rate=3e-5,\n    metric_name='accuracy',\n    fp16=False,\n    logging_steps=2,\n    eval_steps=4\n)\n\n# 3. Прогнозирование и получение эмбеддингов\nlabels = model.predict(submission_data)\nprint(labels)\n\nembeddings = model.get_embeddings(submission_data, batch_size=2)\nprint(f'Форма эмбеддингов: {embeddings.shape}')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение ViT'а и AST'а для регрессии звуков.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport librosa\nfrom transformers import AutoFeatureExtractor, AutoModelForAudioClassification, Trainer, TrainingArguments\nimport evaluate\nfrom datasets import Dataset, Audio\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split # Для корректного разделения данных\n\nclass AudioRegression:\n    \"\"\"\n    Pipeline, благодаря которому можно быстро и удобно загружать аудио-энкодер из transformers и дообучать его для\n    задачи регрессии (предсказания непрерывного значения) по аудиофайлам.\n    Добавлена возможность аугментации данных \"на лету\" с помощью librosa и numpy.\n\n    Необходимые импорты:\n    import pandas as pd\n    import numpy as np\n    import torch\n    import librosa\n    from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, Trainer, TrainingArguments\n    import evaluate\n    from datasets import Dataset, Audio\n    from tqdm.auto import tqdm\n    from sklearn.model_selection import train_test_split\n    \"\"\"\n\n    def __init__(self, checkpoint, target_column_name, audio_path_column_name, audio_freq=16000, use_augmentation=False):\n        \"\"\"\n        :param checkpoint: Название предобученной модели из Hugging Face Hub (например, 'MIT/ast-finetuned-audioset-10-10-0.4593').\n        :param target_column_name: Имя столбца с целевой переменной (непрерывное значение).\n        :param audio_path_column_name: Имя столбца с путями к аудиофайлам.\n        :param audio_freq: Частота дискретизации, к которой будут приведены все аудио. По умолчанию 16000.\n        :param use_augmentation: Использовать ли аугментацию данных при обучении. По умолчанию False.\n        \"\"\"\n        self.feature_extractor = AutoFeatureExtractor.from_pretrained(checkpoint)\n        self.model = AutoModelForAudioClassification.from_pretrained(\n            checkpoint,\n            num_labels=1,  # Для регрессии предсказываем одно значение\n            ignore_mismatched_sizes=True  # Позволяет заменить \"голову\" классификатора на регрессионную\n        )\n        self.target_column_name = target_column_name\n        self.audio_path_column_name = audio_path_column_name\n        self.audio_freq = audio_freq\n        self.use_augmentation = use_augmentation\n\n        self.trainer = None  # будет создан после обучения\n        self.compute_metrics = None  # будет создана после выбора метрики\n\n    def _apply_augmentations(self, samples, sample_rate):\n        \"\"\"\n        Внутренний метод для применения серии аугментаций к одному аудиофайлу.\n        Каждая аугментация применяется с вероятностью 20%.\n        \"\"\"\n        # 1. AddGaussianNoise\n        if np.random.rand() < 0.2:\n            noise_amplitude = np.random.uniform(0.001, 0.015)\n            noise = np.random.randn(len(samples))\n            samples = samples + noise_amplitude * noise\n\n        # 2. TimeStretch\n        if np.random.rand() < 0.2:\n            rate = np.random.uniform(0.8, 1.25)\n            samples = librosa.effects.time_stretch(y=samples, rate=rate)\n\n        # 3. PitchShift\n        if np.random.rand() < 0.2:\n            n_steps = np.random.uniform(-4, 4)\n            samples = librosa.effects.pitch_shift(y=samples, sr=sample_rate, n_steps=n_steps)\n\n        # 4. Shift (циклический сдвиг)\n        if np.random.rand() < 0.2:\n            shift_fraction = np.random.uniform(-0.5, 0.5)\n            shift_samples = int(len(samples) * shift_fraction)\n            samples = np.roll(samples, shift_samples)\n\n        return samples\n\n    def _prepare_dataset(self, df, augment=False):\n        \"\"\"Внутренний метод для подготовки датасета.\"\"\"\n        df_copy = df.copy()\n        # Убедимся, что таргет имеет тип float32, что стандартно для регрессии в PyTorch\n        df_copy[self.target_column_name] = df_copy[self.target_column_name].astype(np.float32)\n\n        dataset = Dataset.from_pandas(df_copy[[self.target_column_name, self.audio_path_column_name]])\n        dataset = dataset.cast_column(self.audio_path_column_name, Audio(sampling_rate=self.audio_freq))\n\n        def preprocess_function(examples):\n            audio_arrays = [x[\"array\"] for x in examples[self.audio_path_column_name]]\n\n            if augment:\n                audio_arrays = [self._apply_augmentations(samples=audio, sample_rate=self.audio_freq) for audio in audio_arrays]\n\n            inputs = self.feature_extractor(\n                audio_arrays,\n                sampling_rate=self.audio_freq,\n                return_tensors=\"pt\"\n            )\n            return inputs\n\n        processed_dataset = dataset.map(preprocess_function, remove_columns=self.audio_path_column_name, batched=True)\n        processed_dataset = processed_dataset.rename_column(self.target_column_name, 'labels')\n\n        return processed_dataset\n\n    def _setup_compute_metrics(self, metric_name):\n        \"\"\"Внутренний метод для настройки функции подсчета метрик регрессии.\"\"\"\n        metric_name = metric_name.lower()\n        if metric_name in ['mse', 'rmse']:\n            metric = evaluate.load('mse')\n            def compute_mse(pred):\n                logits, labels = pred\n                predictions = logits.squeeze(-1)\n                result = metric.compute(predictions=predictions, references=labels)\n                # Добавляем RMSE, если нужно\n                if metric_name == 'rmse':\n                    result['rmse'] = np.sqrt(result['mse'])\n                return result\n            self.compute_metrics = compute_mse\n        elif metric_name == 'mae':\n            metric = evaluate.load('mae')\n            def compute_mae(pred):\n                logits, labels = pred\n                predictions = logits.squeeze(-1)\n                return metric.compute(predictions=predictions, references=labels)\n            self.compute_metrics = compute_mae\n        else:\n            raise ValueError('Параметр \"metric_name\" может быть только \"mse\", \"rmse\" или \"mae\".')\n\n\n    def fit(self, train_data, epochs=3, test_size=0.2, per_device_train_batch_size=16,\n            gradient_accumulation_steps=1, learning_rate=2e-5, metric_name='mse', fp16=True,\n            logging_steps=50, eval_steps=100, output_dir='./result'):\n        \"\"\"\n        Дообучает модель на предоставленных данных.\n        \"\"\"\n        # Для регрессии стратификация не используется\n        train_df, test_df = train_test_split(\n            train_data,\n            test_size=test_size,\n            random_state=42 # для воспроизводимости\n        )\n\n        train_dataset = self._prepare_dataset(train_df, augment=self.use_augmentation)\n        eval_dataset = self._prepare_dataset(test_df, augment=False) # Аугментация на тесте не нужна\n\n        self._setup_compute_metrics(metric_name=metric_name.lower())\n        \n        # Для метрик, где чем меньше, тем лучше, Trainer сам это определяет, но можно указать явно\n        metric_to_track = 'rmse' if metric_name == 'rmse' else metric_name\n        \n        training_arguments = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type='cosine',\n            weight_decay=0.01,\n            eval_strategy='steps',\n            save_strategy='steps',\n            eval_steps=eval_steps,\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=metric_to_track,\n            save_total_limit=1,\n            logging_strategy='steps',\n            logging_steps=logging_steps,\n            report_to='none',\n            fp16=fp16,\n            greater_is_better=False # Для MSE, RMSE, MAE чем меньше, тем лучше\n        )\n\n        self.trainer = Trainer(\n            model=self.model,\n            args=training_arguments,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=self.feature_extractor,\n            compute_metrics=self.compute_metrics\n        )\n\n        self.trainer.train()\n\n        return self\n\n    def predict(self, df):\n        \"\"\"\n        Делает предсказания (регрессию) для новых данных.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель еще не обучена. Вызовите метод .fit() перед предсказанием.\")\n\n        df_copy = df.copy()\n        df_copy[self.target_column_name] = 0.0  # фиктивный таргет типа float\n\n        predict_dataset = self._prepare_dataset(df_copy, augment=False)\n        predictions = self.trainer.predict(predict_dataset)\n        \n        # Для регрессии возвращаем сами значения, убирая лишнюю размерность\n        return predictions.predictions.squeeze(-1)\n\n    def get_embeddings(self, df, batch_size=32):\n        \"\"\"\n        Извлекает эмбеддинги для каждого аудиофайла.\n        Этот метод идеально подходит для создания признаков для других моделей (CatBoost, LightGBM).\n        Работает идентично, независимо от задачи (классификация или регрессия).\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель должна быть обучена. Вызовите .fit() перед извлечением эмбеддингов.\")\n\n        device = self.trainer.model.device\n        self.trainer.model.eval()\n\n        paths = df[self.audio_path_column_name].tolist()\n        all_embeddings = []\n\n        for i in tqdm(range(0, len(paths), batch_size), desc=\"Извлечение эмбеддингов\"):\n            batch_paths = paths[i:i + batch_size]\n            \n            batch_audio = [librosa.load(p, sr=self.audio_freq)[0] for p in batch_paths]\n            \n            inputs = self.feature_extractor(\n                batch_audio,\n                sampling_rate=self.audio_freq,\n                return_tensors=\"pt\"\n            ).to(device)\n\n            with torch.no_grad():\n                base_model_prefix = self.trainer.model.base_model_prefix\n                base_model = getattr(self.trainer.model, base_model_prefix)\n                outputs = base_model(**inputs)\n\n            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n            all_embeddings.append(cls_embeddings.cpu().numpy())\n\n        return np.vstack(all_embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport soundfile as sf\nimport pandas as pd\n\nTEMP_AUDIO_DIR = 'temp_audio_regression'\n\nif os.path.exists(TEMP_AUDIO_DIR):\n    shutil.rmtree(TEMP_AUDIO_DIR)\nos.makedirs(TEMP_AUDIO_DIR)\n\nSAMPLE_RATE = 16000\nDURATION = 2\nN_SAMPLES = SAMPLE_RATE * DURATION\nAMPLITUDE = np.iinfo(np.int16).max * 0.3\nt = np.linspace(0., DURATION, N_SAMPLES)\n\nall_files_data = []\nNUM_FILES_TOTAL = 31\n\nfor i in range(NUM_FILES_TOTAL):\n    # Генерируем случайную частоту - это и будет наш таргет для регрессии\n    current_freq = np.random.uniform(200.0, 1200.0)\n    \n    audio_data = (AMPLITUDE * np.sin(2. * np.pi * current_freq * t)).astype(np.int16)\n    \n    file_path = os.path.join(TEMP_AUDIO_DIR, f\"freq_{current_freq:.2f}_{i}.wav\")\n    \n    sf.write(file_path, audio_data, SAMPLE_RATE)\n    \n    # В DataFrame сохраняем путь и реальную частоту\n    all_files_data.append({'path': file_path, 'frequency': current_freq})\n\nfinal_df = pd.DataFrame(all_files_data).sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Подготовка данных в формате pd.DataFrame\n# В реальном сценарии данные загружаются из CSV файла.\ntrain_data = final_df[:25]\nsubmission_data = final_df[25:]\n\n# 3. Инициализация и обучение модели для РЕГРЕССИИ\nmodel = AudioRegression(\n    checkpoint='MIT/ast-finetuned-audioset-10-10-0.4593',\n    target_column_name='frequency',  # Название колонки с таргетом\n    audio_path_column_name='path',\n    use_augmentation=True\n)\n\nmodel.fit(\n    train_data,\n    epochs=5,\n    test_size=0.25,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=1,\n    learning_rate=3e-5,\n    metric_name='mae',\n    fp16=False\n)\npreds = model.predict(submission_data)\nembeddings = model.get_embeddings(submission_data, batch_size=2)\n\nprint(preds)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора картинок.","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\nimport pandas as pd\nimport numpy as np\nimport torch\nimport albumentations as A\nfrom PIL import Image\nfrom transformers import (\n    AutoImageProcessor,\n    AutoModelForImageClassification,\n    Trainer,\n    TrainingArguments\n)\nimport evaluate\nfrom datasets import Dataset, ClassLabel, Image as DSImage\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom transformers.trainer_utils import get_last_checkpoint\nimport os\nimport json\n\nclass WeightedCETrainer(Trainer):\n    \"\"\"\n    Кастомный Trainer с автоматическим вычислением весов классов.\n    weight_i = N / (K * n_i), где:\n      N — число обучающих примеров,\n      K — число классов,\n      n_i — число примеров класса i.\n    Отсутствующие классы получают вес 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        train_data_df=None,\n        target_column_name=None,\n        num_labels=None,\n        **kwargs\n    ):\n        # тихо переводим устаревший аргумент tokenizer в processing_class,\n        # чтобы убрать FutureWarning и ничего не менять в вызывающем коде\n        processing = kwargs.pop(\"tokenizer\", None)\n        if processing is not None and \"processing_class\" not in kwargs:\n            kwargs[\"processing_class\"] = processing\n\n        super().__init__(*args, **kwargs)\n\n        self.num_labels = num_labels or getattr(self.model.config, \"num_labels\", None)\n        self.class_weights = None\n\n        if train_data_df is not None and target_column_name is not None and self.num_labels is not None:\n            labels = np.asarray(train_data_df[target_column_name].values)\n\n            # на всякий: если метки не целочисленные — факторизуем\n            if not np.issubdtype(labels.dtype, np.integer):\n                _, labels = np.unique(labels, return_inverse=True)\n\n            labels = labels.astype(int)\n            counts = np.bincount(labels, minlength=self.num_labels)\n            n_samples = counts.sum()\n\n            weights = np.zeros(self.num_labels, dtype=np.float32)\n            nonzero = counts > 0\n            weights[nonzero] = n_samples / (self.num_labels * counts[nonzero].astype(np.float32))\n\n            self.class_weights = torch.tensor(weights, dtype=torch.float32)\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss_fct = torch.nn.CrossEntropyLoss(weight=weight)\n        loss = loss_fct(logits, labels.long())\n\n        return (loss, outputs) if return_outputs else loss\n\nclass ImageClassification:\n    \"\"\"\n    Pipeline, благодаря которому можно быстро и удобно загружать энкодер изображений из transformers и дообучать его для\n    классификации картинок. Проблема дисбаланса классов решается с помощью взвешенной функции потерь (автоматически),\n    а также дополнительно оценивается метрикой F1-weighted. Добавлена возможность аугментации данных \"на лету\"\n    с помощью albumentations и numpy.\n\n    Необходимые импорты:\n    import pandas as pd\n    import numpy as np\n    import torch\n    import albumentations as A\n    from PIL import Image\n    from transformers import AutoImageProcessor, AutoModelForImageClassification, Trainer, TrainingArguments\n    import evaluate\n    from datasets import Dataset, ClassLabel, Image as DSImage\n    from tqdm.auto import tqdm\n    from sklearn.model_selection import train_test_split\n    \"\"\"\n\n    def __init__(self, checkpoint, num_labels, target_column_name,\n                 image_path_column_name, use_augmentation=False, target_size=(224, 224)):\n        \"\"\"\n        :param checkpoint: Название предобученной модели из Hugging Face Hub.\n        :param num_labels: Количество классов в задаче классификации.\n        :param target_column_name: Имя столбца с целевой переменной.\n        :param image_path_column_name: Имя столбца с путями к файлам изображений.\n        :param use_augmentation: Использовать ли аугментацию данных при обучении. По умолчанию False.\n        :param target_size: Размер, к которому будут приводится все картинки. По умолчанию (224, 224).\n        \"\"\"\n        # стараемся использовать быстрый image processor, если доступен\n        try:\n            self.image_processor = AutoImageProcessor.from_pretrained(checkpoint, use_fast=True)\n        except TypeError:\n            self.image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n\n        self.model = AutoModelForImageClassification.from_pretrained(\n            checkpoint,\n            num_labels=num_labels,\n            ignore_mismatched_sizes=True\n        )\n        self.target_column_name = target_column_name\n        self.image_path_column_name = image_path_column_name\n        self.num_labels = num_labels\n        self.use_augmentation = use_augmentation\n        self.target_size = target_size\n\n        self.augment_transform = A.Compose([\n            A.HorizontalFlip(p=0.5),\n            A.ShiftScaleRotate(p=0.3, shift_limit=0.1, scale_limit=0.2, rotate_limit=20),\n            A.RandomBrightnessContrast(p=0.3),\n            A.CoarseDropout(p=0.3, max_holes=8, max_height=16, max_width=16),\n            A.ToGray(p=0.2)\n        ])\n\n        self.trainer = None\n        self.compute_metrics = None\n\n    def _apply_augmentations(self, image):\n        image_np = np.array(image)\n        augmented_image_np = self.augment_transform(image=image_np)['image']\n        return Image.fromarray(augmented_image_np)\n\n    def _prepare_dataset(self, df):\n        df_copy = df.copy()\n        dataset = Dataset.from_pandas(\n            df_copy[[self.target_column_name, self.image_path_column_name]],\n            preserve_index=False\n        )\n        dataset = dataset.rename_columns({\n            self.image_path_column_name: \"image\",\n            self.target_column_name: \"labels\"\n        })\n        dataset = dataset.cast_column(\"image\", DSImage(decode=True))\n        return dataset\n\n    def _make_transform(self, augment=False):\n        def transform(batch):\n            images = [img.convert(\"RGB\").resize(self.target_size, Image.Resampling.BICUBIC)\n                      for img in batch[\"image\"]]\n            if augment:\n                images = [self._apply_augmentations(img) for img in images]\n            inputs = self.image_processor(images, return_tensors=\"pt\")\n            # возвращаем список тензоров по одному на пример, чтобы collate_fn корректно склеил\n            batch[\"pixel_values\"] = [x for x in inputs[\"pixel_values\"]]\n            return batch\n        return transform\n\n    def _setup_compute_metrics(self, metric_name):\n        if metric_name == 'f1':\n            metric = evaluate.load('f1')\n            def compute_f1(pred):\n                if isinstance(pred, (tuple, list)):\n                    logits, labels = pred\n                else:\n                    logits, labels = pred.predictions, pred.label_ids\n                predictions = np.argmax(logits, axis=-1)\n                return metric.compute(predictions=predictions, references=labels, average='weighted')\n            self.compute_metrics = compute_f1\n\n        elif metric_name == 'accuracy':\n            metric = evaluate.load('accuracy')\n            def compute_accuracy(pred):\n                if isinstance(pred, (tuple, list)):\n                    logits, labels = pred\n                else:\n                    logits, labels = pred.predictions, pred.label_ids\n                predictions = np.argmax(logits, axis=-1)\n                return metric.compute(predictions=predictions, references=labels)\n            self.compute_metrics = compute_accuracy\n        else:\n            raise ValueError('Параметр \"metric_name\" может быть только \"f1\" или \"accuracy\".')\n\n    def fit(self, train_data, epochs=3, test_size=0.2, per_device_train_batch_size=16,\n            gradient_accumulation_steps=1, learning_rate=2e-5, metric_name='f1', fp16=True,\n            logging_steps=50, eval_steps=100, output_dir='./result'):\n\n        # стратифицированный сплит по исходным меткам\n        train_df, test_df = train_test_split(\n            train_data,\n            test_size=test_size,\n            stratify=train_data[self.target_column_name],\n            random_state=42\n        )\n\n        # маппинг меток -> id (на случай строковых/не 0..K-1 меток)\n        classes = sorted(train_df[self.target_column_name].unique().tolist())\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n\n        label2id = {c: i for i, c in enumerate(classes)}\n        id2label = {i: str(c) for c, i in label2id.items()}\n\n        # применяем маппинг к train/test\n        for df_ in (train_df, test_df):\n            df_[self.target_column_name] = df_[self.target_column_name].map(label2id).astype(int)\n\n        # сохраняем маппинги в конфиг модели\n        self.model.config.label2id = {str(k): int(v) for k, v in label2id.items()}\n        self.model.config.id2label = {int(k): str(v) for k, v in id2label.items()}\n\n        train_dataset = self._prepare_dataset(train_df)\n        eval_dataset = self._prepare_dataset(test_df)\n\n        # приводим labels к ClassLabel с явными именами классов\n        label_casting = ClassLabel(num_classes=self.num_labels, names=[id2label[i] for i in range(self.num_labels)])\n        train_dataset = train_dataset.cast_column(\"labels\", label_casting)\n        eval_dataset = eval_dataset.cast_column(\"labels\", label_casting)\n\n        train_dataset = train_dataset.with_transform(self._make_transform(augment=self.use_augmentation))\n        eval_dataset = eval_dataset.with_transform(self._make_transform(augment=False))\n\n        def collate_fn(batch):\n            return {\n                'pixel_values': torch.stack([ex['pixel_values'] for ex in batch]),\n                'labels': torch.tensor([int(ex['labels']) for ex in batch], dtype=torch.long)\n            }\n\n        self._setup_compute_metrics(metric_name=metric_name)\n\n        def _safe_resume_ckpt(out_dir, expected_num_labels):\n            if not os.path.isdir(out_dir):\n                return None\n            ckpt = get_last_checkpoint(out_dir)\n            if ckpt is None:\n                return None\n            cfg_path = os.path.join(ckpt, \"config.json\")\n            try:\n                with open(cfg_path, \"r\") as f:\n                    cfg = json.load(f)\n                ckpt_labels = int(cfg.get(\"num_labels\", -1))\n            except Exception:\n                ckpt_labels = -1\n            if ckpt_labels == expected_num_labels:\n                print(f\"Resuming from checkpoint: {ckpt}\")\n                return ckpt\n            print(f\"Ignoring checkpoint {ckpt}: num_labels mismatch ({ckpt_labels} vs {expected_num_labels})\")\n            return None\n\n        training_arguments = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type='cosine',\n            weight_decay=0.01,\n            eval_strategy='steps',\n            save_strategy='steps',\n            load_best_model_at_end=True,\n            metric_for_best_model=metric_name,\n            greater_is_better=True,\n            save_total_limit=1,\n            logging_strategy='steps',\n            logging_steps=logging_steps,\n            eval_steps=eval_steps,\n            save_steps=eval_steps,\n            report_to='none',\n            fp16=fp16 and torch.cuda.is_available(),\n            bf16=False,\n            remove_unused_columns=False,\n            dataloader_num_workers=1,\n            dataloader_pin_memory=True\n        )\n\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=training_arguments,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            processing_class=self.image_processor,  # вместо tokenizer=...\n            compute_metrics=self.compute_metrics,\n            data_collator=collate_fn,\n            train_data_df=train_df,                       # для вычисления весов классов\n            target_column_name=self.target_column_name,\n            num_labels=self.num_labels\n        )\n\n        last_ckpt = _safe_resume_ckpt(output_dir, self.num_labels)\n        self.trainer.train(resume_from_checkpoint=last_ckpt)\n\n        return self\n\n    def predict(self, df):\n        if self.trainer is None:\n            raise RuntimeError(\"Модель еще не обучена. Вызовите .fit() перед предсказанием.\")\n\n        df_copy = df.copy()\n        df_copy[self.target_column_name] = 0  # фиктивная метка для совместимости с коллатором\n        predict_dataset = self._prepare_dataset(df_copy)\n        predict_dataset = predict_dataset.with_transform(self._make_transform(augment=False))\n\n        predictions = self.trainer.predict(predict_dataset)\n        return np.argmax(predictions.predictions, axis=-1)\n\n    def get_embeddings(self, df, batch_size=32):\n        \"\"\"\n        Извлекает эмбеддинги для каждого изображения.\n        Этот метод идеально подходит для создания признаков для других моделей (CatBoost, LightGBM).\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель должна быть обучена. Вызовите .fit() перед извлечением эмбеддингов.\")\n\n        device = self.trainer.model.device\n        self.trainer.model.eval()\n\n        paths = df[self.image_path_column_name].tolist()\n        all_embeddings = []\n\n        for i in tqdm(range(0, len(paths), batch_size), desc=\"Извлечение эмбеддингов\"):\n            batch_paths = paths[i:i + batch_size]\n            batch_images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n\n            inputs = self.image_processor(batch_images, return_tensors=\"pt\").to(device)\n\n            with torch.no_grad():\n                base_model_prefix = self.trainer.model.base_model_prefix\n                base_model = getattr(self.trainer.model, base_model_prefix)\n                outputs = base_model(**inputs)\n\n            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n            all_embeddings.append(cls_embeddings.cpu().numpy())\n\n        return np.vstack(all_embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\n\nif not os.path.exists('dummy_images'):\n    os.makedirs('dummy_images')\n\nimage_paths = []\ncolors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]\nfor i in range(20):\n    path = f'dummy_images/img_{i}.png'\n    color = colors[i % 4] \n    Image.new('RGB', (224, 224), color=color).save(path)\n    image_paths.append(path)\n\ndata = {\n    'image_path': image_paths,\n    'label': [random.randint(0, 3) for i in range(20)]\n}\nfull_df = pd.DataFrame(data)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline = ImageClassification(\n    checkpoint='facebook/deit-tiny-patch16-224' ,\n    num_labels=4,\n    target_column_name='label',\n    image_path_column_name='image_path',\n    use_augmentation=True\n)\n\npipeline.fit(\n    train_data=full_df, \n    epochs=2,\n    per_device_train_batch_size=4,\n    fp16=torch.cuda.is_available(),\n    logging_steps=2,\n    eval_steps=4\n)\n\ntest_df = full_df.head(5).copy()\npredicted_labels = pipeline.predict(test_df)\n\nprint(f\"Реальные метки: {test_df['label'].tolist()}\")\nprint(f\"Предсказанные метки: {predicted_labels.tolist()}\")\n\nembeddings = pipeline.get_embeddings(test_df, batch_size=5)\n\nprint(f\"Форма массива эмбеддингов: {embeddings.shape}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение регрессора картинок.","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\nimport pandas as pd\nimport numpy as np\nimport torch\nimport albumentations as A\nfrom PIL import Image\nfrom transformers import (\n    AutoImageProcessor,\n    AutoModelForImageClassification,\n    Trainer,\n    TrainingArguments\n)\nimport evaluate\nfrom datasets import Dataset, Image as DSImage, Value, Sequence\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom transformers.trainer_utils import get_last_checkpoint\nimport os\n\nclass ImageRegression:\n    \"\"\"\n    Класс-пайплайн для дообучения энкодеров изображений из Hugging Face Transformers\n    на задачу регрессии. Поддерживает on-the-fly аугментации через Albumentations,\n    автоматическую подготовку датасета и вычисление метрик (RMSE или MAE),\n    а также извлечение эмбеддингов из базовой модели.\n\n    Необходимые импорты:\n    import pandas as pd\n    import numpy as np\n    import torch\n    import albumentations as A\n    from PIL import Image\n    from transformers import (\n        AutoImageProcessor,\n        AutoModelForImageClassification,\n        Trainer,\n        TrainingArguments\n    )\n    import evaluate\n    from datasets import Dataset, Image as DSImage, Value, Sequence\n    from tqdm.auto import tqdm\n    from sklearn.model_selection import train_test_split\n    from transformers.trainer_utils import get_last_checkpoint\n    import os\n    \"\"\"\n\n    def __init__(\n        self,\n        checkpoint,\n        target_columns_names,\n        image_path_column_name,\n        use_augmentation=False,\n        target_size=(224, 224),\n    ):\n        \"\"\"\n        Инициализация пайплайна.\n\n        :param checkpoint: Имя предобученной модели из Hugging Face Hub.\n        :param target_columns_names: Имя столбца (str) или список имен столбцов (list[str])\n                                     с целевыми переменными в DataFrame.\n        :param image_path_column_name: Имя столбца с путями к изображениям в DataFrame.\n        :param use_augmentation: Использовать ли аугментацию при обучении.\n        :param target_size: Размер изображения (width, height), к которому приводятся все картинки.\n        \"\"\"\n        if isinstance(target_columns_names, str):\n            target_columns_names = [target_columns_names]\n        self.target_columns_names = list(target_columns_names)\n        self.image_path_column_name = image_path_column_name\n        self.use_augmentation = use_augmentation\n        self.target_size = target_size\n\n        # Вычисляем число регрессионных таргетов\n        self.num_labels = len(self.target_columns_names)\n\n        self.image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n\n        self.model = AutoModelForImageClassification.from_pretrained(\n            checkpoint,\n            num_labels=self.num_labels,\n            ignore_mismatched_sizes=True,\n        )\n        self.model.config.problem_type = \"regression\"\n\n        self.augment_transform = A.Compose(\n            [\n                A.HorizontalFlip(p=0.5),\n                A.ShiftScaleRotate(\n                    p=0.3,\n                    shift_limit=0.1,\n                    scale_limit=0.2,\n                    rotate_limit=20,\n                ),\n                A.RandomBrightnessContrast(p=0.3),\n                A.CoarseDropout(\n                    p=0.3,\n                    max_holes=8,\n                    max_height=16,\n                    max_width=16,\n                ),\n                A.ToGray(p=0.2),\n            ]\n        )\n\n        self.trainer = None\n        self.compute_metrics = None\n\n    def _apply_augmentations(self, image):\n        \"\"\"\n        Применение аугментаций к одному изображению.\n\n        :param image: PIL.Image.\n        :return: Аугментированное PIL.Image.\n        \"\"\"\n        image_np = np.array(image)\n        augmented_image_np = self.augment_transform(image=image_np)[\"image\"]\n        return Image.fromarray(augmented_image_np)\n\n    def _prepare_dataset(self, df):\n        \"\"\"\n        Подготовка датасета Hugging Face Datasets из DataFrame.\n\n        :param df: pandas.DataFrame с колонками таргетов и путей к изображениям.\n        :return: datasets.Dataset с колонками image и labels (Sequence[float32] длины num_labels).\n        \"\"\"\n        df_copy = df.copy()\n\n        # Собираем labels как список float32 (мультитаргет поддерживается из коробки)\n        labels = df_copy[self.target_columns_names].astype(np.float32).values.tolist()\n        base_df = pd.DataFrame(\n            {\n                \"image\": df_copy[self.image_path_column_name].values,\n                \"labels\": labels,\n            }\n        )\n\n        dataset = Dataset.from_pandas(base_df, preserve_index=False)\n        dataset = dataset.cast_column(\"image\", DSImage(decode=True))\n\n        # Приводим метки к Sequence(float32) фиксированной длины = num_labels\n        dataset = dataset.cast_column(\n            \"labels\",\n            Sequence(feature=Value(\"float32\"), length=self.num_labels),\n        )\n\n        return dataset\n\n    def _make_transform(self, augment=False):\n        \"\"\"\n        Создание функции трансформации для with_transform.\n\n        :param augment: Применять ли аугментации.\n        :return: Функция-трансформ для батча.\n        \"\"\"\n        def transform(batch):\n            images = [\n                img.convert(\"RGB\").resize(\n                    self.target_size,\n                    Image.Resampling.BICUBIC,\n                )\n                for img in batch[\"image\"]\n            ]\n\n            if augment:\n                images = [self._apply_augmentations(img) for img in images]\n\n            inputs = self.image_processor(\n                images=images,\n                return_tensors=\"pt\",\n            )\n\n            batch[\"pixel_values\"] = [x for x in inputs[\"pixel_values\"]]\n            return batch\n\n        return transform\n\n    def _setup_compute_metrics(self, metric_name):\n        \"\"\"\n        Настройка функции вычисления метрик.\n\n        :param metric_name: 'rmse' или 'mae'.\n        \"\"\"\n        def _extract(eval_pred):\n            if isinstance(eval_pred, (tuple, list)):\n                logits, labels = eval_pred\n            else:\n                logits = eval_pred.predictions\n                labels = eval_pred.label_ids\n            preds = np.array(logits)\n            labels = np.array(labels)\n            return preds, labels\n\n        if metric_name == \"rmse\":\n            def compute(eval_pred):\n                preds, labels = _extract(eval_pred)\n                rmse = float(np.sqrt(np.mean((preds - labels) ** 2)))\n                return {\"rmse\": rmse}\n            self.compute_metrics = compute\n\n        elif metric_name == \"mae\":\n            def compute(eval_pred):\n                preds, labels = _extract(eval_pred)\n                mae = float(np.mean(np.abs(preds - labels)))\n                return {\"mae\": mae}\n            self.compute_metrics = compute\n\n        else:\n            raise ValueError(\n                'Параметр \"metric_name\" может быть только \"rmse\" или \"mae\".'\n            )\n\n    def fit(\n        self,\n        train_data,\n        epochs=3,\n        test_size=0.2,\n        per_device_train_batch_size=16,\n        gradient_accumulation_steps=1,\n        learning_rate=2e-5,\n        metric_name=\"rmse\",\n        fp16=True,\n        logging_steps=50,\n        eval_steps=100,\n        output_dir='./result'\n    ):\n        \"\"\"\n        Обучение модели.\n\n        :param train_data: pandas.DataFrame с колонками таргетов и путей к изображениям.\n        :param epochs: Количество эпох.\n        :param test_size: Размер валидационной выборки.\n        :param per_device_train_batch_size: Размер батча на устройство для обучения.\n        :param gradient_accumulation_steps: Шаги аккумуляции градиента.\n        :param learning_rate: Скорость обучения.\n        :param metric_name: Метрика для отслеживания лучшей модели: 'rmse' или 'mae'.\n        :param fp16: Включить FP16 при наличии CUDA.\n        :param logging_steps: Раз во сколько шагов записывать результаты модели.\n        :param eval_steps: Раз во сколько шагов проверять модель на тестовой выборке.\n        :return: self.\n        \"\"\"\n        train_df, test_df = train_test_split(\n            train_data,\n            test_size=test_size,\n            random_state=42,\n            shuffle=True,\n        )\n\n        train_dataset = self._prepare_dataset(train_df)\n        eval_dataset = self._prepare_dataset(test_df)\n\n        train_dataset = train_dataset.with_transform(\n            self._make_transform(augment=self.use_augmentation)\n        )\n        eval_dataset = eval_dataset.with_transform(\n            self._make_transform(augment=False)\n        )\n\n        def collate_fn(batch):\n            return {\n                \"pixel_values\": torch.stack(\n                    [ex[\"pixel_values\"] for ex in batch]\n                ),\n                \"labels\": torch.tensor(\n                    [ex[\"labels\"] for ex in batch],\n                    dtype=torch.float32,\n                ),\n            }\n\n        self._setup_compute_metrics(metric_name=metric_name)\n\n        greater_is_better = False if metric_name in {\"rmse\", \"mae\"} else True\n\n        training_arguments = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            save_strategy=\"steps\",\n            load_best_model_at_end=True,\n            metric_for_best_model=metric_name,\n            greater_is_better=greater_is_better,\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            eval_steps=eval_steps,\n            save_steps=eval_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            bf16=False,\n            remove_unused_columns=False,\n            dataloader_num_workers=2,\n            dataloader_pin_memory=True,\n        )\n\n        self.trainer = Trainer(\n            model=self.model,\n            args=training_arguments,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            tokenizer=self.image_processor,\n            compute_metrics=self.compute_metrics,\n            data_collator=collate_fn,\n        )\n\n        last_ckpt = (\n            get_last_checkpoint(output_dir) if os.path.isdir(output_dir) else None\n        )\n\n        self.trainer.train(resume_from_checkpoint=last_ckpt)\n        return self\n\n    def predict(self, df):\n        \"\"\"\n        Предсказание численных значений для набора изображений.\n\n        :param df: pandas.DataFrame с колонкой путей к изображениям.\n        :return: numpy.array с предсказаниями формы (n_samples, num_labels);\n                 если num_labels == 1, вернется вектор формы (n_samples,).\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\n                \"Модель еще не обучена. Вызовите .fit() перед предсказанием.\"\n            )\n\n        df_copy = df.copy()\n        # Создаем фиктивные таргеты для унифицированной подготовки датасета\n        for col in self.target_columns_names:\n            df_copy[col] = 0.0\n\n        predict_dataset = self._prepare_dataset(df_copy)\n        predict_dataset = predict_dataset.with_transform(\n            self._make_transform(augment=False)\n        )\n\n        predictions = self.trainer.predict(predict_dataset)\n        preds = np.array(predictions.predictions)\n\n        # Для единичного таргета вернем 1D-вектор\n        if preds.ndim == 2 and preds.shape[1] == 1:\n            preds = preds.squeeze(1)\n\n        return preds\n\n    def get_embeddings(self, df, batch_size=32):\n        \"\"\"\n        Извлечение эмбеддингов базовой модели для каждого изображения.\n\n        :param df: pandas.DataFrame с колонкой путей к изображениям.\n        :param batch_size: Размер батча.\n        :return: numpy.array формы (n_samples, hidden_size) с эмбеддингами.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\n                \"Модель должна быть обучена. Вызовите .fit() перед извлечением эмбеддингов.\"\n            )\n\n        device = self.trainer.model.device\n        self.trainer.model.eval()\n\n        paths = df[self.image_path_column_name].tolist()\n        all_embeddings = []\n\n        for i in tqdm(\n            range(0, len(paths), batch_size),\n            desc=\"Извлечение эмбеддингов\",\n        ):\n            batch_paths = paths[i: i + batch_size]\n\n            batch_images = [\n                Image.open(p)\n                .convert(\"RGB\")\n                .resize(self.target_size, Image.Resampling.BICUBIC)\n                for p in batch_paths\n            ]\n\n            inputs = self.image_processor(\n                batch_images,\n                return_tensors=\"pt\",\n            ).to(device)\n\n            with torch.no_grad():\n                base_model_prefix = self.trainer.model.base_model_prefix\n                base_model = getattr(self.trainer.model, base_model_prefix)\n\n                outputs = base_model(\n                    **inputs,\n                    return_dict=True,\n                )\n\n                if (\n                    hasattr(outputs, \"pooler_output\")\n                    and outputs.pooler_output is not None\n                ):\n                    emb = outputs.pooler_output\n\n                elif hasattr(outputs, \"last_hidden_state\"):\n                    hs = outputs.last_hidden_state\n\n                    if hs.ndim == 3:\n                        emb = hs[:, 0, :]\n\n                    elif hs.ndim == 4:\n                        emb = hs.mean(dim=(2, 3))\n\n                    else:\n                        raise ValueError(\n                            \"Неизвестная форма last_hidden_state \"\n                            \"для извлечения эмбеддингов.\"\n                        )\n                else:\n                    raise ValueError(\n                        \"Модель не вернула подходящие тензоры \"\n                        \"для эмбеддингов.\"\n                    )\n\n            all_embeddings.append(emb.cpu().numpy())\n\n        return np.vstack(all_embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"import os\n\nif not os.path.exists('dummy_images'):\n    os.makedirs('dummy_images')\n\nimage_paths = []\ncolors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]\nfor i in range(20):\n    path = f'dummy_images/img_{i}.png'\n    color = colors[i % 4] \n    Image.new('RGB', (224, 224), color=color).save(path)\n    image_paths.append(path)\n\ndata = {\n    'image_path': image_paths,\n    'target_1': [i for i in range(20)],\n    'target_2': [i**2 for i in range(20)]\n}\nfull_df = pd.DataFrame(data)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline = ImageRegression(\n    checkpoint='facebook/deit-tiny-patch16-224',\n    target_columns_names=['target_1', 'target_2'],  # можно передать и строку, и список — класс поддерживает оба варианта\n    image_path_column_name='image_path',\n    use_augmentation=True\n)\n\npipeline.fit(\n    train_data=full_df,\n    epochs=2,\n    per_device_train_batch_size=4,\n    fp16=False,\n    logging_steps=2,\n    eval_steps=4\n)\n\ntest_df = full_df.head(5).copy()\npreds = pipeline.predict(test_df)\n\nprint(preds)\n\nembeddings = pipeline.get_embeddings(test_df, batch_size=5)\nprint(f\"Форма массива эмбеддингов: {embeddings.shape}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"code","source":"!pip install wav2clip torchaudio transformers evaluate pillow\n\nimport math\nfrom typing import List, Dict, Any, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport evaluate\nfrom transformers import Trainer, TrainingArguments\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Устанавливает фиксированное зерно для Python, NumPy и PyTorch.\n\n    :param seed: Значение зерна (по умолчанию 42).\n    \"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef to_pil(x: Union[str, np.ndarray, Image.Image]) -> Image.Image:\n    \"\"\"\n    Приводит вход к PIL.Image в формате RGB.\n\n    :param x: Путь к изображению, массив NumPy (H,W[,C]) либо PIL.Image.\n    :return: PIL.Image в RGB.\n    :raises ValueError: Если тип входа не поддерживается.\n    \"\"\"\n    if isinstance(x, Image.Image):\n        return x.convert(\"RGB\")\n    if isinstance(x, str):\n        return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray):\n        return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    \"\"\"\n    Загружает аудиофайл и при необходимости ресемплирует до target_sr.\n\n    :param path: Путь к аудиофайлу (wav/flac и т.п.).\n    :param target_sr: Целевая частота дискретизации.\n    :return: Одноканальный сигнал формы [T] в float32.\n    :raises RuntimeError: Если не установлен torchaudio.\n    \"\"\"\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)  # [C, T]\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr:\n        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\nclass MultiComboDataset(Dataset):\n    \"\"\"\n    Датасет для различных комбинаций модальностей (text/image/audio).\n    Теперь поддерживает несколько изображений и аудио в одном сэмпле (через списки или несколько колонок).\n\n    :param df: Исходный датафрейм.\n    :param target_col: Имя колонки с таргетом.\n    :param label2id: Маппинг \"значение метки -> id\".\n    :param text_columns: Список текстовых колонок (склеиваются). Если None/[], модальность 'text' не используется.\n    :param image_columns: Список колонок с изображениями; каждая ячейка может быть одиночным значением или списком.\n    :param audio_columns: Список колонок с аудио; каждая ячейка может быть одиночным значением или списком.\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        label2id: Dict[Any, int],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None\n    ):\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.label2id = label2id\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.sep = \" [SEP] \"\n\n    def __len__(self) -> int:\n        \"\"\"\n        :return: Количество элементов в датасете.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        \"\"\"\n        Возвращает элемент датасета.\n\n        :param idx: Индекс строки.\n        :return: Словарь с ключами:\n                 'labels' (int), а также при наличии модальностей 'text', 'images', 'audios'.\n        \"\"\"\n        row = self.df.iloc[idx]\n        item = {\"labels\": int(self.label2id[row[self.target_col]]) if self.target_col in row else 0}\n        if self.text_columns:\n            item[\"text\"] = self.sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n        def _as_list(v):\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                return []\n            if isinstance(v, (list, tuple)):\n                return list(v)\n            return [v]\n\n        if self.image_columns:\n            imgs = []\n            for c in self.image_columns:\n                if c in row:\n                    imgs.extend(_as_list(row[c]))\n            item[\"images\"] = imgs\n        if self.audio_columns:\n            auds = []\n            for c in self.audio_columns:\n                if c in row:\n                    auds.extend(_as_list(row[c]))\n            item[\"audios\"] = auds\n        return item\n\n\nclass BaseBackend(nn.Module):\n    \"\"\"\n    Базовый класс бэкенда для единой мультимодальной модели.\n\n    Атрибуты:\n      - name: Название бэкенда.\n      - supported: Набор поддерживаемых модальностей, например {'text','image'}.\n      - embed_dim: Базовая размерность эмбеддингов модальностей.\n      - out_dim_per_modality: Реальные выходные размерности по модальностям (с учётом агрегации/concat).\n    \"\"\"\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Собирает батч из списка элементов MultiComboDataset для текущего бэкенда.\n\n        :param batch: Список элементов датасета.\n        :return: Содержит 'labels' (torch.LongTensor) и 'backend_inputs' (dict тензоров/сырья).\n        \"\"\"\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует модальности и возвращает эмбеддинги по доступным модальностям.\n\n        :param backend_inputs: Предобработанные входы (из collate).\n        :param device: Девайс для инференса.\n        :return: Словарь {'text':[B,*], 'image':[B,*], 'audio':[B,*]} по имеющимся модальностям.\n        \"\"\"\n        raise NotImplementedError\n\n    def freeze_all(self):\n        \"\"\"\n        Замораживает все параметры бэкенда (requires_grad=False).\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        \"\"\"\n        Возвращает реальную размерность выходного эмбеддинга по модальности.\n\n        :param modality: Имя модальности ('text','image','audio').\n        :return: Размерность выходного вектора для заданной модальности.\n        \"\"\"\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n\nclass ClipBackend(BaseBackend):\n    \"\"\"\n    Бэкенд на основе CLIP (HF) для пары модальностей: текст + изображение.\n    Поддерживает несколько изображений на сэмпл через агрегацию (concat или mean).\n\n    :param checkpoint: Имя/путь чекпоинта CLIP (HF Hub). По умолчанию 'openai/clip-vit-base-patch32'.\n    :param max_length: Максимальная длина текстовых токенов.\n    :param freeze: Замораживать ли веса CLIP (linear probing).\n    :param max_images: Максимальное число изображений на сэмпл (для concat/mean агрегации).\n    :param image_agg: Тип агрегации изображений ('concat' или 'mean').\n    \"\"\"\n    name = \"clip\"\n    supported = {\"text\", \"image\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        max_images: int = 1,\n        image_agg: str = \"concat\"\n    ):\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.max_images = int(max_images)\n        self.image_agg = image_agg\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Подготавливает батч для CLIP. Робастен к отсутствующим ключам 'text'/'images'.\n\n        :param batch: Элементы с ключами 'text', 'images', 'labels'.\n        :return: 'labels' и 'backend_inputs' с полями:\n                 'text_inputs' (dict тензоров для текста),\n                 'image_inputs' (dict с pixel_values или None),\n                 'image_counts' (LongTensor [B]).\n        \"\"\"\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        # Списки картинок per-сэмпл -> плоский список + счётчики\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n\n        text_inputs = self.processor(\n            text=texts, padding=True, truncation=True,\n            max_length=self.max_length, return_tensors=\"pt\"\n        )\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенирует до max_k эмбеддингов на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги изображений [M, D], где M = сумма counts.\n        :param counts: Список количеств изображений на сэмпл (длина B).\n        :param max_k: Максимальное число изображений на сэмпл.\n        :return: Нормализованный тензор [B, D*max_k].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усредняет эмбеддинги изображений на сэмпл.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества изображений на сэмпл (длина B).\n        :return: Нормализованный тензор [B, D].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Вычисляет эмбеддинги текста и изображений (с агрегацией изображений).\n\n        :param backend_inputs: Тензоры CLIPProcessor для текста и картинок.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'image':[B,D*max_images] или [B,D]} — L2-нормированные эмбеддинги.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                img_z = self._concat_padded(img_flat, counts, self.max_images)\n            else:\n                img_z = self._mean_pool(img_flat, counts)\n        else:\n            if self.image_agg == \"concat\":\n                img_z = torch.zeros((len(counts), self.embed_dim * self.max_images), device=device)\n            else:\n                img_z = torch.zeros((len(counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"image\": img_z}\n\n\nclass ClapBackend(BaseBackend):\n    \"\"\"\n    Бэкенд на основе CLAP (HF) для пары модальностей: текст + аудио.\n    Поддерживает несколько аудио на сэмпл через агрегацию (concat или mean).\n\n    :param checkpoint: Имя/путь чекпоинта CLAP (HF Hub). По умолчанию 'laion/clap-htsat-unfused'.\n    :param freeze: Замораживать ли веса CLAP.\n    :param max_audios: Максимальное число аудио на сэмпл (для concat/mean агрегации).\n    :param audio_agg: Тип агрегации аудио ('concat' или 'mean').\n    \"\"\"\n    name = \"clap\"\n    supported = {\"text\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"laion/clap-htsat-unfused\",\n        freeze: bool = True,\n        max_audios: int = 1,\n        audio_agg: str = \"concat\"\n    ):\n        super().__init__()\n        from transformers import ClapModel, ClapProcessor\n        self.model = ClapModel.from_pretrained(checkpoint)\n        self.processor = ClapProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(getattr(self.model.config, \"projection_dim\", 512))\n        sr = getattr(self.processor, \"sampling_rate\", None)\n        if sr is None:\n            fe = getattr(self.processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n        self.sr = int(sr)\n        self.max_audios = int(max_audios)\n        self.audio_agg = audio_agg\n        if freeze:\n            self.freeze_all()\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"audio\": aud_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Подготавливает батч для CLAP (робастно к пропускам, поддерживает списки аудио per-сэмпл).\n\n        :param batch: элементы с 'text','audios','labels'\n        :return: 'labels' и 'backend_inputs' с полями:\n                 'text_inputs' (dict для текста),\n                 'audio_inputs' (dict с input_features или None),\n                 'audio_counts' (LongTensor [B]).\n        \"\"\"\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"CLAP ожидает путь к аудио или numpy.ndarray\")\n\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_audios):\n            aud_proc = self.processor(audios=flat_audios, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n            audio_inputs = {\"input_features\": aud_proc[\"input_features\"]}\n        else:\n            audio_inputs = {\"input_features\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенирует до max_k эмбеддингов аудио на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги аудио [M, D], где M = сумма counts.\n        :param counts: Список количеств аудио на сэмпл (длина B).\n        :param max_k: Максимальное число аудио на сэмпл.\n        :return: Нормализованный тензор [B, D*max_k].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усредняет эмбеддинги аудио на сэмпл.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества аудио на сэмпл (длина B).\n        :return: Нормализованный тензор [B, D].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Вычисляет эмбеддинги текста и аудио (с агрегацией аудио).\n\n        :param backend_inputs: Тензоры ClapProcessor для текста и аудио.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'audio':[B,D*max_audios] или [B,D]} — L2-нормированные эмбеддинги.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"audio_counts\"].tolist()\n        af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n        if af is not None:\n            af = af.to(device)\n            aud_flat = self.model.get_audio_features(input_features=af)\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                aud_z = self._concat_padded(aud_flat, counts, self.max_audios)\n            else:\n                aud_z = self._mean_pool(aud_flat, counts)\n        else:\n            if self.audio_agg == \"concat\":\n                aud_z = torch.zeros((len(counts), self.embed_dim * self.max_audios), device=device)\n            else:\n                aud_z = torch.zeros((len(counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"audio\": aud_z}\n\n\nclass ClipWav2CLIPBackend(BaseBackend):\n    \"\"\"\n    Бэкенд: CLIP (text+image) + Wav2CLIP (audio->CLIP пространство). Поддерживает ['text','image','audio'].\n    Поддержка нескольких изображений и аудио на сэмпл через агрегацию (concat или mean).\n\n    :param checkpoint: Чекпоинт CLIP (HF), по умолчанию 'openai/clip-vit-base-patch32'.\n    :param max_length: Максимальная длина текстовых токенов.\n    :param freeze: Замораживать ли веса CLIP.\n    :param audio_sr: Частота дискретизации для аудио (для Wav2CLIP).\n    :param max_images: Максимум изображений на сэмпл (для агрегации).\n    :param max_audios: Максимум аудио на сэмпл (для агрегации).\n    :param image_agg: Агрегация изображений ('concat' или 'mean').\n    :param audio_agg: Агрегация аудио ('concat' или 'mean').\n    \"\"\"\n    name = \"clip_wav2clip\"\n    supported = {\"text\", \"image\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        audio_sr: int = 16000,\n        max_images: int = 1,\n        max_audios: int = 1,\n        image_agg: str = \"concat\",\n        audio_agg: str = \"concat\"\n    ):\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.audio_sr = int(audio_sr)\n        self.max_images = int(max_images)\n        self.max_audios = int(max_audios)\n        self.image_agg = image_agg\n        self.audio_agg = audio_agg\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out, \"audio\": aud_out}\n        # wav2clip lazy\n        self._w2c_model = None\n        self._w2c_mod = None\n        self._w2c_api = None\n        self._w2c_device = None\n\n    def _ensure_w2c(self, device: torch.device):\n        \"\"\"\n        Подгружает wav2clip и модель с учётом разных API версий.\n\n        :param device: torch.device для инференса.\n        :raises RuntimeError: Если пакет wav2clip не найден или не удалось загрузить модель.\n        \"\"\"\n        if self._w2c_model is not None and self._w2c_device == str(device):\n            return\n        import importlib\n        try:\n            w2c = importlib.import_module(\"wav2clip\")\n        except Exception as e:\n            raise RuntimeError(\"Не найден пакет 'wav2clip'. Установите: pip install wav2clip\") from e\n        dev_str = str(device) if device.type == \"cuda\" else \"cpu\"\n        if hasattr(w2c, \"load_model\"):\n            model = w2c.load_model(device=dev_str); api_kind = \"func\"\n        elif hasattr(w2c, \"get_model\"):\n            model = w2c.get_model(device=dev_str); api_kind = \"func\"\n        elif hasattr(w2c, \"Wav2CLIP\"):\n            try:\n                model = w2c.Wav2CLIP(dev_str)\n            except TypeError:\n                model = w2c.Wav2CLIP(device=dev_str)\n            api_kind = \"method\"\n        else:\n            raise RuntimeError(\"wav2clip установлен, но нет способов загрузки (load_model/get_model/Wav2CLIP)\")\n        self._w2c_mod = w2c\n        self._w2c_model = model\n        self._w2c_api = api_kind\n        self._w2c_device = str(device)\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Готовит батч: CLIPProcessor для текста/картинки + список аудио (пути/массивы).\n\n        :param batch: элементы с 'text','images','audios','labels' (text можно опустить — подставится пустая строка).\n        :return: 'labels' и 'backend_inputs' с полями для текста/изображений/аудио и счётчиками.\n        \"\"\"\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        # images (flatten + counts)\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, img_counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            img_counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        # audios (flatten + counts)\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, aud_counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            aud_counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.audio_sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"Ожидается путь к аудио или numpy.ndarray\")\n\n        if len(flat_audios):\n            Lmax = max(len(a) for a in flat_audios)\n            wav = np.zeros((len(flat_audios), Lmax), dtype=np.float32)\n            lens = np.zeros((len(flat_audios),), dtype=np.int64)\n            for i, a in enumerate(flat_audios):\n                L = len(a)\n                wav[i, :L] = a\n                lens[i] = L\n            audio_inputs = {\n                \"waveforms\": torch.from_numpy(wav),   # [M, Lmax]\n                \"lengths\": torch.from_numpy(lens)     # [M]\n            }\n        else:\n            audio_inputs = {\"waveforms\": None, \"lengths\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(img_counts, dtype=torch.long),\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(aud_counts, dtype=torch.long),\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _agg_concat(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенирует до max_k эмбеддингов на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Список количеств элементов на сэмпл (длина B).\n        :param max_k: Максимальное число элементов на сэмпл.\n        :return: Нормализованный тензор [B, D*max_k].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset+c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _agg_mean(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усредняет эмбеддинги на сэмпл.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества элементов на сэмпл (длина B).\n        :return: Нормализованный тензор [B, D].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset+c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует модальности в CLIP‑пространство (текст/изображение через CLIP, аудио через Wav2CLIP).\n\n        :param backend_inputs: Входы для бэкенда.\n        :param device: torch.device.\n        :return: {'text':[B,*], 'image':[B,*], 'audio':[B,*]} (L2-нормированные эмбеддинги).\n        \"\"\"\n        # text\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        # image\n        img_counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                image_z = self._agg_concat(img_flat, img_counts, self.max_images)\n            else:\n                image_z = self._agg_mean(img_flat, img_counts)\n        else:\n            if self.image_agg == \"concat\":\n                image_z = torch.zeros((len(img_counts), self.embed_dim * self.max_images), device=device)\n            else:\n                image_z = torch.zeros((len(img_counts), self.embed_dim), device=device)\n\n        # audio via wav2clip\n        aud_counts = backend_inputs[\"audio_counts\"].tolist()\n        wav = backend_inputs[\"audio_inputs\"][\"waveforms\"]\n        lens = backend_inputs[\"audio_inputs\"][\"lengths\"]\n        if wav is not None and lens is not None and (lens.numel() if torch.is_tensor(lens) else len(lens)) > 0:\n            self._ensure_w2c(device)\n            w2c = self._w2c_mod\n            waves = wav  # [M, Lmax] (CPU тензор — нормально)\n            lens_np = lens.cpu().numpy()\n            embs = []\n            for i in range(waves.size(0)):\n                L = int(lens_np[i])\n                a_np = waves[i, :L].detach().cpu().numpy()\n                e = None\n                if self._w2c_api == \"func\":\n                    if hasattr(w2c, \"embed_audio\"):\n                        try:\n                            e = w2c.embed_audio(a_np, self._w2c_model)\n                        except TypeError:\n                            e = w2c.embed_audio(a_np, self.audio_sr, self._w2c_model)\n                    elif hasattr(w2c, \"get_audio_embedding\"):\n                        try:\n                            e = w2c.get_audio_embedding(a_np, self._w2c_model)\n                        except TypeError:\n                            e = w2c.get_audio_embedding(a_np, self.audio_sr, self._w2c_model)\n                if e is None and self._w2c_api == \"method\" and hasattr(self._w2c_model, \"embed_audio\"):\n                    try:\n                        e = self._w2c_model.embed_audio(a_np)\n                    except TypeError:\n                        e = self._w2c_model.embed_audio(a_np, sr=self.audio_sr)\n                if e is None:\n                    raise RuntimeError(\"Не удалось получить аудио‑эмбеддинг через wav2clip.\")\n                e = np.asarray(e)\n                if e.ndim == 2:\n                    e = e.mean(axis=0)\n                elif e.ndim > 2:\n                    e = e.reshape(-1, e.shape[-1]).mean(axis=0)\n                embs.append(e.astype(np.float32))\n            aud_flat = torch.tensor(np.stack(embs, axis=0), dtype=torch.float32, device=device)\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                audio_z = self._agg_concat(aud_flat, aud_counts, self.max_audios)\n            else:\n                audio_z = self._agg_mean(aud_flat, aud_counts)\n        else:\n            if self.audio_agg == \"concat\":\n                audio_z = torch.zeros((len(aud_counts), self.embed_dim * self.max_audios), device=device)\n            else:\n                audio_z = torch.zeros((len(aud_counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"image\": image_z, \"audio\": audio_z}\n\n\nclass SingleBackboneClassifier(nn.Module):\n    \"\"\"\n    Классификатор поверх одного мультимодального бэкенда: фьюжн эмбеддингов -> MLP.\n\n    :param backend: Инициализированный бэкенд (CLIP/CLAP/ClipWav2CLIP).\n    :param modalities: Список активных модальностей; порядок учитывается при concat ('image','text','audio').\n    :param num_labels: Количество классов.\n    :param fusion: Тип фьюжна — 'concat' или 'mean'.\n    :param hidden: Размер скрытого слоя головы.\n    :param dropout: Дропаут в голове.\n    \"\"\"\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_labels: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        # Реальная входная размерность головы с учётом агрегации по модальностям\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать, а у нас: {dict(zip(order, dims))}')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        \"\"\"\n        Рекурсивно находит девайс по первому попавшемуся тензору во входах.\n        Если тензоры не найдены — отдаёт доступный cuda/cpu.\n\n        :param obj: Произвольная структура входов (тензоры/словари/списки).\n        :return: torch.device для выполнения.\n        \"\"\"\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Объединяет эмбеддинги модальностей согласно self.fusion.\n\n        :param z: Эмбеддинги по модальностям.\n        :return: Слитый эмбеддинг [B, D*|mods|] для concat или [B, D] для mean.\n        \"\"\"\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        for m in order:\n            t = z[m]\n            if t.dim() == 3:\n                t = t.mean(dim=1)\n            elif t.dim() > 3:\n                t = t.view(t.size(0), -1)\n            feats.append(t)\n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            sizes = [f.size(-1) for f in feats]\n            if len(set(sizes)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать, а у нас: {sizes}')\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        \"\"\"\n        Прямой проход: кодирование модальностей -> фьюжн -> классификационная голова.\n\n        :param backend_inputs: Входы для бэкенда (из его collate).\n        :param labels: Игнорируется (loss считает Trainer).\n        :return: SequenceClassifierOutput с logits [B, num_labels].\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        \"\"\"\n        Извлекает fused эмбеддинги (и опционально per-modality).\n\n        :param backend_inputs: Входы для бэкенда.\n        :param return_per_modality: Вернуть также словарь эмбеддингов по модальностям.\n        :return: fused [B, *] или (fused, {'text':[B,*], 'image':[B,*], 'audio':[B,*]}).\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\nclass WeightedCETrainer(Trainer):\n    \"\"\"\n    Trainer с CrossEntropyLoss и поддержкой весов классов.\n    Умеет автоматически вычислять class_weights по частотам классов, если они не переданы.\n\n    :param num_labels: Количество классов.\n    :param train_labels: Список/массив меток train (для авто-вычисления весов).\n    :param class_weights: Веса классов (np.ndarray/список длины num_labels) или None.\n    \"\"\"\n    def __init__(self, *args, num_labels=None, train_labels=None, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_labels = num_labels\n        if class_weights is not None:\n            w = torch.as_tensor(class_weights, dtype=torch.float32)\n        else:\n            w = None\n            if train_labels is not None and num_labels is not None:\n                train_labels = np.asarray(train_labels).astype(int)\n                counts = np.bincount(train_labels, minlength=num_labels)\n                n = counts.sum()\n                weights = np.zeros(num_labels, dtype=np.float32)\n                nonzero = counts > 0\n                weights[nonzero] = n / (num_labels * counts[nonzero].astype(np.float32))\n                w = torch.tensor(weights, dtype=torch.float32)\n        self.class_weights = w\n        self._warned_label_tiling = False\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        \"\"\"\n        Считает CrossEntropyLoss с опциональными весами классов.\n\n        :param model: Модель.\n        :param inputs: Батч, содержащий 'labels' (LongTensor) и аргументы для model.forward.\n        :param return_outputs: Возвращать ли также outputs.\n        :param num_items_in_batch: Совместимость с Trainer API (не используется).\n        :return: loss (и outputs, если return_outputs=True).\n        :raises ValueError: Если размеры logits и labels не совпадают (с учётом DP).\n        \"\"\"\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        # Переносим метки на тот же девайс, что и логиты\n        labels = labels.to(logits.device)\n\n        # Защита от DP-дублирования батча\n        if logits.size(0) != labels.size(0):\n            ngpu = torch.cuda.device_count()\n            if ngpu > 1 and logits.size(0) == labels.size(0) * ngpu:\n                # Дублируем метки по числу GPU\n                labels = labels.repeat_interleave(ngpu)\n                if not self._warned_label_tiling:\n                    print(f\"[Warning] DataParallel удвоил batch для logits. \"\n                          f\"Повторяем labels x{ngpu} для выравнивания. \"\n                          f\"logits: {tuple(logits.shape)}, labels: {tuple(labels.shape)}\")\n                    self._warned_label_tiling = True\n            else:\n                raise ValueError(f\"Batch size mismatch: logits {tuple(logits.shape)} vs labels {tuple(labels.shape)}\")\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = nn.CrossEntropyLoss(weight=weight)(logits, labels.long())\n        return (loss, outputs) if return_outputs else loss\n\n\nclass SingleModelMultiComboClassification:\n    \"\"\"\n    Пайплайн с одной мультимодальной моделью (бэкендом) под заданную комбинацию модальностей.\n\n    Поддерживаемые комбинации:\n      - ['text','image']         -> CLIP (HF)\n      - ['text','audio']         -> CLAP (HF)\n      - ['image','audio']        -> ClipWav2CLIP\n      - ['text','image','audio'] -> ClipWav2CLIP\n\n    Поддерживает мульти-изображения/аудио с concat-агрегацией (паддинг до max_*).\n\n    :param modalities: Активные модальности ('text','image','audio') в любом порядке.\n    :param num_labels: Количество классов.\n    :param target_column_name: Имя столбца таргета в DataFrame.\n    :param text_columns: Имена текстовых колонок (склеиваются).\n    :param image_columns: Список колонок с изображениями (ячейки — одиночные значения или списки).\n    :param audio_columns: Список колонок с аудио (ячейки — одиночные значения или списки).\n    :param backend: 'auto' | 'clip' | 'clap' | 'clip_wav2clip'. 'auto' подбирает по комбинации модальностей.\n    :param clip_checkpoint: Чекпоинт CLIP (HF). По умолчанию 'openai/clip-vit-base-patch32'.\n    :param clap_checkpoint: Чекпоинт CLAP (HF). По умолчанию 'laion/clap-htsat-unfused'.\n    :param fusion: Тип фьюжна ('concat' или 'mean').\n    :param freeze_backbone: Заморозить веса бэкенда (linear probing).\n    :param clip_max_length: Максимальная длина токенов для CLIP.\n    :param max_images_per_sample: Максимум изображений на сэмпл при агрегации.\n    :param max_audios_per_sample: Максимум аудио на сэмпл при агрегации.\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        num_labels: int,\n        target_column_name: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1\n    ):\n        self.modalities = sorted(list(set(modalities)))\n        self.num_labels = num_labels\n        self.target_column_name = target_column_name\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.backend_name = backend\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n\n        self.label2id: Optional[Dict[Any, int]] = None\n        self.id2label: Optional[Dict[int, str]] = None\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        \"\"\"\n        Инициализирует бэкенд согласно backend/auto и проверяет совместимость с выбранными модальностями.\n\n        :raises ValueError: Если комбинация модальностей не поддерживается выбранным бэкендом.\n        \"\"\"\n        mods = set(self.modalities)\n        name = self.backend_name\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                name = \"clip\"\n            elif mods == {\"text\", \"audio\"}:\n                name = \"clap\"\n            elif mods in ({\"image\", \"audio\"}, {\"text\", \"image\", \"audio\"}):\n                name = \"clip_wav2clip\"\n            else:\n                raise ValueError(f\"Неподдерживаемая комбинация: {mods}\")\n\n        if name == \"clip\":\n            self.backend = ClipBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                max_images=self.max_images_per_sample,\n                image_agg=\"concat\"\n            )\n        elif name == \"clap\":\n            self.backend = ClapBackend(\n                checkpoint=self.clap_checkpoint,\n                freeze=self.freeze_backbone,\n                max_audios=self.max_audios_per_sample,\n                audio_agg=\"concat\"\n            )\n        elif name == \"clip_wav2clip\":\n            self.backend = ClipWav2CLIPBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                audio_sr=16000,\n                max_images=self.max_images_per_sample,\n                max_audios=self.max_audios_per_sample,\n                image_agg=\"concat\",\n                audio_agg=\"concat\"\n            )\n        else:\n            raise ValueError(f\"Неизвестный backend: {name}\")\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}. Поддерживает: {self.backend.supported}\")\n\n    def _setup_metrics(self, metric_name: str):\n        \"\"\"\n        Создаёт функцию подсчёта метрик для Trainer.\n\n        :param metric_name: 'f1' или 'accuracy'.\n        :raises ValueError: Если метрика не поддерживается.\n        \"\"\"\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            metric = evaluate.load(\"f1\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids, average=\"weighted\")\n        elif metric_name == \"accuracy\":\n            metric = evaluate.load(\"accuracy\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids)\n        else:\n            raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        \"\"\"\n        Перемешивает и делит DataFrame на train/eval.\n\n        :param df: Исходный датафрейм.\n        :param test_size: Доля валидации (0..1).\n        :param seed: Зерно для перемешивания.\n        :return: (df_train, df_eval).\n        \"\"\"\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data(self, df: pd.DataFrame):\n        \"\"\"\n        Проверяет соответствие колонок выбранным модальностям.\n        Бросает ValueError с понятной подсказкой, если что-то не так.\n\n        :param df: Исходный датафрейм.\n        :raises ValueError: При отсутствии нужных колонок/настроек.\n        \"\"\"\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"f1\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        \"\"\"\n        Обучает классификационную голову поверх выбранного бэкенда.\n\n        :param train_data: Данные обучения с нужными колонками.\n        :param epochs: Количество эпох.\n        :param test_size: Доля валидации.\n        :param per_device_train_batch_size: Размер батча на устройство.\n        :param gradient_accumulation_steps: Градиентная аккумуляция.\n        :param learning_rate: Learning rate для AdamW.\n        :param metric_name: 'f1' или 'accuracy' для ранжирования чекпоинтов.\n        :param fp16: Использовать ли fp16 при наличии CUDA.\n        :param logging_steps: Шаг логирования.\n        :param eval_steps: Шаг валидации/сохранения.\n        :param output_dir: Папка для логов/чекпоинтов.\n        :param seed: Зерно.\n        :param hidden: Размер скрытого слоя головы.\n        :param dropout: Дропаут в голове.\n        :return: self.\n        :raises ValueError: При проблемах с данными/параметрами.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        classes = sorted(train_data[self.target_column_name].unique().tolist())\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n        df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n        ds_train = MultiComboDataset(df_train, self.target_column_name, self.label2id, self.text_columns, self.image_columns, self.audio_columns)\n        ds_eval  = MultiComboDataset(df_eval,  self.target_column_name, self.label2id, self.text_columns, self.image_columns, self.audio_columns)\n\n        y_train = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n        counts = np.bincount(y_train, minlength=self.num_labels)\n        n = counts.sum()\n        class_weights = np.zeros(self.num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        class_weights[nonzero] = n / (self.num_labels * counts[nonzero].astype(np.float32))\n\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n\n        self._setup_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=0,\n            seed=seed,\n            remove_unused_columns=False\n        )\n\n        def data_collator(batch_list):\n            \"\"\"\n            Хук для Trainer: вызывает backend.collate.\n\n            :param batch_list: Элементы из датасета.\n            :return: Батч для model.forward() (labels, backend_inputs).\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            num_labels=self.num_labels,\n            train_labels=y_train,\n            class_weights=class_weights\n        )\n\n        self.trainer.train()\n        return self\n\n    def predict(self, df: pd.DataFrame, return_label_str: bool = False) -> np.ndarray:\n        \"\"\"\n        Делает предсказания классов на новых данных.\n\n        :param df: Датафрейм с нужными колонками модальностей.\n        :param return_label_str: Вернуть строковые метки (True) или id (False).\n        :return: Вектор предсказанных меток (id или строки).\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = 0\n        ds = MultiComboDataset(df_c, self.target_column_name, self.label2id, self.text_columns, self.image_columns, self.audio_columns)\n        preds = self.trainer.predict(test_dataset=ds)\n        y_pred = np.argmax(preds.predictions, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        \"\"\"\n        Извлекает эмбеддинги для новых данных.\n\n        :param df: Датафрейм с нужными колонками модальностей.\n        :param batch_size: Размер батча для извлечения эмбеддингов.\n        :param return_per_modality: Вернуть также эмбеддинги по модальностям.\n        :return: fused эмбеддинги [N, D_fused] или (fused, {'text':[N,*], 'image':[N,*], 'audio':[N,*]}).\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        device = getattr(self.trainer.args, \"device\", None)\n        if device is None:\n            try:\n                device = next(self.trainer.model.parameters()).device\n            except StopIteration:\n                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device)\n        self.model.eval()\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = 0\n\n        ds = MultiComboDataset(\n            df_c,\n            self.target_column_name,\n            self.label2id,\n            self.text_columns,\n            self.image_columns,\n            self.audio_columns\n        )\n\n        def collate(batch_list):\n            \"\"\"\n            Collate для DataLoader при извлечении эмбеддингов.\n\n            :param batch_list: Элементы из датасета.\n            :return: Батч (labels, backend_inputs).\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device):\n            \"\"\"\n            Рекурсивно переносит тензоры на нужный девайс.\n\n            :param obj: Тензор/словарь/список/кортеж/прочее.\n            :param device: torch.device.\n            :return: Объект с перенесёнными тензорами.\n            \"\"\"\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        with torch.no_grad():\n            for batch in loader:\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torchaudio\n\nHAVE_TORCHAUDIO = True\n\nrandom.seed(42); np.random.seed(42); torch.manual_seed(42)\n\nBASE_DIR = \"./dummy_data\"\nIMG_DIR  = os.path.join(BASE_DIR, \"images\")\nAUD_DIR  = os.path.join(BASE_DIR, \"audio\")\nos.makedirs(IMG_DIR, exist_ok=True)\nos.makedirs(AUD_DIR, exist_ok=True)\n\n# Сколько элементов на модальность в каждой строке\nK_PER_MODALITY = 3\n\ndef make_dummy_images(n=12, size=(256, 256)):\n    paths = []\n    for i in range(n):\n        color = tuple(np.random.randint(0, 255, size=3).tolist())\n        img = Image.new(\"RGB\", size, color=color)\n        path = os.path.join(IMG_DIR, f\"img_{i:02d}.png\")\n        img.save(path)\n        paths.append(path)\n    return paths\n\ndef make_dummy_audios(n=12, sr=48000, duration_sec=0.6):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Нельзя сгенерировать аудио без torchaudio. Установите 'pip install torchaudio'.\")\n    paths = []\n    t = torch.linspace(0, duration_sec, int(sr * duration_sec))\n    for i in range(n):\n        freq = random.choice([220, 330, 440, 550, 660, 880])\n        wave = 0.2 * torch.sin(2 * np.pi * freq * t)  # амплитуда 0.2\n        wave = wave.unsqueeze(0)  # [1, T] mono\n        path = os.path.join(AUD_DIR, f\"tone_{i:02d}.wav\")\n        torchaudio.save(path, wave, sample_rate=sr)\n        paths.append(path)\n    return paths\n\nimg_paths = make_dummy_images(n=12)\naudio_paths = make_dummy_audios(n=12) if HAVE_TORCHAUDIO else []\n\n# Вспомогательные тексты\nTITLES = [\"Red fox\", \"Blue sky\", \"Green field\", \"Yellow sun\", \"Purple rain\", \"Silver line\"]\nBODIES = [\"quick brown\", \"lazy dog\", \"jumps high\", \"runs fast\", \"stays calm\", \"shines bright\"]\nQUERIES = [\"find tone\", \"classify sound\", \"describe image\", \"retrieve pair\", \"detect event\"]\n\ndef rand_title(): return random.choice(TITLES)\ndef rand_body(): return random.choice(BODIES)\ndef rand_query(): return random.choice(QUERIES)\n\n# Универсальные хелперы\ndef sample_k(seq, k):\n    if len(seq) >= k:\n        return random.sample(seq, k)  # без повторов\n    else:\n        return [random.choice(seq) for _ in range(k)]  # с повторами, если мало исходников\n\ndef as_cols(prefix, values):\n    # {\"prefix_1\": values[0], ..., \"prefix_k\": values[k-1]}\n    return {f\"{prefix}_{i+1}\": v for i, v in enumerate(values)}\n\ndef pick_text_desc(k=K_PER_MODALITY):\n    # Текстовое описание: \"Title | body\"\n    vals = [f\"{rand_title()} | {rand_body()}\" for _ in range(k)]\n    return as_cols(\"text\", vals)\n\ndef pick_text_query(k=K_PER_MODALITY):\n    vals = [rand_query() for _ in range(k)]\n    return as_cols(\"text\", vals)\n\ndef pick_images(k=K_PER_MODALITY):\n    vals = sample_k(img_paths, k)\n    return as_cols(\"image_path\", vals)\n\ndef pick_audios(k=K_PER_MODALITY):\n    vals = sample_k(audio_paths, k)\n    return as_cols(\"audio_path\", vals)\n\n# 1) Текст + Картинка -> по 3 текстовых и 3 картинок\ndef build_df_text_image(n=24, k=K_PER_MODALITY):\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_desc(k))\n        row.update(pick_images(k))\n        row[\"label\"] = random.choice([\"class_a\", \"class_b\", \"class_c\"])\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 2) Текст + Звук -> по 3 текста (query) и 3 аудио\ndef build_df_text_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для text+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_query(k))\n        row.update(pick_audios(k))\n        row[\"label\"] = random.choice([\"ok\", \"ng\"])\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 3) Картинка + Звук -> по 3 картинки и 3 аудио\ndef build_df_image_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для image+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_images(k))\n        row.update(pick_audios(k))\n        row[\"label\"] = random.choice([\"dog\", \"cat\", \"bird\"])\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 4) Текст + Картинка + Звук -> по 3 на каждую модальность\ndef build_df_text_image_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для text+image+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_desc(k))\n        row.update(pick_images(k))\n        row.update(pick_audios(k))\n        row[\"label\"] = random.choice([\"A\", \"B\", \"C\", \"D\"])\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# Собираем 4 датасета\ndf_text_image = build_df_text_image(24, K_PER_MODALITY)\ndf_text_audio = build_df_text_audio(24, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\ndf_image_audio = build_df_image_audio(24, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\ndf_text_image_audio  = build_df_text_image_audio(24, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\n\nprint(\"df_text_image columns:\", list(df_text_image.columns))\nif HAVE_TORCHAUDIO:\n    print(\"df_text_audio columns:\", list(df_text_audio.columns))\n    print(\"df_image_audio columns:\", list(df_image_audio.columns))\n    print(\"df_text_image_audio columns:\", list(df_text_image_audio.columns))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования на данных текст + картинка.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\"],\n    num_labels=3,\n    target_column_name=\"label\",\n    text_columns=[\"text_1\", \"text_2\"],\n    image_columns=[\"image_path_1\", \"image_path_2\", \"image_path_3\"],\n    backend=\"auto\",\n    fusion=\"concat\",\n    freeze_backbone=True,\n    max_images_per_sample=2\n)\npipeline.fit(df_text_image, epochs=2, per_device_train_batch_size=8, logging_steps=2, eval_steps=4)\npreds = pipeline.predict(df_text_image.head(5), return_label_str=True)\nemb = pipeline.get_embeddings(df_text_image.head(8), batch_size=8)\nprint(preds)\nprint(emb.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования на данных текст + звук.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"audio\"],\n    num_labels=2,\n    target_column_name=\"label\",\n    text_columns=[\"text_1\"],\n    audio_columns=[\"audio_path_1\"],\n    backend=\"auto\",\n    fusion=\"concat\",\n    freeze_backbone=True,\n    max_audios_per_sample=1\n)\npipeline.fit(df_text_audio, epochs=1, per_device_train_batch_size=8)\npreds = pipeline.predict(df_text_audio.head(5), return_label_str=True)\nemb = pipeline.get_embeddings(df_text_audio.head(8), batch_size=8)\nprint(preds)\nprint(emb.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования на данных картинка + звук.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboClassification(\n    modalities=[\"image\", \"audio\"],\n    num_labels=3,\n    target_column_name=\"label\",\n    image_columns=[\"image_path_1\"],\n    audio_columns=[\"audio_path_1\", \"audio_path_2\"],\n    backend=\"auto\",\n    fusion=\"concat\",\n    freeze_backbone=True,\n    max_images_per_sample=1,\n    max_audios_per_sample=1\n)\npipeline.fit(df_image_audio, epochs=1, per_device_train_batch_size=8)\npreds = pipeline.predict(df_image_audio.head(5), return_label_str=True)\nemb = pipeline.get_embeddings(df_image_audio.head(8), batch_size=8)\nprint(preds)\nprint(emb)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования на данных текст + картинка + звук.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\", \"audio\"],\n    num_labels=4,\n    target_column_name=\"label\",\n    text_columns=[\"text_1\"],\n    image_columns=[\"image_path_1\", \"image_path_2\"],\n    audio_columns=[\"audio_path_1\"],\n    backend=\"auto\",\n    fusion=\"concat\",\n    freeze_backbone=True,\n    max_images_per_sample=2,\n    max_audios_per_sample=2\n)\npipeline.fit(df_text_image_audio, epochs=2, per_device_train_batch_size=8, logging_steps=3, eval_steps=6)\npreds = pipeline.predict(df_text_image_audio.head(5), return_label_str=True)\nemb = pipeline.get_embeddings(df_text_image_audio.head(8), batch_size=8)\nprint(preds)\nprint(emb)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение регрессора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"code","source":"!pip install -q wav2clip torchaudio transformers evaluate pillow\n\nimport math\nfrom typing import List, Dict, Any, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport evaluate\nfrom transformers import Trainer, TrainingArguments\nfrom transformers.modeling_outputs import ModelOutput\n\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Устанавливает фиксированное зерно для Python, NumPy и PyTorch (включая все доступные CUDA-устройства).\n\n    :param seed: Значение зерна.\n    \"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef to_pil(x: Union[str, np.ndarray, Image.Image]) -> Image.Image:\n    \"\"\"\n    Приводит входное изображение к PIL.Image в формате RGB.\n\n    Поддерживаемые форматы:\n      - путь к файлу (str);\n      - NumPy массив (H, W[, C]) со значениями uint8;\n      - PIL.Image (любого режима).\n\n    :param x: Источник изображения.\n    :return: Изображение PIL.Image в RGB.\n    :raises ValueError: Если тип входа не поддерживается.\n    \"\"\"\n    if isinstance(x, Image.Image):\n        return x.convert(\"RGB\")\n    if isinstance(x, str):\n        return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray):\n        return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    \"\"\"\n    Загружает аудио и (при необходимости) ресемплирует до target_sr.\n\n    - Монофонизирует вход через усреднение каналов.\n    - Возвращает float32 массив формы [T].\n\n    :param path: Путь к аудиофайлу (wav/flac/…).\n    :param target_sr: Целевая частота дискретизации.\n    :return: Сигнал формы [T] float32.\n    :raises RuntimeError: Если torchaudio недоступен.\n    \"\"\"\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)  # [C, T]\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr:\n        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\nclass MultiComboRegDataset(Dataset):\n    \"\"\"\n    Регрессионный датасет с поддержкой нескольких изображений и аудио на сэмпл.\n\n    Ячейки колонок изображений/аудио могут содержать:\n      - одиночное значение (путь/np.ndarray/PIL для изображения; путь/np.ndarray для аудио);\n      - список таких значений.\n\n    :param df: Исходный DataFrame.\n    :param target_cols: Список целевых колонок (поддерживается многомерная регрессия).\n    :param text_columns: Список текстовых колонок. Их значения конкатенируются через [SEP].\n    :param image_columns: Список колонок изображений.\n    :param audio_columns: Список колонок аудио.\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_cols: List[str],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None\n    ):\n        \"\"\"\n        Инициализация датасета.\n\n        :param df: Исходный DataFrame.\n        :param target_cols: Названия колонок целей.\n        :param text_columns: Текстовые колонки (склеиваются).\n        :param image_columns: Колонки изображений (значение или список значений).\n        :param audio_columns: Колонки аудио (значение или список значений).\n        \"\"\"\n        self.df = df.reset_index(drop=True)\n        self.target_cols = target_cols\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.sep = \" [SEP] \"\n\n    def __len__(self) -> int:\n        \"\"\"\n        Количество элементов датасета.\n\n        :return: Длина датасета.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        \"\"\"\n        Возвращает один элемент датасета.\n\n        Ключи в словаре:\n          - 'labels': np.ndarray формы [T], float32;\n          - 'text': строка (если заданы text_columns);\n          - 'images': список изображений (пути/np.ndarray/PIL) или пустой список;\n          - 'audios': список аудио (пути/np.ndarray) или пустой список.\n\n        :param idx: Индекс строки.\n        :return: Словарь для последующего collate бэкенда.\n        \"\"\"\n        row = self.df.iloc[idx]\n        y = np.array([float(row[c]) for c in self.target_cols], dtype=np.float32)\n\n        def _as_list(v):\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                return []\n            if isinstance(v, (list, tuple)):\n                return list(v)\n            return [v]\n\n        item = {\"labels\": y}\n        if self.text_columns:\n            item[\"text\"] = self.sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n        if self.image_columns:\n            imgs = []\n            for c in self.image_columns:\n                if c in row:\n                    imgs.extend(_as_list(row[c]))\n            item[\"images\"] = imgs\n\n        if self.audio_columns:\n            auds = []\n            for c in self.audio_columns:\n                if c in row:\n                    auds.extend(_as_list(row[c]))\n            item[\"audios\"] = auds\n\n        return item\n\n\nclass BaseBackend(nn.Module):\n    \"\"\"\n    Базовый класс бэкенда для единой мультимодальной модели.\n\n    Атрибуты:\n      - name: Название бэкенда.\n      - supported: Набор поддерживаемых модальностей (например, {'text','image'}).\n      - embed_dim: Базовая размерность эмбеддингов модальностей.\n      - out_dim_per_modality: Словарь фактических размерностей выходных эмбеддингов по модальностям\n                              (с учётом агрегации/concat).\n    \"\"\"\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Собирает батч для текущего бэкенда.\n\n        :param batch: Список элементов MultiComboRegDataset.\n        :return: Словарь вида:\n                 {\n                   'labels': torch.FloatTensor [B, T],\n                   'backend_inputs': dict(...)\n                 }\n        \"\"\"\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует модальности и возвращает эмбеддинги по доступным модальностям.\n\n        :param backend_inputs: Предобработанные входы (из collate).\n        :param device: Целевой девайс.\n        :return: Словарь {'text':[B,*], 'image':[B,*], 'audio':[B,*]} по имеющимся модальностям.\n        \"\"\"\n        raise NotImplementedError\n\n    def freeze_all(self):\n        \"\"\"\n        Замораживает все параметры бэкенда (requires_grad=False).\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad = False\n\n    @staticmethod\n    def _stack_labels_float(batch: List[Dict[str, Any]]) -> torch.Tensor:\n        \"\"\"\n        Преобразует список 'labels' из элементов батча в тензор float32 [B, T] с выравниванием по T.\n\n        Если длины таргетов различны, добивает меньшие нулями справа до максимальной длины.\n\n        :param batch: Список элементов датасета.\n        :return: torch.FloatTensor [B, T].\n        \"\"\"\n        ys = []\n        for b in batch:\n            y = b.get(\"labels\", None)\n            if y is None:\n                ys.append(np.array([0.0], dtype=np.float32))\n            else:\n                arr = np.array(y, dtype=np.float32).reshape(-1)\n                ys.append(arr)\n        max_t = max(a.shape[0] for a in ys)\n        ys_padded = []\n        for a in ys:\n            if a.shape[0] == max_t:\n                ys_padded.append(a)\n            else:\n                pad = np.zeros(max_t, dtype=np.float32)\n                pad[:a.shape[0]] = a\n                ys_padded.append(pad)\n        return torch.from_numpy(np.stack(ys_padded, axis=0))\n\n    def get_out_dim(self, modality: str) -> int:\n        \"\"\"\n        Возвращает итоговую размерность выходного вектора по модальности\n        (с учётом агрегации и параметров max_*).\n\n        :param modality: Имя модальности: 'text' | 'image' | 'audio'.\n        :return: Размерность эмбеддинга.\n        \"\"\"\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n\nclass ClipBackend(BaseBackend):\n    \"\"\"\n    CLIP-бэкенд для текста и изображений с поддержкой нескольких изображений на сэмпл.\n\n    Агрегация изображений:\n      - 'concat': конкатенация эмбеддингов N изображений в один вектор (с паддингом нулями до max_images);\n      - 'mean': усреднение эмбеддингов изображений.\n\n    :param checkpoint: Имя/путь чекпоинта CLIP (HF Hub).\n    :param max_length: Максимальная длина токенов текста.\n    :param freeze: Замораживать ли веса CLIP (linear probing).\n    :param max_images: Максимальное число изображений на сэмпл для агрегации.\n    :param image_agg: Тип агрегации изображений ('concat' или 'mean').\n    \"\"\"\n    name = \"clip\"\n    supported = {\"text\", \"image\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        max_images: int = 1,\n        image_agg: str = \"concat\"\n    ):\n        \"\"\"\n        Инициализация CLIP-бэкенда.\n\n        :param checkpoint: Чекпоинт CLIP.\n        :param max_length: Максимальная длина текста (токенов).\n        :param freeze: Замораживать ли веса.\n        :param max_images: Максимум изображений на сэмпл.\n        :param image_agg: Агрегация изображений.\n        \"\"\"\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.max_images = int(max_images)\n        self.image_agg = image_agg\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Готовит батч для CLIP.\n\n        Формирует:\n          - text_inputs: токены текста;\n          - image_inputs: pixel_values (возможен None);\n          - image_counts: количество изображений на каждый сэмпл (для агрегации).\n\n        :param batch: Элементы с ключами 'text', 'images', 'labels'.\n        :return: {'labels': FloatTensor [B,T], 'backend_inputs': dict}.\n        \"\"\"\n        labels = self._stack_labels_float(batch)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n\n        text_inputs = self.processor(\n            text=texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n        )\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(counts, dtype=torch.long),\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенирует до max_k эмбеддингов изображений на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги изображений [M, D], где M = сумма(counts).\n        :param counts: Список количеств изображений на сэмпл.\n        :param max_k: Максимум изображений на сэмпл (для паддинга/отрезания).\n        :return: Нормализованный тензор [B, D*max_k].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усредняет эмбеддинги изображений на сэмпл.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества изображений на сэмпл.\n        :return: Нормализованный тензор [B, D].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Вычисляет эмбеддинги текста и изображений (с агрегацией изображений).\n\n        :param backend_inputs: Тензоры CLIPProcessor для текста и картинок.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'image':[B,D*max_images] или [B,D]} — L2-нормированные эмбеддинги.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                img_z = self._concat_padded(img_flat, counts, self.max_images)\n            else:\n                img_z = self._mean_pool(img_flat, counts)\n        else:\n            img_z = torch.zeros(\n                (len(counts), self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim),\n                device=device\n            )\n\n        return {\"text\": text_z, \"image\": img_z}\n\n\nclass ClapBackend(BaseBackend):\n    \"\"\"\n    CLAP-бэкенд для текста и аудио с поддержкой нескольких аудио на сэмпл.\n\n    Агрегация аудио:\n      - 'concat': конкатенация эмбеддингов N аудиоклипов в один вектор (с паддингом нулями до max_audios);\n      - 'mean': усреднение эмбеддингов аудио.\n\n    :param checkpoint: Имя/путь чекпоинта CLAP (HF Hub).\n    :param freeze: Замораживать ли веса CLAP.\n    :param max_audios: Максимальное число аудио на сэмпл для агрегации.\n    :param audio_agg: Тип агрегации аудио ('concat' или 'mean').\n    \"\"\"\n    name = \"clap\"\n    supported = {\"text\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"laion/clap-htsat-unfused\",\n        freeze: bool = True,\n        max_audios: int = 1,\n        audio_agg: str = \"concat\"\n    ):\n        \"\"\"\n        Инициализация CLAP-бэкенда.\n\n        :param checkpoint: Чекпоинт CLAP.\n        :param freeze: Замораживать ли веса.\n        :param max_audios: Максимум аудио на сэмпл.\n        :param audio_agg: Агрегация аудио.\n        \"\"\"\n        super().__init__()\n        from transformers import ClapModel, ClapProcessor\n        self.model = ClapModel.from_pretrained(checkpoint)\n        self.processor = ClapProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(getattr(self.model.config, \"projection_dim\", 512))\n        sr = getattr(self.processor, \"sampling_rate\", None)\n        if sr is None:\n            fe = getattr(self.processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n        self.sr = int(sr)\n        self.max_audios = int(max_audios)\n        self.audio_agg = audio_agg\n        if freeze:\n            self.freeze_all()\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"audio\": aud_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Готовит батч для CLAP.\n\n        Формирует:\n          - text_inputs: токены текста;\n          - audio_inputs: input_features (или None);\n          - audio_counts: число аудио на сэмпл.\n\n        :param batch: Элементы с ключами 'text','audios','labels'.\n        :return: {'labels': FloatTensor [B,T], 'backend_inputs': dict}.\n        \"\"\"\n        labels = self._stack_labels_float(batch)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"Ожидается путь к аудио или numpy.ndarray\")\n\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_audios):\n            aud_proc = self.processor(audios=flat_audios, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n            audio_inputs = {\"input_features\": aud_proc[\"input_features\"]}\n        else:\n            audio_inputs = {\"input_features\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенирует до max_k эмбеддингов аудио на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги аудио [M, D].\n        :param counts: Список количеств аудио на сэмпл.\n        :param max_k: Максимум аудио на сэмпл.\n        :return: Нормализованный тензор [B, D*max_k].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усредняет эмбеддинги аудио на сэмпл.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества аудио на сэмпл.\n        :return: Нормализованный тензор [B, D].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Вычисляет эмбеддинги текста и аудио (с агрегацией аудио).\n\n        :param backend_inputs: Тензоры ClapProcessor для текста и аудио.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'audio':[B,D*max_audios] или [B,D]} — L2-нормированные эмбеддинги.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        if hasattr(self.model, \"get_text_features\") and hasattr(self.model, \"get_audio_features\"):\n            text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        else:\n            # совместимость со старыми версиями transformers\n            out = self.model(**ti, output_attentions=False, output_hidden_states=False, return_dict=True)\n            text_z = out.text_embeds\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"audio_counts\"].tolist()\n        af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n        if af is not None:\n            af = af.to(device)\n            if hasattr(self.model, \"get_audio_features\"):\n                aud_flat = self.model.get_audio_features(input_features=af)\n            else:\n                out = self.model(input_features=af, output_attentions=False, output_hidden_states=False, return_dict=True)\n                aud_flat = out.audio_embeds\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                aud_z = self._concat_padded(aud_flat, counts, self.max_audios)\n            else:\n                aud_z = self._mean_pool(aud_flat, counts)\n        else:\n            aud_z = torch.zeros(\n                (len(counts), self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim),\n                device=device\n            )\n\n        return {\"text\": text_z, \"audio\": aud_z}\n\n\nclass ClipWav2CLIPBackend(BaseBackend):\n    \"\"\"\n    Комбинированный бэкенд: CLIP для текста/изображения + Wav2CLIP для аудио (в CLIP-пространство).\n\n    Поддержка нескольких изображений и аудио на сэмпл с агрегацией 'concat' или 'mean'.\n\n    :param checkpoint: Чекпоинт CLIP (HF).\n    :param max_length: Максимальная длина токенов для CLIP.\n    :param freeze: Замораживать ли веса CLIP.\n    :param audio_sr: Частота дискретизации для Wav2CLIP.\n    :param max_images: Максимум изображений на сэмпл.\n    :param max_audios: Максимум аудио на сэмпл.\n    :param image_agg: Тип агрегации изображений ('concat' | 'mean').\n    :param audio_agg: Тип агрегации аудио ('concat' | 'mean').\n    \"\"\"\n    name = \"clip_wav2clip\"\n    supported = {\"text\", \"image\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        audio_sr: int = 16000,\n        max_images: int = 1,\n        max_audios: int = 1,\n        image_agg: str = \"concat\",\n        audio_agg: str = \"concat\"\n    ):\n        \"\"\"\n        Инициализация ClipWav2CLIP-бэкенда.\n\n        :param checkpoint: Чекпоинт CLIP.\n        :param max_length: Максимальная длина текста.\n        :param freeze: Замораживать ли веса CLIP.\n        :param audio_sr: Частота дискретизации для Wav2CLIP.\n        :param max_images: Максимум изображений на сэмпл.\n        :param max_audios: Максимум аудио на сэмпл.\n        :param image_agg: Агрегация изображений.\n        :param audio_agg: Агрегация аудио.\n        \"\"\"\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.audio_sr = int(audio_sr)\n        self.max_images = int(max_images)\n        self.max_audios = int(max_audios)\n        self.image_agg = image_agg\n        self.audio_agg = audio_agg\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out, \"audio\": aud_out}\n        # ленивое подключение Wav2CLIP\n        self._w2c_model = None\n        self._w2c_mod = None\n        self._w2c_api = None\n        self._w2c_device = None\n\n    def _ensure_w2c(self, device: torch.device):\n        \"\"\"\n        Ленивая загрузка wav2clip и его модели для текущего устройства.\n\n        Поддерживаются разные версии API:\n          - функции load_model/get_model + embed_audio/get_audio_embedding;\n          - класс Wav2CLIP с методом embed_audio.\n\n        :param device: Текущее устройство.\n        :raises RuntimeError: Если wav2clip недоступен или не удалось инициализировать модель.\n        \"\"\"\n        if self._w2c_model is not None and self._w2c_device == str(device):\n            return\n        import importlib\n        try:\n            w2c = importlib.import_module(\"wav2clip\")\n        except Exception as e:\n            raise RuntimeError(\"Не найден пакет 'wav2clip'. Установите: pip install wav2clip\") from e\n        dev_str = str(device) if device.type == \"cuda\" else \"cpu\"\n        if hasattr(w2c, \"load_model\"):\n            model = w2c.load_model(device=dev_str); api_kind = \"func\"\n        elif hasattr(w2c, \"get_model\"):\n            model = w2c.get_model(device=dev_str); api_kind = \"func\"\n        elif hasattr(w2c, \"Wav2CLIP\"):\n            try:\n                model = w2c.Wav2CLIP(dev_str)\n            except TypeError:\n                model = w2c.Wav2CLIP(device=dev_str)\n            api_kind = \"method\"\n        else:\n            raise RuntimeError(\"wav2clip установлен, но нет способов загрузки (load_model/get_model/Wav2CLIP)\")\n        self._w2c_mod = w2c\n        self._w2c_model = model\n        self._w2c_api = api_kind\n        self._w2c_device = str(device)\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Готовит батч для текста/изображений (через CLIPProcessor) и аудио (через паддинг numpy->Tensor).\n\n        Возвращает:\n          - text_inputs: словарь тензоров для текста;\n          - image_inputs: словарь с pixel_values или None;\n          - image_counts: LongTensor [B] — количество изображений на сэмпл;\n          - audio_inputs: {'waveforms': Tensor [M,Lmax] или None, 'lengths': LongTensor [M] или None};\n          - audio_counts: LongTensor [B] — количество аудио на сэмпл;\n          - labels: FloatTensor [B, T].\n\n        :param batch: Элементы с 'text','images','audios','labels'.\n        :return: Словарь {'labels', 'backend_inputs'}.\n        \"\"\"\n        labels = self._stack_labels_float(batch)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        # images (flatten + counts)\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, img_counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            img_counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        # audios (flatten + counts)\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, aud_counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            aud_counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.audio_sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"Ожидается путь к аудио или numpy.ndarray\")\n\n        if len(flat_audios):\n            Lmax = max(len(a) for a in flat_audios)\n            wav = np.zeros((len(flat_audios), Lmax), dtype=np.float32)\n            lens = np.zeros((len(flat_audios),), dtype=np.int64)\n            for i, a in enumerate(flat_audios):\n                L = len(a)\n                wav[i, :L] = a\n                lens[i] = L\n            audio_inputs = {\"waveforms\": torch.from_numpy(wav), \"lengths\": torch.from_numpy(lens)}\n        else:\n            audio_inputs = {\"waveforms\": None, \"lengths\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(img_counts, dtype=torch.long),\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(aud_counts, dtype=torch.long),\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _agg_concat(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенирует до max_k эмбеддингов на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества элементов на сэмпл.\n        :param max_k: Максимум элементов на сэмпл (для паддинга).\n        :return: Нормализованный тензор [B, D*max_k].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset+c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _agg_mean(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усредняет эмбеддинги на сэмпл.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества элементов на сэмпл.\n        :return: Нормализованный тензор [B, D].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset+c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует модальности: текст/изображение — через CLIP; аудио — через Wav2CLIP.\n\n        :param backend_inputs: Входы из collate().\n        :param device: Целевой девайс.\n        :return: Словарь {'text',[B,D]; 'image',[B,*]; 'audio',[B,*]} в CLIP-пространстве.\n        \"\"\"\n        # text + image via CLIP\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        img_counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                image_z = self._agg_concat(img_flat, img_counts, self.max_images)\n            else:\n                image_z = self._agg_mean(img_flat, img_counts)\n        else:\n            image_z = torch.zeros(\n                (len(img_counts), self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim),\n                device=device\n            )\n\n        # audio via wav2clip\n        aud_counts = backend_inputs[\"audio_counts\"].tolist()\n        wav = backend_inputs[\"audio_inputs\"][\"waveforms\"]\n        lens = backend_inputs[\"audio_inputs\"][\"lengths\"]\n        if wav is not None and lens is not None and (lens.numel() if torch.is_tensor(lens) else len(lens)) > 0:\n            self._ensure_w2c(device)\n            w2c = self._w2c_mod\n            embs = []\n            for i in range(wav.size(0)):\n                L = int(lens[i].item())\n                a_np = wav[i, :L].detach().cpu().numpy()\n                e = None\n                if self._w2c_api == \"func\":\n                    if hasattr(w2c, \"embed_audio\"):\n                        try:\n                            e = w2c.embed_audio(a_np, self._w2c_model)\n                        except TypeError:\n                            e = w2c.embed_audio(a_np, self.audio_sr, self._w2c_model)\n                    elif hasattr(w2c, \"get_audio_embedding\"):\n                        try:\n                            e = w2c.get_audio_embedding(a_np, self._w2c_model)\n                        except TypeError:\n                            e = w2c.get_audio_embedding(a_np, self.audio_sr, self._w2c_model)\n                if e is None and self._w2c_api == \"method\" and hasattr(self._w2c_model, \"embed_audio\"):\n                    try:\n                        e = self._w2c_model.embed_audio(a_np)\n                    except TypeError:\n                        e = self._w2c_model.embed_audio(a_np, sr=self.audio_sr)\n                if e is None:\n                    raise RuntimeError(\"Не удалось получить аудио‑эмбеддинг через wav2clip.\")\n                e = np.asarray(e)\n                if e.ndim == 2:\n                    e = e.mean(axis=0)\n                elif e.ndim > 2:\n                    e = e.reshape(-1, e.shape[-1]).mean(axis=0)\n                embs.append(e.astype(np.float32))\n            aud_flat = torch.tensor(np.stack(embs, axis=0), dtype=torch.float32, device=device)\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                audio_z = self._agg_concat(aud_flat, aud_counts, self.max_audios)\n            else:\n                audio_z = self._agg_mean(aud_flat, aud_counts)\n        else:\n            audio_z = torch.zeros(\n                (len(aud_counts), self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim),\n                device=device\n            )\n\n        return {\"text\": text_z, \"image\": image_z, \"audio\": audio_z}\n\n\nclass SingleBackboneRegressor(nn.Module):\n    \"\"\"\n    Регрессор поверх одного мультимодального бэкенда:\n    фьюжн эмбеддингов модальностей -> MLP-голова -> предсказание R^T.\n    \"\"\"\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_targets: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        \"\"\"\n        Инициализирует регрессионную голову.\n\n        :param backend: Инициализированный бэкенд (CLIP/CLAP/ClipWav2CLIP).\n        :param modalities: Список активных модальностей (учитывается порядок: image, text, audio).\n        :param num_targets: Число целевых признаков (T).\n        :param fusion: Тип фьюжна — 'concat' или 'mean'.\n        :param hidden: Размер скрытого слоя головы.\n        :param dropout: Дропаут в голове.\n        :raises ValueError: Если fusion='mean' при несовпадающих размерах модальностей.\n        \"\"\"\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_targets = num_targets\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать: {dict(zip(order, dims))}')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_targets)\n        )\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        \"\"\"\n        Пытается определить целевой девайс по первому найденному тензору во входах.\n\n        :param obj: Произвольная вложенная структура (тензоры/словари/списки).\n        :return: torch.device (cuda при наличии, иначе cpu).\n        \"\"\"\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Объединяет эмбеддинги модальностей согласно self.fusion.\n\n        - concat: конкатенация по последней оси;\n        - mean: среднее по модальностям (требует совпадения размерностей).\n\n        :param z: Словарь эмбеддингов по модальностям.\n        :return: Слитый эмбеддинг [B, D*|mods|] (concat) или [B, D] (mean).\n        \"\"\"\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        for m in order:\n            t = z[m]\n            if t.dim() == 3:\n                t = t.mean(dim=1)\n            elif t.dim() > 3:\n                t = t.view(t.size(0), -1)\n            feats.append(t)\n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            sizes = [f.size(-1) for f in feats]\n            if len(set(sizes)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать: {sizes}')\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None) -> ModelOutput:\n        \"\"\"\n        Прямой проход: кодирование модальностей -> фьюжн -> регрессионная голова.\n\n        :param backend_inputs: Входы для бэкенда (из его collate()).\n        :param labels: Не используется (Trainer читает labels отдельно).\n        :return: ModelOutput с полем logits [B, T].\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        preds = self.head(fused)\n        return ModelOutput(logits=preds)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        \"\"\"\n        Извлекает fused эмбеддинги (и, опционально, эмбеддинги по модальностям).\n\n        :param backend_inputs: Входы для бэкенда.\n        :param return_per_modality: Вернуть также словарь эмбеддингов по модальностям.\n        :return: fused [B, *] или (fused, {'text':[B,*], 'image':[B,*], 'audio':[B,*]}).\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\nclass MSETrainer(Trainer):\n    \"\"\"\n    Trainer для регрессии на основе MSE loss.\n\n    :param reduction: Редукция лосса ('mean' по умолчанию).\n    \"\"\"\n    def __init__(self, *args, reduction: str = \"mean\", **kwargs):\n        \"\"\"\n        Инициализация MSETrainer.\n\n        :param args: Аргументы Trainer.\n        :param reduction: Тип редукции MSELoss ('mean' | 'sum' | 'none').\n        :param kwargs: Аргументы Trainer.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self._reduction = reduction\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        \"\"\"\n        Вычисляет MSE лосс между предсказаниями и целевыми значениями.\n\n        :param model: Модель.\n        :param inputs: Батч, содержащий 'labels' (float [B,T]) и аргументы для model.forward().\n        :param return_outputs: Возвращать ли также outputs.\n        :param num_items_in_batch: Совместимость с API Trainer (не используется).\n        :return: loss (и outputs, если return_outputs=True).\n        \"\"\"\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        preds = outputs.logits\n        labels = labels.to(preds.device)\n        loss = nn.MSELoss(reduction=self._reduction)(preds, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\nclass SingleModelMultiComboRegression:\n    \"\"\"\n    Регрессионный пайплайн на одной мультимодальной модели (бэкенде) под заданные комбинации модальностей.\n\n    Поддерживаемые комбинации (auto-подбор бэкенда):\n      - ['text','image']         -> CLIP (HF)\n      - ['text','audio']         -> CLAP (HF)\n      - ['image','audio']        -> ClipWav2CLIP\n      - ['text','image','audio'] -> ClipWav2CLIP\n\n    Поддержка нескольких изображений/аудио на сэмпл с агрегацией 'concat' или 'mean'.\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        target_columns_names: List[str],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1,\n        image_agg: str = \"concat\",\n        audio_agg: str = \"concat\"\n    ):\n        \"\"\"\n        Инициализация пайплайна регрессии.\n\n        :param modalities: Активные модальности ('text','image','audio') в любом порядке.\n        :param target_columns_names: Имена целевых колонок (num_targets = len(target_columns_names)).\n        :param text_columns: Имена текстовых колонок (склеиваются).\n        :param image_columns: Список колонок изображений (ячейки — значение или список значений).\n        :param audio_columns: Список колонок аудио (ячейки — значение или список значений).\n        :param backend: 'auto' | 'clip' | 'clap' | 'clip_wav2clip'.\n        :param clip_checkpoint: Чекпоинт CLIP (HF).\n        :param clap_checkpoint: Чекпоинт CLAP (HF).\n        :param fusion: Тип фьюжна ('concat' или 'mean').\n        :param freeze_backbone: Заморозить веса бэкенда (linear probing).\n        :param clip_max_length: Максимальная длина токенов для CLIP.\n        :param max_images_per_sample: Максимум изображений на сэмпл при агрегации.\n        :param max_audios_per_sample: Максимум аудио на сэмпл при агрегации.\n        :param image_agg: Агрегация изображений ('concat' | 'mean').\n        :param audio_agg: Агрегация аудио ('concat' | 'mean').\n        \"\"\"\n        self.modalities = sorted(list(set(modalities)))\n        self.target_columns_names = target_columns_names\n        self.num_targets = len(target_columns_names)\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.backend_name = backend\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n        self.image_agg = image_agg\n        self.audio_agg = audio_agg\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneRegressor] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        \"\"\"\n        Инициализирует бэкенд согласно настройке backend/auto и проверяет совместимость с модальностями.\n\n        :raises ValueError: Если комбинация модальностей не поддерживается выбранным бэкендом.\n        \"\"\"\n        mods = set(self.modalities)\n        name = self.backend_name\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                name = \"clip\"\n            elif mods == {\"text\", \"audio\"}:\n                name = \"clap\"\n            elif mods in ({\"image\", \"audio\"}, {\"text\", \"image\", \"audio\"}):\n                name = \"clip_wav2clip\"\n            else:\n                raise ValueError(f\"Неподдерживаемая комбинация: {mods}\")\n\n        if name == \"clip\":\n            self.backend = ClipBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                max_images=self.max_images_per_sample,\n                image_agg=self.image_agg\n            )\n        elif name == \"clap\":\n            self.backend = ClapBackend(\n                checkpoint=self.clap_checkpoint,\n                freeze=self.freeze_backbone,\n                max_audios=self.max_audios_per_sample,\n                audio_agg=self.audio_agg\n            )\n        elif name == \"clip_wav2clip\":\n            self.backend = ClipWav2CLIPBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                audio_sr=16000,\n                max_images=self.max_images_per_sample,\n                max_audios=self.max_audios_per_sample,\n                image_agg=self.image_agg,\n                audio_agg=self.audio_agg\n            )\n        else:\n            raise ValueError(f\"Неизвестный backend: {name}\")\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(\n                f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}. \"\n                f\"Поддерживает: {self.backend.supported}\"\n            )\n\n    def _validate_data(self, df: pd.DataFrame):\n        \"\"\"\n        Проверяет соответствие колонок в DataFrame выбранным модальностям и целям.\n\n        :param df: Исходный датафрейм.\n        :raises ValueError: При отсутствии обязательных колонок.\n        \"\"\"\n        missing_targets = [c for c in self.target_columns_names if c not in df.columns]\n        if missing_targets:\n            raise ValueError(f\"В DataFrame отсутствуют целевые колонки: {missing_targets}\")\n\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        \"\"\"\n        Перемешивает и делит DataFrame на обучающую и валидационную части.\n\n        :param df: Исходный датафрейм.\n        :param test_size: Доля валидации (0..1).\n        :param seed: Зерно для перемешивания.\n        :return: (df_train, df_eval).\n        \"\"\"\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _setup_compute_metrics(self, metric_name: str):\n        \"\"\"\n        Создаёт функцию подсчёта метрик для Trainer.\n\n        Поддерживаемые итоговые метрики:\n          - 'rmse' (минимизируется),\n          - 'mae'  (минимизируется),\n          - 'mse'  (минимизируется),\n          - 'r2'   (максимизируется).\n\n        :param metric_name: Название основной метрики.\n        :raises RuntimeError: Если недоступны r_squared и sklearn для R2.\n        \"\"\"\n        name = metric_name.lower()\n    \n        def compute(p):\n            preds = np.asarray(p.predictions)\n            refs  = np.asarray(p.label_ids)\n    \n            # гарантируем форму [N, T]\n            if preds.ndim == 1: preds = preds[:, None]\n            if refs.ndim  == 1: refs  = refs[:,  None]\n    \n            T = min(preds.shape[1], refs.shape[1])\n            mse_list, mae_list, r2_list = [], [], []\n    \n            for t in range(T):\n                y_true = refs[:, t].astype(np.float64)\n                y_pred = preds[:, t].astype(np.float64)\n    \n                err = y_pred - y_true\n                mse = float(np.mean(err**2))\n                mae = float(np.mean(np.abs(err)))\n    \n                # R^2: 1 - SS_res/SS_tot; если дисперсия нулевая, даём 0.0 (как безопасный фолбэк)\n                var = float(np.var(y_true))\n                if var == 0.0:\n                    r2 = 0.0\n                else:\n                    ss_res = float(np.sum(err**2))\n                    ss_tot = float(np.sum((y_true - np.mean(y_true))**2))\n                    r2 = 1.0 - (ss_res / ss_tot if ss_tot > 0 else 0.0)\n    \n                mse_list.append(mse)\n                mae_list.append(mae)\n                r2_list.append(r2)\n    \n            mse_avg = float(np.mean(mse_list))\n            rmse_avg = float(np.sqrt(mse_avg))\n            mae_avg = float(np.mean(mae_list))\n            r2_avg = float(np.mean(r2_list))\n            return {\"rmse\": rmse_avg, \"mse\": mse_avg, \"mae\": mae_avg, \"r2\": r2_avg}\n    \n        self.compute_metrics = compute\n        self._primary_metric = name\n        self._greater_is_better = True if name == \"r2\" else False\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"rmse\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./reg_result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        \"\"\"\n        Обучает регрессионную голову поверх выбранного бэкенда.\n\n        :param train_data: Данные обучения с необходимыми колонками модальностей и целями.\n        :param epochs: Количество эпох обучения.\n        :param test_size: Доля валидации.\n        :param per_device_train_batch_size: Размер батча на устройство.\n        :param gradient_accumulation_steps: Шаги аккумуляции градиентов.\n        :param learning_rate: Learning rate для AdamW.\n        :param metric_name: Основная метрика ('rmse' | 'mae' | 'mse' | 'r2') для выбора лучшей модели.\n        :param fp16: Использовать ли fp16 при наличии CUDA.\n        :param logging_steps: Периодичность логирования.\n        :param eval_steps: Периодичность валидации/сохранения чекпоинтов.\n        :param output_dir: Папка для логов/чекпоинтов.\n        :param seed: Зерно.\n        :param hidden: Размер скрытого слоя головы.\n        :param dropout: Дропаут в голове.\n        :return: self.\n        :raises ValueError: При проблемах с данными или параметрами.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n        ds_train = MultiComboRegDataset(df_train, self.target_columns_names, self.text_columns, self.image_columns, self.audio_columns)\n        ds_eval  = MultiComboRegDataset(df_eval,  self.target_columns_names, self.text_columns, self.image_columns, self.audio_columns)\n\n        self.model = SingleBackboneRegressor(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_targets=self.num_targets,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n\n        self._setup_compute_metrics(metric_name)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{self._primary_metric}\",\n            greater_is_better=self._greater_is_better,\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=fp16 and torch.cuda.is_available(),\n            dataloader_num_workers=0,\n            seed=seed,\n            remove_unused_columns=False\n        )\n\n        def data_collator(batch_list):\n            \"\"\"\n            Collate-хук для Trainer: делегирует сборку батча в backend.collate().\n\n            :param batch_list: Список элементов датасета.\n            :return: Батч словаря {'labels', 'backend_inputs'}.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        self.trainer = MSETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics\n        )\n        self.trainer.train()\n        return self\n\n    def predict(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Делает предсказания регрессионных таргетов на новых данных.\n\n        :param df: Датафрейм с нужными колонками модальностей (целевые колонки могут отсутствовать).\n        :return: Массив предсказаний формы [N, T].\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        df_c = df.copy()\n        for c in self.target_columns_names:\n            if c not in df_c.columns:\n                df_c[c] = 0.0\n        ds = MultiComboRegDataset(df_c, self.target_columns_names, self.text_columns, self.image_columns, self.audio_columns)\n        preds = self.trainer.predict(test_dataset=ds)\n        return preds.predictions\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        \"\"\"\n        Извлекает эмбеддинги для новых данных (fused и, опционально, по модальностям).\n\n        :param df: Датафрейм с нужными колонками модальностей.\n        :param batch_size: Размер батча для извлечения эмбеддингов.\n        :param return_per_modality: Вернуть также по-модальные эмбеддинги.\n        :return: fused [N, D_fused] или (fused, {'text':[N,*], 'image':[N,*], 'audio':[N,*]}).\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        device = getattr(self.trainer.args, \"device\", None)\n        if device is None:\n            try:\n                device = next(self.trainer.model.parameters()).device\n            except StopIteration:\n                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device)\n        self.model.eval()\n\n        df_c = df.copy()\n        for c in self.target_columns_names:\n            if c not in df_c.columns:\n                df_c[c] = 0.0\n\n        ds = MultiComboRegDataset(\n            df_c,\n            self.target_columns_names,\n            self.text_columns,\n            self.image_columns,\n            self.audio_columns\n        )\n\n        def collate(batch_list):\n            \"\"\"\n            Collate для DataLoader при извлечении эмбеддингов (использует backend.collate()).\n\n            :param batch_list: Элементы датасета.\n            :return: Батч {'labels', 'backend_inputs'}.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device):\n            \"\"\"\n            Рекурсивно переносит тензоры в структуре на заданный девайс.\n\n            :param obj: Тензор/словарь/список/кортеж/прочее.\n            :param device: torch.device ('cpu' или 'cuda').\n            :return: Объект с перенесёнными тензорами.\n            \"\"\"\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        with torch.no_grad():\n            for batch in loader:\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torchaudio\n\nHAVE_TORCHAUDIO = True\n\nrandom.seed(42); np.random.seed(42); torch.manual_seed(42)\n\nBASE_DIR = \"./dummy_data\"\nIMG_DIR  = os.path.join(BASE_DIR, \"images\")\nAUD_DIR  = os.path.join(BASE_DIR, \"audio\")\nos.makedirs(IMG_DIR, exist_ok=True)\nos.makedirs(AUD_DIR, exist_ok=True)\n\n# Сколько элементов на модальность в каждой строке\nK_PER_MODALITY = 3\n\ndef make_dummy_images(n=12, size=(256, 256)):\n    paths = []\n    for i in range(n):\n        color = tuple(np.random.randint(0, 255, size=3).tolist())\n        img = Image.new(\"RGB\", size, color=color)\n        path = os.path.join(IMG_DIR, f\"img_{i:02d}.png\")\n        img.save(path)\n        paths.append(path)\n    return paths\n\ndef make_dummy_audios(n=12, sr=48000, duration_sec=0.6):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Нельзя сгенерировать аудио без torchaudio. Установите 'pip install torchaudio'.\")\n    paths = []\n    t = torch.linspace(0, duration_sec, int(sr * duration_sec))\n    for i in range(n):\n        freq = random.choice([220, 330, 440, 550, 660, 880])\n        wave = 0.2 * torch.sin(2 * np.pi * freq * t)  # амплитуда 0.2\n        wave = wave.unsqueeze(0)  # [1, T] mono\n        path = os.path.join(AUD_DIR, f\"tone_{i:02d}.wav\")\n        torchaudio.save(path, wave, sample_rate=sr)\n        paths.append(path)\n    return paths\n\nimg_paths = make_dummy_images(n=12)\naudio_paths = make_dummy_audios(n=12) if HAVE_TORCHAUDIO else []\n\n# Вспомогательные тексты\nTITLES = [\"Red fox\", \"Blue sky\", \"Green field\", \"Yellow sun\", \"Purple rain\", \"Silver line\"]\nBODIES = [\"quick brown\", \"lazy dog\", \"jumps high\", \"runs fast\", \"stays calm\", \"shines bright\"]\nQUERIES = [\"find tone\", \"classify sound\", \"describe image\", \"retrieve pair\", \"detect event\"]\n\ndef rand_title(): return random.choice(TITLES)\ndef rand_body(): return random.choice(BODIES)\ndef rand_query(): return random.choice(QUERIES)\n\n# Универсальные хелперы\ndef sample_k(seq, k):\n    if len(seq) >= k:\n        return random.sample(seq, k)  # без повторов\n    else:\n        return [random.choice(seq) for _ in range(k)]  # с повторами, если мало исходников\n\ndef as_cols(prefix, values):\n    # {\"prefix_1\": values[0], ..., \"prefix_k\": values[k-1]}\n    return {f\"{prefix}_{i+1}\": v for i, v in enumerate(values)}\n\ndef pick_text_desc(k=K_PER_MODALITY):\n    # Текстовое описание: \"Title | body\"\n    vals = [f\"{rand_title()} | {rand_body()}\" for _ in range(k)]\n    return as_cols(\"text\", vals)\n\ndef pick_text_query(k=K_PER_MODALITY):\n    vals = [rand_query() for _ in range(k)]\n    return as_cols(\"text\", vals)\n\ndef pick_images(k=K_PER_MODALITY):\n    vals = sample_k(img_paths, k)\n    return as_cols(\"image_path\", vals)\n\ndef pick_audios(k=K_PER_MODALITY):\n    vals = sample_k(audio_paths, k)\n    return as_cols(\"audio_path\", vals)\n\n# 1) Текст + Картинка -> по 3 текстовых и 3 картинок\ndef build_df_text_image(n=24, k=K_PER_MODALITY):\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_desc(k))\n        row.update(pick_images(k))\n        row[\"y1\"] = random.uniform(-10, 10)\n        row[\"y2\"] = random.uniform(-10, 10)\n        row[\"y3\"] = random.uniform(-10, 10)\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 2) Текст + Звук -> по 3 текста (query) и 3 аудио\ndef build_df_text_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для text+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_query(k))\n        row.update(pick_audios(k))\n        row[\"y1\"] = random.uniform(-10, 10)\n        row[\"y2\"] = random.uniform(-10, 10)\n        row[\"y3\"] = random.uniform(-10, 10)\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 3) Картинка + Звук -> по 3 картинки и 3 аудио\ndef build_df_image_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для image+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_images(k))\n        row.update(pick_audios(k))\n        row[\"y1\"] = random.uniform(-10, 10)\n        row[\"y2\"] = random.uniform(-10, 10)\n        row[\"y3\"] = random.uniform(-10, 10)\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 4) Текст + Картинка + Звук -> по 3 на каждую модальность\ndef build_df_text_image_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для text+image+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_desc(k))\n        row.update(pick_images(k))\n        row.update(pick_audios(k))\n        row[\"y1\"] = random.uniform(-10, 10)\n        row[\"y2\"] = random.uniform(-10, 10)\n        row[\"y3\"] = random.uniform(-10, 10)\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# Собираем 4 датасета\ndf_text_image = build_df_text_image(24, K_PER_MODALITY)\ndf_text_audio = build_df_text_audio(24, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\ndf_image_audio = build_df_image_audio(24, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\ndf_text_image_audio  = build_df_text_image_audio(24, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\n\nprint(\"df_text_image columns:\", list(df_text_image.columns))\nif HAVE_TORCHAUDIO:\n    print(\"df_text_audio columns:\", list(df_text_audio.columns))\n    print(\"df_image_audio columns:\", list(df_image_audio.columns))\n    print(\"df_text_image_audio columns:\", list(df_text_image_audio.columns))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования на данных текст + картинка.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboRegression(\n    modalities=[\"text\", \"image\"],\n    target_columns_names=[\"y1\", \"y2\"],\n    text_columns=[\"text_1\", \"text_2\"],\n    image_columns=[\"image_path_1\"],\n    backend=\"auto\",\n    clip_checkpoint=\"openai/clip-vit-base-patch32\",  # необязательно\n    fusion=\"concat\",  # можно другое сделать\n    freeze_backbone=True\n)\n\npipeline.fit(df_text_image, epochs=1, per_device_train_batch_size=8)\npreds = pipeline.predict(df_text_image.head(5))\nembeddings = pipeline.get_embeddings(df_text_image.head(8))\n\nprint(preds)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования на данных текст + звук.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboRegression(\n    modalities=[\"text\", \"audio\"],\n    target_columns_names=[\"y1\"],\n    text_columns=[\"text_1\"],\n    audio_columns=[\"audio_path_1\", \"audio_path_2\"],\n    backend=\"auto\",  # выберет CLAP\n    clap_checkpoint=\"laion/clap-htsat-unfused\",  # необязательно\n    fusion=\"concat\",\n    freeze_backbone=True\n)\n\npipeline.fit(df_text_audio, epochs=1, per_device_train_batch_size=8)\npreds = pipeline.predict(df_text_audio.head(5))\nembeddings = pipeline.get_embeddings(df_text_audio.head(8))\n\nprint(preds)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования на данных картинка + звук.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboRegression(\n    modalities=[\"image\", \"audio\"],\n    target_columns_names=[\"y1\", \"y2\", \"y3\"],\n    image_columns=[\"image_path_1\", \"image_path_2\", \"image_path_3\"],\n    audio_columns=[\"audio_path_1\", \"audio_path_2\", \"audio_path_3\"],\n    backend=\"auto\",  # выберет ClipWav2CLIP\n    clip_checkpoint=\"openai/clip-vit-base-patch32\",  # необязательно\n    fusion=\"concat\",\n    freeze_backbone=True\n)\n\npipeline.fit(df_image_audio, epochs=1, per_device_train_batch_size=8)\npreds = pipeline.predict(df_image_audio.head(5))\nembeddings = pipeline.get_embeddings(df_image_audio.head(8))\n\nprint(preds)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования на данных текст + картинка + звук.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboRegression(\n    modalities=[\"text\", \"image\", \"audio\"],\n    target_columns_names=[\"y1\", \"y2\"],\n    text_columns=[\"text_1\"],\n    image_columns=[\"image_path_1\"],\n    audio_columns=[\"audio_path_1\"],\n    backend=\"auto\",  # выберет ClipWav2CLIP\n    clip_checkpoint=\"openai/clip-vit-base-patch32\",\n    fusion=\"concat\",\n    freeze_backbone=True\n)\n\npipeline.fit(df_text_image_audio, epochs=4, per_device_train_batch_size=8,\n             logging_steps=3, eval_steps=6)\npreds = pipeline.predict(df_text_image_audio.head(5))\nembeddings = pipeline.get_embeddings(df_text_image_audio.head(8))\n\nprint(preds)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}