{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install transformers>=4.38.0 trl>=0.8.0 peft>=0.9.0 bitsandbytes evaluate\n\nimport evaluate\nfrom datasets import load_dataset\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\nfrom peft import LoraConfig, get_peft_model\nimport torch\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:04:40.917287Z","iopub.execute_input":"2025-11-12T14:04:40.917539Z","iopub.status.idle":"2025-11-12T14:06:50.236538Z","shell.execute_reply.started":"2025-11-12T14:04:40.917513Z","shell.execute_reply":"2025-11-12T14:06:50.235407Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-12 14:06:31.036038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762956391.273378      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762956391.333744      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"checkpoint = 'google-t5/t5-3b'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\nMAX_LENGTH = model.config.task_specific_params['summarization']['max_length']\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:06:50.238385Z","iopub.execute_input":"2025-11-12T14:06:50.239137Z","iopub.status.idle":"2025-11-12T14:07:29.041455Z","shell.execute_reply.started":"2025-11-12T14:06:50.239109Z","shell.execute_reply":"2025-11-12T14:07:29.040448Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d930035768942e79d2893dd0cd3a844"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/11.4G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"668b0ddfd13248afb0e61ba2d9923360"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bbcfe792ad0462cadd1128a385b3310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b40d3e31d8364b11b65087e4c129c162"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"dataset = load_dataset('cnn_dailymail', '3.0.0').remove_columns('id')\n\ndataset['train'] = dataset['train'].select(range(10000))\ndataset['validation'] = dataset['validation'].select(range(1000))\n\ndef tokenize_function(examples):\n    inputs = tokenizer(\n        ['summarize: ' + text for text in examples['article']],\n        max_length=MAX_LENGTH,\n        truncation=True\n    )\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            [example[0] for example in examples['highlights']],\n            max_length=MAX_LENGTH,\n            truncation=True\n        )\n\n    return {\n        'input_ids': inputs['input_ids'],\n        'attention_mask': inputs['attention_mask'],\n        'labels': labels['input_ids']\n    }\n\nnot_preprocessed_val_data = dataset['validation']\ndataset = dataset.map(tokenize_function, batched=True, batch_size=1000)\ndataset = dataset.remove_columns(['article', 'highlights'])\ntrain_dataset, val_dataset = dataset['train'], dataset['validation']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:07:29.042405Z","iopub.execute_input":"2025-11-12T14:07:29.042765Z","iopub.status.idle":"2025-11-12T14:08:17.482565Z","shell.execute_reply.started":"2025-11-12T14:07:29.042737Z","shell.execute_reply":"2025-11-12T14:08:17.481819Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a50f7ca6f44a919843799b481297bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"3.0.0/train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"463850f0960c4ca1b562aee40aff75a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"3.0.0/train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42b406d5ff0644b0a8f2d3749b552f10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"3.0.0/train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e963e8122f4e5e97bab3b71ec377f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"3.0.0/validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4518f130bd3e4478875eebd0afa30984"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"3.0.0/test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"120ae3da60a646ec9b11a137b9aa24e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"919e85eb068449aab0450d6bdc060585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9ac36d7dc40483eb93c82661c70f2df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc2cf5b009542b5a9353b0e6fa80e26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b78b4a854f7e4c41a770a73305c0109f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8a8adc2ee7437aac360639a43c9203"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7584027e57cf4d9baf56364b65689f1e"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def generate_response(promt):\n    inputs = tokenizer(\n        promt,\n        max_length=MAX_LENGTH,\n        truncation=True,\n        padding='longest',\n        return_tensors='pt'\n    )\n\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=MAX_LENGTH,\n            early_stopping=True,\n            num_beams=4,\n            # temperature=0.7,\n            # top_p=0.9,\n            length_penalty=1.1,\n            no_repeat_ngram_size=3\n        )\n\n    texts = [\n        tokenizer.decode(output, skip_special_tokens=True)\n        for output in outputs\n    ]\n\n    return texts\n\nrouge = evaluate.load('rouge')\ndef evaluate_model(model, tokenizer, val_data, batch_size=8):\n    predictions = []\n\n    for i in tqdm(range(0, len(val_data), batch_size)):\n        batch = val_data.select(range(i, min(i+batch_size, len(val_data))))\n        batch_predictions = generate_response(['summarize: ' + item['article'] for item in batch])\n        predictions.extend(batch_predictions)\n\n    references = [item['highlights'] for item in val_data]\n\n    return rouge.compute(predictions=predictions, references=references)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:08:17.489097Z","iopub.execute_input":"2025-11-12T14:08:17.489379Z","iopub.status.idle":"2025-11-12T14:08:19.079049Z","shell.execute_reply.started":"2025-11-12T14:08:17.489352Z","shell.execute_reply":"2025-11-12T14:08:19.078228Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c5a825b4c046de9473b8dba0885f46"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"model.eval()\nmodel.to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:08:19.079922Z","iopub.execute_input":"2025-11-12T14:08:19.080138Z","iopub.status.idle":"2025-11-12T14:08:30.211666Z","shell.execute_reply.started":"2025-11-12T14:08:19.080121Z","shell.execute_reply":"2025-11-12T14:08:30.210892Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 1024)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"promts = ['summarize: One side of the armed conflicts is composed mainly of the Sudanese military and the Janjaweed, a Sudanese militia group recruited mostly from the Afro-Arab Abbala tribes of the northern Rizeigat region in Sudan.']\ngenerated_texts = generate_response(promts)\nprint(generated_texts[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:08:30.213669Z","iopub.execute_input":"2025-11-12T14:08:30.213915Z","iopub.status.idle":"2025-11-12T14:08:33.618442Z","shell.execute_reply.started":"2025-11-12T14:08:30.213896Z","shell.execute_reply":"2025-11-12T14:08:33.617399Z"}},"outputs":[{"name":"stdout","text":"Sudanese military and the Janjaweed, a sudanese militia group recruited mostly from the Afro-Arab Abbala tribes of the northern Rizeigat region in Sudan.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import gc; gc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:08:33.619194Z","iopub.execute_input":"2025-11-12T14:08:33.619435Z","iopub.status.idle":"2025-11-12T14:08:34.022949Z","shell.execute_reply.started":"2025-11-12T14:08:33.619414Z","shell.execute_reply":"2025-11-12T14:08:34.022210Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"scores = evaluate_model(model, tokenizer, not_preprocessed_val_data.select(range(30)), batch_size=1)\nprint(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:08:34.024024Z","iopub.execute_input":"2025-11-12T14:08:34.024388Z","iopub.status.idle":"2025-11-12T14:10:08.657964Z","shell.execute_reply.started":"2025-11-12T14:08:34.024361Z","shell.execute_reply":"2025-11-12T14:10:08.657081Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5095904730674fbfba304dc496550fb7"}},"metadata":{}},{"name":"stdout","text":"{'rouge1': 0.40551666036183737, 'rouge2': 0.22058954375166137, 'rougeL': 0.31871927831710123, 'rougeLsum': 0.3651192863596151}\n","output_type":"stream"}],"execution_count":9}]}