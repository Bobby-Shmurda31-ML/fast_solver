{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install transformers>=4.38.0 trl>=0.8.0 peft>=0.9.0 bitsandbytes evaluate\n\nimport evaluate\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nimport torch\nfrom trl import SFTTrainer\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:14:35.184878Z","iopub.execute_input":"2025-11-16T13:14:35.185441Z","iopub.status.idle":"2025-11-16T13:16:32.431795Z","shell.execute_reply.started":"2025-11-16T13:14:35.185413Z","shell.execute_reply":"2025-11-16T13:16:32.431194Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-16 13:16:12.110438: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763298972.330903      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763298972.391427      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"checkpoint = 'Qwen/Qwen2.5-1.5B-Instruct'\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'left'\n\ndef concat_question_and_context(context, question):\n    prompt = f\"\"\"Context:\n{context}\n\nQuestion:\n{question}\n\nAnswer the question very briefly:\n\"\"\"\n    return prompt\n\ndef template_processing(question, answer=None):\n    if answer is None:\n        return tokenizer.apply_chat_template(\n            [{'role': 'user', 'content': question}],\n            tokenize=False,\n            add_generation_promt=True\n        )\n    else:\n        return tokenizer.apply_chat_template(\n            [{'role': 'user', 'content': question},\n             {'role': 'assistant', 'content': answer}],\n            tokenize=False\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:26:06.325867Z","iopub.execute_input":"2025-11-16T14:26:06.326701Z","iopub.status.idle":"2025-11-16T14:26:06.787531Z","shell.execute_reply.started":"2025-11-16T14:26:06.326674Z","shell.execute_reply":"2025-11-16T14:26:06.786697Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"dataset = load_dataset(\"squad\", split=\"train[:1000]\").remove_columns(['id', 'title'])\n\ndataset = dataset.train_test_split(test_size=0.2, seed=42, shuffle=True)\ndataset['train'] = dataset['train'].train_test_split(test_size=0.2, seed=42, shuffle=True)\ndataset['validation'] = dataset['train']['test']\ndataset['train'] = dataset['train']['train']\n\ndataset = dataset.map(\n    lambda samples: {'question': [concat_question_and_context(context, question)\n                                  for context, question in zip(samples['context'], samples['question'])]},\n    batched=True, batch_size=1000\n).remove_columns('context')\n\ndataset = dataset.map(\n    lambda samples: {'answer': [answers['text'][-1] for answers in samples['answers']]},\n    batched=True, batch_size=1000\n).remove_columns('answers')\n\ntest_data = dataset['test']\n\ndataset = dataset.map(\n    lambda samples: {'text': [template_processing(question, answer)\n                              for question, answer in zip(samples['question'], samples['answer'])]},\n    batched=True, batch_size=1000\n).remove_columns(['question', 'answer'])\n\ntrain_dataset, val_dataset = dataset['train'], dataset['validation']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:18:23.415143Z","iopub.execute_input":"2025-11-16T14:18:23.415410Z","iopub.status.idle":"2025-11-16T14:18:24.354142Z","shell.execute_reply.started":"2025-11-16T14:18:23.415391Z","shell.execute_reply":"2025-11-16T14:18:24.353561Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_type=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\n    quantization_config=bnb_config,\n    device_map='auto'\n)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout=0.05,\n    bias='none',\n    task_type='CAUSAL_LM'\n)\nargs = TrainingArguments(\n    gradient_accumulation_steps=16,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=2,\n    output_dir='./result',\n    report_to='none',\n    learning_rate=1.5e-4,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.1,\n    logging_strategy='steps',\n    eval_strategy='steps',\n    save_strategy='steps',\n    logging_steps=25,\n    eval_steps=25,\n    save_steps=100,\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_mean_token_accuracy',\n    greater_is_better=True,\n    num_train_epochs=1,\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=4,\n    eval_on_start=True\n)\ntrainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=args,\n    peft_config=lora_config\n)\n# trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:26:09.014664Z","iopub.execute_input":"2025-11-16T14:26:09.014944Z","iopub.status.idle":"2025-11-16T14:26:13.792815Z","shell.execute_reply.started":"2025-11-16T14:26:09.014924Z","shell.execute_reply":"2025-11-16T14:26:13.792019Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"trainer.model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:25:42.697643Z","iopub.execute_input":"2025-11-16T14:25:42.698446Z","iopub.status.idle":"2025-11-16T14:25:42.717416Z","shell.execute_reply.started":"2025-11-16T14:25:42.698412Z","shell.execute_reply":"2025-11-16T14:25:42.716556Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 896)\n        (layers): ModuleList(\n          (0-23): 24 x Qwen2DecoderLayer(\n            (self_attn): Qwen2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=896, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=896, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=896, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=896, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=128, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=896, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=128, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=896, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=896, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=896, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n              (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n              (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":107},{"cell_type":"code","source":"import gc\n\nprint(gc.collect())\n\nfor i in range(torch.cuda.device_count()):\n    with torch.cuda.device(i):\n        torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:25:42.718344Z","iopub.execute_input":"2025-11-16T14:25:42.718663Z","iopub.status.idle":"2025-11-16T14:25:43.293829Z","shell.execute_reply.started":"2025-11-16T14:25:42.718638Z","shell.execute_reply":"2025-11-16T14:25:43.292871Z"}},"outputs":[{"name":"stdout","text":"7269\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"def normalize_response(text):\n    return text.replace('user\\n', '')\n\ndef generate_response(\n    model, tokenizer, prompts,\n    temperature=0.7, top_p=0.9, repetition_penalty=1.3, \n    length_penalty=1.0, num_beams=4, num_return_sequences=1, max_new_tokens=256,\n    *args, **kwargs\n):\n    inputs = tokenizer(\n        prompts,\n        return_tensors='pt',\n        padding=True,\n        truncation=True, \n        max_length=512\n    ).to(model.device)\n\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model.generate(\n            *args,\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            do_sample=True,\n            repetition_penalty=repetition_penalty,\n            length_penalty=length_penalty,\n            num_beams=max(num_beams, num_return_sequences),\n            num_return_sequences=num_return_sequences,\n            **kwargs\n        )\n\n    results = []\n    for i in range(len(prompts)):\n        prompt_results = []\n        for j in range(num_return_sequences):\n            idx = i * num_return_sequences + j\n            text = tokenizer.decode(outputs[idx][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n            prompt_results.append(normalize_response(text))\n        results.append(prompt_results)\n\n    return results\n\nrouge = evaluate.load('rouge')\ndef evaluate_model(model, tokenizer, val_data, batch_size=8, *args, **kwargs):\n    predictions = []\n\n    for i in tqdm(range(0, len(val_data), batch_size)):\n        batch = [val_data[j] for j in range(i, min(i + batch_size, len(val_data)))]\n        batch_predictions = generate_response(model, tokenizer, [template_processing(item['question']) for item in batch], *args, **kwargs)\n        batch_predictions = np.array(batch_predictions).ravel().tolist()\n        predictions.extend(batch_predictions)\n\n    references = [item['answer'] for item in val_data]\n\n    return rouge.compute(predictions=predictions, references=references), predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:25:43.295677Z","iopub.execute_input":"2025-11-16T14:25:43.295955Z","iopub.status.idle":"2025-11-16T14:25:43.811477Z","shell.execute_reply.started":"2025-11-16T14:25:43.295936Z","shell.execute_reply":"2025-11-16T14:25:43.810630Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"generated_text = generate_response(\n    trainer.model.half(), tokenizer,\n    [template_processing(test_data['question'][0])],\n    num_return_sequences=10,\n    num_beams=4,\n    temperature=0.8,\n    top_k=10,\n    top_p=0.9,\n    max_new_tokens=40\n)\nfor text in generated_text[0]:\n    print(text)\n    print('#########################')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:28:16.721016Z","iopub.execute_input":"2025-11-16T14:28:16.721625Z","iopub.status.idle":"2025-11-16T14:28:21.904206Z","shell.execute_reply.started":"2025-11-16T14:28:16.721593Z","shell.execute_reply":"2025-11-16T14:28:21.903299Z"}},"outputs":[{"name":"stdout","text":"Brazil\nBrazil\n#########################\nBrazil\n#########################\nTo what country did Beyoncé release Dereon in 2010?\nBrazil\n#########################\nTo what country did Beyoncé release Dereon in 2010?\nBrazil.\n#########################\nBeyoncé released Dereon to Brazil.\nBrazil is a country in South America.\n#########################\nBeyoncé released Dereon to Brazil.\n#########################\nTo what country did Beyoncé release Dereon in 2010?\n#########################\nBeyoncé released Dereon to Brazil.\nBrazil.\n#########################\nBeyoncé released Dereon at C&A stores in Brazil.\n#########################\nBrazil\nBrazil.\n#########################\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"scores, preds = evaluate_model(trainer.model.half(), tokenizer, test_data.select(range(20)), batch_size=1, num_beams=2)\n\nprint(scores)\nfor pred in preds:\n    print('\\n########################\\n')\n    print(pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:26:18.967702Z","iopub.execute_input":"2025-11-16T14:26:18.967973Z","iopub.status.idle":"2025-11-16T14:27:19.998924Z","shell.execute_reply.started":"2025-11-16T14:26:18.967950Z","shell.execute_reply":"2025-11-16T14:27:19.998068Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8303365ea0a94305b5e04cada22721d2"}},"metadata":{}},{"name":"stdout","text":"{'rouge1': 0.6996379233511586, 'rouge2': 0.20445526695526695, 'rougeL': 0.7038645790484026, 'rougeLsum': 0.7007873988756341}\n\n########################\n\nBrazil\n\n########################\n\nEmporio Armani's Diamonds\n\n########################\n\n2014\n\n########################\n\n2002\n\n########################\n\nSaint Mary's College\n\n########################\n\nBeyoncé described a miscarriage in 2010 or 2011 as the saddest thing she had ever endured.\n\n########################\n\nThe Guardian named Beyoncé the Artist of the Decade.\n\n########################\n\nAirplane!\n\n########################\n\nThe annual budget of Notre Dame's LaFortune Center is $1.2 million.\n\n########################\n\nPeople\n\n########################\n\nBeyoncé sang \"America the Beautiful\" during the 2009 presidential inauguration.\n\n########################\n\nIn 2005\n\n########################\n\nTidal\n\n########################\n\nParkwood Topshop Athletic Ltd\n\n########################\n\n5 Grammy Awards\nBeyoncé won 5 Grammy Awards for her first solo album \"Dangerously in Love\" (2003).\n\n########################\n\nApril 15, 2011\n\n########################\n\nObsessed\n\n########################\n\nMathew Knowles managed the Destiny's Child group.\n\n########################\n\nProfessor Jerome Green sent the first wireless message in the USA around 1899.\n\n########################\n\n1950\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"# import re\n# import numpy as np\n# from scipy.optimize import differential_evolution\n# from scipy.stats import spearmanr\n# import pickle\n# from pathlib import Path\n# from tqdm.auto import tqdm\n# from collections import Counter\n# import evaluate\n# import torch\n# from rouge_score import rouge_scorer\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\n# class UniversalTextReRanker:\n#     \"\"\"универсальный re-ranker для генераций текста\"\"\"\n    \n#     def __init__(\n#         self,\n#         use_coverage=False,\n#         use_compression_ratio=False,\n#         use_extractive_coverage=False,\n#         use_fluency=False,\n#         use_grammar=False,\n#         use_length_simple=False,\n#         use_lexical_diversity=False,\n#         use_repetition_penalty=False,\n#         use_rouge_with_source=False,\n#         use_semantic_coherence=False,\n#         coverage_params=None,\n#         compression_params=None,\n#         length_params=None,\n#         weights=None,\n#         device='cuda' if torch.cuda.is_available() else 'cpu',\n#         cache_dir='./reranker_cache'\n#     ):\n#         self.use_coverage = use_coverage\n#         self.use_compression_ratio = use_compression_ratio\n#         self.use_extractive_coverage = use_extractive_coverage\n#         self.use_fluency = use_fluency\n#         self.use_grammar = use_grammar\n#         self.use_length_simple = use_length_simple\n#         self.use_lexical_diversity = use_lexical_diversity\n#         self.use_repetition_penalty = use_repetition_penalty\n#         self.use_rouge_with_source = use_rouge_with_source\n#         self.use_semantic_coherence = use_semantic_coherence\n        \n#         self.coverage_params = coverage_params or {'concepts_key': 'concepts'}\n#         self.compression_params = compression_params or {'optimal_ratio': 0.15, 'sigma': 0.05}\n#         self.length_params = length_params or {'target_min': 10, 'target_max': 20}\n        \n#         self.weights = weights or self._get_default_weights()\n#         self.device = device\n#         self.cache_dir = Path(cache_dir)\n#         self.cache_dir.mkdir(exist_ok=True)\n        \n#         # ленивая инициализация\n#         self._rouge = None\n#         self._fluency_model = None\n#         self._fluency_tokenizer = None\n    \n#     def __getstate__(self):\n#         state = self.__dict__.copy()\n#         state['_rouge'] = None\n#         state['_fluency_model'] = None\n#         state['_fluency_tokenizer'] = None\n#         return state\n    \n#     def __setstate__(self, state):\n#         self.__dict__.update(state)\n    \n#     def _get_default_weights(self):\n#         \"\"\"равномерное распределение весов для активных метрик\"\"\"\n#         active_metrics = self._get_active_metrics()\n#         if not active_metrics:\n#             return {}\n#         weight = 1.0 / len(active_metrics)\n#         return {metric: weight for metric in active_metrics}\n    \n#     def _get_active_metrics(self):\n#         \"\"\"список активных метрик\"\"\"\n#         metrics = []\n#         if self.use_coverage:\n#             metrics.append('coverage')\n#         if self.use_compression_ratio:\n#             metrics.append('compression_ratio')\n#         if self.use_extractive_coverage:\n#             metrics.append('extractive_coverage')\n#         if self.use_fluency:\n#             metrics.append('fluency')\n#         if self.use_grammar:\n#             metrics.append('grammar')\n#         if self.use_length_simple:\n#             metrics.append('length_simple')\n#         if self.use_lexical_diversity:\n#             metrics.append('lexical_diversity')\n#         if self.use_repetition_penalty:\n#             metrics.append('repetition_penalty')\n#         if self.use_rouge_with_source:\n#             metrics.append('rouge_with_source')\n#         if self.use_semantic_coherence:\n#             metrics.append('semantic_coherence')\n#         return metrics\n    \n#     # def _load_rouge(self):\n#     #     if self._rouge is None:\n#     #         self._rouge = evaluate.load('rouge')\n#     #     return self._rouge\n\n#     def _load_rouge_scorer(self):\n#         \"\"\"быстрая альтернатива evaluate.load('rouge')\"\"\"\n#         if not hasattr(self, '_rouge_scorer'):\n#             self._rouge_scorer = rouge_scorer.RougeScorer(['rouge2', 'rougeL'], use_stemmer=True)\n#         return self._rouge_scorer\n    \n#     def _load_fluency_model(self):\n#         if self._fluency_model is None:\n#             print(\"загрузка модели для fluency (GPT-2)...\")\n#             self._fluency_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n#             self._fluency_model = AutoModelForCausalLM.from_pretrained('gpt2').to(self.device)\n#             self._fluency_model.eval()\n#         return self._fluency_model, self._fluency_tokenizer\n    \n#     @staticmethod\n#     def _get_ngrams_fast(text, n):\n#         \"\"\"быстрое извлечение n-грамм\"\"\"\n#         words = text.lower().split()\n#         return Counter([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])\n    \n#     def _compute_coverage(self, candidates, context):\n#         \"\"\"\n#         покрытие обязательных элементов (слов/концептов)\n        \n#         применение:\n#         - commongen: все ли слова из списка использованы\n#         - keyword-to-text: присутствуют ли ключевые слова\n        \n#         значения: 0.0 (нет покрытия) - 1.0 (все элементы использованы)\n        \n#         args:\n#             context: str - слова/концепты через запятую или пробел\n#                      например: \"dog, run, park\" или \"dog run park\"\n#         \"\"\"\n#         if context is None:\n#             return [0.0] * len(candidates)\n        \n#         # парсим слова из строки\n#         # убираем запятые и разбиваем по пробелам\n#         words = context.replace(',', '').lower().split()\n        \n#         if not words:\n#             return [0.0] * len(candidates)\n        \n#         scores = []\n#         for candidate in candidates:\n#             candidate_lower = candidate.lower()\n#             covered = sum(1 for word in words if word in candidate_lower)\n#             score = covered / len(words)\n#             scores.append(score)\n        \n#         return scores\n\n#     def _compute_compression_ratio(self, candidates, source_text):\n#         \"\"\"\n#         соответствие целевому коэффициенту сжатия\n        \n#         применение:\n#         - суммаризация: контроль длины summary относительно источника\n#         - compression tasks\n        \n#         параметры (compression_params):\n#         - optimal_ratio: целевое отношение длин (например, 0.15 = 15% от оригинала)\n#         - sigma: допустимое отклонение\n        \n#         метод: gaussian penalty относительно оптимального ratio\n        \n#         значения: 0.0 (далеко от целевого) - 1.0 (оптимальное сжатие)\n#         \"\"\"\n#         if source_text is None:\n#             return [0.0] * len(candidates)\n        \n#         source_len = len(source_text.split())\n#         optimal = self.compression_params['optimal_ratio']\n#         sigma = self.compression_params['sigma']\n        \n#         scores = []\n#         for candidate in candidates:\n#             cand_len = len(candidate.split())\n#             ratio = cand_len / source_len if source_len > 0 else 0\n#             score = np.exp(-((ratio - optimal) ** 2) / (2 * sigma ** 2))\n#             scores.append(score)\n        \n#         return scores\n    \n#     def _compute_extractive_coverage(self, candidates, source_text):\n#         \"\"\"\n#         покрытие n-грамм из исходного текста\n        \n#         применение:\n#         - extractive summarization\n#         - faithful generation: генерация должна основываться на источнике\n#         - qa: ответ должен содержать фразы из контекста\n        \n#         метод: процент n-грамм кандидата, присутствующих в источнике\n        \n#         значения: 0.0 (ничего не извлечено) - 1.0 (все из источника)\n#         \"\"\"\n#         if source_text is None:\n#             return [0.0] * len(candidates)\n        \n#         source_ngrams = {\n#             1: self._get_ngrams_fast(source_text, 1),\n#             2: self._get_ngrams_fast(source_text, 2),\n#             3: self._get_ngrams_fast(source_text, 3),\n#             4: self._get_ngrams_fast(source_text, 4),\n#         }\n        \n#         scores = []\n#         for candidate in candidates:\n#             ngram_scores = []\n#             for n in [1, 2, 3, 4]:\n#                 cand_ngrams = self._get_ngrams_fast(candidate, n)\n#                 if not cand_ngrams:\n#                     continue\n#                 overlap = sum((cand_ngrams & source_ngrams[n]).values())\n#                 total = sum(cand_ngrams.values())\n#                 ngram_scores.append(overlap / total)\n#             score = np.mean(ngram_scores) if ngram_scores else 0.0\n#             scores.append(score)\n        \n#         return scores\n    \n#     def _compute_fluency(self, candidates):\n#         \"\"\"\n#         беглость (naturalness) текста на основе perplexity\n        \n#         применение:\n#         - все задачи генерации (универсальная метрика)\n#         - commongen, диалоги, перефразирование\n        \n#         метод: используется языковая модель (gpt-2) для оценки вероятности текста\n#         низкий perplexity = высокая fluency = естественный текст\n        \n#         значения: 0.0 (плохо) - 1.0 (отлично)\n#         \"\"\"\n#         model, tokenizer = self._load_fluency_model()\n        \n#         scores = []\n#         for candidate in candidates:\n#             inputs = tokenizer(candidate, return_tensors='pt', truncation=True, max_length=512)\n#             inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            \n#             with torch.no_grad():\n#                 outputs = model(**inputs, labels=inputs['input_ids'])\n#                 loss = outputs.loss\n            \n#             perplexity = torch.exp(loss).item()\n#             # нормализуем: хороший текст имеет perplexity 20-50\n#             fluency_score = 1.0 / (1.0 + perplexity / 30.0)\n#             scores.append(fluency_score)\n        \n#         return scores\n    \n#     def _compute_grammar(self, candidates):\n#         \"\"\"\n#         грамматическая корректность\n        \n#         применение:\n#         - все задачи генерации\n#         - особенно важно для образовательных приложений\n        \n#         метод: простая эвристика - проверка базовых паттернов\n#         для продакшна лучше использовать специализированные модели (languagetool, grammarbot)\n        \n#         значения: 0.0 (много ошибок) - 1.0 (нет ошибок)\n#         \"\"\"\n#         scores = []\n#         for candidate in candidates:\n#             score = 1.0\n            \n#             # начинается с заглавной буквы\n#             if candidate and not candidate[0].isupper():\n#                 score -= 0.2\n            \n#             # заканчивается на точку/!/?\n#             if candidate and not candidate.rstrip()[-1] in '.!?':\n#                 score -= 0.2\n            \n#             # нет двойных пробелов\n#             if '  ' in candidate:\n#                 score -= 0.1\n            \n#             # баланс кавычек\n#             if candidate.count('\"') % 2 != 0:\n#                 score -= 0.1\n            \n#             # слишком короткое предложение\n#             if len(candidate.split()) < 3:\n#                 score -= 0.3\n            \n#             scores.append(max(0.0, score))\n        \n#         return scores\n    \n#     def _compute_length_simple(self, candidates):\n#         \"\"\"\n#         соответствие целевой длине текста\n        \n#         применение:\n#         - commongen: предложения должны быть 10-20 слов\n#         - заголовки: 5-10 слов\n#         - твиты: до 280 символов\n#         - суммаризация: зависит от задачи\n        \n#         параметры (length_params):\n#         - target_min: минимальная желаемая длина\n#         - target_max: максимальная желаемая длина\n        \n#         значения: 0.0 (слишком короткое/длинное) - 1.0 (оптимальная длина)\n#         \"\"\"\n#         target_min = self.length_params['target_min']\n#         target_max = self.length_params['target_max']\n        \n#         scores = []\n#         for candidate in candidates:\n#             words = len(candidate.split())\n            \n#             if target_min <= words <= target_max:\n#                 score = 1.0\n#             elif words < target_min:\n#                 score = words / target_min\n#             else:\n#                 # штраф за превышение\n#                 score = max(0, 1 - (words - target_max) / (target_max * 0.5))\n            \n#             scores.append(score)\n        \n#         return scores\n    \n#     def _compute_lexical_diversity(self, candidates):\n#         \"\"\"\n#         лексическое разнообразие (богатство словаря)\n        \n#         применение:\n#         - creative writing\n#         - диалоги: избегать однообразных ответов\n#         - генерация вариаций текста\n        \n#         метод: type-token ratio (ttr) и вариации\n        \n#         значения: 0.0 (бедный словарь) - 1.0 (богатый словарь)\n#         \"\"\"\n#         scores = []\n#         for candidate in candidates:\n#             words = candidate.lower().split()\n            \n#             if len(words) < 5:\n#                 scores.append(0.5)\n#                 continue\n            \n#             # type-token ratio\n#             unique_words = len(set(words))\n#             total_words = len(words)\n#             ttr = unique_words / total_words\n            \n#             # нормализуем относительно длины\n#             normalized_diversity = min(1.0, ttr * (1 + np.log(total_words) / 5))\n#             scores.append(normalized_diversity)\n        \n#         return scores\n    \n#     def _compute_repetition_penalty(self, candidates):\n#         \"\"\"\n#         штраф за повторяющиеся слова и фразы\n        \n#         применение:\n#         - все задачи генерации\n#         - особенно для борьбы с вырожденными генерациями (llm bugs)\n        \n#         метод: анализирует повторы unigrams, bigrams, trigrams\n        \n#         значения: 0.0 (много повторов) - 1.0 (нет повторов)\n#         \"\"\"\n#         scores = []\n#         for candidate in candidates:\n#             words = candidate.lower().split()\n            \n#             if len(words) < 3:\n#                 scores.append(1.0)\n#                 continue\n            \n#             # unigram repetition\n#             unique_words = len(set(words))\n#             total_words = len(words)\n#             unigram_diversity = unique_words / total_words if total_words > 0 else 0\n            \n#             # bigram repetition\n#             bigrams = self._get_ngrams_fast(candidate, 2)\n#             unique_bigrams = len(bigrams)\n#             total_bigrams = sum(bigrams.values())\n#             bigram_diversity = unique_bigrams / total_bigrams if total_bigrams > 0 else 0\n            \n#             # комбинированный скор\n#             score = 0.5 * unigram_diversity + 0.5 * bigram_diversity\n#             scores.append(score)\n        \n#         return scores\n    \n#     def _compute_rouge_with_source(self, candidates, source_text):\n#         \"\"\"\n#         rouge overlap с исходным текстом\n        \n#         применение:\n#         - суммаризация (extractive)\n#         - перефразирование: должно быть высокое совпадение\n#         - compression tasks\n        \n#         метод: rouge-l (longest common subsequence)\n        \n#         значения: 0.0 (нет совпадений) - 1.0 (полное совпадение)\n#         \"\"\"\n#         if source_text is None:\n#             return [0.0] * len(candidates)\n        \n#         rouge = self._load_rouge()\n#         scores = []\n        \n#         for candidate in candidates:\n#             result = rouge.compute(\n#                 predictions=[candidate],\n#                 references=[source_text],\n#                 rouge_types=['rougeL']\n#             )\n#             scores.append(result['rougeL'])\n        \n#         return scores\n    \n#     def _compute_semantic_coherence(self, candidates):\n#         \"\"\"\n#         семантическая связность текста\n        \n#         применение:\n#         - длинные генерации (абзацы, статьи)\n#         - диалоги: логичность ответа\n#         - story generation\n        \n#         метод (упрощенный): анализ переходов между предложениями,\n#         наличие связующих слов, структура\n        \n#         для продакшна: используйте sentence embeddings и косинусное сходство\n#         между последовательными предложениями\n        \n#         значения: 0.0 (несвязный текст) - 1.0 (хорошая связность)\n#         \"\"\"\n#         scores = []\n#         for candidate in candidates:\n#             # разбиваем на предложения\n#             sentences = re.split(r'[.!?]+', candidate)\n#             sentences = [s.strip() for s in sentences if s.strip()]\n            \n#             if len(sentences) <= 1:\n#                 scores.append(1.0)  # одно предложение - всегда связно\n#                 continue\n            \n#             score = 1.0\n            \n#             # есть ли связующие слова\n#             connectives = [\n#                 'however', 'therefore', 'moreover', 'furthermore',\n#                 'additionally', 'consequently', 'thus', 'hence',\n#                 'because', 'since', 'although', 'while', 'but', 'and'\n#             ]\n            \n#             candidate_lower = candidate.lower()\n#             has_connectives = any(conn in candidate_lower for conn in connectives)\n            \n#             if len(sentences) > 2 and not has_connectives:\n#                 score -= 0.2\n            \n#             # слишком резкие переходы (эвристика)\n#             lengths = [len(s.split()) for s in sentences]\n#             if len(lengths) > 1:\n#                 length_variance = np.std(lengths) / (np.mean(lengths) + 1e-6)\n#                 if length_variance > 1.5:\n#                     score -= 0.2\n            \n#             scores.append(max(0.0, score))\n        \n#         return scores\n    \n#     def compute_batch(self, candidates, context=None):\n#         \"\"\"\n#         вычислить все метрики для батча кандидатов\n        \n#         args:\n#             candidates: list of str - генерации для ранжирования\n#             context: str или None - контекст для метрик\n#                 - для commongen: \"dog, run, park\" (слова через запятую)\n#                 - для суммаризации: исходная статья\n#                 - для диалогов: история диалога\n        \n#         returns:\n#             list of dict: [{metric_name: score}, ...]\n#         \"\"\"\n#         if not candidates:\n#             return []\n        \n#         all_scores = {}\n        \n#         # все метрики теперь работают со строкой\n#         if self.use_coverage:\n#             all_scores['coverage'] = self._compute_coverage(candidates, context)\n        \n#         if self.use_compression_ratio:\n#             all_scores['compression_ratio'] = self._compute_compression_ratio(candidates, context)\n        \n#         if self.use_extractive_coverage:\n#             all_scores['extractive_coverage'] = self._compute_extractive_coverage(candidates, context)\n        \n#         if self.use_fluency:\n#             all_scores['fluency'] = self._compute_fluency(candidates)\n        \n#         if self.use_grammar:\n#             all_scores['grammar'] = self._compute_grammar(candidates)\n        \n#         if self.use_length_simple:\n#             all_scores['length_simple'] = self._compute_length_simple(candidates)\n        \n#         if self.use_lexical_diversity:\n#             all_scores['lexical_diversity'] = self._compute_lexical_diversity(candidates)\n        \n#         if self.use_repetition_penalty:\n#             all_scores['repetition_penalty'] = self._compute_repetition_penalty(candidates)\n        \n#         if self.use_rouge_with_source:\n#             all_scores['rouge_with_source'] = self._compute_rouge_with_source(candidates, source_text)\n        \n#         if self.use_semantic_coherence:\n#             all_scores['semantic_coherence'] = self._compute_semantic_coherence(candidates)\n        \n#         # агрегируем результаты\n#         results = []\n#         for i in range(len(candidates)):\n#             candidate_scores = {metric: all_scores[metric][i] for metric in all_scores}\n#             results.append(candidate_scores)\n        \n#         return results\n    \n#     def compute(self, text, context=None, return_individual=True, return_weighted=True):\n#         \"\"\"вычислить метрики для одного текста\"\"\"\n#         batch_results = self.compute_batch([text], context)\n#         scores = batch_results[0]\n        \n#         result = {}\n#         if return_individual:\n#             result['scores'] = scores\n        \n#         if return_weighted:\n#             weighted_score = sum(scores[m] * self.weights.get(m, 0) for m in scores)\n#             result['weighted_score'] = weighted_score\n        \n#         return result\n    \n#     def rank_candidates(self, candidates, context=None):\n#         \"\"\"\n#         ранжировать кандидатов по взвешенному скору\n        \n#         returns:\n#             list of tuple: [(idx, weighted_score, individual_scores), ...]\n#             отсортировано по убыванию weighted_score\n#         \"\"\"\n#         if not candidates:\n#             return []\n        \n#         batch_scores = self.compute_batch(candidates, context)\n        \n#         results = []\n#         for idx, scores in enumerate(batch_scores):\n#             weighted_score = sum(scores[m] * self.weights.get(m, 0) for m in scores)\n#             results.append((idx, weighted_score, scores))\n        \n#         results.sort(key=lambda x: x[1], reverse=True)\n#         return results\n    \n#     def get_best_candidate(self, candidates, context=None):\n#         \"\"\"получить лучшего кандидата\"\"\"\n#         results = self.rank_candidates(candidates, context)\n#         if not results:\n#             return None\n#         best_idx = results[0][0]\n#         return candidates[best_idx]\n    \n#     def fit(\n#         self,\n#         candidates_list,\n#         contexts,\n#         y_texts,\n#         metric='rouge2',\n#         cache_name=None,\n#         use_cache=True,\n#         max_iter=50,\n#         popsize=15,\n#         seed=42,\n#         n_workers=1,\n#         print_correlations=True\n#     ):\n#         \"\"\"\n#         оптимизировать веса метрик на основе целевой метрики\n        \n#         args:\n#             candidates_list: list of list of str - для каждого примера список кандидатов\n#             contexts: list - контексты для каждого примера\n#             y_texts: list of str - референсные тексты\n#             metric: str - целевая метрика для оптимизации ('rouge2', 'rougeL', etc.)\n        \n#         returns:\n#             dict, float: оптимальные веса и достигнутый скор\n#         \"\"\"\n#         if len(candidates_list) != len(contexts) or len(candidates_list) != len(y_texts):\n#             raise ValueError(\n#                 f\"length mismatch: candidates_list={len(candidates_list)}, \"\n#                 f\"contexts={len(contexts)}, y_texts={len(y_texts)}\"\n#             )\n        \n#         # предвычисление метрик\n#         precomputed_metrics = self._load_or_compute_metrics(\n#             candidates_list, contexts, cache_name, use_cache\n#         )\n        \n#         # предвычисление целевой метрики\n#         print(f\"предвычисление целевой метрики ({metric})...\")\n#         target_scores_cache = self._precompute_target_scores(\n#             candidates_list, y_texts, metric\n#         )\n        \n#         metric_names = list(precomputed_metrics[0][0].keys())\n#         n_metrics = len(metric_names)\n        \n#         print(f\"\\nактивные метрики: {metric_names}\")\n#         print(f\"целевая метрика: {metric}\")\n        \n#         if print_correlations:\n#             self._print_correlations_fast(\n#                 precomputed_metrics, target_scores_cache,\n#                 metric_names, n_samples=min(50, len(candidates_list))\n#             )\n        \n#         # оптимизация\n#         best_weights, best_score = self._optimize_weights_fast(\n#             candidates_list, precomputed_metrics, target_scores_cache,\n#             metric_names, max_iter, popsize, seed, n_workers\n#         )\n        \n#         self.weights = {\n#             metric_names[i]: best_weights[i]\n#             for i in range(n_metrics)\n#         }\n        \n#         print(\"\\n\" + \"=\"*60)\n#         print(\"РЕЗУЛЬТАТЫ КАЛИБРОВКИ\")\n#         print(\"=\"*60)\n#         print(f\"best {metric}: {best_score:.4f}\")\n#         print(f\"\\nоптимальные веса:\")\n#         for name, weight in sorted(self.weights.items(), key=lambda x: x[1], reverse=True):\n#             print(f\"  {name:25s}: {weight:.4f}\")\n#         print(\"=\"*60)\n        \n#         return self.weights, best_score\n    \n#     def _load_or_compute_metrics(self, candidates_list, contexts, cache_name, use_cache):\n#         cache_path = None\n#         if cache_name and use_cache:\n#             cache_path = self.cache_dir / f\"{cache_name}_metrics.pkl\"\n        \n#         if cache_path and cache_path.exists():\n#             print(f\"загрузка метрик из кэша: {cache_path}\")\n#             with open(cache_path, 'rb') as f:\n#                 return pickle.load(f)\n        \n#         print(\"предвычисление метрик...\")\n#         precomputed = []\n        \n#         for candidates, context in tqdm(list(zip(candidates_list, contexts)), desc=\"computing metrics\"):\n#             candidate_metrics = self.compute_batch(candidates, context)\n#             precomputed.append(candidate_metrics)\n        \n#         if cache_path:\n#             with open(cache_path, 'wb') as f:\n#                 pickle.dump(precomputed, f)\n#             print(f\"метрики сохранены в кэш: {cache_path}\")\n        \n#         return precomputed\n    \n#     def _precompute_target_scores(self, candidates_list, y_texts, metric='rouge2'):\n#         scorer = self._load_rouge_scorer()\n#         target_scores = []\n        \n#         for candidates, y_text in tqdm(zip(candidates_list, y_texts),\n#                                        total=len(candidates_list),\n#                                        desc=\"target metric\"):\n#             candidate_scores = []\n#             for candidate in candidates:\n#                 scores = scorer.score(y_text, candidate)\n#                 # rouge_score возвращает .fmeasure\n#                 candidate_scores.append(scores[metric].fmeasure)\n            \n#             target_scores.append(candidate_scores)\n        \n#         return target_scores\n\n#     def _print_correlations_fast(self, precomputed_metrics, target_scores_cache,\n#                                  metric_names, n_samples=30):\n#         print(\"\\n\" + \"=\"*60)\n#         print(\"КОРРЕЛЯЦИЯ МЕТРИК С ЦЕЛЕВОЙ МЕТРИКОЙ (spearman)\")\n#         print(\"=\"*60)\n        \n#         metric_spearman = {name: [] for name in metric_names}\n        \n#         for i in range(min(n_samples, len(precomputed_metrics))):\n#             target_values = target_scores_cache[i]\n            \n#             for metric_name in metric_names:\n#                 metric_values = [\n#                     precomputed_metrics[i][j][metric_name]\n#                     for j in range(len(precomputed_metrics[i]))\n#                 ]\n                \n#                 if np.std(metric_values) > 1e-10 and np.std(target_values) > 1e-10:\n#                     spearman, _ = spearmanr(metric_values, target_values)\n#                     metric_spearman[metric_name].append(spearman)\n        \n#         # сортируем по корреляции\n#         correlations = []\n#         for metric_name in metric_names:\n#             if metric_spearman[metric_name]:\n#                 avg_spearman = np.mean(metric_spearman[metric_name])\n#                 std_spearman = np.std(metric_spearman[metric_name])\n#                 correlations.append((metric_name, avg_spearman, std_spearman))\n        \n#         correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n        \n#         for metric_name, avg_spearman, std_spearman in correlations:\n#             if abs(avg_spearman) > 0.4:\n#                 status = \"✓✓✓ сильная\"\n#             elif abs(avg_spearman) > 0.25:\n#                 status = \"✓✓  средняя\"\n#             elif abs(avg_spearman) > 0.15:\n#                 status = \"✓   слабая\"\n#             else:\n#                 status = \"⚠️  очень слабая\"\n            \n#             print(f\"{status:20s} {metric_name:25s}: {avg_spearman:+.3f} (±{std_spearman:.3f})\")\n        \n#         print(\"=\"*60)\n    \n#     def _optimize_weights_fast(self, candidates_list, precomputed_metrics,\n#                                target_scores_cache, metric_names,\n#                                max_iter, popsize, seed, n_workers):\n#         n_metrics = len(metric_names)\n        \n#         # сохраняем для objective function\n#         self._opt_precomputed_metrics = precomputed_metrics\n#         self._opt_target_scores_cache = target_scores_cache\n#         self._opt_metric_names = metric_names\n#         self._opt_n_metrics = n_metrics\n        \n#         print(f\"\\nоптимизация весов (max_iter={max_iter}, popsize={popsize}, workers={n_workers})...\")\n        \n#         result = differential_evolution(\n#             self._objective_function_fast,\n#             bounds=[(-1, 1)] * n_metrics,\n#             strategy='best1bin',\n#             maxiter=max_iter,\n#             popsize=popsize,\n#             tol=0.001,\n#             mutation=(0.5, 1.5),\n#             recombination=0.7,\n#             seed=seed,\n#             workers=n_workers,\n#             updating='deferred' if n_workers > 1 else 'immediate',\n#             polish=True,\n#             disp=True\n#         )\n        \n#         # очистка\n#         del self._opt_precomputed_metrics\n#         del self._opt_target_scores_cache\n#         del self._opt_metric_names\n#         del self._opt_n_metrics\n        \n#         weights_raw = result.x\n#         weights_sum = np.sum(np.abs(weights_raw))\n        \n#         if weights_sum < 1e-10:\n#             print(f\"⚠️ warning: все веса близки к нулю, веса до нормализации: {weights_raw}\")\n#             weights_normalized = np.ones(n_metrics) / n_metrics\n#         else:\n#             weights_normalized = weights_raw / weights_sum\n        \n#         best_score = -result.fun\n#         return weights_normalized, best_score\n    \n#     def _objective_function_fast(self, weights):\n#         \"\"\"функция для оптимизации: максимизирует среднюю целевую метрику\"\"\"\n#         total_score = 0.0\n        \n#         for metrics_list, target_scores in zip(\n#             self._opt_precomputed_metrics,\n#             self._opt_target_scores_cache\n#         ):\n#             # вычисляем взвешенный скор для каждого кандидата\n#             weighted_scores = [\n#                 sum(weights[i] * metrics_dict.get(self._opt_metric_names[i], 0)\n#                     for i in range(self._opt_n_metrics))\n#                 for metrics_dict in metrics_list\n#             ]\n            \n#             # выбираем лучшего кандидата\n#             best_idx = np.argmax(weighted_scores)\n            \n#             # добавляем его целевую метрику\n#             total_score += target_scores[best_idx]\n        \n#         avg_score = total_score / len(self._opt_precomputed_metrics)\n#         return -avg_score  # минимизируем отрицательное значение\n    \n#     def save(self, path):\n#         \"\"\"сохранить re-ranker\"\"\"\n#         with open(path, 'wb') as f:\n#             pickle.dump(self, f)\n#         print(f\"re-ranker сохранен: {path}\")\n    \n#     @staticmethod\n#     def load(path):\n#         \"\"\"загрузить re-ranker\"\"\"\n#         with open(path, 'rb') as f:\n#             ranker = pickle.load(f)\n#         print(f\"re-ranker загружен: {path}\")\n#         return ranker","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:59:54.726602Z","iopub.execute_input":"2025-11-16T15:59:54.727331Z","iopub.status.idle":"2025-11-16T15:59:54.783296Z","shell.execute_reply.started":"2025-11-16T15:59:54.727302Z","shell.execute_reply":"2025-11-16T15:59:54.782469Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":159},{"cell_type":"code","source":"import re\nimport numpy as np\nfrom scipy.optimize import differential_evolution\nfrom scipy.stats import spearmanr\nimport pickle\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom collections import Counter\nimport torch\nfrom rouge_score import rouge_scorer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nclass MetricsComputer:\n    \"\"\"класс для вычисления метрик качества текста\"\"\"\n    \n    ALL_METRICS = [\n        'coverage',\n        'compression_ratio',\n        'extractive_coverage',\n        'fluency',\n        'grammar',\n        'length_simple',\n        'lexical_diversity',\n        'repetition_penalty',\n        'rouge_with_source',\n        'semantic_coherence'\n    ]\n    \n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.device = device\n        self._fluency_model = None\n        self._fluency_tokenizer = None\n        self._rouge_scorer = None\n    \n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state['_fluency_model'] = None\n        state['_fluency_tokenizer'] = None\n        state['_rouge_scorer'] = None\n        return state\n    \n    def __setstate__(self, state):\n        self.__dict__.update(state)\n    \n    def _load_rouge_scorer(self):\n        \"\"\"загрузка rouge scorer\"\"\"\n        if self._rouge_scorer is None:\n            self._rouge_scorer = rouge_scorer.RougeScorer(\n                ['rouge2', 'rougeL'], \n                use_stemmer=True\n            )\n        return self._rouge_scorer\n    \n    def _load_fluency_model(self):\n        \"\"\"загрузка модели для fluency\"\"\"\n        if self._fluency_model is None:\n            print(\"загрузка модели для fluency (gpt-2)...\")\n            self._fluency_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n            self._fluency_model = AutoModelForCausalLM.from_pretrained('gpt2').to(self.device)\n            self._fluency_model.eval()\n        return self._fluency_model, self._fluency_tokenizer\n    \n    @staticmethod\n    def _get_ngrams(text, n):\n        \"\"\"извлечение n-грамм\"\"\"\n        words = text.lower().split()\n        return Counter([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])\n    \n    def compute_coverage(self, candidates, context, params=None):\n        \"\"\"\n        покрытие обязательных элементов\n        \n        применение:\n        - commongen: все ли слова использованы\n        - keyword-to-text: присутствуют ли ключевые слова\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        if context is None:\n            return [0.0] * len(candidates)\n        \n        words = context.replace(',', '').lower().split()\n        if not words:\n            return [0.0] * len(candidates)\n        \n        scores = []\n        for candidate in candidates:\n            candidate_lower = candidate.lower()\n            covered = sum(1 for word in words if word in candidate_lower)\n            scores.append(covered / len(words))\n        \n        return scores\n    \n    def compute_compression_ratio(self, candidates, source_text, params=None):\n        \"\"\"\n        соответствие целевому коэффициенту сжатия\n        \n        применение:\n        - суммаризация: контроль длины\n        - compression tasks\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        if source_text is None:\n            return [0.0] * len(candidates)\n        \n        params = params or {'optimal_ratio': 0.15, 'sigma': 0.05}\n        source_len = len(source_text.split())\n        optimal = params['optimal_ratio']\n        sigma = params['sigma']\n        \n        scores = []\n        for candidate in candidates:\n            cand_len = len(candidate.split())\n            ratio = cand_len / source_len if source_len > 0 else 0\n            score = np.exp(-((ratio - optimal) ** 2) / (2 * sigma ** 2))\n            scores.append(score)\n        \n        return scores\n    \n    def compute_extractive_coverage(self, candidates, source_text, params=None):\n        \"\"\"\n        покрытие n-грамм из исходного текста\n        \n        применение:\n        - extractive summarization\n        - faithful generation\n        - qa: ответ из контекста\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        if source_text is None:\n            return [0.0] * len(candidates)\n        \n        source_ngrams = {\n            n: self._get_ngrams(source_text, n)\n            for n in [1, 2, 3, 4]\n        }\n        \n        scores = []\n        for candidate in candidates:\n            ngram_scores = []\n            for n in [1, 2, 3, 4]:\n                cand_ngrams = self._get_ngrams(candidate, n)\n                if not cand_ngrams:\n                    continue\n                overlap = sum((cand_ngrams & source_ngrams[n]).values())\n                total = sum(cand_ngrams.values())\n                ngram_scores.append(overlap / total)\n            \n            scores.append(np.mean(ngram_scores) if ngram_scores else 0.0)\n        \n        return scores\n    \n    def compute_fluency(self, candidates, params=None):\n        \"\"\"\n        беглость текста на основе perplexity\n        \n        применение:\n        - все задачи генерации\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        model, tokenizer = self._load_fluency_model()\n        \n        scores = []\n        for candidate in candidates:\n            inputs = tokenizer(candidate, return_tensors='pt', truncation=True, max_length=512)\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = model(**inputs, labels=inputs['input_ids'])\n                loss = outputs.loss\n            \n            perplexity = torch.exp(loss).item()\n            fluency_score = 1.0 / (1.0 + perplexity / 30.0)\n            scores.append(fluency_score)\n        \n        return scores\n    \n    def compute_grammar(self, candidates, params=None):\n        \"\"\"\n        грамматическая корректность (эвристика)\n        \n        применение:\n        - все задачи генерации\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        scores = []\n        for candidate in candidates:\n            score = 1.0\n            \n            if candidate and not candidate[0].isupper():\n                score -= 0.2\n            \n            if candidate and not candidate.rstrip()[-1] in '.!?':\n                score -= 0.2\n            \n            if '  ' in candidate:\n                score -= 0.1\n            \n            if candidate.count('\"') % 2 != 0:\n                score -= 0.1\n            \n            if len(candidate.split()) < 3:\n                score -= 0.3\n            \n            scores.append(max(0.0, score))\n        \n        return scores\n    \n    def compute_length_simple(self, candidates, params=None):\n        \"\"\"\n        соответствие целевой длине\n        \n        применение:\n        - commongen: 10-20 слов\n        - заголовки: 5-10 слов\n        - qa: 1-10 слов\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        params = params or {'target_min': 10, 'target_max': 20}\n        target_min = params['target_min']\n        target_max = params['target_max']\n        \n        scores = []\n        for candidate in candidates:\n            words = len(candidate.split())\n            \n            if target_min <= words <= target_max:\n                score = 1.0\n            elif words < target_min:\n                score = words / target_min\n            else:\n                score = max(0, 1 - (words - target_max) / (target_max * 0.5))\n            \n            scores.append(score)\n        \n        return scores\n    \n    def compute_lexical_diversity(self, candidates, params=None):\n        \"\"\"\n        лексическое разнообразие\n        \n        применение:\n        - creative writing\n        - диалоги\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        scores = []\n        for candidate in candidates:\n            words = candidate.lower().split()\n            \n            if len(words) < 5:\n                scores.append(0.5)\n                continue\n            \n            unique_words = len(set(words))\n            total_words = len(words)\n            ttr = unique_words / total_words\n            \n            normalized_diversity = min(1.0, ttr * (1 + np.log(total_words) / 5))\n            scores.append(normalized_diversity)\n        \n        return scores\n    \n    def compute_repetition_penalty(self, candidates, params=None):\n        \"\"\"\n        штраф за повторы\n        \n        применение:\n        - все задачи генерации\n        - борьба с вырожденными генерациями\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        scores = []\n        for candidate in candidates:\n            words = candidate.lower().split()\n            \n            if len(words) < 3:\n                scores.append(1.0)\n                continue\n            \n            unique_words = len(set(words))\n            total_words = len(words)\n            unigram_diversity = unique_words / total_words if total_words > 0 else 0\n            \n            bigrams = self._get_ngrams(candidate, 2)\n            unique_bigrams = len(bigrams)\n            total_bigrams = sum(bigrams.values())\n            bigram_diversity = unique_bigrams / total_bigrams if total_bigrams > 0 else 0\n            \n            score = 0.5 * unigram_diversity + 0.5 * bigram_diversity\n            scores.append(score)\n        \n        return scores\n    \n    def compute_rouge_with_source(self, candidates, source_text, params=None):\n        \"\"\"\n        rouge overlap с исходным текстом\n        \n        применение:\n        - extractive summarization\n        - перефразирование\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        if source_text is None:\n            return [0.0] * len(candidates)\n        \n        scorer = self._load_rouge_scorer()\n        scores = []\n        \n        for candidate in candidates:\n            result = scorer.score(source_text, candidate)\n            scores.append(result['rougeL'].fmeasure)\n        \n        return scores\n    \n    def compute_semantic_coherence(self, candidates, params=None):\n        \"\"\"\n        семантическая связность (эвристика)\n        \n        применение:\n        - длинные генерации\n        - диалоги\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        scores = []\n        for candidate in candidates:\n            sentences = re.split(r'[.!?]+', candidate)\n            sentences = [s.strip() for s in sentences if s.strip()]\n            \n            if len(sentences) <= 1:\n                scores.append(1.0)\n                continue\n            \n            score = 1.0\n            \n            connectives = [\n                'however', 'therefore', 'moreover', 'furthermore',\n                'additionally', 'consequently', 'thus', 'hence',\n                'because', 'since', 'although', 'while', 'but', 'and'\n            ]\n            \n            candidate_lower = candidate.lower()\n            has_connectives = any(conn in candidate_lower for conn in connectives)\n            \n            if len(sentences) > 2 and not has_connectives:\n                score -= 0.2\n            \n            lengths = [len(s.split()) for s in sentences]\n            if len(lengths) > 1:\n                length_variance = np.std(lengths) / (np.mean(lengths) + 1e-6)\n                if length_variance > 1.5:\n                    score -= 0.2\n            \n            scores.append(max(0.0, score))\n        \n        return scores\n    \n    def compute_metric(self, metric_name, candidates, context, params=None):\n        \"\"\"вычислить одну метрику по имени\"\"\"\n        method_name = f'compute_{metric_name}'\n        if not hasattr(self, method_name):\n            raise ValueError(f\"unknown metric: {metric_name}\")\n        \n        method = getattr(self, method_name)\n        \n        # метрики требующие context\n        if metric_name in ['coverage', 'compression_ratio', 'extractive_coverage', 'rouge_with_source']:\n            return method(candidates, context, params)\n        else:\n            return method(candidates, params)\n\n\nclass ReRankingModel:\n    \"\"\"линейная модель для ранжирования\"\"\"\n    \n    def __init__(self, metric_names, weights=None):\n        self.metric_names = metric_names\n        self.weights = weights or self._init_uniform_weights()\n    \n    def _init_uniform_weights(self):\n        \"\"\"равномерные веса\"\"\"\n        n = len(self.metric_names)\n        return {name: 1.0 / n for name in self.metric_names}\n    \n    def predict(self, metrics_dict):\n        \"\"\"вычислить взвешенный скор\"\"\"\n        return sum(metrics_dict.get(m, 0) * self.weights.get(m, 0) for m in self.metric_names)\n    \n    def predict_batch(self, metrics_list):\n        \"\"\"вычислить скоры для батча\"\"\"\n        return [self.predict(m) for m in metrics_list]\n    \n    def set_weights(self, weights):\n        \"\"\"установить веса\"\"\"\n        self.weights = weights\n\n\nclass UniversalTextReRanker:\n    \"\"\"универсальный re-ranker для генераций текста\"\"\"\n    \n    def __init__(\n        self,\n        use_coverage=False,\n        use_compression_ratio=False,\n        use_extractive_coverage=False,\n        use_fluency=False,\n        use_grammar=False,\n        use_length_simple=False,\n        use_lexical_diversity=False,\n        use_repetition_penalty=False,\n        use_rouge_with_source=False,\n        use_semantic_coherence=False,\n        coverage_params=None,\n        compression_params=None,\n        length_params=None,\n        weights=None,\n        metric_selection='manual',\n        min_correlation=0.15,\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        cache_dir='./reranker_cache'\n    ):\n        \"\"\"\n        args:\n            use_*: флаги для включения метрик\n            *_params: параметры для метрик\n            weights: предустановленные веса\n            metric_selection: 'manual', 'auto' или 'auto_all'\n                - 'manual': используем только метрики с use_*=True\n                - 'auto': из метрик с use_*=True отбираем по корреляции\n                - 'auto_all': тестируем все метрики, отбираем по корреляции\n            min_correlation: порог корреляции для auto режимов\n            device: устройство для вычислений\n            cache_dir: директория для кэша\n        \"\"\"\n        self.use_coverage = use_coverage\n        self.use_compression_ratio = use_compression_ratio\n        self.use_extractive_coverage = use_extractive_coverage\n        self.use_fluency = use_fluency\n        self.use_grammar = use_grammar\n        self.use_length_simple = use_length_simple\n        self.use_lexical_diversity = use_lexical_diversity\n        self.use_repetition_penalty = use_repetition_penalty\n        self.use_rouge_with_source = use_rouge_with_source\n        self.use_semantic_coherence = use_semantic_coherence\n        \n        self.coverage_params = coverage_params or {}\n        self.compression_params = compression_params or {'optimal_ratio': 0.15, 'sigma': 0.05}\n        self.length_params = length_params or {'target_min': 10, 'target_max': 20}\n        \n        if metric_selection not in ['manual', 'auto', 'auto_all']:\n            raise ValueError(f\"metric_selection must be 'manual', 'auto' or 'auto_all', got {metric_selection}\")\n        \n        self.metric_selection = metric_selection\n        self.min_correlation = min_correlation\n        self.device = device\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n        \n        self.metrics_computer = MetricsComputer(device=device)\n        \n        active_metrics = self._get_active_metrics()\n        self.model = ReRankingModel(active_metrics, weights)\n    \n    def __getstate__(self):\n        state = self.__dict__.copy()\n        return state\n    \n    def __setstate__(self, state):\n        self.__dict__.update(state)\n    \n    def _get_active_metrics(self):\n        \"\"\"получить список активных метрик\"\"\"\n        metrics = []\n        if self.use_coverage:\n            metrics.append('coverage')\n        if self.use_compression_ratio:\n            metrics.append('compression_ratio')\n        if self.use_extractive_coverage:\n            metrics.append('extractive_coverage')\n        if self.use_fluency:\n            metrics.append('fluency')\n        if self.use_grammar:\n            metrics.append('grammar')\n        if self.use_length_simple:\n            metrics.append('length_simple')\n        if self.use_lexical_diversity:\n            metrics.append('lexical_diversity')\n        if self.use_repetition_penalty:\n            metrics.append('repetition_penalty')\n        if self.use_rouge_with_source:\n            metrics.append('rouge_with_source')\n        if self.use_semantic_coherence:\n            metrics.append('semantic_coherence')\n        return metrics\n    \n    def _get_metric_params(self, metric_name):\n        \"\"\"получить параметры для метрики\"\"\"\n        if metric_name == 'coverage':\n            return self.coverage_params\n        elif metric_name == 'compression_ratio':\n            return self.compression_params\n        elif metric_name == 'length_simple':\n            return self.length_params\n        else:\n            return None\n    \n    def compute_batch(self, candidates, context=None):\n        \"\"\"\n        вычислить все метрики для батча кандидатов\n        \n        args:\n            candidates: list of str\n            context: str или None\n        \n        returns:\n            list of dict: [{metric_name: score}, ...]\n        \"\"\"\n        if not candidates:\n            return []\n        \n        all_scores = {}\n        \n        for metric_name in self.model.metric_names:\n            params = self._get_metric_params(metric_name)\n            all_scores[metric_name] = self.metrics_computer.compute_metric(\n                metric_name, candidates, context, params\n            )\n        \n        results = []\n        for i in range(len(candidates)):\n            candidate_scores = {\n                metric: all_scores[metric][i] \n                for metric in all_scores\n            }\n            results.append(candidate_scores)\n        \n        return results\n    \n    def compute(self, text, context=None, return_individual=True, return_weighted=True):\n        \"\"\"вычислить метрики для одного текста\"\"\"\n        batch_results = self.compute_batch([text], context)\n        scores = batch_results[0]\n        \n        result = {}\n        if return_individual:\n            result['scores'] = scores\n        \n        if return_weighted:\n            result['weighted_score'] = self.model.predict(scores)\n        \n        return result\n    \n    def rank_candidates(self, candidates, context=None):\n        \"\"\"\n        ранжировать кандидатов\n        \n        returns:\n            list of tuple: [(idx, weighted_score, individual_scores), ...]\n        \"\"\"\n        if not candidates:\n            return []\n        \n        batch_scores = self.compute_batch(candidates, context)\n        \n        results = []\n        for idx, scores in enumerate(batch_scores):\n            weighted_score = self.model.predict(scores)\n            results.append((idx, weighted_score, scores))\n        \n        results.sort(key=lambda x: x[1], reverse=True)\n        return results\n    \n    def get_best_candidate(self, candidates, context=None):\n        \"\"\"получить лучшего кандидата\"\"\"\n        results = self.rank_candidates(candidates, context)\n        if not results:\n            return None\n        best_idx = results[0][0]\n        return candidates[best_idx]\n    \n    def fit(\n        self,\n        candidates_list,\n        contexts,\n        y_texts,\n        metric='rouge2',\n        cache_name=None,\n        use_cache=True,\n        max_iter=50,\n        popsize=15,\n        seed=42,\n        n_workers=1,\n        print_correlations=True\n    ):\n        \"\"\"\n        оптимизировать веса метрик\n        \n        args:\n            candidates_list: list of list of str\n            contexts: list\n            y_texts: list of str\n            metric: str - целевая метрика ('rouge2', 'rougeL')\n        \n        returns:\n            dict, float: оптимальные веса и достигнутый скор\n        \"\"\"\n        if len(candidates_list) != len(contexts) or len(candidates_list) != len(y_texts):\n            raise ValueError(\n                f\"length mismatch: candidates_list={len(candidates_list)}, \"\n                f\"contexts={len(contexts)}, y_texts={len(y_texts)}\"\n            )\n        \n        # определяем метрики для вычисления\n        if self.metric_selection == 'auto_all':\n            # вычисляем ВСЕ метрики\n            metrics_to_compute = MetricsComputer.ALL_METRICS\n            print(f\"\\nрежим auto_all: вычисляем все {len(metrics_to_compute)} метрик\")\n        else:\n            # вычисляем только активные (use_*=True)\n            metrics_to_compute = self._get_active_metrics()\n            if self.metric_selection == 'auto':\n                print(f\"\\nрежим auto: из {len(metrics_to_compute)} активных метрик отберём по корреляции >= {self.min_correlation}\")\n            else:\n                print(f\"\\nрежим manual: используем {len(metrics_to_compute)} метрик\")\n        \n        # предвычисление метрик\n        precomputed_metrics = self._load_or_compute_metrics(\n            candidates_list, contexts, cache_name, use_cache, metrics_to_compute\n        )\n        \n        # предвычисление целевой метрики\n        print(f\"предвычисление целевой метрики ({metric})...\")\n        target_scores_cache = self._precompute_target_scores(\n            candidates_list, y_texts, metric\n        )\n        \n        metric_names = list(precomputed_metrics[0][0].keys())\n        \n        print(f\"целевая метрика: {metric}\")\n        \n        # вычисление корреляций\n        correlations_dict = {}\n        if print_correlations or self.metric_selection in ['auto', 'auto_all']:\n            correlations_dict = self._compute_correlations(\n                precomputed_metrics, target_scores_cache, metric_names\n            )\n        \n        if print_correlations:\n            self._print_correlations(correlations_dict)\n        \n        # автоматический отбор метрик\n        if self.metric_selection in ['auto', 'auto_all']:\n            selected_metrics = self._select_metrics_by_correlation(\n                correlations_dict, self.min_correlation\n            )\n            \n            if not selected_metrics:\n                print(f\"warning: ни одна метрика не прошла порог {self.min_correlation}\")\n                print(f\"используем все {len(metric_names)} метрик\")\n                selected_metrics = metric_names\n            else:\n                dropped = set(metric_names) - set(selected_metrics)\n                if dropped:\n                    print(f\"\\nотброшены ({len(dropped)}):\")\n                    for m in sorted(dropped):\n                        corr = correlations_dict[m]['mean']\n                        print(f\"  {m:25s}: корреляция {corr:+.3f}\")\n                \n                print(f\"\\nотобрано: {len(selected_metrics)}/{len(metric_names)} метрик\")\n                print(f\"метрики: {selected_metrics}\")\n            \n            # фильтруем precomputed_metrics\n            precomputed_metrics = [\n                [{k: v for k, v in m.items() if k in selected_metrics} for m in batch]\n                for batch in precomputed_metrics\n            ]\n            metric_names = selected_metrics\n            \n            # обновляем активные метрики в объекте (для auto_all)\n            if self.metric_selection == 'auto_all':\n                self._update_active_metrics(selected_metrics)\n        \n        n_metrics = len(metric_names)\n        \n        # оптимизация весов\n        best_weights, best_score = self._optimize_weights(\n            precomputed_metrics, target_scores_cache,\n            metric_names, max_iter, popsize, seed, n_workers\n        )\n        \n        # обновление модели\n        weights_dict = {\n            metric_names[i]: best_weights[i]\n            for i in range(n_metrics)\n        }\n        \n        # добавляем нулевые веса для неиспользуемых метрик\n        for metric in MetricsComputer.ALL_METRICS:\n            if metric not in weights_dict:\n                weights_dict[metric] = 0.0\n        \n        # обновляем модель с новыми метриками\n        self.model = ReRankingModel(metric_names, weights_dict)\n        \n        self._print_results(metric, best_score, weights_dict)\n        \n        return weights_dict, best_score\n    \n    def _update_active_metrics(self, selected_metrics):\n        \"\"\"обновить флаги use_* (для auto_all режима)\"\"\"\n        for metric in MetricsComputer.ALL_METRICS:\n            attr_name = f'use_{metric}'\n            setattr(self, attr_name, metric in selected_metrics)\n    \n    def _compute_correlations(self, precomputed_metrics, target_scores_cache, metric_names):\n        \"\"\"вычислить корреляции метрик с целевой метрикой\"\"\"\n        metric_correlations = {name: [] for name in metric_names}\n        \n        for i in range(len(precomputed_metrics)):\n            target_values = target_scores_cache[i]\n            \n            for metric_name in metric_names:\n                metric_values = [\n                    precomputed_metrics[i][j][metric_name]\n                    for j in range(len(precomputed_metrics[i]))\n                ]\n                \n                if np.std(metric_values) > 1e-10 and np.std(target_values) > 1e-10:\n                    corr, _ = spearmanr(metric_values, target_values)\n                    metric_correlations[metric_name].append(corr)\n        \n        # усреднение\n        avg_correlations = {}\n        for metric_name in metric_names:\n            if metric_correlations[metric_name]:\n                avg_correlations[metric_name] = {\n                    'mean': np.mean(metric_correlations[metric_name]),\n                    'std': np.std(metric_correlations[metric_name])\n                }\n        \n        return avg_correlations\n    \n    def _select_metrics_by_correlation(self, correlations_dict, min_correlation):\n        \"\"\"отобрать метрики по корреляции\"\"\"\n        selected = []\n        for metric_name, stats in correlations_dict.items():\n            if abs(stats['mean']) >= min_correlation:\n                selected.append(metric_name)\n        return selected\n    \n    def _print_correlations(self, correlations_dict):\n        \"\"\"вывести корреляции\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"корреляция метрик с целевой метрикой (spearman)\")\n        print(\"=\"*60)\n        \n        correlations = [\n            (name, stats['mean'], stats['std'])\n            for name, stats in correlations_dict.items()\n        ]\n        correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n        \n        for metric_name, avg_corr, std_corr in correlations:\n            if abs(avg_corr) > 0.4:\n                status = \"сильная\"\n            elif abs(avg_corr) > 0.25:\n                status = \"средняя\"\n            elif abs(avg_corr) > 0.15:\n                status = \"слабая\"\n            else:\n                status = \"очень слабая\"\n            \n            print(f\"{status:15s} {metric_name:25s}: {avg_corr:+.3f} (±{std_corr:.3f})\")\n        \n        print(\"=\"*60)\n    \n    def _print_results(self, metric, best_score, weights_dict):\n        \"\"\"вывести результаты калибровки\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"результаты калибровки\")\n        print(\"=\"*60)\n        print(f\"best {metric}: {best_score:.4f}\")\n        print(f\"\\nоптимальные веса:\")\n        \n        sorted_weights = sorted(\n            weights_dict.items(), \n            key=lambda x: abs(x[1]), \n            reverse=True\n        )\n        \n        for name, weight in sorted_weights:\n            if abs(weight) > 1e-6:\n                print(f\"  {name:25s}: {weight:+.4f}\")\n        \n        print(\"=\"*60)\n    \n    def _load_or_compute_metrics(self, candidates_list, contexts, cache_name, use_cache, metrics_to_compute):\n        \"\"\"загрузить или вычислить метрики\"\"\"\n        cache_path = None\n        if cache_name and use_cache:\n            # добавляем метрики в имя кэша для различения\n            metrics_hash = hash(tuple(sorted(metrics_to_compute)))\n            cache_path = self.cache_dir / f\"{cache_name}_metrics_{metrics_hash}.pkl\"\n        \n        if cache_path and cache_path.exists():\n            print(f\"загрузка метрик из кэша: {cache_path}\")\n            with open(cache_path, 'rb') as f:\n                return pickle.load(f)\n        \n        print(f\"предвычисление {len(metrics_to_compute)} метрик...\")\n        precomputed = []\n        \n        for candidates, context in tqdm(\n            list(zip(candidates_list, contexts)), \n            desc=\"computing metrics\"\n        ):\n            candidate_metrics = []\n            for candidate in candidates:\n                metrics_dict = {}\n                for metric_name in metrics_to_compute:\n                    params = self._get_metric_params(metric_name)\n                    scores = self.metrics_computer.compute_metric(\n                        metric_name, [candidate], context, params\n                    )\n                    metrics_dict[metric_name] = scores[0]\n                candidate_metrics.append(metrics_dict)\n            \n            precomputed.append(candidate_metrics)\n        \n        if cache_path:\n            with open(cache_path, 'wb') as f:\n                pickle.dump(precomputed, f)\n            print(f\"метрики сохранены в кэш: {cache_path}\")\n        \n        return precomputed\n    \n    def _precompute_target_scores(self, candidates_list, y_texts, metric='rouge2'):\n        \"\"\"предвычислить целевую метрику\"\"\"\n        scorer = self.metrics_computer._load_rouge_scorer()\n        target_scores = []\n        \n        for candidates, y_text in tqdm(\n            zip(candidates_list, y_texts),\n            total=len(candidates_list),\n            desc=\"target metric\"\n        ):\n            candidate_scores = []\n            for candidate in candidates:\n                scores = scorer.score(y_text, candidate)\n                candidate_scores.append(scores[metric].fmeasure)\n            \n            target_scores.append(candidate_scores)\n        \n        return target_scores\n    \n    def _optimize_weights(\n        self, \n        precomputed_metrics, \n        target_scores_cache, \n        metric_names,\n        max_iter, \n        popsize, \n        seed, \n        n_workers\n    ):\n        \"\"\"оптимизация весов\"\"\"\n        n_metrics = len(metric_names)\n        \n        # сохраняем для objective function\n        self._opt_precomputed_metrics = precomputed_metrics\n        self._opt_target_scores_cache = target_scores_cache\n        self._opt_metric_names = metric_names\n        self._opt_n_metrics = n_metrics\n        \n        print(f\"\\nоптимизация весов (max_iter={max_iter}, popsize={popsize}, workers={n_workers})...\")\n        \n        result = differential_evolution(\n            self._objective_function,\n            bounds=[(-1, 1)] * n_metrics,\n            strategy='best1bin',\n            maxiter=max_iter,\n            popsize=popsize,\n            tol=0.001,\n            mutation=(0.5, 1.5),\n            recombination=0.7,\n            seed=seed,\n            workers=n_workers,\n            updating='deferred' if n_workers > 1 else 'immediate',\n            polish=True,\n            disp=True\n        )\n        \n        # очистка\n        del self._opt_precomputed_metrics\n        del self._opt_target_scores_cache\n        del self._opt_metric_names\n        del self._opt_n_metrics\n        \n        weights_raw = result.x\n        weights_sum = np.sum(np.abs(weights_raw))\n        \n        if weights_sum < 1e-10:\n            print(f\"warning: все веса близки к нулю\")\n            weights_normalized = np.ones(n_metrics) / n_metrics\n        else:\n            weights_normalized = weights_raw / weights_sum\n        \n        best_score = -result.fun\n        return weights_normalized, best_score\n    \n    def _objective_function(self, weights):\n        \"\"\"функция для оптимизации\"\"\"\n        total_score = 0.0\n        \n        for metrics_list, target_scores in zip(\n            self._opt_precomputed_metrics,\n            self._opt_target_scores_cache\n        ):\n            # вычисляем взвешенный скор\n            weighted_scores = [\n                sum(\n                    weights[i] * metrics_dict.get(self._opt_metric_names[i], 0)\n                    for i in range(self._opt_n_metrics)\n                )\n                for metrics_dict in metrics_list\n            ]\n            \n            # выбираем лучшего\n            best_idx = np.argmax(weighted_scores)\n            total_score += target_scores[best_idx]\n        \n        avg_score = total_score / len(self._opt_precomputed_metrics)\n        return -avg_score\n    \n    def save(self, path):\n        \"\"\"сохранить re-ranker\"\"\"\n        with open(path, 'wb') as f:\n            pickle.dump(self, f)\n        print(f\"re-ranker сохранен: {path}\")\n    \n    @staticmethod\n    def load(path):\n        \"\"\"загрузить re-ranker\"\"\"\n        with open(path, 'rb') as f:\n            ranker = pickle.load(f)\n        print(f\"re-ranker загружен: {path}\")\n        return ranker","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T16:19:26.500353Z","iopub.execute_input":"2025-11-16T16:19:26.500655Z","iopub.status.idle":"2025-11-16T16:19:26.567432Z","shell.execute_reply.started":"2025-11-16T16:19:26.500632Z","shell.execute_reply":"2025-11-16T16:19:26.566860Z"}},"outputs":[],"execution_count":164},{"cell_type":"code","source":"def predict_with_reranker(promts):\n    results = []\n    for promt in promts:\n        generated_texts = generate_response(\n            trainer.model.half(), \n            tokenizer,\n            [promt],\n            num_return_sequences=10,\n            num_beams=4,\n            temperature=0.8,\n            top_k=10,\n            top_p=0.9,\n            max_new_tokens=40\n        )[0]\n        result = ranker.get_best_candidate(generated_texts, promt)\n        results.append(result)\n    return results\n\ndef evaluate_model_with_reranker(model, tokenizer, val_data, batch_size=8):\n    predictions = []\n\n    for i in tqdm(range(0, len(val_data), batch_size)):\n        batch = [val_data[j] for j in range(i, min(i + batch_size, len(val_data)))]\n        batch_predictions = predict_with_reranker([template_processing(item['question']) for item in batch])\n        predictions.extend(batch_predictions)\n\n    references = [item['answer'] for item in val_data]\n\n    rouge = evaluate.load('rouge')\n    final_scores = rouge.compute(predictions=predictions, references=references)\n\n    return final_scores, predictions\n\ndef get_ranker_train_data(indexes):\n    dev_candidates = []\n    dev_x_texts = []\n    dev_y_texts = []\n\n    for sample in tqdm(test_data.select(indexes)):\n        candidates = generate_response(\n            trainer.model.half(), tokenizer, [template_processing(sample['question'])],\n            num_return_sequences=10,\n            num_beams=2,\n            temperature=0.8,\n            top_k=10,\n            top_p=0.9,\n            max_new_tokens=40\n        )[0]\n\n        dev_candidates.append(candidates)\n        dev_x_texts.append(sample['question'])\n        dev_y_texts.append(sample['answer'])\n\n    return dev_candidates, dev_x_texts, dev_y_texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:08:13.289275Z","iopub.execute_input":"2025-11-16T15:08:13.289975Z","iopub.status.idle":"2025-11-16T15:08:13.297840Z","shell.execute_reply.started":"2025-11-16T15:08:13.289951Z","shell.execute_reply":"2025-11-16T15:08:13.297033Z"}},"outputs":[],"execution_count":143},{"cell_type":"code","source":"# ranker = UniversalTextReRanker(\n#     use_coverage=True,\n#     use_fluency=True,\n#     # use_grammar=True,\n#     use_length_simple=True,\n#     use_repetition_penalty=True,\n#     length_params={'target_min': 1, 'target_max': 15}\n# )\n\nranker = UniversalTextReRanker(\n    metric_selection='auto_all',\n    min_correlation=0.15\n)\n\n# dev_candidates, dev_x_texts, dev_y_texts = get_ranker_train_data(list(range(200)))\n\noptimal_weights, best_score = ranker.fit(\n    candidates_list=dev_candidates,\n    contexts=dev_x_texts,\n    y_texts=dev_y_texts,\n    metric='rouge2',\n    cache_name='commongen_dev_v9',\n    max_iter=50,\n    popsize=30,\n    print_correlations=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T16:19:58.137181Z","iopub.execute_input":"2025-11-16T16:19:58.137508Z","iopub.status.idle":"2025-11-16T16:21:37.869918Z","shell.execute_reply.started":"2025-11-16T16:19:58.137483Z","shell.execute_reply":"2025-11-16T16:21:37.869095Z"}},"outputs":[{"name":"stdout","text":"\nрежим auto_all: вычисляем все 10 метрик\nпредвычисление 10 метрик...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"computing metrics:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac93ccf73354c238346507b8627a6a7"}},"metadata":{}},{"name":"stdout","text":"загрузка модели для fluency (gpt-2)...\nметрики сохранены в кэш: reranker_cache/commongen_dev_v9_metrics_-4645364536204692210.pkl\nпредвычисление целевой метрики (rouge2)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"target metric:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfe0f6d431ca44758e4d36d60ac744e2"}},"metadata":{}},{"name":"stdout","text":"целевая метрика: rouge2\n\n============================================================\nкорреляция метрик с целевой метрикой (spearman)\n============================================================\nсильная         compression_ratio        : -0.596 (±0.523)\nсильная         coverage                 : -0.527 (±0.406)\nсильная         rouge_with_source        : -0.504 (±0.425)\nсильная         semantic_coherence       : +0.444 (±0.371)\nсильная         repetition_penalty       : +0.424 (±0.403)\nсредняя         fluency                  : -0.334 (±0.446)\nслабая          lexical_diversity        : -0.245 (±0.445)\nочень слабая    length_simple            : -0.077 (±0.620)\nочень слабая    extractive_coverage      : +0.050 (±0.465)\nочень слабая    grammar                  : +0.002 (±0.454)\n============================================================\n\nотброшены (3):\n  extractive_coverage      : корреляция +0.050\n  grammar                  : корреляция +0.002\n  length_simple            : корреляция -0.077\n\nотобрано: 7/10 метрик\nметрики: ['coverage', 'compression_ratio', 'fluency', 'lexical_diversity', 'repetition_penalty', 'rouge_with_source', 'semantic_coherence']\n\nоптимизация весов (max_iter=50, popsize=30, workers=1)...\ndifferential_evolution step 1: f(x)= -0.3983757324374973\ndifferential_evolution step 2: f(x)= -0.40027544176757546\ndifferential_evolution step 3: f(x)= -0.40027544176757546\ndifferential_evolution step 4: f(x)= -0.40027544176757546\ndifferential_evolution step 5: f(x)= -0.40027544176757546\ndifferential_evolution step 6: f(x)= -0.4004926106573168\ndifferential_evolution step 7: f(x)= -0.4019163202597348\ndifferential_evolution step 8: f(x)= -0.4027102662023999\ndifferential_evolution step 9: f(x)= -0.403312830304964\ndifferential_evolution step 10: f(x)= -0.403312830304964\ndifferential_evolution step 11: f(x)= -0.403312830304964\ndifferential_evolution step 12: f(x)= -0.403547055144452\ndifferential_evolution step 13: f(x)= -0.403547055144452\ndifferential_evolution step 14: f(x)= -0.403547055144452\ndifferential_evolution step 15: f(x)= -0.403547055144452\ndifferential_evolution step 16: f(x)= -0.403547055144452\ndifferential_evolution step 17: f(x)= -0.40446784195997565\ndifferential_evolution step 18: f(x)= -0.40446784195997565\ndifferential_evolution step 19: f(x)= -0.40446784195997565\ndifferential_evolution step 20: f(x)= -0.40446784195997565\ndifferential_evolution step 21: f(x)= -0.40446784195997565\ndifferential_evolution step 22: f(x)= -0.40446784195997565\ndifferential_evolution step 23: f(x)= -0.40446784195997565\ndifferential_evolution step 24: f(x)= -0.40446784195997565\ndifferential_evolution step 25: f(x)= -0.40446784195997565\ndifferential_evolution step 26: f(x)= -0.40585841435054804\ndifferential_evolution step 27: f(x)= -0.40585841435054804\ndifferential_evolution step 28: f(x)= -0.40585841435054804\ndifferential_evolution step 29: f(x)= -0.40585841435054804\ndifferential_evolution step 30: f(x)= -0.40585841435054804\ndifferential_evolution step 31: f(x)= -0.40585841435054804\ndifferential_evolution step 32: f(x)= -0.40585841435054804\ndifferential_evolution step 33: f(x)= -0.40585841435054804\ndifferential_evolution step 34: f(x)= -0.40585841435054804\ndifferential_evolution step 35: f(x)= -0.40585841435054804\ndifferential_evolution step 36: f(x)= -0.4059493234414571\ndifferential_evolution step 37: f(x)= -0.40712954672694357\ndifferential_evolution step 38: f(x)= -0.40712954672694357\ndifferential_evolution step 39: f(x)= -0.40712954672694357\ndifferential_evolution step 40: f(x)= -0.40712954672694357\ndifferential_evolution step 41: f(x)= -0.40712954672694357\ndifferential_evolution step 42: f(x)= -0.40712954672694357\ndifferential_evolution step 43: f(x)= -0.40712954672694357\ndifferential_evolution step 44: f(x)= -0.40712954672694357\ndifferential_evolution step 45: f(x)= -0.40714469824209504\ndifferential_evolution step 46: f(x)= -0.40714469824209504\ndifferential_evolution step 47: f(x)= -0.40714469824209504\ndifferential_evolution step 48: f(x)= -0.40714469824209504\ndifferential_evolution step 49: f(x)= -0.40714469824209504\ndifferential_evolution step 50: f(x)= -0.40714469824209504\nPolishing solution with 'L-BFGS-B'\n\n============================================================\nрезультаты калибровки\n============================================================\nbest semantic_coherence: 0.4071\n\nоптимальные веса:\n  lexical_diversity        : -0.2480\n  repetition_penalty       : +0.2437\n  semantic_coherence       : +0.2390\n  rouge_with_source        : +0.0923\n  compression_ratio        : -0.0900\n  coverage                 : -0.0801\n  fluency                  : -0.0069\n============================================================\n","output_type":"stream"}],"execution_count":166},{"cell_type":"code","source":"def evaluate_on_cached_candidates(dev_candidates, dev_x_texts, dev_y_texts):\n    predictions = []\n\n    for candidates, x_text in tqdm(list(zip(dev_candidates, dev_x_texts))):\n        best = ranker.get_best_candidate(candidates, x_text)\n        predictions.append(best)\n\n    rouge = evaluate.load('rouge')\n    scores = rouge.compute(predictions=predictions, references=dev_y_texts)\n\n    return scores, predictions\n\nscores, preds = evaluate_on_cached_candidates(dev_candidates, dev_x_texts, dev_y_texts)\nprint(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T16:21:37.871208Z","iopub.execute_input":"2025-11-16T16:21:37.871437Z","iopub.status.idle":"2025-11-16T16:22:05.247717Z","shell.execute_reply.started":"2025-11-16T16:21:37.871419Z","shell.execute_reply":"2025-11-16T16:22:05.246800Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a094492c4414f1a9120905342c5429b"}},"metadata":{}},{"name":"stdout","text":"{'rouge1': 0.607136387972391, 'rouge2': 0.40260392064501627, 'rougeL': 0.6058775773495776, 'rougeLsum': 0.604456859396507}\n","output_type":"stream"}],"execution_count":167},{"cell_type":"code","source":"# проверить catboostranker\n\nscores, preds = evaluate_model_with_reranker(trainer.model.half(), tokenizer, test_data.select(range(200)), batch_size=1)\n\nprint(scores)\nfor pred in preds:\n    print('\\n########################\\n')\n    print(pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T16:32:12.580359Z","iopub.execute_input":"2025-11-16T16:32:12.581066Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7219107fe1c74f85a6c63ada08c7f0a6"}},"metadata":{}}],"execution_count":null}]}