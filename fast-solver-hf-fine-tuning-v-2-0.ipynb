{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Дообучение encoder'а для классификации токенов.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"# Установка зависимостей (при необходимости)\n!pip install -q evaluate seqeval transformers datasets\n\nimport gc\nimport math\nfrom typing import List, Dict, Any, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport evaluate\nfrom transformers import (\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForTokenClassification,\n    TrainerCallback,\n    PrinterCallback,\n)\nfrom transformers.modeling_outputs import ModelOutput\nimport pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\n\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Устанавливает фиксированное зерно для Python, NumPy и PyTorch (включая все доступные CUDA-устройства).\n\n    :param seed: Значение зерна.\n    \"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    \"\"\"\n    Внешний прогресс‑бар и консольный логгер метрик/лоссов для стабильного отображения на больших данных.\n    \"\"\"\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                try:\n                    parts.append(f\"{k.replace('eval_', '')} {float(v):.4f}\")\n                except Exception:\n                    pass\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    try:\n                        extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n                    except Exception:\n                        pass\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\nclass WeightedTokenCETrainer(Trainer):\n    \"\"\"\n    Кастомный Trainer для токен-классификации с взвешенной CrossEntropy.\n    Веса считаются с учетом числа токенов класса (после разметки и чанкинга):\n      weight_i = N / (K * n_i),\n    где K — число классов, n_i — число токенов класса i, N — сумма всех n_i.\n    Отсутствующие классы получают вес 0. Метки -100 игнорируются в потере.\n\n    Поддержка DataParallel: если logits дублируются по числу GPU, метки тайлятся соответствующим образом.\n    \"\"\"\n    def __init__(self, *args, class_weights=None, **kwargs):\n        # Тихо переводим устаревший аргумент tokenizer в processing_class (совместимость)\n        processing = kwargs.pop(\"tokenizer\", None)\n        if processing is not None and \"processing_class\" not in kwargs:\n            kwargs[\"processing_class\"] = processing\n\n        super().__init__(*args, **kwargs)\n\n        self.class_weights = None\n        if class_weights is not None:\n            self.class_weights = torch.as_tensor(class_weights, dtype=torch.float32)\n        self._warned_label_tiling = False\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Считает взвешенный CrossEntropyLoss по токенам, игнорируя -100.\n        Корректно обрабатывает случай DataParallel (дублирование по batch-оси).\n        \"\"\"\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[\"logits\"]  # [B, L, C]\n\n        # Приводим размеры в соответствие при DataParallel (если нужно)\n        if logits.size(0) != labels.size(0):\n            ngpu = torch.cuda.device_count()\n            if ngpu > 1 and logits.size(0) == labels.size(0) * ngpu:\n                labels = labels.repeat_interleave(ngpu, dim=0)\n                if not self._warned_label_tiling:\n                    print(f\"[Warning] DataParallel удвоил batch для logits. \"\n                          f\"Повторяем labels x{ngpu}. logits: {tuple(logits.shape)}, labels: {tuple(labels.shape)}\")\n                    self._warned_label_tiling = True\n            else:\n                raise ValueError(f\"Batch size mismatch: logits {tuple(logits.shape)} vs labels {tuple(labels.shape)}\")\n\n        # Взвешенный CE с ignore_index=-100\n        loss_fct = torch.nn.CrossEntropyLoss(\n            weight=(self.class_weights.to(logits.device) if self.class_weights is not None else None),\n            ignore_index=-100,\n        )\n        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\n\nclass TokenClassification:\n    \"\"\"\n    Пайплайн токен‑классификации с поддержкой больших данных (чанковое обучение по документам и sliding window).\n\n    Возможности:\n      - Автоматическая разметка (sliding window) с перекрытием stride.\n      - Взвешенная CrossEntropy по токенам (по всему train) с игнорированием -100.\n      - Чанковое обучение: подстановка train_dataset кусками документов (fit_chunk_size_docs).\n      - Внешний tqdm прогресс‑бар (стабильный) + консольный лог метрик.\n\n    Необходимые импорты:\n    import numpy as np\n    import torch\n    import evaluate\n    from transformers import (\n        AutoModelForTokenClassification,\n        AutoTokenizer,\n        Trainer,\n        TrainingArguments,\n        DataCollatorForTokenClassification,\n    )\n    import pandas as pd\n    from datasets import Dataset\n    from sklearn.model_selection import train_test_split\n    from tqdm.auto import tqdm\n    \"\"\"\n    def __init__(\n        self,\n        checkpoint: str,\n        label2id: Dict[str, int],\n        tokens_column_name: str,\n        tags_column_name: str\n    ):\n        \"\"\"\n        Инициализация модели и токенизатора.\n\n        :param checkpoint: Имя/путь чекпоинта (HF Hub).\n        :param label2id: Отображение тегов в id.\n        :param tokens_column_name: Имя колонки с токенами (список строк).\n        :param tags_column_name: Имя колонки с метками (список тегов или id).\n        \"\"\"\n        self.id2label = {v: k for k, v in label2id.items()}\n        self.label2id = label2id\n\n        self.model = AutoModelForTokenClassification.from_pretrained(\n            checkpoint,\n            num_labels=len(self.id2label),\n            id2label=self.id2label,\n            label2id=self.label2id,\n            ignore_mismatched_sizes=True\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.tokens_column_name = tokens_column_name\n        self.tags_column_name = tags_column_name\n\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.data_collator = DataCollatorForTokenClassification(tokenizer=self.tokenizer)\n        self.progress_callback: Optional[TrainerCallback] = None\n\n    @staticmethod\n    def _align_labels_with_word_ids(labels_ids: List[int], word_ids: List[Optional[int]]) -> List[int]:\n        \"\"\"\n        Выравнивает метки по word_ids от токенизатора: на первый токен слова ставится метка,\n        остальные токены слова получают -100. Спец‑токены (None) получают -100.\n\n        :param labels_ids: Список id‑меток длины = числу слов в документе.\n        :param word_ids: Список индексов слов длины = числу токенов в чанке (или None для спец‑токенов).\n        :return: Список меток длины = числу токенов в чанке.\n        \"\"\"\n        new_labels = []\n        prev_word_id = None\n        for wid in word_ids:\n            if wid is None:\n                new_labels.append(-100)\n            else:\n                if wid != prev_word_id:\n                    new_labels.append(labels_ids[wid])\n                else:\n                    new_labels.append(-100)\n            prev_word_id = wid\n        return new_labels\n\n    def _tokenize_and_align_chunk(\n        self,\n        docs_tokens: List[List[str]],\n        docs_labels_ids: List[List[int]],\n        max_length: int,\n        stride: int\n    ) -> Dataset:\n        \"\"\"\n        Токенизирует список документов (списки токенов) с возвратом переполнений (sliding window),\n        выравнивает метки по word_ids, формирует Dataset для обучения/валидации.\n\n        :param docs_tokens: Список документов; каждый документ — список токенов (слов).\n        :param docs_labels_ids: Список документов; каждый — список id‑меток по словам.\n        :param max_length: Максимальная длина последовательности модели.\n        :param stride: Перекрытие при нарезке (число токенов).\n        :return: datasets.Dataset с полями input_ids, attention_mask, labels.\n        \"\"\"\n        enc = self.tokenizer(\n            docs_tokens,\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True\n        )\n        mapping = enc.pop(\"overflow_to_sample_mapping\")  # len = кол-во получившихся чанков\n        num_chunks = len(enc[\"input_ids\"])\n\n        all_labels = []\n        for i in range(num_chunks):\n            doc_idx = int(mapping[i])\n            word_ids = enc.word_ids(batch_index=i)\n            aligned = self._align_labels_with_word_ids(docs_labels_ids[doc_idx], word_ids)\n            all_labels.append(aligned)\n\n        return Dataset.from_dict({\n            \"input_ids\": enc[\"input_ids\"],\n            \"attention_mask\": enc[\"attention_mask\"],\n            \"labels\": all_labels\n        })\n\n    def _count_total_chunks(\n        self,\n        docs_tokens: List[List[str]],\n        max_length: int,\n        stride: int,\n        batch_docs: int = 64\n    ) -> int:\n        \"\"\"\n        Оценивает общее число чанков (последовательностей) после sliding window\n        для списка документов, без хранения признаков.\n\n        :param docs_tokens: Список документов (список токенов-слов).\n        :param max_length: Максимальная длина последовательности модели.\n        :param stride: Перекрытие при нарезке.\n        :param batch_docs: Размер пачки документов для ускорения токенизации.\n        :return: Общее число последовательностей.\n        \"\"\"\n        total = 0\n        for i in range(0, len(docs_tokens), batch_docs):\n            batch = docs_tokens[i:i + batch_docs]\n            enc = self.tokenizer(\n                batch,\n                is_split_into_words=True,\n                return_overflowing_tokens=True,\n                max_length=max_length,\n                stride=stride,\n                truncation=True\n            )\n            total += len(enc[\"input_ids\"])\n        return total\n\n    def _compute_class_weights_over_docs(\n        self,\n        docs_tokens: List[List[str]],\n        docs_labels_ids: List[List[int]],\n        max_length: int,\n        stride: int,\n        batch_docs: int = 32\n    ) -> np.ndarray:\n        \"\"\"\n        Считает веса классов по ВСЕМ токенам тренировки, выравнивая метки и\n        не сохраняя полный датасет в памяти.\n\n        :param docs_tokens: Список документов: список токенов (слов).\n        :param docs_labels_ids: Список документов: список id‑меток по словам.\n        :param max_length: Максимальная длина последовательности.\n        :param stride: Перекрытие при нарезке (в токенах).\n        :param batch_docs: Размер батча документов при токенизации.\n        :return: Вектор весов классов формы [K].\n        \"\"\"\n        num_labels = len(self.id2label)\n        counts = np.zeros(num_labels, dtype=np.int64)\n\n        for i in tqdm(range(0, len(docs_tokens), batch_docs), desc=\"Подсчет частот классов (token-level)\"):\n            toks = docs_tokens[i:i + batch_docs]\n            labs = docs_labels_ids[i:i + batch_docs]\n\n            enc = self.tokenizer(\n                toks,\n                is_split_into_words=True,\n                return_overflowing_tokens=True,\n                max_length=max_length,\n                stride=stride,\n                truncation=True\n            )\n            mapping = enc.pop(\"overflow_to_sample_mapping\")\n            num_chunks = len(enc[\"input_ids\"])\n\n            for j in range(num_chunks):\n                doc_idx = int(mapping[j])\n                word_ids = enc.word_ids(batch_index=j)\n                aligned = self._align_labels_with_word_ids(labs[doc_idx], word_ids)\n                arr = np.asarray(aligned, dtype=np.int64)\n                arr = arr[arr >= 0]  # игнорируем -100\n                if arr.size > 0:\n                    bc = np.bincount(arr, minlength=num_labels)\n                    counts += bc\n\n        N = counts.sum()\n        weights = np.zeros(num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        if N > 0:\n            weights[nonzero] = N / (num_labels * counts[nonzero].astype(np.float32))\n        return weights\n\n    def _setup_compute_metrics(self):\n        \"\"\"\n        Создает функцию подсчета seqeval-метрик (precision/recall/f1/accuracy).\n        \"\"\"\n        metric = evaluate.load(\"seqeval\")\n\n        def compute_seqeval_metrics(p):\n            # Поддержка EvalPrediction и tuple\n            if isinstance(p, (tuple, list)):\n                predictions, labels = p\n            else:\n                predictions, labels = p.predictions, p.label_ids\n\n            predictions = np.argmax(predictions, axis=2)\n\n            true_predictions = [\n                [self.id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n\n            true_labels = [\n                [self.id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n                for prediction, label in zip(predictions, labels)\n            ]\n\n            results = metric.compute(predictions=true_predictions, references=true_labels)\n            return {\n                \"precision\": results.get(\"overall_precision\", 0.0),\n                \"recall\": results.get(\"overall_recall\", 0.0),\n                \"f1\": results.get(\"overall_f1\", 0.0),\n                \"accuracy\": results.get(\"overall_accuracy\", 0.0),\n            }\n\n        self.compute_metrics = compute_seqeval_metrics\n\n    def _prepare_dataset_with_sliding_window(self, df: pd.DataFrame, max_length: int, stride: int) -> Dataset:\n        \"\"\"\n        Готовит токенизированный Dataset для списка документов (используется, например, для валидации).\n        Выполняет:\n          - маппинг строковых тегов -> id при необходимости,\n          - токенизацию с переполнениями,\n          - выравнивание меток по word_ids.\n\n        :param df: Датафрейм с колонками токенов и меток.\n        :param max_length: Максимальная длина модели.\n        :param stride: Перекрытие sliding window.\n        :return: datasets.Dataset с полями input_ids, attention_mask, labels.\n        \"\"\"\n        docs_tokens = df[self.tokens_column_name].tolist()\n        docs_labels = df[self.tags_column_name].tolist()\n\n        # Если метки строковые — переводим в id\n        if len(docs_labels) and isinstance(docs_labels[0][0], str):\n            docs_labels = [[self.label2id[tag] for tag in tags] for tags in docs_labels]\n\n        return self._tokenize_and_align_chunk(docs_tokens, docs_labels, max_length, stride)\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        test_size: float = 0.2,\n        learning_rate: float = 2e-5,\n        fp16: bool = True,\n        stride: int = 128,\n        logging_steps: int = 50,\n        eval_steps: int = 100,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        fit_chunk_size_docs: Optional[int] = None\n    ):\n        \"\"\"\n        Обучает модель токен‑классификации с поддержкой больших данных:\n        train_dataset подставляется чанками документов, внутри которых выполняется sliding window.\n\n        :param train_data: Датафрейм: колонки с токенами и метками на уровне слов.\n        :param epochs: Кол-во эпох.\n        :param per_device_train_batch_size: Размер батча на устройство.\n        :param gradient_accumulation_steps: Шаги аккумуляции градиентов.\n        :param test_size: Доля валидации (по документам).\n        :param learning_rate: LR для AdamW.\n        :param fp16: Использовать fp16 (если bf16 не доступен).\n        :param stride: Перекрытие при нарезке (в токенах).\n        :param logging_steps: Частота логирования.\n        :param eval_steps: Частота валидации/сохранения.\n        :param output_dir: Папка для артефактов.\n        :param seed: Зерно.\n        :param fit_chunk_size_docs: Сколько документов подставлять в один тренировочный чанк. Если None — все.\n        :return: self.\n        \"\"\"\n        set_seed(seed)\n        max_length = int(getattr(self.model.config, \"max_position_embeddings\", 512))\n\n        # Маппинг строковых тегов -> id (все данные)\n        df_all = train_data.copy()\n        if len(df_all) and len(df_all[self.tags_column_name]) and isinstance(df_all[self.tags_column_name].iloc[0][0], str):\n            df_all[self.tags_column_name] = df_all[self.tags_column_name].apply(\n                lambda tags: [self.label2id[tag] for tag in tags]\n            )\n\n        # Сплит по документам (потом каждый набор будет чанковаться по sliding window)\n        df_train, df_eval = train_test_split(df_all, test_size=test_size, random_state=seed, shuffle=True)\n\n        # Eval датасет можно подготовить целиком (обычно компактнее train)\n        eval_dataset = self._prepare_dataset_with_sliding_window(df_eval, max_length, stride)\n\n        # Документы тренировки (списки токенов и меток)\n        train_docs_tokens = df_train[self.tokens_column_name].tolist()\n        train_docs_labels = df_train[self.tags_column_name].tolist()\n\n        # Веса классов на всём train (по токенам, без хранения полного tokenized train)\n        class_weights = self._compute_class_weights_over_docs(\n            docs_tokens=train_docs_tokens,\n            docs_labels_ids=train_docs_labels,\n            max_length=max_length,\n            stride=stride,\n            batch_docs=32\n        )\n\n        # Настройка метрик\n        self._setup_compute_metrics()\n\n        # bf16 если доступен, иначе fp16 при флаге\n        bf16_ok = bool(torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",         # <- по вашей просьбе используем eval_strategy\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"f1\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=bool(fp16 and torch.cuda.is_available() and not bf16_ok),\n            bf16=bool(bf16_ok and not fp16),\n            dataloader_num_workers=0,\n            seed=seed,\n            remove_unused_columns=False,\n            disable_tqdm=True  # используем внешний tqdm\n        )\n\n        data_collator = self.data_collator\n\n        # Вспомогательные: шаги по количеству сэмплов (последовательностей), а не документов\n        def steps_for_size(n_samples: int, bsz: int, accum: int) -> int:\n            \"\"\"\n            Оценивает число оптимизационных шагов на чанке из n_samples последовательностей.\n            \"\"\"\n            return max(0, math.ceil(math.ceil(n_samples / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(n_docs: int, chunk_docs: int):\n            \"\"\"\n            Генератор срезов индексов документов по chunk_docs.\n            \"\"\"\n            for i in range(0, n_docs, chunk_docs):\n                yield slice(i, min(i + chunk_docs, n_docs))\n\n        # Объем чанка по документам (по умолчанию — все документы)\n        n_docs = len(train_docs_tokens)\n        chunk_docs = int(fit_chunk_size_docs) if (fit_chunk_size_docs and fit_chunk_size_docs > 0) else n_docs\n\n        # Предварительный расчет total_steps (по числу последовательностей после токенизации)\n        total_steps = 0\n        rng = np.random.default_rng(seed)\n        doc_indices = np.arange(n_docs)\n\n        for _ in range(epochs):\n            rng.shuffle(doc_indices)\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = doc_indices[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                # считаем, сколько получится последовательностей в этом чанке документов\n                n_samples = self._count_total_chunks(toks_chunk, max_length, stride, batch_docs=64)\n                total_steps += steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n\n        # Инициализируем Trainer с \"минимальным\" train_dataset (пустой/минимальный чанк)\n        # чтобы не держать всю тренировочную выборку\n        if n_docs > 0:\n            init_chunk_ds = self._tokenize_and_align_chunk(\n                [train_docs_tokens[0]], [train_docs_labels[0]], max_length, stride\n            )\n        else:\n            init_chunk_ds = eval_dataset  # fallback\n\n        self.trainer = WeightedTokenCETrainer(\n            model=self.model,\n            args=args,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            train_dataset=init_chunk_ds,\n            eval_dataset=eval_dataset,\n            tokenizer=self.tokenizer,\n            class_weights=class_weights\n        )\n        # Удаляем стандартный принтер логов, чтобы не конфликтовал с внешним tqdm\n        try:\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n\n        # Планировщик под рассчитанное число шагов\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        # Внешний прогресс-бар\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        # Основной цикл обучения по эпохам/чанкам документов\n        steps_done = 0\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            order = np.arange(n_docs)\n            rng.shuffle(order)\n\n            for slc in chunk_slices(n_docs, chunk_docs):\n                idx = order[slc]\n                toks_chunk = [train_docs_tokens[i] for i in idx]\n                labs_chunk = [train_docs_labels[i] for i in idx]\n\n                # Готовим датасет последовательностей для этого чанка документов\n                ds_chunk = self._tokenize_and_align_chunk(toks_chunk, labs_chunk, max_length, stride)\n                self.trainer.train_dataset = ds_chunk\n\n                # Шаги на текущем чанке\n                n_samples = len(ds_chunk)  # число последовательностей после sliding window\n                chunk_steps = steps_for_size(n_samples, per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk\n                    continue\n\n                # Дообучаем до steps_done + chunk_steps\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                # Очистка памяти\n                del ds_chunk\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n        return self\n\n    def _predict_single_document(self, tokens: List[str], stride: int) -> List[str]:\n        \"\"\"\n        Предсказывает метки на уровне слов для одного документа с помощью sliding window.\n\n        :param tokens: Список токенов (слов) документа.\n        :param stride: Перекрытие при нарезке.\n        :return: Список предсказанных тегов (строки) длиной = числу слов.\n        \"\"\"\n        max_length = int(getattr(self.model.config, \"max_position_embeddings\", 512))\n\n        tokenized_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n        )\n        tokenized_inputs.pop(\"overflow_to_sample_mapping\", None)\n        chunk_dataset = Dataset.from_dict(tokenized_inputs)\n\n        outputs = self.trainer.predict(chunk_dataset)\n        predictions = np.argmax(outputs.predictions, axis=2)\n\n        num_original_words = len(tokens)\n        final_predictions = np.full(num_original_words, -1, dtype=np.int32)\n\n        for i, chunk_preds in enumerate(predictions):\n            chunk_word_ids = tokenized_inputs.word_ids(batch_index=i)\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if word_id is not None and final_predictions[word_id] == -1:\n                    final_predictions[word_id] = int(chunk_preds[token_pos])\n\n        return [self.id2label.get(pid, 'O') for pid in final_predictions]\n\n    def predict(self, df: pd.DataFrame, stride: int = 128) -> List[List[str]]:\n        \"\"\"\n        Делает предсказания на уровне слов для набора документов (через sliding window).\n\n        :param df: Датафрейм с колонкой токенов (список слов).\n        :param stride: Перекрытие при нарезке.\n        :return: Список предсказанных последовательностей тегов по документам.\n        \"\"\"\n        all_final_labels = []\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Предсказание (sliding window)\"):\n            original_tokens = row[self.tokens_column_name]\n            if not original_tokens:\n                all_final_labels.append([])\n                continue\n            document_labels = self._predict_single_document(original_tokens, stride)\n            all_final_labels.append(document_labels)\n        return all_final_labels\n\n    def _get_embeddings_single_document(self, tokens: List[str], stride: int, device: torch.device) -> np.ndarray:\n        \"\"\"\n        Возвращает усредненные эмбеддинги токенов на уровне слов (после объединения частей\n        от sliding window) для одного документа.\n\n        :param tokens: Список токенов (слов).\n        :param stride: Перекрытие при нарезке.\n        :param device: Целевой девайс модели.\n        :return: Массив [num_words, hidden_size].\n        \"\"\"\n        max_length = int(getattr(self.model.config, \"max_position_embeddings\", 512))\n        num_original_words = len(tokens)\n\n        chunk_inputs = self.tokenizer(\n            [tokens],\n            is_split_into_words=True,\n            return_overflowing_tokens=True,\n            max_length=max_length,\n            stride=stride,\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        chunk_inputs.pop(\"overflow_to_sample_mapping\")\n\n        with torch.no_grad():\n            base_model = getattr(self.trainer.model, self.trainer.model.base_model_prefix)\n            outputs = base_model(**chunk_inputs)\n\n        chunk_embeddings = outputs.last_hidden_state  # [num_chunks, seq_len, hidden]\n\n        hidden_size = int(self.model.config.hidden_size)\n        final_word_embeddings = torch.zeros(num_original_words, hidden_size, device=device)\n        word_counts = torch.zeros(num_original_words, device=device)\n\n        for i in range(len(chunk_embeddings)):\n            chunk_embeds = chunk_embeddings[i]\n            chunk_word_ids = chunk_inputs.word_ids(batch_index=i)\n\n            for token_pos, word_id in enumerate(chunk_word_ids):\n                if word_id is not None:\n                    final_word_embeddings[word_id] += chunk_embeds[token_pos]\n                    # считаем количество фрагментов для усреднения по слову\n                    if token_pos == 0 or chunk_word_ids[token_pos - 1] != word_id:\n                        word_counts[word_id] += 1\n\n        average_embeddings = final_word_embeddings / (word_counts.unsqueeze(1) + 1e-8)\n        return average_embeddings.detach().cpu().numpy()\n\n    def get_embeddings(self, df: pd.DataFrame, stride: int = 128) -> List[np.ndarray]:\n        \"\"\"\n        Генерирует эмбеддинги на уровне слов для набора документов (через sliding window).\n\n        :param df: Датафрейм с колонкой токенов (список слов).\n        :param stride: Перекрытие при нарезке.\n        :return: Список массивов [num_words, hidden_size] для каждого документа.\n        :raises RuntimeError: Если модель не обучена.\n        \"\"\"\n        if self.trainer is None or self.trainer.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        self.trainer.model.eval()\n        device = self.trainer.model.device\n        all_final_embeddings = []\n\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Генерация эмбеддингов (sliding window)\"):\n            original_tokens = row[self.tokens_column_name]\n            if not original_tokens:\n                all_final_embeddings.append(np.zeros((0, int(self.model.config.hidden_size)), dtype=np.float32))\n                continue\n\n            document_embeddings = self._get_embeddings_single_document(original_tokens, stride, device)\n            all_final_embeddings.append(document_embeddings)\n\n        return all_final_embeddings","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nПример использования.","metadata":{}},{"cell_type":"code","source":"train_data = pd.DataFrame({\n    'tokens': [\n        ['Федор', 'Достоевский', 'родился', 'в', 'Москве', '.'],\n        ['Анна', 'Керн', 'была', 'музой', 'Пушкина', '.'],\n        ['Компания', 'Яндекс', 'представила', 'Алису', '.'],\n        ['Илон', 'Маск', 'основал', 'SpaceX', 'и', 'Tesla', '.']\n    ],\n    'ner_tags': [\n        ['B-PER', 'I-PER', 'O', 'O', 'B-LOC', 'O'],\n        ['B-PER', 'I-PER', 'O', 'O', 'B-PER', 'O'],\n        ['O', 'B-ORG', 'O', 'B-PER', 'O'],\n        ['B-PER', 'I-PER', 'O', 'B-ORG', 'O', 'B-ORG', 'O']\n    ]\n})\n\n# Для предсказания нам нужны только токены\nsubmission_data = pd.DataFrame({\n    'tokens': [\n        ['Лев', 'Толстой', 'написал', 'роман', '\"', 'Война', 'и', 'мир', '\"', '.'],\n        ['Сергей', 'Королев', 'работал', 'в', 'РКК', '\"', 'Энергия', '\"', '.']\n    ]\n})\n\ntrain_data = pd.concat([train_data] * 15, axis=0)\nsubmission_data = pd.concat([submission_data] * 15, axis=0)\n\n# 2. Создание и обучение модели\n# Создаем маппинг из тегов в ID\ntags = train_data['ner_tags'].explode().unique()\nlabel2id = {tag: i for i, tag in enumerate(tags)}\n\nmodel = TokenClassification(\n    checkpoint='DeepPavlov/rubert-base-cased',\n    label2id=label2id,\n    tokens_column_name='tokens',\n    tags_column_name='ner_tags'\n)\n\nmodel.fit(\n    train_data,\n    epochs=3,\n    test_size=0.25,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-5,\n    fp16=False,\n    logging_steps=10,\n    eval_steps=10\n)\n\n# 3. Прогнозирование и получение эмбеддингов\nlabels = model.predict(submission_datap[:5])\nembeddings = model.get_embeddings(submission_data[:5])\n\nprint(labels)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение классификатора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация классификатора.","metadata":{}},{"cell_type":"code","source":"!pip install -q wav2clip torchaudio evaluate pillow\n\nimport gc\nimport math\nfrom typing import List, Dict, Any, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport evaluate\nfrom transformers import Trainer, TrainingArguments, TrainerCallback, PrinterCallback\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom tqdm.auto import tqdm\n\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Фиксирует зерно для воспроизводимости.\n\n    :param seed: Целое число для инициализации генераторов случайных чисел.\n    \"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef to_pil(x: Union[str, np.ndarray, Image.Image]) -> Image.Image:\n    \"\"\"\n    Приводит вход к PIL.Image в формате RGB.\n\n    :param x: Путь к изображению, np.ndarray (H,W[,C]) или PIL.Image.\n    :return: PIL.Image (RGB).\n    :raises ValueError: Если тип входа не поддерживается.\n    \"\"\"\n    if isinstance(x, Image.Image):\n        return x.convert(\"RGB\")\n    if isinstance(x, str):\n        return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray):\n        return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    \"\"\"\n    Загружает аудиофайл и при необходимости ресемплирует до target_sr.\n\n    :param path: Путь к файлу (wav/flac и т.п.).\n    :param target_sr: Целевая частота дискретизации.\n    :return: Одноканальный сигнал формы [T] float32.\n    :raises RuntimeError: Если torchaudio не установлен.\n    \"\"\"\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)  # [C, T]\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr:\n        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\nclass MultiComboDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset для различных комбинаций модальностей (text/image/audio).\n    Поддерживает несколько изображений/аудио per-сэмпл за счёт того, что ячейки могут быть списками.\n\n    Важно: сам датасет хранит лишь ссылки/пути/строки. Реальная загрузка PIL/аудио происходит в collate бэкенда\n    (лениво и батчево), что позволяет работать с большими данными.\n\n    :param df: Источник данных (DataFrame).\n    :param target_col: Имя колонки с таргетом (классовой меткой).\n    :param label2id: Словарь {значение_метки -> id}. Значения меток в df[target_col] должны встречаться в ключах.\n    :param text_columns: Текстовые колонки; их значения склеиваются в одну строку.\n    :param image_columns: Колонки с изображениями (значение — путь/PIL/numpy или список таковых).\n    :param audio_columns: Колонки с аудио (значение — путь/массив или список таковых).\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_col: str,\n        label2id: Dict[Any, int],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None\n    ):\n        self.df = df.reset_index(drop=True)\n        self.target_col = target_col\n        self.label2id = label2id\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.sep = \" [SEP] \"\n\n    def __len__(self) -> int:\n        \"\"\"\n        :return: Количество элементов.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        \"\"\"\n        Возвращает элемент датасета.\n\n        :param idx: Индекс строки.\n        :return: Словарь с ключами:\n                 - 'labels' (int id)\n                 - 'text' (str), если есть текстовые колонки\n                 - 'images' (list), если есть колонки картинок\n                 - 'audios' (list), если есть колонки аудио\n        \"\"\"\n        row = self.df.iloc[idx]\n        item = {\"labels\": int(self.label2id[row[self.target_col]]) if self.target_col in row else 0}\n\n        if self.text_columns:\n            item[\"text\"] = self.sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n        def _as_list(v):\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                return []\n            if isinstance(v, (list, tuple)):\n                return list(v)\n            return [v]\n\n        if self.image_columns:\n            imgs = []\n            for c in self.image_columns:\n                if c in row:\n                    imgs.extend(_as_list(row[c]))\n            item[\"images\"] = imgs\n\n        if self.audio_columns:\n            auds = []\n            for c in self.audio_columns:\n                if c in row:\n                    auds.extend(_as_list(row[c]))\n            item[\"audios\"] = auds\n\n        return item\n\n\nclass BaseBackend(nn.Module):\n    \"\"\"\n    Базовый класс мультимодального бэкенда.\n\n    Атрибуты:\n      - name: Название бэкенда (str).\n      - supported: Набор поддерживаемых модальностей, например {'text','image'}.\n      - embed_dim: Базовая размерность эмбеддингов (int).\n      - out_dim_per_modality: Реальные выходные размерности (dict modality->int), учитывая агрегацию (concat/mean).\n    \"\"\"\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Собирает батч для Trainer из списка элементов Dataset.\n\n        :param batch: Список элементов (из MultiComboDataset.__getitem__).\n        :return: Словарь с 'labels' (LongTensor) и 'backend_inputs' (dict).\n        \"\"\"\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует модальности и возвращает L2-нормированные эмбеддинги.\n\n        :param backend_inputs: Подготовленные входы (collate).\n        :param device: Девайс для инференса.\n        :return: Словарь {'text':[B,*], 'image':[B,*], 'audio':[B,*]} по доступным модальностям.\n        \"\"\"\n        raise NotImplementedError\n\n    def freeze_all(self):\n        \"\"\"\n        Замораживает параметры бэкенда (requires_grad=False), полезно для linear probing.\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def get_out_dim(self, modality: str) -> int:\n        \"\"\"\n        Возвращает выходную размерность эмбеддинга по модальности с учётом агрегации.\n\n        :param modality: 'text' | 'image' | 'audio'.\n        :return: Размерность вектора.\n        \"\"\"\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n\nclass ClipBackend(BaseBackend):\n    \"\"\"\n    Бэкенд CLIP (HF) для модальностей: text + image.\n    Поддерживает несколько изображений per-сэмпл с агрегацией (concat или mean).\n\n    :param checkpoint: Модель CLIP на HF (например, 'openai/clip-vit-base-patch32').\n    :param max_length: Максимальная длина текстовых токенов.\n    :param freeze: Заморозить ли веса CLIP.\n    :param max_images: Максимум картинок на сэмпл при concat-паде.\n    :param image_agg: 'concat' или 'mean' — как агрегировать несколько изображений.\n    \"\"\"\n    name = \"clip\"\n    supported = {\"text\", \"image\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        max_images: int = 1,\n        image_agg: str = \"concat\"\n    ):\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.max_images = int(max_images)\n        self.image_agg = image_agg\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Collate для CLIP: ленивая загрузка изображений, подготовка токенов текста.\n\n        :param batch: Список элементов датасета.\n        :return: {'labels': LongTensor[B], 'backend_inputs': {...}}\n        \"\"\"\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n\n        text_inputs = self.processor(\n            text=texts, padding=True, truncation=True,\n            max_length=self.max_length, return_tensors=\"pt\"\n        )\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенация до max_k эмбеддингов на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги изображений [M, D], где M = сумма counts.\n        :param counts: Количество изображений на сэмпл (длина B).\n        :param max_k: Максимум картинок на сэмпл.\n        :return: [B, D*max_k], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усреднение эмбеддингов изображений по сэмплу.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количество изображений на сэмпл (длина B).\n        :return: [B, D], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует текст/изображения через CLIP и агрегирует изображения.\n\n        :param backend_inputs: Выход collate.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'image':[B, D*max_images] или [B,D]}.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                img_z = self._concat_padded(img_flat, counts, self.max_images)\n            else:\n                img_z = self._mean_pool(img_flat, counts)\n        else:\n            if self.image_agg == \"concat\":\n                img_z = torch.zeros((len(counts), self.embed_dim * self.max_images), device=device)\n            else:\n                img_z = torch.zeros((len(counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"image\": img_z}\n\n\nclass ClapBackend(BaseBackend):\n    \"\"\"\n    Бэкенд CLAP (HF) для модальностей: text + audio.\n    Поддерживает несколько аудио per-сэмпл (concat/mean).\n\n    :param checkpoint: Модель CLAP (например, 'laion/clap-htsat-unfused').\n    :param freeze: Заморозить ли веса CLAP.\n    :param max_audios: Максимум аудио на сэмпл при concat-паде.\n    :param audio_agg: 'concat' или 'mean' — как агрегировать несколько аудио.\n    \"\"\"\n    name = \"clap\"\n    supported = {\"text\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"laion/clap-htsat-unfused\",\n        freeze: bool = True,\n        max_audios: int = 1,\n        audio_agg: str = \"concat\"\n    ):\n        super().__init__()\n        from transformers import ClapModel, ClapProcessor\n        self.model = ClapModel.from_pretrained(checkpoint)\n        self.processor = ClapProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(getattr(self.model.config, \"projection_dim\", 512))\n        sr = getattr(self.processor, \"sampling_rate\", None)\n        if sr is None:\n            fe = getattr(self.processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n        self.sr = int(sr)\n        self.max_audios = int(max_audios)\n        self.audio_agg = audio_agg\n        if freeze:\n            self.freeze_all()\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"audio\": aud_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Collate для CLAP: ленивая загрузка и препроцессинг аудио, токены текста.\n\n        :param batch: Список элементов датасета.\n        :return: {'labels': LongTensor[B], 'backend_inputs': {...}}\n        \"\"\"\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"CLAP ожидает путь к аудио или numpy.ndarray\")\n\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_audios):\n            aud_proc = self.processor(audios=flat_audios, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n            audio_inputs = {\"input_features\": aud_proc[\"input_features\"]}\n        else:\n            audio_inputs = {\"input_features\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенация аудио-эмбеддингов (до max_k) с нулевым паддингом.\n\n        :param embs: Плоские эмбеддинги аудио [M, D].\n        :param counts: Кол-во аудио на сэмпл (B).\n        :param max_k: Максимум аудио на сэмпл.\n        :return: [B, D*max_k], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усреднение аудио-эмбеддингов по сэмплу.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Кол-во аудио на сэмпл (B).\n        :return: [B, D], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует текст/аудио через CLAP и агрегирует аудио.\n\n        :param backend_inputs: Выход collate.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'audio':[B, D*max_audios] или [B,D]}.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"audio_counts\"].tolist()\n        af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n        if af is not None:\n            af = af.to(device)\n            aud_flat = self.model.get_audio_features(input_features=af)\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                aud_z = self._concat_padded(aud_flat, counts, self.max_audios)\n            else:\n                aud_z = self._mean_pool(aud_flat, counts)\n        else:\n            if self.audio_agg == \"concat\":\n                aud_z = torch.zeros((len(counts), self.embed_dim * self.max_audios), device=device)\n            else:\n                aud_z = torch.zeros((len(counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"audio\": aud_z}\n\n\nclass ClipWav2CLIPBackend(BaseBackend):\n    \"\"\"\n    Бэкенд: CLIP (text+image) + Wav2CLIP (audio->CLIP пространство). Модальности: ['text','image','audio'].\n    Поддержка мульти‑изображений/аудио через concat/mean агрегацию.\n\n    :param checkpoint: Чекпоинт CLIP (HF).\n    :param max_length: Максимальная длина токенов текста.\n    :param freeze: Замораживать ли веса CLIP.\n    :param audio_sr: Частота дискретизации для аудио в wav2clip.\n    :param max_images: Максимум изображений на сэмпл (concat).\n    :param max_audios: Максимум аудио на сэмпл (concat).\n    :param image_agg: 'concat' или 'mean' — агрегация изображений.\n    :param audio_agg: 'concat' или 'mean' — агрегация аудио.\n    \"\"\"\n    name = \"clip_wav2clip\"\n    supported = {\"text\", \"image\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        audio_sr: int = 16000,\n        max_images: int = 1,\n        max_audios: int = 1,\n        image_agg: str = \"concat\",\n        audio_agg: str = \"concat\"\n    ):\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.audio_sr = int(audio_sr)\n        self.max_images = int(max_images)\n        self.max_audios = int(max_audios)\n        self.image_agg = image_agg\n        self.audio_agg = audio_agg\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out, \"audio\": aud_out}\n        # Lazy инициализация wav2clip\n        self._w2c_model = None\n        self._w2c_mod = None\n        self._w2c_api = None\n        self._w2c_device = None\n\n    def _ensure_w2c(self, device: torch.device):\n        \"\"\"\n        Готовит wav2clip к использованию с учётом разных API вариантов (load_model/get_model/Wav2CLIP).\n\n        :param device: Девайс выполнения.\n        :raises RuntimeError: Если wav2clip не установлен или не удалось загрузить модель.\n        \"\"\"\n        if self._w2c_model is not None and self._w2c_device == str(device):\n            return\n        import importlib\n        try:\n            w2c = importlib.import_module(\"wav2clip\")\n        except Exception as e:\n            raise RuntimeError(\"Не найден пакет 'wav2clip'. Установите: pip install wav2clip\") from e\n        dev_str = str(device) if device.type == \"cuda\" else \"cpu\"\n        if hasattr(w2c, \"load_model\"):\n            model = w2c.load_model(device=dev_str); api_kind = \"func\"\n        elif hasattr(w2c, \"get_model\"):\n            model = w2c.get_model(device=dev_str); api_kind = \"func\"\n        elif hasattr(w2c, \"Wav2CLIP\"):\n            try:\n                model = w2c.Wav2CLIP(dev_str)\n            except TypeError:\n                model = w2c.Wav2CLIP(device=dev_str)\n            api_kind = \"method\"\n        else:\n            raise RuntimeError(\"wav2clip установлен, но нет способов загрузки (load_model/get_model/Wav2CLIP)\")\n        self._w2c_mod = w2c\n        self._w2c_model = model\n        self._w2c_api = api_kind\n        self._w2c_device = str(device)\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Collate: готовит токены текста/пиксели изображения, пакует аудио в padded-матрицу.\n\n        :param batch: Список элементов датасета.\n        :return: {'labels': LongTensor[B], 'backend_inputs': {...}}\n        \"\"\"\n        labels = torch.tensor([b.get(\"labels\", 0) for b in batch], dtype=torch.long)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        # images (flatten + counts)\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, img_counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            img_counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        # audios (flatten + counts)\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, aud_counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            aud_counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.audio_sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"Ожидается путь к аудио или numpy.ndarray\")\n\n        if len(flat_audios):\n            Lmax = max(len(a) for a in flat_audios)\n            wav = np.zeros((len(flat_audios), Lmax), dtype=np.float32)\n            lens = np.zeros((len(flat_audios),), dtype=np.int64)\n            for i, a in enumerate(flat_audios):\n                L = len(a)\n                wav[i, :L] = a\n                lens[i] = L\n            audio_inputs = {\n                \"waveforms\": torch.from_numpy(wav),   # [M, Lmax]\n                \"lengths\": torch.from_numpy(lens)     # [M]\n            }\n        else:\n            audio_inputs = {\"waveforms\": None, \"lengths\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(img_counts, dtype=torch.long),\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(aud_counts, dtype=torch.long),\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _agg_concat(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенация эмбеддингов (до max_k) с нулевым паддингом.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Кол-во элементов на сэмпл (B).\n        :param max_k: Максимум элементов на сэмпл.\n        :return: [B, D*max_k], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset+c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _agg_mean(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усреднение эмбеддингов по сэмплу.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Кол-во элементов на сэмпл (B).\n        :return: [B, D], L2-нормированный.\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset+c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует текст/изображение через CLIP и аудио через wav2clip, далее агрегирует.\n\n        :param backend_inputs: Выход collate.\n        :param device: Девайс.\n        :return: {'text':[B,*], 'image':[B,*], 'audio':[B,*]}.\n        \"\"\"\n        # text\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        # image\n        img_counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                image_z = self._agg_concat(img_flat, img_counts, self.max_images)\n            else:\n                image_z = self._agg_mean(img_flat, img_counts)\n        else:\n            if self.image_agg == \"concat\":\n                image_z = torch.zeros((len(img_counts), self.embed_dim * self.max_images), device=device)\n            else:\n                image_z = torch.zeros((len(img_counts), self.embed_dim), device=device)\n\n        # audio via wav2clip\n        aud_counts = backend_inputs[\"audio_counts\"].tolist()\n        wav = backend_inputs[\"audio_inputs\"][\"waveforms\"]\n        lens = backend_inputs[\"audio_inputs\"][\"lengths\"]\n        if wav is not None and lens is not None and (lens.numel() if torch.is_tensor(lens) else len(lens)) > 0:\n            self._ensure_w2c(device)\n            w2c = self._w2c_mod\n            waves = wav  # [M, Lmax] (CPU тензор — ок)\n            lens_np = lens.cpu().numpy()\n            embs = []\n            for i in range(waves.size(0)):\n                L = int(lens_np[i])\n                a_np = waves[i, :L].detach().cpu().numpy()\n                e = None\n                if self._w2c_api == \"func\":\n                    if hasattr(w2c, \"embed_audio\"):\n                        try:\n                            e = w2c.embed_audio(a_np, self._w2c_model)\n                        except TypeError:\n                            e = w2c.embed_audio(a_np, self.audio_sr, self._w2c_model)\n                    elif hasattr(w2c, \"get_audio_embedding\"):\n                        try:\n                            e = w2c.get_audio_embedding(a_np, self._w2c_model)\n                        except TypeError:\n                            e = w2c.get_audio_embedding(a_np, self.audio_sr, self._w2c_model)\n                if e is None and self._w2c_api == \"method\" and hasattr(self._w2c_model, \"embed_audio\"):\n                    try:\n                        e = self._w2c_model.embed_audio(a_np)\n                    except TypeError:\n                        e = self._w2c_model.embed_audio(a_np, sr=self.audio_sr)\n                if e is None:\n                    raise RuntimeError(\"Не удалось получить аудио‑эмбеддинг через wav2clip.\")\n                e = np.asarray(e)\n                if e.ndim == 2:\n                    e = e.mean(axis=0)\n                elif e.ndim > 2:\n                    e = e.reshape(-1, e.shape[-1]).mean(axis=0)\n                embs.append(e.astype(np.float32))\n            aud_flat = torch.tensor(np.stack(embs, axis=0), dtype=torch.float32, device=device)\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                audio_z = self._agg_concat(aud_flat, aud_counts, self.max_audios)\n            else:\n                audio_z = self._agg_mean(aud_flat, aud_counts)\n        else:\n            if self.audio_agg == \"concat\":\n                audio_z = torch.zeros((len(aud_counts), self.embed_dim * self.max_audios), device=device)\n            else:\n                audio_z = torch.zeros((len(aud_counts), self.embed_dim), device=device)\n\n        return {\"text\": text_z, \"image\": image_z, \"audio\": audio_z}\n\n\nclass SingleBackboneClassifier(nn.Module):\n    \"\"\"\n    Классификатор поверх одного мультимодального бэкенда: encode -> fuse -> MLP голова.\n\n    :param backend: Экземпляр бэкенда (CLIP/CLAP/ClipWav2CLIP).\n    :param modalities: Активные модальности (учёт порядка важен при concat): подмножество ['image','text','audio'].\n    :param num_labels: Количество классов.\n    :param fusion: 'concat' (объединение признаков) или 'mean' (среднее по модальностям).\n    :param hidden: Размер скрытого слоя головы.\n    :param dropout: Дропаут в голове.\n    \"\"\"\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_labels: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_labels = num_labels\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать, а у нас: {dict(zip(order, dims))}')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_labels)\n        )\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        \"\"\"\n        Находит девайс по первому тензору во входах; иначе выбирает доступный cuda/cpu.\n\n        :param obj: Любая структура с тензорами.\n        :return: torch.device.\n        \"\"\"\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Объединяет эмбеддинги модальностей согласно self.fusion.\n\n        :param z: Словарь эмбеддингов по модальностям.\n        :return: Fused тензор [B, *].\n        \"\"\"\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        for m in order:\n            t = z[m]\n            if t.dim() == 3:\n                t = t.mean(dim=1)\n            elif t.dim() > 3:\n                t = t.view(t.size(0), -1)\n            feats.append(t)\n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            sizes = [f.size(-1) for f in feats]\n            if len(set(sizes)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать, а у нас: {sizes}')\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None):\n        \"\"\"\n        Прямой проход модели.\n\n        :param backend_inputs: Входы для бэкенда (из его collate).\n        :param labels: Игнорируется (loss считает Trainer).\n        :return: SequenceClassifierOutput с logits [B, num_labels].\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        logits = self.head(fused)\n        return SequenceClassifierOutput(logits=logits)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        \"\"\"\n        Извлекает fused эмбеддинги (и опционально по модальностям).\n\n        :param backend_inputs: Входы для бэкенда.\n        :param return_per_modality: Вернуть также словарь {'text','image','audio'}.\n        :return: fused [B, *] или (fused, per_modality).\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\nclass WeightedCETrainer(Trainer):\n    \"\"\"\n    Trainer с CrossEntropyLoss и поддержкой весов классов.\n    При отсутствии class_weights может вычислять их по частотам train-меток.\n\n    :param num_labels: Количество классов.\n    :param train_labels: Список/массив train-меток (int).\n    :param class_weights: Веса классов (list/np.ndarray/torch.Tensor).\n    \"\"\"\n    def __init__(self, *args, num_labels=None, train_labels=None, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_labels = num_labels\n        if class_weights is not None:\n            w = torch.as_tensor(class_weights, dtype=torch.float32)\n        else:\n            w = None\n            if train_labels is not None and num_labels is not None:\n                train_labels = np.asarray(train_labels).astype(int)\n                counts = np.bincount(train_labels, minlength=num_labels)\n                n = counts.sum()\n                weights = np.zeros(num_labels, dtype=np.float32)\n                nonzero = counts > 0\n                weights[nonzero] = n / (num_labels * counts[nonzero].astype(np.float32))\n                w = torch.tensor(weights, dtype=torch.float32)\n        self.class_weights = w\n        self._warned_label_tiling = False\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        \"\"\"\n        Считает CrossEntropyLoss с опциональными весами классов. Защищается от случая DataParallel,\n        когда logits дублируются по числу GPU.\n\n        :param model: Модель.\n        :param inputs: Батч: {'labels': LongTensor[B], 'backend_inputs': {...}}.\n        :param return_outputs: Возвращать ли outputs вместе с loss.\n        :param num_items_in_batch: Совместимость с Trainer API (не используется).\n        :return: loss (и outputs, если return_outputs=True).\n        :raises ValueError: Если размеры batch не согласованы.\n        \"\"\"\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        labels = labels.to(logits.device)\n\n        if logits.size(0) != labels.size(0):\n            ngpu = torch.cuda.device_count()\n            if ngpu > 1 and logits.size(0) == labels.size(0) * ngpu:\n                labels = labels.repeat_interleave(ngpu)\n                if not self._warned_label_tiling:\n                    print(f\"[Warning] DataParallel удвоил batch для logits. \"\n                          f\"Повторяем labels x{ngpu}. logits: {tuple(logits.shape)}, labels: {tuple(labels.shape)}\")\n                    self._warned_label_tiling = True\n            else:\n                raise ValueError(f\"Batch size mismatch: logits {tuple(logits.shape)} vs labels {tuple(labels.shape)}\")\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        loss = nn.CrossEntropyLoss(weight=weight)(logits, labels.long())\n        return (loss, outputs) if return_outputs else loss\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                parts.append(f\"{k.replace('eval_', '')} {v:.4f}\")\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\nclass SingleModelMultiComboClassification:\n    \"\"\"\n    Пайплайн: одна мультимодальная модель (бэкенд) + классификационная голова + HuggingFace Trainer.\n\n    Поддерживаемые комбинации модальностей:\n      - ['text','image']         -> ClipBackend (CLIP, HF)\n      - ['text','audio']         -> ClapBackend (CLAP, HF)\n      - ['image','audio']        -> ClipWav2CLIPBackend\n      - ['text','image','audio'] -> ClipWav2CLIPBackend\n\n    Возможности:\n      - Мульти-изображения/аудио per-сэмпл (concat/mean агрегация).\n      - Обучение на больших данных: чанковая подстановка train_dataset, стабильный прогресс‑бар и логи.\n      - Взвешенная CrossEntropy по частотам классов (для дисбаланса).\n\n    Необходимые импорты:\n    !pip install -q wav2clip torchaudio evaluate pillow\n    import gc\n    import math\n    from typing import List, Dict, Any, Optional, Union\n    import numpy as np\n    import pandas as pd\n    from PIL import Image\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.utils.data import Dataset, DataLoader\n    import evaluate\n    from transformers import Trainer, TrainingArguments, TrainerCallback, PrinterCallback\n    from transformers.modeling_outputs import SequenceClassifierOutput\n    from tqdm.auto import tqdm\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        num_labels: int,\n        target_column_name: str,\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1\n    ):\n        \"\"\"\n            :param modalities: Список модальностей ('text','image','audio') в любом порядке.\n        :param num_labels: Количество классов.\n        :param target_column_name: Имя столбца таргета в DataFrame.\n        :param text_columns: Имена текстовых колонок (склеиваются).\n        :param image_columns: Имена колонок изображений (значения — пути или списки путей/объектов).\n        :param audio_columns: Имена колонок аудио (значения — пути/массивы или списки).\n        :param backend: 'auto' | 'clip' | 'clap' | 'clip_wav2clip'.\n        :param clip_checkpoint: Чекпоинт CLIP.\n        :param clap_checkpoint: Чекпоинт CLAP.\n        :param fusion: 'concat' или 'mean' — тип фьюжна эмбеддингов.\n        :param freeze_backbone: Заморозить веса бэкенда (linear probing).\n        :param clip_max_length: Максимальная длина токенов в CLIP.\n        :param max_images_per_sample: Максимум картинок при concat-агрегации.\n        :param max_audios_per_sample: Максимум аудио при concat-агрегации.\n        \"\"\"\n        self.modalities = sorted(list(set(modalities)))\n        self.num_labels = num_labels\n        self.target_column_name = target_column_name\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.backend_name = backend\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n\n        self.label2id: Optional[Dict[Any, int]] = None\n        self.id2label: Optional[Dict[int, str]] = None\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneClassifier] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[PbarConsoleLogger] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        \"\"\"\n        Инициализирует бэкенд согласно backend='auto'|'clip'|'clap'|'clip_wav2clip' и проверяет совместимость модальностей.\n\n        :raises ValueError: При неподдерживаемой комбинации модальностей.\n        \"\"\"\n        mods = set(self.modalities)\n        name = self.backend_name\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                name = \"clip\"\n            elif mods == {\"text\", \"audio\"}:\n                name = \"clap\"\n            elif mods in ({\"image\", \"audio\"}, {\"text\", \"image\", \"audio\"}):\n                name = \"clip_wav2clip\"\n            else:\n                raise ValueError(f\"Неподдерживаемая комбинация модальностей: {mods}\")\n\n        if name == \"clip\":\n            self.backend = ClipBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                max_images=self.max_images_per_sample,\n                image_agg=\"concat\"\n            )\n        elif name == \"clap\":\n            self.backend = ClapBackend(\n                checkpoint=self.clap_checkpoint,\n                freeze=self.freeze_backbone,\n                max_audios=self.max_audios_per_sample,\n                audio_agg=\"concat\"\n            )\n        elif name == \"clip_wav2clip\":\n            self.backend = ClipWav2CLIPBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                audio_sr=16000,\n                max_images=self.max_images_per_sample,\n                max_audios=self.max_audios_per_sample,\n                image_agg=\"concat\",\n                audio_agg=\"concat\"\n            )\n        else:\n            raise ValueError(f\"Неизвестный backend: {name}\")\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}. \"\n                             f\"Поддерживает: {self.backend.supported}\")\n\n    def _setup_metrics(self, metric_name: str):\n        \"\"\"\n        Создаёт функцию подсчёта метрик для Trainer.\n\n        :param metric_name: 'f1' или 'accuracy'.\n        :raises ValueError: Если метрика не поддерживается.\n        \"\"\"\n        metric_name = metric_name.lower()\n        if metric_name == \"f1\":\n            metric = evaluate.load(\"f1\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids, average=\"weighted\")\n        elif metric_name == \"accuracy\":\n            metric = evaluate.load(\"accuracy\")\n            def compute(p):\n                preds = p.predictions.argmax(-1)\n                return metric.compute(predictions=preds, references=p.label_ids)\n        else:\n            raise ValueError('metric_name должен быть \"f1\" или \"accuracy\"')\n        self.compute_metrics = compute\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        \"\"\"\n        Перемешивает и делит DataFrame на train/eval.\n\n        :param df: Полный датафрейм.\n        :param test_size: Доля валидации (0..1).\n        :param seed: Зерно.\n        :return: (df_train, df_eval).\n        \"\"\"\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _validate_data(self, df: pd.DataFrame):\n        \"\"\"\n        Проверяет соответствие колонок выбранным модальностям.\n\n        :param df: Источник данных.\n        :raises ValueError: При отсутствии необходимых колонок.\n        \"\"\"\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"f1\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1,\n        fit_chunk_size: Optional[int] = None\n    ):\n        \"\"\"\n        Обучает классификационную голову поверх выбранного бэкенда.\n        Поддерживает обучение на больших данных за счёт чанков: train_dataset подставляется кусками.\n\n        :param train_data: Полный датафрейм с данными и таргетом.\n        :param epochs: Количество эпох.\n        :param test_size: Доля валидации.\n        :param per_device_train_batch_size: Размер батча на устройство.\n        :param gradient_accumulation_steps: Шаги аккумуляции градиента.\n        :param learning_rate: Learning rate для оптимизатора.\n        :param metric_name: 'f1' или 'accuracy' — метрика выбора лучшей модели.\n        :param fp16: Использовать fp16 при наличии CUDA (если доступен bf16 — он будет использован вместо fp16).\n        :param logging_steps: Частота логирования шагов.\n        :param eval_steps: Шаги между валидациями/сохранениями.\n        :param output_dir: Каталог для артефактов.\n        :param seed: Зерно.\n        :param hidden: Размер скрытого слоя головы.\n        :param dropout: Дропаут в голове.\n        :param fit_chunk_size: Размер чанка обучающей выборки. Если None — весь train как один чанк.\n        :return: self.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        classes = sorted(train_data[self.target_column_name].unique().tolist())\n        if self.num_labels != len(classes):\n            print(f\"Warning: num_labels={self.num_labels} != len(classes)={len(classes)}\")\n        self.label2id = {c: i for i, c in enumerate(classes)}\n        self.id2label = {i: str(c) for c, i in self.label2id.items()}\n\n        df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n\n        # Датасет валидации держим целиком (обычно небольшой).\n        ds_eval = MultiComboDataset(\n            df_eval, self.target_column_name, self.label2id,\n            self.text_columns, self.image_columns, self.audio_columns\n        )\n\n        # Веса классов по всему train (не по чанку).\n        y_train_all = np.array([self.label2id[y] for y in df_train[self.target_column_name].tolist()], dtype=int)\n        counts = np.bincount(y_train_all, minlength=self.num_labels)\n        n_all = counts.sum()\n        class_weights = np.zeros(self.num_labels, dtype=np.float32)\n        nonzero = counts > 0\n        class_weights[nonzero] = n_all / (self.num_labels * counts[nonzero].astype(np.float32))\n\n        # Модель и метрики\n        self.model = SingleBackboneClassifier(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_labels=self.num_labels,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n        self._setup_metrics(metric_name)\n\n        # Настройки точности\n        bf16_ok = bool(torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{metric_name}\",\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=bool(fp16 and torch.cuda.is_available() and not bf16_ok),\n            bf16=bool(bf16_ok and not fp16),\n            dataloader_num_workers=0,\n            seed=seed,\n            remove_unused_columns=False,\n            disable_tqdm=True  # используем свой внешний tqdm\n        )\n\n        def data_collator(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            \"\"\"\n            Адаптер collate для Trainer: делегирует бэкенду сборку батча.\n\n            :param batch_list: Список элементов Dataset.\n            :return: Батч для model.forward(): {'labels': LongTensor, 'backend_inputs': {...}}.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        # Вспомогательные функции для чанков\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            \"\"\"\n            Оценивает число шагов оптимизации на чанке размера sz.\n\n            :param sz: Количество примеров в чанке.\n            :param bsz: Размер батча.\n            :param accum: Шаги аккумуляции.\n            :return: Число оптимизационных шагов.\n            \"\"\"\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            \"\"\"\n            Генератор срезов индексов по chunk_size.\n\n            :param index_array: Индексы обучающей выборки.\n            :param chunk_size: Размер чанка.\n            :yield: Срез индексов.\n            \"\"\"\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        # Индексы train\n        n_train = len(df_train)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n\n        # Чанк по умолчанию — весь train\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        # Предварительный рассчёт общего числа шагов (для прогресс‑бара и планировщика)\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        # Инициализация Trainer с «пустым» train датасетом (минимальный чанк), чтобы не держать весь train\n        dummy_idx = np.arange(min(len(df_train), 1))\n        ds_train_init = MultiComboDataset(\n            df_train.iloc[dummy_idx], self.target_column_name, self.label2id,\n            self.text_columns, self.image_columns, self.audio_columns\n        ) if len(dummy_idx) > 0 else ds_eval\n\n        self.trainer = WeightedCETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics,\n            num_labels=self.num_labels,\n            train_labels=y_train_all,\n            class_weights=class_weights\n        )\n        self.trainer.remove_callback(PrinterCallback)\n\n        # Планировщик на рассчитанное количество шагов\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        # Внешний прогресс‑бар + консольный лог\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        # Основной цикл обучения по эпохам и чанкам\n        steps_done = 0\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n\n            for slc in chunk_slices(shuffled, chunk_size):\n                # Подставляем чанк\n                chunk_df = df_train.iloc[slc]\n                ds_chunk = MultiComboDataset(\n                    chunk_df, self.target_column_name, self.label2id,\n                    self.text_columns, self.image_columns, self.audio_columns\n                )\n                self.trainer.train_dataset = ds_chunk\n\n                # Считаем шаги на чанке и настраиваем max_steps Trainer\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk, chunk_df\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                # Очистка памяти между чанками\n                del ds_chunk, chunk_df\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n        return self\n\n    def predict(self, df: pd.DataFrame, return_label_str: bool = False) -> np.ndarray:\n        \"\"\"\n        Делает предсказания классов на новых данных.\n\n        :param df: Датафрейм с теми же колонками модальностей, что и при обучении.\n        :param return_label_str: Если True — вернуть строковые метки; иначе — id.\n        :return: np.ndarray предсказанных меток.\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n        ds = MultiComboDataset(\n            df_c, self.target_column_name, self.label2id,\n            self.text_columns, self.image_columns, self.audio_columns\n        )\n        preds = self.trainer.predict(test_dataset=ds)\n        y_pred = np.argmax(preds.predictions, axis=-1)\n        if return_label_str:\n            return np.array([self.id2label[int(i)] for i in y_pred])\n        return y_pred\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        \"\"\"\n        Извлекает fused эмбеддинги (и опционально по модальностям) для новых данных.\n\n        :param df: Датафрейм с нужными колонками модальностей.\n        :param batch_size: Размер батча при инференсе.\n        :param return_per_modality: Вернуть также словарь эмбеддингов {'text','image','audio'}.\n        :return: np.ndarray fused [N, D_fused] или (fused, per_modality_dict).\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        # Определяем девайс модели\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = df.copy()\n        if self.target_column_name not in df_c.columns:\n            df_c[self.target_column_name] = list(self.label2id.keys())[0]\n\n        ds = MultiComboDataset(\n            df_c, self.target_column_name, self.label2id,\n            self.text_columns, self.image_columns, self.audio_columns\n        )\n\n        def collate(batch_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n            \"\"\"\n            Collate для DataLoader при извлечении эмбеддингов.\n\n            :param batch_list: Список элементов.\n            :return: Батч 'backend_inputs' для модели.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device: torch.device):\n            \"\"\"\n            Рекурсивно переносит тензоры на device.\n\n            :param obj: Тензор/словарь/список/кортеж/прочее.\n            :param device: torch.device.\n            :return: Объект с перенесёнными тензорами.\n            \"\"\"\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        with torch.no_grad():\n            for batch in loader:\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        return fused_arr, per_mod\n\n    def get_eval_history(self) -> pd.DataFrame:\n        \"\"\"\n        Возвращает историю метрик на валидации, собранную коллбэком (если он добавлен).\n\n        :return: DataFrame со строками вида {'step','epoch', 'eval_loss', 'eval_f1'/...}.\n        \"\"\"\n        cb = getattr(self, \"progress_callback\", None)\n        if cb is None:\n            return pd.DataFrame()\n        # История собирается через on_log печать; здесь можно расширить для хранения, если потребуется.\n        # Сейчас возвращаем пустой каркас (расширяемость для будущей версии).\n        return pd.DataFrame()  # при желании можно дополнить сохранением истории внутри коллбэка","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Создание фиктивных данных.","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torchaudio\n\nHAVE_TORCHAUDIO = True\n\nrandom.seed(42); np.random.seed(42); torch.manual_seed(42)\n\nBASE_DIR = \"./dummy_data\"\nIMG_DIR  = os.path.join(BASE_DIR, \"images\")\nAUD_DIR  = os.path.join(BASE_DIR, \"audio\")\nos.makedirs(IMG_DIR, exist_ok=True)\nos.makedirs(AUD_DIR, exist_ok=True)\n\n# Сколько элементов на модальность в каждой строке\nK_PER_MODALITY = 3\n\ndef make_dummy_images(n=12, size=(256, 256)):\n    paths = []\n    for i in range(n):\n        color = tuple(np.random.randint(0, 255, size=3).tolist())\n        img = Image.new(\"RGB\", size, color=color)\n        path = os.path.join(IMG_DIR, f\"img_{i:02d}.png\")\n        img.save(path)\n        paths.append(path)\n    return paths\n\ndef make_dummy_audios(n=12, sr=48000, duration_sec=0.6):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Нельзя сгенерировать аудио без torchaudio. Установите 'pip install torchaudio'.\")\n    paths = []\n    t = torch.linspace(0, duration_sec, int(sr * duration_sec))\n    for i in range(n):\n        freq = random.choice([220, 330, 440, 550, 660, 880])\n        wave = 0.2 * torch.sin(2 * np.pi * freq * t)  # амплитуда 0.2\n        wave = wave.unsqueeze(0)  # [1, T] mono\n        path = os.path.join(AUD_DIR, f\"tone_{i:02d}.wav\")\n        torchaudio.save(path, wave, sample_rate=sr)\n        paths.append(path)\n    return paths\n\nimg_paths = make_dummy_images(n=12)\naudio_paths = make_dummy_audios(n=12) if HAVE_TORCHAUDIO else []\n\n# Вспомогательные тексты\nTITLES = [\"Red fox\", \"Blue sky\", \"Green field\", \"Yellow sun\", \"Purple rain\", \"Silver line\"]\nBODIES = [\"quick brown\", \"lazy dog\", \"jumps high\", \"runs fast\", \"stays calm\", \"shines bright\"]\nQUERIES = [\"find tone\", \"classify sound\", \"describe image\", \"retrieve pair\", \"detect event\"]\n\ndef rand_title(): return random.choice(TITLES)\ndef rand_body(): return random.choice(BODIES)\ndef rand_query(): return random.choice(QUERIES)\n\n# Универсальные хелперы\ndef sample_k(seq, k):\n    if len(seq) >= k:\n        return random.sample(seq, k)  # без повторов\n    else:\n        return [random.choice(seq) for _ in range(k)]  # с повторами, если мало исходников\n\ndef as_cols(prefix, values):\n    # {\"prefix_1\": values[0], ..., \"prefix_k\": values[k-1]}\n    return {f\"{prefix}_{i+1}\": v for i, v in enumerate(values)}\n\ndef pick_text_desc(k=K_PER_MODALITY):\n    # Текстовое описание: \"Title | body\"\n    vals = [f\"{rand_title()} | {rand_body()}\" for _ in range(k)]\n    return as_cols(\"text\", vals)\n\ndef pick_text_query(k=K_PER_MODALITY):\n    vals = [rand_query() for _ in range(k)]\n    return as_cols(\"text\", vals)\n\ndef pick_images(k=K_PER_MODALITY):\n    vals = sample_k(img_paths, k)\n    return as_cols(\"image_path\", vals)\n\ndef pick_audios(k=K_PER_MODALITY):\n    vals = sample_k(audio_paths, k)\n    return as_cols(\"audio_path\", vals)\n\n# 1) Текст + Картинка -> по 3 текстовых и 3 картинок\ndef build_df_text_image(n=24, k=K_PER_MODALITY):\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_desc(k))\n        row.update(pick_images(k))\n        row[\"label\"] = random.choice([\"class_a\", \"class_b\", \"class_c\"])\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 2) Текст + Звук -> по 3 текста (query) и 3 аудио\ndef build_df_text_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для text+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_query(k))\n        row.update(pick_audios(k))\n        row[\"label\"] = random.choice([\"ok\", \"ng\"])\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 3) Картинка + Звук -> по 3 картинки и 3 аудио\ndef build_df_image_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для image+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_images(k))\n        row.update(pick_audios(k))\n        row[\"label\"] = random.choice([\"dog\", \"cat\", \"bird\"])\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 4) Текст + Картинка + Звук -> по 3 на каждую модальность\ndef build_df_text_image_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для text+image+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_desc(k))\n        row.update(pick_images(k))\n        row.update(pick_audios(k))\n        row[\"label\"] = random.choice([\"A\", \"B\", \"C\", \"D\"])\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# Собираем 4 датасета\ndf_text_image = build_df_text_image(290, K_PER_MODALITY)\ndf_text_audio = build_df_text_audio(230, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\ndf_image_audio = build_df_image_audio(150, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\ndf_text_image_audio  = build_df_text_image_audio(200, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\n\nprint(\"df_text_image columns:\", list(df_text_image.columns))\nif HAVE_TORCHAUDIO:\n    print(\"df_text_audio columns:\", list(df_text_audio.columns))\n    print(\"df_image_audio columns:\", list(df_image_audio.columns))\n    print(\"df_text_image_audio columns:\", list(df_text_image_audio.columns))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboClassification(\n    modalities=[\"text\", \"image\", \"audio\"],\n    num_labels=4,\n    target_column_name=\"label\",\n\n    # Колонки данных\n    text_columns=[\"text_1\", \"text_2\", \"text_3\"],\n    image_columns=[\"image_path_1\", \"image_path_2\"],\n    audio_columns=[\"audio_path_1\", \"audio_path_2\"],\n\n    # Бэкенд и фьюжн\n    backend=\"auto\",     # для ['text','image','audio'] автоматически выберется clip_wav2clip\n    fusion=\"concat\",\n    freeze_backbone=True,   # linear probing; поставьте False для тонкой донастройки энкодеров\n\n    # Ограничения по количеству мультимодальных входов (для concat-агрегации)\n    max_images_per_sample=2,  # под две картинки\n    max_audios_per_sample=1    # под одно аудио (если две — выставьте 2 и добавьте вторую колонку)\n)\n\n# Обучение (с поддержкой «больших данных» за счёт чанков)\npipeline.fit(\n    df_text_image_audio,\n    epochs=2,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    metric_name=\"f1\",\n    fp16=True,\n    logging_steps=10,\n    eval_steps=10,\n    fit_chunk_size=25,\n    output_dir=\"./result\",\n    seed=42\n)\n\npreds = pipeline.predict(df_text_image_audio[:5], return_label_str=True)\nemb = pipeline.get_embeddings(df_text_image_audio[:5], batch_size=8)\n\nprint(f'preds: {preds}')\nprint(f'embeddings: {emb}')\n\n# При желании — эмбеддинги по модальностям отдельно\nfused, per_mod = pipeline.get_embeddings(\n    df_text_image_audio.head(8),\n    batch_size=8,\n    return_per_modality=True\n)\nprint(\"fused.shape:\", fused.shape)\nfor m, arr in per_mod.items():\n    print(f\"{m} emb shape:\", arr.shape)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Дообучение регрессора, который работает с данными разной модальностью.","metadata":{}},{"cell_type":"markdown","source":"Реализация регрессора.","metadata":{}},{"cell_type":"code","source":"!pip install -q wav2clip torchaudio transformers evaluate pillow\n\nimport gc\nimport math\nfrom typing import List, Dict, Any, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport evaluate\nfrom transformers import Trainer, TrainingArguments, TrainerCallback, PrinterCallback\nfrom transformers.modeling_outputs import ModelOutput\nfrom tqdm.auto import tqdm\n\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Устанавливает фиксированное зерно для Python, NumPy и PyTorch (включая все доступные CUDA-устройства).\n\n    :param seed: Значение зерна.\n    \"\"\"\n    import random\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef to_pil(x: Union[str, np.ndarray, Image.Image]) -> Image.Image:\n    \"\"\"\n    Приводит входное изображение к PIL.Image в формате RGB.\n\n    Поддерживаемые форматы:\n      - путь к файлу (str);\n      - NumPy массив (H, W[, C]) со значениями uint8;\n      - PIL.Image (любого режима).\n\n    :param x: Источник изображения.\n    :return: Изображение PIL.Image в RGB.\n    :raises ValueError: Если тип входа не поддерживается.\n    \"\"\"\n    if isinstance(x, Image.Image):\n        return x.convert(\"RGB\")\n    if isinstance(x, str):\n        return Image.open(x).convert(\"RGB\")\n    if isinstance(x, np.ndarray):\n        return Image.fromarray(x).convert(\"RGB\")\n    raise ValueError(\"Ожидается путь/np.ndarray/PIL.Image\")\n\n\ndef load_audio(path: str, target_sr: int) -> np.ndarray:\n    \"\"\"\n    Загружает аудио и (при необходимости) ресемплирует до target_sr.\n\n    - Монофонизирует вход через усреднение каналов.\n    - Возвращает float32 массив формы [T].\n\n    :param path: Путь к аудиофайлу (wav/flac/…).\n    :param target_sr: Целевая частота дискретизации.\n    :return: Сигнал формы [T] float32.\n    :raises RuntimeError: Если torchaudio недоступен.\n    \"\"\"\n    try:\n        import torchaudio\n    except Exception as e:\n        raise RuntimeError(\"Требуется torchaudio: pip install torchaudio\") from e\n    waveform, sr = torchaudio.load(path)  # [C, T]\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n    if sr != target_sr:\n        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n    return waveform.squeeze(0).numpy().astype(np.float32)\n\n\nclass MultiComboRegDataset(Dataset):\n    \"\"\"\n    Регрессионный датасет с поддержкой нескольких изображений и аудио на сэмпл.\n\n    Ячейки колонок изображений/аудио могут содержать:\n      - одиночное значение (путь/np.ndarray/PIL для изображения; путь/np.ndarray для аудио);\n      - список таких значений.\n\n    :param df: Исходный DataFrame.\n    :param target_cols: Список целевых колонок (поддерживается многомерная регрессия).\n    :param text_columns: Список текстовых колонок. Их значения конкатенируются через [SEP].\n    :param image_columns: Список колонок изображений.\n    :param audio_columns: Список колонок аудио.\n    \"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        target_cols: List[str],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None\n    ):\n        \"\"\"\n        Инициализация датасета.\n\n        :param df: Исходный DataFrame.\n        :param target_cols: Названия колонок целей.\n        :param text_columns: Текстовые колонки (склеиваются).\n        :param image_columns: Колонки изображений (значение или список значений).\n        :param audio_columns: Колонки аудио (значение или список значений).\n        \"\"\"\n        self.df = df.reset_index(drop=True)\n        self.target_cols = target_cols\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.sep = \" [SEP] \"\n\n    def __len__(self) -> int:\n        \"\"\"\n        Количество элементов датасета.\n\n        :return: Длина датасета.\n        \"\"\"\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> Dict[str, Any]:\n        \"\"\"\n        Возвращает один элемент датасета.\n\n        Ключи в словаре:\n          - 'labels': np.ndarray формы [T], float32;\n          - 'text': строка (если заданы text_columns);\n          - 'images': список изображений (пути/np.ndarray/PIL) или пустой список;\n          - 'audios': список аудио (пути/np.ndarray) или пустой список.\n\n        :param idx: Индекс строки.\n        :return: Словарь для последующего collate бэкенда.\n        \"\"\"\n        row = self.df.iloc[idx]\n        y = np.array([float(row[c]) for c in self.target_cols], dtype=np.float32)\n\n        def _as_list(v):\n            if v is None or (isinstance(v, float) and np.isnan(v)):\n                return []\n            if isinstance(v, (list, tuple)):\n                return list(v)\n            return [v]\n\n        item = {\"labels\": y}\n        if self.text_columns:\n            item[\"text\"] = self.sep.join([str(row[c]) if pd.notnull(row[c]) else \"\" for c in self.text_columns])\n\n        if self.image_columns:\n            imgs = []\n            for c in self.image_columns:\n                if c in row:\n                    imgs.extend(_as_list(row[c]))\n            item[\"images\"] = imgs\n\n        if self.audio_columns:\n            auds = []\n            for c in self.audio_columns:\n                if c in row:\n                    auds.extend(_as_list(row[c]))\n            item[\"audios\"] = auds\n\n        return item\n\n\nclass BaseBackend(nn.Module):\n    \"\"\"\n    Базовый класс бэкенда для единой мультимодальной модели.\n\n    Атрибуты:\n      - name: Название бэкенда.\n      - supported: Набор поддерживаемых модальностей (например, {'text','image'}).\n      - embed_dim: Базовая размерность эмбеддингов модальностей.\n      - out_dim_per_modality: Словарь фактических размерностей выходных эмбеддингов по модальностям\n                              (с учётом агрегации/concat).\n    \"\"\"\n    name: str = \"base\"\n    supported: set = set()\n    embed_dim: int = 0\n    out_dim_per_modality: Dict[str, int] = {}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Собирает батч для текущего бэкенда.\n\n        :param batch: Список элементов MultiComboRegDataset.\n        :return: Словарь вида:\n                 {\n                   'labels': torch.FloatTensor [B, T],\n                   'backend_inputs': dict(...)\n                 }\n        \"\"\"\n        raise NotImplementedError\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует модальности и возвращает эмбеддинги по доступным модальностям.\n\n        :param backend_inputs: Предобработанные входы (из collate).\n        :param device: Целевой девайс.\n        :return: Словарь {'text':[B,*], 'image':[B,*], 'audio':[B,*]} по имеющимся модальностям.\n        \"\"\"\n        raise NotImplementedError\n\n    def freeze_all(self):\n        \"\"\"\n        Замораживает все параметры бэкенда (requires_grad=False).\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad = False\n\n    @staticmethod\n    def _stack_labels_float(batch: List[Dict[str, Any]]) -> torch.Tensor:\n        \"\"\"\n        Преобразует список 'labels' из элементов батча в тензор float32 [B, T] с выравниванием по T.\n\n        Если длины таргетов различны, добивает меньшие нулями справа до максимальной длины.\n\n        :param batch: Список элементов датасета.\n        :return: torch.FloatTensor [B, T].\n        \"\"\"\n        ys = []\n        for b in batch:\n            y = b.get(\"labels\", None)\n            if y is None:\n                ys.append(np.array([0.0], dtype=np.float32))\n            else:\n                arr = np.array(y, dtype=np.float32).reshape(-1)\n                ys.append(arr)\n        max_t = max(a.shape[0] for a in ys)\n        ys_padded = []\n        for a in ys:\n            if a.shape[0] == max_t:\n                ys_padded.append(a)\n            else:\n                pad = np.zeros(max_t, dtype=np.float32)\n                pad[:a.shape[0]] = a\n                ys_padded.append(pad)\n        return torch.from_numpy(np.stack(ys_padded, axis=0))\n\n    def get_out_dim(self, modality: str) -> int:\n        \"\"\"\n        Возвращает итоговую размерность выходного вектора по модальности\n        (с учётом агрегации и параметров max_*).\n\n        :param modality: Имя модальности: 'text' | 'image' | 'audio'.\n        :return: Размерность эмбеддинга.\n        \"\"\"\n        return self.out_dim_per_modality.get(modality, self.embed_dim)\n\n\nclass ClipBackend(BaseBackend):\n    \"\"\"\n    CLIP-бэкенд для текста и изображений с поддержкой нескольких изображений на сэмпл.\n\n    Агрегация изображений:\n      - 'concat': конкатенация эмбеддингов N изображений в один вектор (с паддингом нулями до max_images);\n      - 'mean': усреднение эмбеддингов изображений.\n\n    :param checkpoint: Имя/путь чекпоинта CLIP (HF Hub).\n    :param max_length: Максимальная длина токенов текста.\n    :param freeze: Замораживать ли веса CLIP (linear probing).\n    :param max_images: Максимальное число изображений на сэмпл для агрегации.\n    :param image_agg: Тип агрегации изображений ('concat' или 'mean').\n    \"\"\"\n    name = \"clip\"\n    supported = {\"text\", \"image\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        max_images: int = 1,\n        image_agg: str = \"concat\"\n    ):\n        \"\"\"\n        Инициализация CLIP-бэкенда.\n\n        :param checkpoint: Чекпоинт CLIP.\n        :param max_length: Максимальная длина текста (токенов).\n        :param freeze: Замораживать ли веса.\n        :param max_images: Максимум изображений на сэмпл.\n        :param image_agg: Агрегация изображений.\n        \"\"\"\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.max_images = int(max_images)\n        self.image_agg = image_agg\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Готовит батч для CLIP.\n\n        Формирует:\n          - text_inputs: токены текста;\n          - image_inputs: pixel_values (возможен None);\n          - image_counts: количество изображений на каждый сэмпл (для агрегации).\n\n        :param batch: Элементы с ключами 'text', 'images', 'labels'.\n        :return: {'labels': FloatTensor [B,T], 'backend_inputs': dict}.\n        \"\"\"\n        labels = self._stack_labels_float(batch)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n\n        text_inputs = self.processor(\n            text=texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n        )\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(counts, dtype=torch.long),\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенирует до max_k эмбеддингов изображений на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги изображений [M, D], где M = сумма(counts).\n        :param counts: Список количеств изображений на сэмпл.\n        :param max_k: Максимум изображений на сэмпл (для паддинга/отрезания).\n        :return: Нормализованный тензор [B, D*max_k].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усредняет эмбеддинги изображений на сэмпл.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества изображений на сэмпл.\n        :return: Нормализованный тензор [B, D].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Вычисляет эмбеддинги текста и изображений (с агрегацией изображений).\n\n        :param backend_inputs: Тензоры CLIPProcessor для текста и картинок.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'image':[B,D*max_images] или [B,D]} — L2-нормированные эмбеддинги.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                img_z = self._concat_padded(img_flat, counts, self.max_images)\n            else:\n                img_z = self._mean_pool(img_flat, counts)\n        else:\n            img_z = torch.zeros(\n                (len(counts), self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim),\n                device=device\n            )\n\n        return {\"text\": text_z, \"image\": img_z}\n\n\nclass ClapBackend(BaseBackend):\n    \"\"\"\n    CLAP-бэкенд для текста и аудио с поддержкой нескольких аудио на сэмпл.\n\n    Агрегация аудио:\n      - 'concat': конкатенация эмбеддингов N аудиоклипов в один вектор (с паддингом нулями до max_audios);\n      - 'mean': усреднение эмбеддингов аудио.\n\n    :param checkpoint: Имя/путь чекпоинта CLAP (HF Hub).\n    :param freeze: Замораживать ли веса CLAP.\n    :param max_audios: Максимальное число аудио на сэмпл для агрегации.\n    :param audio_agg: Тип агрегации аудио ('concat' или 'mean').\n    \"\"\"\n    name = \"clap\"\n    supported = {\"text\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"laion/clap-htsat-unfused\",\n        freeze: bool = True,\n        max_audios: int = 1,\n        audio_agg: str = \"concat\"\n    ):\n        \"\"\"\n        Инициализация CLAP-бэкенда.\n\n        :param checkpoint: Чекпоинт CLAP.\n        :param freeze: Замораживать ли веса.\n        :param max_audios: Максимум аудио на сэмпл.\n        :param audio_agg: Агрегация аудио.\n        \"\"\"\n        super().__init__()\n        from transformers import ClapModel, ClapProcessor\n        self.model = ClapModel.from_pretrained(checkpoint)\n        self.processor = ClapProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(getattr(self.model.config, \"projection_dim\", 512))\n        sr = getattr(self.processor, \"sampling_rate\", None)\n        if sr is None:\n            fe = getattr(self.processor, \"feature_extractor\", None)\n            sr = getattr(fe, \"sampling_rate\", 48000) if fe is not None else 48000\n        self.sr = int(sr)\n        self.max_audios = int(max_audios)\n        self.audio_agg = audio_agg\n        if freeze:\n            self.freeze_all()\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"audio\": aud_out}\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Готовит батч для CLAP.\n\n        Формирует:\n          - text_inputs: токены текста;\n          - audio_inputs: input_features (или None);\n          - audio_counts: число аудио на сэмпл.\n\n        :param batch: Элементы с ключами 'text','audios','labels'.\n        :return: {'labels': FloatTensor [B,T], 'backend_inputs': dict}.\n        \"\"\"\n        labels = self._stack_labels_float(batch)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"Ожидается путь к аудио или numpy.ndarray\")\n\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n\n        if len(flat_audios):\n            aud_proc = self.processor(audios=flat_audios, sampling_rate=self.sr, padding=True, return_tensors=\"pt\")\n            audio_inputs = {\"input_features\": aud_proc[\"input_features\"]}\n        else:\n            audio_inputs = {\"input_features\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(counts, dtype=torch.long)\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _concat_padded(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенирует до max_k эмбеддингов аудио на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги аудио [M, D].\n        :param counts: Список количеств аудио на сэмпл.\n        :param max_k: Максимум аудио на сэмпл.\n        :return: Нормализованный тензор [B, D*max_k].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset + c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _mean_pool(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усредняет эмбеддинги аудио на сэмпл.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества аудио на сэмпл.\n        :return: Нормализованный тензор [B, D].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset + c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Вычисляет эмбеддинги текста и аудио (с агрегацией аудио).\n\n        :param backend_inputs: Тензоры ClapProcessor для текста и аудио.\n        :param device: Девайс.\n        :return: {'text':[B,D], 'audio':[B,D*max_audios] или [B,D]} — L2-нормированные эмбеддинги.\n        \"\"\"\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        if hasattr(self.model, \"get_text_features\") and hasattr(self.model, \"get_audio_features\"):\n            text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        else:\n            out = self.model(**ti, output_attentions=False, output_hidden_states=False, return_dict=True)\n            text_z = out.text_embeds\n        text_z = F.normalize(text_z, dim=-1)\n\n        counts = backend_inputs[\"audio_counts\"].tolist()\n        af = backend_inputs[\"audio_inputs\"][\"input_features\"]\n        if af is not None:\n            af = af.to(device)\n            if hasattr(self.model, \"get_audio_features\"):\n                aud_flat = self.model.get_audio_features(input_features=af)\n            else:\n                out = self.model(input_features=af, output_attentions=False, output_hidden_states=False, return_dict=True)\n                aud_flat = out.audio_embeds\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                aud_z = self._concat_padded(aud_flat, counts, self.max_audios)\n            else:\n                aud_z = self._mean_pool(aud_flat, counts)\n        else:\n            aud_z = torch.zeros(\n                (len(counts), self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim),\n                device=device\n            )\n\n        return {\"text\": text_z, \"audio\": aud_z}\n\n\nclass ClipWav2CLIPBackend(BaseBackend):\n    \"\"\"\n    Комбинированный бэкенд: CLIP для текста/изображения + Wav2CLIP для аудио (в CLIP-пространство).\n\n    Поддержка нескольких изображений и аудио на сэмпл с агрегацией 'concat' или 'mean'.\n\n    :param checkpoint: Чекпоинт CLIP (HF).\n    :param max_length: Максимальная длина токенов для CLIP.\n    :param freeze: Замораживать ли веса CLIP.\n    :param audio_sr: Частота дискретизации для Wav2CLIP.\n    :param max_images: Максимум изображений на сэмпл.\n    :param max_audios: Максимум аудио на сэмпл.\n    :param image_agg: Тип агрегации изображений ('concat' | 'mean').\n    :param audio_agg: Тип агрегации аудио ('concat' | 'mean').\n    \"\"\"\n    name = \"clip_wav2clip\"\n    supported = {\"text\", \"image\", \"audio\"}\n\n    def __init__(\n        self,\n        checkpoint: str = \"openai/clip-vit-base-patch32\",\n        max_length: int = 77,\n        freeze: bool = True,\n        audio_sr: int = 16000,\n        max_images: int = 1,\n        max_audios: int = 1,\n        image_agg: str = \"concat\",\n        audio_agg: str = \"concat\"\n    ):\n        \"\"\"\n        Инициализация ClipWav2CLIP-бэкенда.\n\n        :param checkpoint: Чекпоинт CLIP.\n        :param max_length: Максимальная длина текста.\n        :param freeze: Замораживать ли веса CLIP.\n        :param audio_sr: Частота дискретизации для Wav2CLIP.\n        :param max_images: Максимум изображений на сэмпл.\n        :param max_audios: Максимум аудио на сэмпл.\n        :param image_agg: Агрегация изображений.\n        :param audio_agg: Агрегация аудио.\n        \"\"\"\n        super().__init__()\n        from transformers import CLIPModel, CLIPProcessor\n        self.model = CLIPModel.from_pretrained(checkpoint)\n        self.processor = CLIPProcessor.from_pretrained(checkpoint)\n        self.embed_dim = int(self.model.config.projection_dim)\n        self.max_length = max_length\n        self.audio_sr = int(audio_sr)\n        self.max_images = int(max_images)\n        self.max_audios = int(max_audios)\n        self.image_agg = image_agg\n        self.audio_agg = audio_agg\n        if freeze:\n            self.freeze_all()\n        img_out = self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim\n        aud_out = self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim\n        self.out_dim_per_modality = {\"text\": self.embed_dim, \"image\": img_out, \"audio\": aud_out}\n        # ленивое подключение Wav2CLIP\n        self._w2c_model = None\n        self._w2c_mod = None\n        self._w2c_api = None\n        self._w2c_device = None\n\n    def _ensure_w2c(self, device: torch.device):\n        \"\"\"\n        Ленивая загрузка wav2clip и его модели для текущего устройства.\n\n        Поддерживаются разные версии API:\n          - функции load_model/get_model + embed_audio/get_audio_embedding;\n          - класс Wav2CLIP с методом embed_audio.\n\n        :param device: Текущее устройство.\n        :raises RuntimeError: Если wav2clip недоступен или не удалось инициализировать модель.\n        \"\"\"\n        if self._w2c_model is not None and self._w2c_device == str(device):\n            return\n        import importlib\n        try:\n            w2c = importlib.import_module(\"wav2clip\")\n        except Exception as e:\n            raise RuntimeError(\"Не найден пакет 'wav2clip'. Установите: pip install wav2clip\") from e\n        dev_str = str(device) if device.type == \"cuda\" else \"cpu\"\n        if hasattr(w2c, \"load_model\"):\n            model = w2c.load_model(device=dev_str); api_kind = \"func\"\n        elif hasattr(w2c, \"get_model\"):\n            model = w2c.get_model(device=dev_str); api_kind = \"func\"\n        elif hasattr(w2c, \"Wav2CLIP\"):\n            try:\n                model = w2c.Wav2CLIP(dev_str)\n            except TypeError:\n                model = w2c.Wav2CLIP(device=dev_str)\n            api_kind = \"method\"\n        else:\n            raise RuntimeError(\"wav2clip установлен, но нет способов загрузки (load_model/get_model/Wav2CLIP)\")\n        self._w2c_mod = w2c\n        self._w2c_model = model\n        self._w2c_api = api_kind\n        self._w2c_device = str(device)\n\n    def collate(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Готовит батч для текста/изображений (через CLIPProcessor) и аудио (через паддинг numpy->Tensor).\n\n        Возвращает:\n          - text_inputs: словарь тензоров для текста;\n          - image_inputs: словарь с pixel_values или None;\n          - image_counts: LongTensor [B] — количество изображений на сэмпл;\n          - audio_inputs: {'waveforms': Tensor [M,Lmax] или None, 'lengths': LongTensor [M] или None};\n          - audio_counts: LongTensor [B] — количество аудио на сэмпл;\n          - labels: FloatTensor [B, T].\n\n        :param batch: Элементы с 'text','images','audios','labels'.\n        :return: Словарь {'labels', 'backend_inputs'}.\n        \"\"\"\n        labels = self._stack_labels_float(batch)\n        texts = [b.get(\"text\", \"\") for b in batch]\n\n        # images (flatten + counts)\n        images_lists = [b.get(\"images\", []) for b in batch]\n        flat_images, img_counts = [], []\n        for lst in images_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            img_counts.append(len(lst))\n            for img in lst:\n                flat_images.append(to_pil(img))\n        text_inputs = self.processor(text=texts, padding=True, truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n        text_inputs = {k: v for k, v in text_inputs.items()}\n        if len(flat_images):\n            img_proc = self.processor(images=flat_images, return_tensors=\"pt\")\n            image_inputs = {\"pixel_values\": img_proc[\"pixel_values\"]}\n        else:\n            image_inputs = {\"pixel_values\": None}\n\n        # audios (flatten + counts)\n        audios_lists = [b.get(\"audios\", []) for b in batch]\n        flat_audios, aud_counts = [], []\n        for lst in audios_lists:\n            lst = lst if isinstance(lst, list) else ([] if lst is None else [lst])\n            aud_counts.append(len(lst))\n            for a in lst:\n                if isinstance(a, str):\n                    flat_audios.append(load_audio(a, self.audio_sr))\n                elif isinstance(a, np.ndarray):\n                    flat_audios.append(a.astype(np.float32))\n                else:\n                    raise ValueError(\"Ожидается путь к аудио или numpy.ndarray\")\n\n        if len(flat_audios):\n            Lmax = max(len(a) for a in flat_audios)\n            wav = np.zeros((len(flat_audios), Lmax), dtype=np.float32)\n            lens = np.zeros((len(flat_audios),), dtype=np.int64)\n            for i, a in enumerate(flat_audios):\n                L = len(a)\n                wav[i, :L] = a\n                lens[i] = L\n            audio_inputs = {\"waveforms\": torch.from_numpy(wav), \"lengths\": torch.from_numpy(lens)}\n        else:\n            audio_inputs = {\"waveforms\": None, \"lengths\": None}\n\n        backend_inputs = {\n            \"text_inputs\": text_inputs,\n            \"image_inputs\": image_inputs,\n            \"image_counts\": torch.tensor(img_counts, dtype=torch.long),\n            \"audio_inputs\": audio_inputs,\n            \"audio_counts\": torch.tensor(aud_counts, dtype=torch.long),\n        }\n        return {\"labels\": labels, \"backend_inputs\": backend_inputs}\n\n    def _agg_concat(self, embs: torch.Tensor, counts: List[int], max_k: int) -> torch.Tensor:\n        \"\"\"\n        Конкатенирует до max_k эмбеддингов на сэмпл с паддингом нулями.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества элементов на сэмпл.\n        :param max_k: Максимум элементов на сэмпл (для паддинга).\n        :return: Нормализованный тензор [B, D*max_k].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D * max_k), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                chunk = embs[offset:offset+c]\n                take = chunk[:max_k]\n                if take.size(0) < max_k:\n                    pad = torch.zeros((max_k - take.size(0), D), device=device, dtype=embs.dtype)\n                    take = torch.cat([take, pad], dim=0)\n                out[i] = take.reshape(-1)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def _agg_mean(self, embs: torch.Tensor, counts: List[int]) -> torch.Tensor:\n        \"\"\"\n        Усредняет эмбеддинги на сэмпл.\n\n        :param embs: Плоские эмбеддинги [M, D].\n        :param counts: Количества элементов на сэмпл.\n        :return: Нормализованный тензор [B, D].\n        \"\"\"\n        device = embs.device if embs is not None else torch.device(\"cpu\")\n        B = len(counts); D = self.embed_dim\n        out = torch.zeros((B, D), device=device, dtype=embs.dtype if embs is not None else torch.float32)\n        if embs is None:\n            return out\n        offset = 0\n        for i, c in enumerate(counts):\n            if c > 0:\n                out[i] = embs[offset:offset+c].mean(dim=0)\n            offset += c\n        return F.normalize(out, dim=-1)\n\n    def encode(self, backend_inputs: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Кодирует модальности: текст/изображение — через CLIP; аудио — через Wav2CLIP.\n\n        :param backend_inputs: Входы из collate().\n        :param device: Целевой девайс.\n        :return: Словарь {'text',[B,D]; 'image',[B,*]; 'audio',[B,*]} в CLIP-пространстве.\n        \"\"\"\n        # text + image via CLIP\n        ti = {k: v.to(device) for k, v in backend_inputs[\"text_inputs\"].items()}\n        text_z = self.model.get_text_features(input_ids=ti[\"input_ids\"], attention_mask=ti[\"attention_mask\"])\n        text_z = F.normalize(text_z, dim=-1)\n\n        img_counts = backend_inputs[\"image_counts\"].tolist()\n        pi = backend_inputs[\"image_inputs\"][\"pixel_values\"]\n        if pi is not None:\n            pi = pi.to(device)\n            img_flat = self.model.get_image_features(pixel_values=pi)\n            img_flat = F.normalize(img_flat, dim=-1)\n            if self.image_agg == \"concat\":\n                image_z = self._agg_concat(img_flat, img_counts, self.max_images)\n            else:\n                image_z = self._agg_mean(img_flat, img_counts)\n        else:\n            image_z = torch.zeros(\n                (len(img_counts), self.embed_dim * self.max_images if self.image_agg == \"concat\" else self.embed_dim),\n                device=device\n            )\n\n        # audio via wav2clip\n        aud_counts = backend_inputs[\"audio_counts\"].tolist()\n        wav = backend_inputs[\"audio_inputs\"][\"waveforms\"]\n        lens = backend_inputs[\"audio_inputs\"][\"lengths\"]\n        if wav is not None and lens is not None and (lens.numel() if torch.is_tensor(lens) else len(lens)) > 0:\n            self._ensure_w2c(device)\n            w2c = self._w2c_mod\n            embs = []\n            for i in range(wav.size(0)):\n                L = int(lens[i].item())\n                a_np = wav[i, :L].detach().cpu().numpy()\n                e = None\n                if self._w2c_api == \"func\":\n                    if hasattr(w2c, \"embed_audio\"):\n                        try:\n                            e = w2c.embed_audio(a_np, self._w2c_model)\n                        except TypeError:\n                            e = w2c.embed_audio(a_np, self.audio_sr, self._w2c_model)\n                    elif hasattr(w2c, \"get_audio_embedding\"):\n                        try:\n                            e = w2c.get_audio_embedding(a_np, self._w2c_model)\n                        except TypeError:\n                            e = w2c.get_audio_embedding(a_np, self.audio_sr, self._w2c_model)\n                if e is None and self._w2c_api == \"method\" and hasattr(self._w2c_model, \"embed_audio\"):\n                    try:\n                        e = self._w2c_model.embed_audio(a_np)\n                    except TypeError:\n                        e = self._w2c_model.embed_audio(a_np, sr=self.audio_sr)\n                if e is None:\n                    raise RuntimeError(\"Не удалось получить аудио‑эмбеддинг через wav2clip.\")\n                e = np.asarray(e)\n                if e.ndim == 2:\n                    e = e.mean(axis=0)\n                elif e.ndim > 2:\n                    e = e.reshape(-1, e.shape[-1]).mean(axis=0)\n                embs.append(e.astype(np.float32))\n            aud_flat = torch.tensor(np.stack(embs, axis=0), dtype=torch.float32, device=device)\n            aud_flat = F.normalize(aud_flat, dim=-1)\n            if self.audio_agg == \"concat\":\n                audio_z = self._agg_concat(aud_flat, aud_counts, self.max_audios)\n            else:\n                audio_z = self._agg_mean(aud_flat, aud_counts)\n        else:\n            audio_z = torch.zeros(\n                (len(aud_counts), self.embed_dim * self.max_audios if self.audio_agg == \"concat\" else self.embed_dim),\n                device=device\n            )\n\n        return {\"text\": text_z, \"image\": image_z, \"audio\": audio_z}\n\n\nclass SingleBackboneRegressor(nn.Module):\n    \"\"\"\n    Регрессор поверх одного мультимодального бэкенда:\n    фьюжн эмбеддингов модальностей -> MLP-голова -> предсказание R^T.\n    \"\"\"\n    def __init__(\n        self,\n        backend: BaseBackend,\n        modalities: List[str],\n        num_targets: int,\n        fusion: str = \"concat\",\n        hidden: int = 512,\n        dropout: float = 0.1\n    ):\n        \"\"\"\n        Инициализирует регрессионную голову.\n\n        :param backend: Инициализированный бэкенд (CLIP/CLAP/ClipWav2CLIP).\n        :param modalities: Список активных модальностей (учитывается порядок: image, text, audio).\n        :param num_targets: Число целевых признаков (T).\n        :param fusion: Тип фьюжна — 'concat' или 'mean'.\n        :param hidden: Размер скрытого слоя головы.\n        :param dropout: Дропаут в голове.\n        :raises ValueError: Если fusion='mean' при несовпадающих размерах модальностей.\n        \"\"\"\n        super().__init__()\n        self.backend = backend\n        self.modalities = modalities\n        self.fusion = fusion\n        self.num_targets = num_targets\n\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        if fusion == \"concat\":\n            in_dim = sum(self.backend.get_out_dim(m) for m in order)\n        elif fusion == \"mean\":\n            dims = [self.backend.get_out_dim(m) for m in order]\n            if len(set(dims)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать: {dict(zip(order, dims))}')\n            in_dim = dims[0]\n        else:\n            raise ValueError('fusion должен быть \"concat\" или \"mean\"')\n\n        self.head = nn.Sequential(\n            nn.LayerNorm(in_dim),\n            nn.Linear(in_dim, hidden),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_targets)\n        )\n\n    def _infer_device_from_inputs(self, obj) -> torch.device:\n        \"\"\"\n        Пытается определить целевой девайс по первому найденному тензору во входах.\n\n        :param obj: Произвольная вложенная структура (тензоры/словари/списки).\n        :return: torch.device (cuda при наличии, иначе cpu).\n        \"\"\"\n        if isinstance(obj, torch.Tensor):\n            return obj.device\n        if isinstance(obj, dict):\n            for v in obj.values():\n                d = self._infer_device_from_inputs(v)\n                if d is not None:\n                    return d\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _fuse(self, z: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Объединяет эмбеддинги модальностей согласно self.fusion.\n\n        - concat: конкатенация по последней оси;\n        - mean: среднее по модальностям (требует совпадения размерностей).\n\n        :param z: Словарь эмбеддингов по модальностям.\n        :return: Слитый эмбеддинг [B, D*|mods|] (concat) или [B, D] (mean).\n        \"\"\"\n        order = [m for m in [\"image\", \"text\", \"audio\"] if m in self.modalities]\n        feats = []\n        for m in order:\n            t = z[m]\n            if t.dim() == 3:\n                t = t.mean(dim=1)\n            elif t.dim() > 3:\n                t = t.view(t.size(0), -1)\n            feats.append(t)\n        if self.fusion == \"concat\":\n            return torch.cat(feats, dim=-1)\n        elif self.fusion == \"mean\":\n            sizes = [f.size(-1) for f in feats]\n            if len(set(sizes)) != 1:\n                raise ValueError(f'Для fusion=\"mean\" размеры модальностей должны совпадать: {sizes}')\n            return torch.stack(feats, dim=0).mean(dim=0)\n\n    def forward(self, backend_inputs: Dict[str, Any], labels: Optional[torch.Tensor] = None) -> ModelOutput:\n        \"\"\"\n        Прямой проход: кодирование модальностей -> фьюжн -> регрессионная голова.\n\n        :param backend_inputs: Входы для бэкенда (из его collate()).\n        :param labels: Не используется (Trainer читает labels отдельно).\n        :return: ModelOutput с полем logits [B, T].\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        preds = self.head(fused)\n        return ModelOutput(logits=preds)\n\n    @torch.no_grad()\n    def get_embeddings(self, backend_inputs: Dict[str, Any], return_per_modality: bool = False):\n        \"\"\"\n        Извлекает fused эмбеддинги (и, опционально, эмбеддинги по модальностям).\n\n        :param backend_inputs: Входы для бэкенда.\n        :param return_per_modality: Вернуть также словарь эмбеддингов по модальностям.\n        :return: fused [B, *] или (fused, {'text':[B,*], 'image':[B,*], 'audio':[B,*]}).\n        \"\"\"\n        device = self._infer_device_from_inputs(backend_inputs)\n        z = self.backend.encode(backend_inputs, device=device)\n        fused = self._fuse(z)\n        if return_per_modality:\n            return fused, z\n        return fused\n\n\nclass MSETrainer(Trainer):\n    \"\"\"\n    Trainer для регрессии на основе MSE loss.\n    \"\"\"\n    def __init__(self, *args, reduction: str = \"mean\", **kwargs):\n        \"\"\"\n        :param reduction: Тип редукции MSELoss ('mean' | 'sum' | 'none').\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self._reduction = reduction\n        self._warned_label_tiling = False\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, num_items_in_batch: Optional[int] = None):\n        \"\"\"\n        Вычисляет MSE лосс между предсказаниями и целевыми значениями.\n        Защищается от случая DataParallel, когда logits дублируются по числу GPU.\n\n        :param model: Модель.\n        :param inputs: Батч, содержащий 'labels' (float [B,T]) и аргументы для model.forward().\n        :param return_outputs: Возвращать ли также outputs.\n        :param num_items_in_batch: Совместимость с API Trainer (не используется).\n        :return: loss (и outputs, если return_outputs=True).\n        \"\"\"\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        preds = outputs.logits\n        labels = labels.to(preds.device)\n\n        if preds.size(0) != labels.size(0):\n            ngpu = torch.cuda.device_count()\n            if ngpu > 1 and preds.size(0) == labels.size(0) * ngpu:\n                labels = labels.repeat_interleave(ngpu, dim=0)\n                if not self._warned_label_tiling:\n                    print(f\"[Warning] DataParallel удвоил batch для logits. \"\n                          f\"Повторяем labels x{ngpu}. preds: {tuple(preds.shape)}, labels: {tuple(labels.shape)}\")\n                    self._warned_label_tiling = True\n            else:\n                raise ValueError(f\"Batch size mismatch: preds {tuple(preds.shape)} vs labels {tuple(labels.shape)}\")\n\n        loss = nn.MSELoss(reduction=self._reduction)(preds, labels)\n        return (loss, outputs) if return_outputs else loss\n\n\nclass PbarConsoleLogger(TrainerCallback):\n    \"\"\"\n    Внешний прогресс‑бар и консольный логгер метрик/лоссов для стабильного отображения на больших данных.\n    \"\"\"\n    def __init__(self, pbar):\n        self.pbar = pbar\n        self.last_logs = {}\n        self.last_train_loss = None\n        self.printed_eval_steps = set()\n\n    def _step(self, state) -> int:\n        return int(state.global_step or 0)\n\n    def _fmt_postfix(self):\n        parts = []\n        if 'loss' in self.last_logs:\n            parts.append(f\"loss {self.last_logs['loss']:.4f}\")\n        if 'eval_loss' in self.last_logs:\n            parts.append(f\"val {self.last_logs['eval_loss']:.4f}\")\n        for k, v in self.last_logs.items():\n            if k.startswith('eval_') and k not in (\n                'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'\n            ):\n                try:\n                    parts.append(f\"{k.replace('eval_', '')} {float(v):.4f}\")\n                except Exception:\n                    pass\n        return \" | \".join(parts)\n\n    def on_step_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        if self.last_logs:\n            self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        for k, v in logs.items():\n            if isinstance(v, (int, float)):\n                self.last_logs[k] = float(v)\n        if 'loss' in logs and isinstance(logs['loss'], (int, float)):\n            self.last_train_loss = float(logs['loss'])\n\n        self.pbar.set_postfix_str(self._fmt_postfix(), refresh=False)\n        self.pbar.refresh()\n\n        if any(k.startswith('eval_') for k in logs.keys()):\n            step = self._step(state)\n            if step in self.printed_eval_steps:\n                return\n            self.printed_eval_steps.add(step)\n\n            train_loss_str = f\"{self.last_train_loss:.10f}\" if self.last_train_loss is not None else \"n/a\"\n            val_loss = logs.get('eval_loss', None)\n            val_loss_str = f\"{float(val_loss):.10g}\" if isinstance(val_loss, (int, float)) else \"n/a\"\n\n            exclude = {'eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'}\n            extra_parts = []\n            for k, v in logs.items():\n                if k.startswith('eval_') and k not in exclude:\n                    metric_name = k.replace('eval_', '')\n                    try:\n                        extra_parts.append(f\"val {metric_name}: {float(v):.10f}\")\n                    except Exception:\n                        pass\n\n            line = f\"step: {step}, train loss: {train_loss_str}, val loss: {val_loss_str}\"\n            if extra_parts:\n                line += \", \" + \", \".join(extra_parts)\n            tqdm.write(line)\n\n    def on_train_end(self, args, state, control, **kwargs):\n        n = min(self._step(state), self.pbar.total)\n        if n > self.pbar.n:\n            self.pbar.update(n - self.pbar.n)\n        self.pbar.refresh()\n\n\nclass SingleModelMultiComboRegression:\n    \"\"\"\n    Регрессионный пайплайн на одной мультимодальной модели (бэкенде) под заданные комбинации модальностей.\n\n    Поддерживаемые комбинации (auto-подбор бэкенда):\n      - ['text','image']         -> CLIP (HF)\n      - ['text','audio']         -> CLAP (HF)\n      - ['image','audio']        -> ClipWav2CLIP\n      - ['text','image','audio'] -> ClipWav2CLIP\n\n    Особенности:\n      - Поддержка нескольких изображений/аудио на сэмпл ('concat' или 'mean' агрегация).\n      - Обучение на больших данных: чанковая подстановка train_dataset, стабильный прогресс‑бар и логи.\n    \"\"\"\n    def __init__(\n        self,\n        modalities: List[str],\n        target_columns_names: List[str],\n        text_columns: Optional[List[str]] = None,\n        image_columns: Optional[List[str]] = None,\n        audio_columns: Optional[List[str]] = None,\n        backend: str = \"auto\",\n        clip_checkpoint: str = \"openai/clip-vit-base-patch32\",\n        clap_checkpoint: str = \"laion/clap-htsat-unfused\",\n        fusion: str = \"concat\",\n        freeze_backbone: bool = True,\n        clip_max_length: int = 77,\n        max_images_per_sample: int = 1,\n        max_audios_per_sample: int = 1,\n        image_agg: str = \"concat\",\n        audio_agg: str = \"concat\"\n    ):\n        \"\"\"\n        Инициализация пайплайна регрессии.\n\n        :param modalities: Активные модальности ('text','image','audio') в любом порядке.\n        :param target_columns_names: Имена целевых колонок (num_targets = len(target_columns_names)).\n        :param text_columns: Имена текстовых колонок (склеиваются).\n        :param image_columns: Список колонок изображений (ячейки — значение или список значений).\n        :param audio_columns: Список колонок аудио (ячейки — значение или список значений).\n        :param backend: 'auto' | 'clip' | 'clap' | 'clip_wav2clip'.\n        :param clip_checkpoint: Чекпоинт CLIP (HF).\n        :param clap_checkpoint: Чекпоинт CLAP (HF).\n        :param fusion: Тип фьюжна ('concat' или 'mean').\n        :param freeze_backbone: Заморозить веса бэкенда (linear probing).\n        :param clip_max_length: Максимальная длина токенов для CLIP.\n        :param max_images_per_sample: Максимум изображений на сэмпл при агрегации.\n        :param max_audios_per_sample: Максимум аудио на сэмпл при агрегации.\n        :param image_agg: Агрегация изображений ('concat' | 'mean').\n        :param audio_agg: Агрегация аудио ('concat' | 'mean').\n        \"\"\"\n        self.modalities = sorted(list(set(modalities)))\n        self.target_columns_names = target_columns_names\n        self.num_targets = len(target_columns_names)\n        self.text_columns = text_columns or []\n        self.image_columns = image_columns or []\n        self.audio_columns = audio_columns or []\n        self.backend_name = backend\n        self.clip_checkpoint = clip_checkpoint\n        self.clap_checkpoint = clap_checkpoint\n        self.fusion = fusion\n        self.freeze_backbone = freeze_backbone\n        self.clip_max_length = clip_max_length\n        self.max_images_per_sample = int(max_images_per_sample)\n        self.max_audios_per_sample = int(max_audios_per_sample)\n        self.image_agg = image_agg\n        self.audio_agg = audio_agg\n\n        self.backend: Optional[BaseBackend] = None\n        self.model: Optional[SingleBackboneRegressor] = None\n        self.trainer: Optional[Trainer] = None\n        self.compute_metrics = None\n        self.progress_callback: Optional[TrainerCallback] = None\n\n        self._build_backend()\n\n    def _build_backend(self):\n        \"\"\"\n        Инициализирует бэкенд согласно настройке backend/auto и проверяет совместимость с модальностями.\n\n        :raises ValueError: Если комбинация модальностей не поддерживается выбранным бэкендом.\n        \"\"\"\n        mods = set(self.modalities)\n        name = self.backend_name\n        if name == \"auto\":\n            if mods == {\"text\", \"image\"}:\n                name = \"clip\"\n            elif mods == {\"text\", \"audio\"}:\n                name = \"clap\"\n            elif mods in ({\"image\", \"audio\"}, {\"text\", \"image\", \"audio\"}):\n                name = \"clip_wav2clip\"\n            else:\n                raise ValueError(f\"Неподдерживаемая комбинация: {mods}\")\n\n        if name == \"clip\":\n            self.backend = ClipBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                max_images=self.max_images_per_sample,\n                image_agg=self.image_agg\n            )\n        elif name == \"clap\":\n            self.backend = ClapBackend(\n                checkpoint=self.clap_checkpoint,\n                freeze=self.freeze_backbone,\n                max_audios=self.max_audios_per_sample,\n                audio_agg=self.audio_agg\n            )\n        elif name == \"clip_wav2clip\":\n            self.backend = ClipWav2CLIPBackend(\n                checkpoint=self.clip_checkpoint,\n                max_length=self.clip_max_length,\n                freeze=self.freeze_backbone,\n                audio_sr=16000,\n                max_images=self.max_images_per_sample,\n                max_audios=self.max_audios_per_sample,\n                image_agg=self.image_agg,\n                audio_agg=self.audio_agg\n            )\n        else:\n            raise ValueError(f\"Неизвестный backend: {name}\")\n\n        if not set(self.modalities).issubset(self.backend.supported):\n            raise ValueError(\n                f\"Бэкенд {self.backend.name} не поддерживает модальности {self.modalities}. \"\n                f\"Поддерживает: {self.backend.supported}\"\n            )\n\n    def _validate_data(self, df: pd.DataFrame):\n        \"\"\"\n        Проверяет соответствие колонок в DataFrame выбранным модальностям и целям.\n\n        :param df: Исходный датафрейм.\n        :raises ValueError: При отсутствии обязательных колонок.\n        \"\"\"\n        missing_targets = [c for c in self.target_columns_names if c not in df.columns]\n        if missing_targets:\n            raise ValueError(f\"В DataFrame отсутствуют целевые колонки: {missing_targets}\")\n\n        if \"text\" in self.modalities:\n            if not self.text_columns:\n                raise ValueError(\"Вы выбрали модальность 'text', но text_columns пустой.\")\n            missing = [c for c in self.text_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют текстовые колонки: {missing}\")\n\n        if \"image\" in self.modalities:\n            if not self.image_columns:\n                raise ValueError(\"Вы выбрали модальность 'image', но image_columns пуст.\")\n            missing = [c for c in self.image_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки изображений: {missing}\")\n\n        if \"audio\" in self.modalities:\n            if not self.audio_columns:\n                raise ValueError(\"Вы выбрали модальность 'audio', но audio_columns пуст.\")\n            missing = [c for c in self.audio_columns if c not in df.columns]\n            if missing:\n                raise ValueError(f\"В DataFrame отсутствуют колонки аудио: {missing}\")\n\n    def _split(self, df: pd.DataFrame, test_size: float = 0.2, seed: int = 42):\n        \"\"\"\n        Перемешивает и делит DataFrame на обучающую и валидационную части.\n\n        :param df: Исходный датафрейм.\n        :param test_size: Доля валидации (0..1).\n        :param seed: Зерно для перемешивания.\n        :return: (df_train, df_eval).\n        \"\"\"\n        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n        n_eval = int(math.ceil(len(df) * test_size))\n        return df.iloc[n_eval:].reset_index(drop=True), df.iloc[:n_eval].reset_index(drop=True)\n\n    def _setup_compute_metrics(self, metric_name: str):\n        \"\"\"\n        Создаёт функцию подсчёта метрик для Trainer.\n\n        Поддерживаемые итоговые метрики:\n          - 'rmse' (минимизируется),\n          - 'mae'  (минимизируется),\n          - 'mse'  (минимизируется),\n          - 'r2'   (максимизируется).\n\n        :param metric_name: Название основной метрики.\n        \"\"\"\n        name = metric_name.lower()\n\n        def compute(p):\n            preds = np.asarray(p.predictions)\n            refs  = np.asarray(p.label_ids)\n\n            # гарантируем форму [N, T]\n            if preds.ndim == 1: preds = preds[:, None]\n            if refs.ndim  == 1: refs  = refs[:,  None]\n\n            T = min(preds.shape[1], refs.shape[1])\n            mse_list, mae_list, r2_list = [], [], []\n\n            for t in range(T):\n                y_true = refs[:, t].astype(np.float64)\n                y_pred = preds[:, t].astype(np.float64)\n\n                err = y_pred - y_true\n                mse = float(np.mean(err**2))\n                mae = float(np.mean(np.abs(err)))\n\n                # R^2: 1 - SS_res/SS_tot; если дисперсия нулевая, даём 0.0 (безопасный фолбэк)\n                var = float(np.var(y_true))\n                if var == 0.0:\n                    r2 = 0.0\n                else:\n                    ss_res = float(np.sum(err**2))\n                    ss_tot = float(np.sum((y_true - np.mean(y_true))**2))\n                    r2 = 1.0 - (ss_res / ss_tot if ss_tot > 0 else 0.0)\n\n                mse_list.append(mse)\n                mae_list.append(mae)\n                r2_list.append(r2)\n\n            mse_avg = float(np.mean(mse_list))\n            rmse_avg = float(np.sqrt(mse_avg))\n            mae_avg = float(np.mean(mae_list))\n            r2_avg = float(np.mean(r2_list))\n            return {\"rmse\": rmse_avg, \"mse\": mse_avg, \"mae\": mae_avg, \"r2\": r2_avg}\n\n        self.compute_metrics = compute\n        self._primary_metric = name\n        self._greater_is_better = True if name == \"r2\" else False\n\n    def fit(\n        self,\n        train_data: pd.DataFrame,\n        epochs: int = 3,\n        test_size: float = 0.2,\n        per_device_train_batch_size: int = 16,\n        gradient_accumulation_steps: int = 1,\n        learning_rate: float = 2e-4,\n        metric_name: str = \"rmse\",\n        fp16: bool = True,\n        logging_steps: int = 50,\n        eval_steps: int = 200,\n        output_dir: str = \"./reg_result\",\n        seed: int = 42,\n        hidden: int = 512,\n        dropout: float = 0.1,\n        fit_chunk_size: Optional[int] = None\n    ):\n        \"\"\"\n        Обучает регрессионную голову поверх выбранного бэкенда.\n        Поддерживает обучение на больших данных за счёт чанков: train_dataset подставляется кусками.\n\n        :param train_data: Данные обучения с необходимыми колонками модальностей и целями.\n        :param epochs: Количество эпох обучения.\n        :param test_size: Доля валидации.\n        :param per_device_train_batch_size: Размер батча на устройство.\n        :param gradient_accumulation_steps: Шаги аккумуляции градиентов.\n        :param learning_rate: Learning rate для AdamW.\n        :param metric_name: Основная метрика ('rmse' | 'mae' | 'mse' | 'r2') для выбора лучшей модели.\n        :param fp16: Использовать ли fp16 при наличии CUDA (если доступен bf16 — он будет использован вместо fp16).\n        :param logging_steps: Периодичность логирования.\n        :param eval_steps: Периодичность валидации/сохранения чекпоинтов.\n        :param output_dir: Папка для логов/чекпоинтов.\n        :param seed: Зерно.\n        :param hidden: Размер скрытого слоя головы.\n        :param dropout: Дропаут в голове.\n        :param fit_chunk_size: Размер чанка обучающей выборки. Если None — весь train как один чанк.\n        :return: self.\n        :raises ValueError: При проблемах с данными или параметрами.\n        \"\"\"\n        self._validate_data(train_data)\n        set_seed(seed)\n\n        df_train, df_eval = self._split(train_data, test_size=test_size, seed=seed)\n\n        # Датасеты: валидацию держим целиком, train будем подставлять чанками\n        ds_eval = MultiComboRegDataset(df_eval, self.target_columns_names, self.text_columns, self.image_columns, self.audio_columns)\n\n        self.model = SingleBackboneRegressor(\n            backend=self.backend,\n            modalities=self.modalities,\n            num_targets=self.num_targets,\n            fusion=self.fusion,\n            hidden=hidden,\n            dropout=dropout\n        )\n\n        self._setup_compute_metrics(metric_name)\n\n        # Настройки точности (bf16, если доступен, иначе fp16 при флаге fp16=True)\n        bf16_ok = bool(torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8)\n\n        args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            per_device_eval_batch_size=max(4, per_device_train_batch_size // 2),\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            eval_accumulation_steps=max(1, gradient_accumulation_steps * 2),\n            learning_rate=learning_rate,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.01,\n            eval_strategy=\"steps\",            # <- по вашей просьбе\n            eval_steps=eval_steps,\n            save_strategy=\"steps\",\n            save_steps=eval_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=f\"eval_{self._primary_metric}\",\n            greater_is_better=self._greater_is_better,\n            save_total_limit=1,\n            logging_strategy=\"steps\",\n            logging_steps=logging_steps,\n            report_to=\"none\",\n            fp16=bool(fp16 and torch.cuda.is_available() and not bf16_ok),\n            bf16=bool(bf16_ok and not fp16),\n            dataloader_num_workers=0,\n            seed=seed,\n            remove_unused_columns=False,\n            disable_tqdm=True  # используем внешний tqdm\n        )\n\n        def data_collator(batch_list):\n            \"\"\"\n            Collate-хук для Trainer: делегирует сборку батча в backend.collate().\n\n            :param batch_list: Список элементов датасета.\n            :return: Батч словаря {'labels', 'backend_inputs'}.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        # Вспомогательные функции для чанков\n        def steps_for_size(sz: int, bsz: int, accum: int) -> int:\n            \"\"\"\n            Оценивает число оптимизационных шагов на чанке размера sz.\n\n            :param sz: Количество примеров в чанке.\n            :param bsz: Размер батча.\n            :param accum: Шаги аккумуляции.\n            :return: Число оптимизационных шагов.\n            \"\"\"\n            return max(0, math.ceil(math.ceil(sz / max(1, bsz)) / max(1, accum)))\n\n        def chunk_slices(index_array: np.ndarray, chunk_size: int):\n            \"\"\"\n            Генератор срезов индексов по chunk_size.\n\n            :param index_array: Индексы обучающей выборки.\n            :param chunk_size: Размер чанка.\n            :yield: Срез индексов.\n            \"\"\"\n            for i in range(0, len(index_array), chunk_size):\n                yield index_array[i:i + chunk_size]\n\n        # Индексы train\n        n_train = len(df_train)\n        rng = np.random.default_rng(seed)\n        train_idx = np.arange(n_train)\n\n        # Чанк по умолчанию — весь train\n        chunk_size = fit_chunk_size if (fit_chunk_size and fit_chunk_size > 0) else len(train_idx)\n\n        # Предварительный рассчёт общего числа шагов (для прогресс‑бара и планировщика)\n        total_steps = 0\n        for _ in range(epochs):\n            rng.shuffle(train_idx)\n            for slc in chunk_slices(train_idx, chunk_size):\n                total_steps += steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n\n        # Инициализация Trainer с «пустым» train датасетом (минимальный чанк), чтобы не держать весь train\n        dummy_idx = np.arange(min(len(df_train), 1))\n        ds_train_init = MultiComboRegDataset(\n            df_train.iloc[dummy_idx], self.target_columns_names, self.text_columns, self.image_columns, self.audio_columns\n        ) if len(dummy_idx) > 0 else ds_eval\n\n        self.trainer = MSETrainer(\n            model=self.model,\n            args=args,\n            train_dataset=ds_train_init,\n            eval_dataset=ds_eval,\n            data_collator=data_collator,\n            compute_metrics=self.compute_metrics\n        )\n        # Убираем стандартный принтер, чтобы не мешал внешнему tqdm\n        try:\n            self.trainer.remove_callback(PrinterCallback)\n        except Exception:\n            pass\n\n        # Планировщик на рассчитанное количество шагов\n        if total_steps > 0:\n            self.trainer.create_optimizer_and_scheduler(num_training_steps=total_steps)\n\n        # Внешний прогресс‑бар + консольный лог\n        pbar = tqdm(total=total_steps, desc=\"Training Progress\", unit=\"step\")\n        cb = PbarConsoleLogger(pbar)\n        self.trainer.add_callback(cb)\n        self.progress_callback = cb\n\n        # Основной цикл обучения по эпохам и чанкам\n        steps_done = 0\n        for ep in range(epochs):\n            rng = np.random.default_rng(seed + ep)\n            shuffled = np.arange(n_train)\n            rng.shuffle(shuffled)\n\n            for slc in chunk_slices(shuffled, chunk_size):\n                # Подставляем чанк\n                chunk_df = df_train.iloc[slc]\n                ds_chunk = MultiComboRegDataset(\n                    chunk_df, self.target_columns_names, self.text_columns, self.image_columns, self.audio_columns\n                )\n                self.trainer.train_dataset = ds_chunk\n\n                # Считаем шаги на чанке и настраиваем max_steps Trainer\n                chunk_steps = steps_for_size(len(slc), per_device_train_batch_size, gradient_accumulation_steps)\n                if chunk_steps == 0:\n                    del ds_chunk, chunk_df\n                    continue\n\n                self.trainer.args.max_steps = steps_done + chunk_steps\n                self.trainer.train()\n                steps_done += chunk_steps\n\n                # Очистка памяти между чанками\n                del ds_chunk, chunk_df\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n        pbar.close()\n        return self\n\n    def predict(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Делает предсказания регрессионных таргетов на новых данных.\n\n        :param df: Датафрейм с нужными колонками модальностей (целевые колонки могут отсутствовать).\n        :return: Массив предсказаний формы [N, T].\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n        df_c = df.copy()\n        for c in self.target_columns_names:\n            if c not in df_c.columns:\n                df_c[c] = 0.0\n        ds = MultiComboRegDataset(df_c, self.target_columns_names, self.text_columns, self.image_columns, self.audio_columns)\n        preds = self.trainer.predict(test_dataset=ds)\n        return preds.predictions\n\n    def get_embeddings(self, df: pd.DataFrame, batch_size: int = 32, return_per_modality: bool = False):\n        \"\"\"\n        Извлекает эмбеддинги для новых данных (fused и, опционально, по модальностям).\n\n        :param df: Датафрейм с нужными колонками модальностей.\n        :param batch_size: Размер батча для извлечения эмбеддингов.\n        :param return_per_modality: Вернуть также по-модальные эмбеддинги.\n        :return: fused [N, D_fused] или (fused, {'text':[N,*], 'image':[N,*], 'audio':[N,*]}).\n        :raises RuntimeError: Если модель ещё не обучена.\n        \"\"\"\n        if self.trainer is None or self.model is None:\n            raise RuntimeError(\"Модель не обучена. Вызовите .fit().\")\n\n        # Определяем девайс модели\n        try:\n            device = next(self.trainer.model.parameters()).device\n        except StopIteration:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.model.to(device).eval()\n\n        df_c = df.copy()\n        for c in self.target_columns_names:\n            if c not in df_c.columns:\n                df_c[c] = 0.0\n\n        ds = MultiComboRegDataset(\n            df_c,\n            self.target_columns_names,\n            self.text_columns,\n            self.image_columns,\n            self.audio_columns\n        )\n\n        def collate(batch_list):\n            \"\"\"\n            Collate для DataLoader при извлечении эмбеддингов (использует backend.collate()).\n\n            :param batch_list: Элементы датасета.\n            :return: Батч {'labels', 'backend_inputs'}.\n            \"\"\"\n            return self.backend.collate(batch_list)\n\n        def move_to_device(obj, device):\n            \"\"\"\n            Рекурсивно переносит тензоры в структуре на заданный девайс.\n\n            :param obj: Тензор/словарь/список/кортеж/прочее.\n            :param device: torch.device ('cpu' или 'cuda').\n            :return: Объект с перенесёнными тензорами.\n            \"\"\"\n            if torch.is_tensor(obj):\n                return obj.to(device)\n            if isinstance(obj, dict):\n                return {k: move_to_device(v, device) for k, v in obj.items()}\n            if isinstance(obj, (list, tuple)):\n                t = [move_to_device(v, device) for v in obj]\n                return type(obj)(t) if not isinstance(obj, list) else t\n            return obj\n\n        loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n        fused_list = []\n        per_mod_lists = {m: [] for m in self.modalities} if return_per_modality else None\n\n        with torch.no_grad():\n            for batch in loader:\n                bi = move_to_device(batch[\"backend_inputs\"], device)\n                fused, per = self.model.get_embeddings(backend_inputs=bi, return_per_modality=True)\n                fused_list.append(fused.cpu().numpy())\n                if return_per_modality:\n                    for m in per_mod_lists.keys():\n                        if m in per:\n                            per_mod_lists[m].append(per[m].cpu().numpy())\n\n        fused_arr = np.vstack(fused_list)\n        if not return_per_modality:\n            return fused_arr\n        per_mod = {m: np.vstack(chunks) for m, chunks in per_mod_lists.items()}\n        return fused_arr, per_mod","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Создание фиктивных данных.","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torchaudio\n\nHAVE_TORCHAUDIO = True\n\nrandom.seed(42); np.random.seed(42); torch.manual_seed(42)\n\nBASE_DIR = \"./dummy_data\"\nIMG_DIR  = os.path.join(BASE_DIR, \"images\")\nAUD_DIR  = os.path.join(BASE_DIR, \"audio\")\nos.makedirs(IMG_DIR, exist_ok=True)\nos.makedirs(AUD_DIR, exist_ok=True)\n\n# Сколько элементов на модальность в каждой строке\nK_PER_MODALITY = 3\n\ndef make_dummy_images(n=12, size=(256, 256)):\n    paths = []\n    for i in range(n):\n        color = tuple(np.random.randint(0, 255, size=3).tolist())\n        img = Image.new(\"RGB\", size, color=color)\n        path = os.path.join(IMG_DIR, f\"img_{i:02d}.png\")\n        img.save(path)\n        paths.append(path)\n    return paths\n\ndef make_dummy_audios(n=12, sr=48000, duration_sec=0.6):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Нельзя сгенерировать аудио без torchaudio. Установите 'pip install torchaudio'.\")\n    paths = []\n    t = torch.linspace(0, duration_sec, int(sr * duration_sec))\n    for i in range(n):\n        freq = random.choice([220, 330, 440, 550, 660, 880])\n        wave = 0.2 * torch.sin(2 * np.pi * freq * t)  # амплитуда 0.2\n        wave = wave.unsqueeze(0)  # [1, T] mono\n        path = os.path.join(AUD_DIR, f\"tone_{i:02d}.wav\")\n        torchaudio.save(path, wave, sample_rate=sr)\n        paths.append(path)\n    return paths\n\nimg_paths = make_dummy_images(n=12)\naudio_paths = make_dummy_audios(n=12) if HAVE_TORCHAUDIO else []\n\n# Вспомогательные тексты\nTITLES = [\"Red fox\", \"Blue sky\", \"Green field\", \"Yellow sun\", \"Purple rain\", \"Silver line\"]\nBODIES = [\"quick brown\", \"lazy dog\", \"jumps high\", \"runs fast\", \"stays calm\", \"shines bright\"]\nQUERIES = [\"find tone\", \"classify sound\", \"describe image\", \"retrieve pair\", \"detect event\"]\n\ndef rand_title(): return random.choice(TITLES)\ndef rand_body(): return random.choice(BODIES)\ndef rand_query(): return random.choice(QUERIES)\n\n# Универсальные хелперы\ndef sample_k(seq, k):\n    if len(seq) >= k:\n        return random.sample(seq, k)  # без повторов\n    else:\n        return [random.choice(seq) for _ in range(k)]  # с повторами, если мало исходников\n\ndef as_cols(prefix, values):\n    # {\"prefix_1\": values[0], ..., \"prefix_k\": values[k-1]}\n    return {f\"{prefix}_{i+1}\": v for i, v in enumerate(values)}\n\ndef pick_text_desc(k=K_PER_MODALITY):\n    # Текстовое описание: \"Title | body\"\n    vals = [f\"{rand_title()} | {rand_body()}\" for _ in range(k)]\n    return as_cols(\"text\", vals)\n\ndef pick_text_query(k=K_PER_MODALITY):\n    vals = [rand_query() for _ in range(k)]\n    return as_cols(\"text\", vals)\n\ndef pick_images(k=K_PER_MODALITY):\n    vals = sample_k(img_paths, k)\n    return as_cols(\"image_path\", vals)\n\ndef pick_audios(k=K_PER_MODALITY):\n    vals = sample_k(audio_paths, k)\n    return as_cols(\"audio_path\", vals)\n\n# 1) Текст + Картинка -> по 3 текстовых и 3 картинок\ndef build_df_text_image(n=24, k=K_PER_MODALITY):\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_desc(k))\n        row.update(pick_images(k))\n        row[\"y1\"] = random.uniform(-10, 10)\n        row[\"y2\"] = random.uniform(-10, 10)\n        row[\"y3\"] = random.uniform(-10, 10)\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 2) Текст + Звук -> по 3 текста (query) и 3 аудио\ndef build_df_text_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для text+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_query(k))\n        row.update(pick_audios(k))\n        row[\"y1\"] = random.uniform(-10, 10)\n        row[\"y2\"] = random.uniform(-10, 10)\n        row[\"y3\"] = random.uniform(-10, 10)\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 3) Картинка + Звук -> по 3 картинки и 3 аудио\ndef build_df_image_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для image+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_images(k))\n        row.update(pick_audios(k))\n        row[\"y1\"] = random.uniform(-10, 10)\n        row[\"y2\"] = random.uniform(-10, 10)\n        row[\"y3\"] = random.uniform(-10, 10)\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# 4) Текст + Картинка + Звук -> по 3 на каждую модальность\ndef build_df_text_image_audio(n=24, k=K_PER_MODALITY):\n    if not HAVE_TORCHAUDIO:\n        raise RuntimeError(\"Для text+image+audio нужен torchaudio.\")\n    rows = []\n    for _ in range(n):\n        row = {}\n        row.update(pick_text_desc(k))\n        row.update(pick_images(k))\n        row.update(pick_audios(k))\n        row[\"y1\"] = random.uniform(-10, 10)\n        row[\"y2\"] = random.uniform(-10, 10)\n        row[\"y3\"] = random.uniform(-10, 10)\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n# Собираем 4 датасета\ndf_text_image = build_df_text_image(243, K_PER_MODALITY)\ndf_text_audio = build_df_text_audio(100, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\ndf_image_audio = build_df_image_audio(300, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\ndf_text_image_audio  = build_df_text_image_audio(190, K_PER_MODALITY) if HAVE_TORCHAUDIO else None\n\nprint(\"df_text_image columns:\", list(df_text_image.columns))\nif HAVE_TORCHAUDIO:\n    print(\"df_text_audio columns:\", list(df_text_audio.columns))\n    print(\"df_image_audio columns:\", list(df_image_audio.columns))\n    print(\"df_text_image_audio columns:\", list(df_text_image_audio.columns))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Пример использования.","metadata":{}},{"cell_type":"code","source":"pipeline = SingleModelMultiComboRegression(\n    modalities=[\"text\", \"image\", \"audio\"],\n    target_columns_names=[\"y1\", \"y2\"],\n    text_columns=[\"text_1\", \"text_2\"],\n    image_columns=[\"image_path_1\", \"image_path_2\"],\n    audio_columns=[\"audio_path_1\", \"audio_path_2\"],\n    backend=\"auto\",\n    clip_checkpoint=\"openai/clip-vit-base-patch32\",\n    fusion=\"concat\",\n    freeze_backbone=True,\n)\npipeline.fit(\n    df_text_image_audio,\n    epochs=3,\n    per_device_train_batch_size=8,\n    logging_steps=10,\n    eval_steps=10,\n    fit_chunk_size=30\n)\npreds = pipeline.predict(df_text_image_audio[:5])\nembeddings = pipeline.get_embeddings(df_text_image_audio[:5])\n\nprint(preds)\nprint(embeddings)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}