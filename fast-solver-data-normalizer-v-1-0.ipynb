{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Hashing, Embedding, Flatten\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Вспомогательные функции.","metadata":{}},{"cell_type":"code","source":"def del_dublicates(lst):\n    seen = set()\n    result = []\n    for item in lst:\n        if item not in seen:\n            result.append(item)\n            seen.add(item)\n\n    return result","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Функции для нормализации данных.","metadata":{}},{"cell_type":"code","source":"def code_hashes(data, output_dim, num_bins, hash_col_name, batch_size=128):\n    \"\"\"\n    Кодирует хэши в ембеддинги.\n\n    Неодходимые импорты:\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Input, Hashing, Embedding, Flatten\n    import tensorflow as tf\n    import pandas as pd\n\n    :param data: Pandas таблица (pd.DataFrame), в котором будут кодироваться хэши одного из столбцов.\n    :param output_dim: Размерность эмбеддингов.\n    :param num_bins: Количество корзин для раскладывания в них хэшей. Лучше поставить 2 * количество уникальных хэшей.\n    :param hash_col_name: Имя столбца, в котором будут кодироваться хэши.\n    :param batch_size: Размер батча, который будет использоваться для кодирования хэшей в keras модели.\n    :return: data с кодированным столбцом, словарь хэш: эмбеддниг, keras модель для кодирования хэшей.\n    \"\"\"\n\n    data = data.copy()  # копирование таблицы\n    data[hash_col_name] = data[hash_col_name].astype(str)  # изменение типа данных для модели\n\n    # создание модели для кодирования хэшей\n    hash_encoder = Sequential()\n    hash_encoder.add(Input((1,), dtype=tf.string))\n    hash_encoder.add(Hashing(num_bins=num_bins))\n    hash_encoder.add(Embedding(input_dim=num_bins, output_dim=output_dim))\n    hash_encoder.add(Flatten())\n    hash_encoder.build()\n\n    # создание словаря с кодированными хэшами\n    hashes = data[hash_col_name].unique()\n    coded_hashes = hash_encoder.predict(hashes, batch_size=batch_size)\n    coded_hashes_dict = dict(zip(hashes, coded_hashes))\n\n    # создание таблицы с кодированными хэшами\n    coded_data = [coded_hashes_dict[hash_] for hash_ in data[hash_col_name]]\n    col_names = [hash_col_name + f'_embedding_{n}' for n in range(1, output_dim + 1)]\n    coded_hashes_df = pd.DataFrame(coded_data, columns=col_names, index=data.index)\n\n    # добавление кодированных хэшей к обучающим данным\n    data = pd.concat([data, coded_hashes_df], axis=1)\n\n    return data, coded_hashes_dict, hash_encoder","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_date(all_data, date_column_name, categorical_columns=None):\n    \"\"\"\n    На вход надо передавать объединённые признаки в обучающих данных и признаки для сабмита.\n\n    Необходимых импортов нет.\n    \"\"\"\n    \n    if categorical_columns is None:\n        categorical_columns = []\n\n    all_data = all_data.copy()\n\n    if pd.api.types.is_datetime64_any_dtype(all_data[date_column_name].dtype) and all_data[date_column_name].dt.tz is not None:\n        all_data[date_column_name] = all_data[date_column_name].dt.tz_localize(None)\n\n    min_date = all_data[date_column_name].min()\n    all_data[date_column_name + '_year'] = all_data[date_column_name].dt.year\n    all_data[date_column_name + '_quarter'] = all_data[date_column_name].dt.quarter\n    all_data[date_column_name + '_month'] = all_data[date_column_name].dt.month\n    all_data[date_column_name + '_day'] = all_data[date_column_name].dt.day\n    # all_data[date_column_name + '_hour'] = all_data[date_column_name].dt.hour\n    # all_data[date_column_name + '_minute'] = all_data[date_column_name].dt.minute\n    all_data[date_column_name + '_month_since_start'] = (all_data[date_column_name].dt.year * 12 + all_data[date_column_name].dt.month\n                                                        ) - (min_date.year * 12 + min_date.month)\n    all_data[date_column_name + '_quarter_since_start'] = (all_data[date_column_name].dt.year * 4 + all_data[date_column_name].dt.quarter\n                                                          ) - (min_date.year * 4 + min_date.quarter)\n    all_data[date_column_name + '_years_since_start'] = all_data[date_column_name].dt.year - min_date.year\n    all_data[date_column_name + '_days_since_start'] = (all_data[date_column_name] - min_date).dt.days\n    all_data[date_column_name + '_is_weekend'] = (all_data[date_column_name].dt.weekday >= 5).astype(int).astype('category')\n    updated_categorical_columns = categorical_columns + [date_column_name + '_is_weekend']\n    all_data[date_column_name + '_weekday'] = all_data[date_column_name].dt.weekday.astype('category')\n    updated_categorical_columns = updated_categorical_columns + [date_column_name + '_weekday']\n    \n    all_data.drop(date_column_name, axis=1, inplace=True)  # удаление этого столбца, так как он больше не нужен\n\n    return all_data, del_dublicates(updated_categorical_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_LabelEncoder(all_data, categorical_columns):\n    \"\"\"\n    На вход надо передавать объединённые признаки в обучающих данных и признаки для сабмита.\n\n    Необходимые импорты:\n    from sklearn.preprocessing import LabelEncoder\n    \"\"\"\n\n    all_data = all_data.copy()\n\n    for cat_col in categorical_columns:\n        le = LabelEncoder()\n        all_data[cat_col] = le.fit_transform(all_data[cat_col])\n        all_data[cat_col] = all_data[cat_col].astype('category')\n\n    return all_data","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_OHE(all_data, categorical_columns):\n    \"\"\"\n    Применяет One-Hot Encoding к указанным категориальным колонкам. NaN значения тоже кодируются,\n    как отдельная категория.\n\n    На вход надо передавать объединённые признаки в обучающих данных и признаки для сабмита,\n    чтобы гарантировать одинаковый набор колонок после кодирования.\n\n    Необходимые импорты:\n    import pandas as pd\n    \"\"\"\n\n    all_data = all_data.copy()\n    all_data = pd.get_dummies(all_data, columns=categorical_columns, dummy_na=False)\n\n    return all_data","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_isna(data, categorical_columns=None):\n    \"\"\"\n    Добавляет в pd.DataFrame столбцы, которые говорят о наличии пропусков.\n\n    Необходимые импорты:\n    import pandas as pd\n    \"\"\"\n\n    if categorical_columns is None:\n        categorical_columns = []\n\n    data = data.copy()\n\n    nan_col_names = [col for col in data.columns if data[col].isna().any()]\n\n    if not nan_col_names:\n        return data, categorical_columns\n\n    isna_df = data[nan_col_names].isna().astype(int)\n\n    new_column_names_map = {col: f\"{col}_isna\" for col in nan_col_names}\n    isna_df = isna_df.rename(columns=new_column_names_map)\n\n    for col in isna_df.columns:\n        isna_df[col] = isna_df[col].astype('category')\n\n    data = pd.concat([data, isna_df], axis=1)\n\n    updated_categorical_columns = categorical_columns + list(isna_df.columns)\n\n    return data, del_dublicates(updated_categorical_columns)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def code_text_with_char_tfidf_svd(data, text_col_name, n_components=20, \n                                  ngram_range=(3, 5), tfidf_max_features=20000, \n                                  random_state=42):\n    \"\"\"\n    Кодирует многоязычные текстовые данные с помощью символьных n-грамм TF-IDF и SVD.\n    Этот метод языконезависим и хорошо подходит как быстрый baseline.\n\n    Надо вот так конкатенировать обучающие и тестовые данные (`ignore_index=True` !!!):\n    all_data = pd.concat([train_data, submission_data], axis=0, ignore_index=True)\n\n    Необходимые импорты:\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.decomposition import TruncatedSVD\n    import pandas as pd\n\n    :param data: pd.DataFrame с текстовыми столбцами.\n    :param text_col_name: Имя столбца (str) или список имен столбцов (list) с текстом.\n    :param n_components: Количество SVD компонент. Это итоговая размерность признаков.\n    :param ngram_range: Кортеж (min_n, max_n). Задает диапазон длин n-грамм символов.\n                        Например, (3, 5) будет использовать 3-, 4- и 5-граммы.\n    :param tfidf_max_features: Максимальное количество самых частых n-грамм, которые\n                               будут включены в словарь. Ограничивает размер матрицы.\n    :param random_state: Random state для SVD для воспроизводимости результатов.\n    :return: pd.DataFrame только с новыми сгенерированными признаками.\n    \"\"\"\n\n    data_copy = data.copy()\n    temp_col_name = None\n    \n    if isinstance(text_col_name, list):\n        output_prefix = '_'.join(text_col_name)\n        temp_col_name = f'__temp_combined_{output_prefix}'\n        data_copy[temp_col_name] = data_copy[text_col_name].fillna('').agg(' '.join, axis=1)\n        processing_col = temp_col_name\n    elif isinstance(text_col_name, str):\n        output_prefix = text_col_name\n        processing_col = text_col_name\n    else:\n        raise TypeError(\"Параметр 'text_col_name' должен быть строкой или списком строк.\")\n\n    unique_texts = data_copy[[processing_col]].drop_duplicates().dropna()\n    \n    if unique_texts.empty:\n        return pd.DataFrame(index=data.index)\n\n    tfidf_vectorizer = TfidfVectorizer(\n        analyzer='char_wb',\n        ngram_range=ngram_range,\n        max_features=tfidf_max_features\n    )\n    tfidf_matrix = tfidf_vectorizer.fit_transform(unique_texts[processing_col])\n\n    svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n    svd_features = svd.fit_transform(tfidf_matrix)\n\n    col_names = [f'{output_prefix}_char_tfidf_svd_{i}' for i in range(n_components)]\n    svd_df = pd.DataFrame(svd_features, index=unique_texts.index, columns=col_names)\n    \n    data_with_features = data_copy.merge(svd_df, left_index=True, right_index=True, how='left')\n\n    return data_with_features.drop(text_col_name, axis=1)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def code_text_with_transformers(data, text_col_name, no_components=None,\n                                model_name='paraphrase-multilingual-MiniLM-L12-v2', \n                                batch_size=32, show_progress_bar=True):\n    \"\"\"\n    Кодирует многоязычные текстовые данные в семантические эмбеддинги с помощью\n    предобученной трансформерной модели и опционально сжимает их.\n\n    Необходимые импорты:\n    from sentence_transformers import SentenceTransformer\n    import pandas as pd\n    from sklearn.decomposition import PCA\n\n    :param data: pd.DataFrame с текстовыми столбцами.\n    :param text_col_name: Имя столбца (str) или список имен столбцов (list) с текстом.\n    :param no_components: (Опционально) Целевое количество признаков. Если задано,\n                          эмбеддинги будут сжаты с помощью PCA до этого размера.\n    :param model_name: Название предобученной многоязычной модели из библиотеки \n                       sentence-transformers. \n                       'paraphrase-multilingual-MiniLM-L12-v2' - хороший баланс скорости и качества.\n                       'paraphrase-multilingual-mpnet-base-v2' - медленнее, но качество выше.\n    :param batch_size: Размер батча для кодирования (влияет на использование VRAM/RAM).\n    :param show_progress_bar: Показывать ли прогресс-бар при кодировании.\n    :return: pd.DataFrame со всеми исходными столбцами и новыми сгенерированными признаками.\n    \"\"\"\n    data_copy = data.copy()\n    temp_col_name = None\n    \n    # если на вход подан список колонок, объединяем их в одну временную\n    if isinstance(text_col_name, list):\n        output_prefix = '_'.join(text_col_name)\n        temp_col_name = f'__temp_combined_{output_prefix}'\n        data_copy[temp_col_name] = data_copy[text_col_name].fillna('').agg(' '.join, axis=1)\n        processing_col = temp_col_name\n    elif isinstance(text_col_name, str):\n        output_prefix = text_col_name\n        processing_col = text_col_name\n    else:\n        raise TypeError(\"параметр 'text_col_name' должен быть строкой или списком строк.\")\n\n    # работаем только с уникальными текстами для экономии ресурсов\n    unique_texts = data_copy[[processing_col]].drop_duplicates().dropna()\n    \n    if unique_texts.empty:\n        # если нет текстов для кодирования, добавляем пустые столбцы и возвращаем копию\n        final_dim = no_components if no_components is not None else 384 # 384 - размерность MiniLM\n        for i in range(final_dim):\n            data_copy[f'{output_prefix}_embed_{i}'] = None\n        return data_copy\n\n    model = SentenceTransformer(model_name)\n\n    # кодируем тексты в эмбеддинги\n    embeddings = model.encode(\n        unique_texts[processing_col].tolist(),\n        batch_size=batch_size, \n        show_progress_bar=show_progress_bar,\n        convert_to_numpy=True\n    )\n\n    # если указан no_components, сжимаем эмбеддинги с помощью pca\n    if no_components is not None and isinstance(no_components, int) and no_components > 0:\n        if no_components < embeddings.shape[1]:\n            print(f\"сжатие эмбеддингов с {embeddings.shape[1]} до {no_components} с помощью pca...\")\n            pca = PCA(n_components=no_components, random_state=42)\n            embeddings = pca.fit_transform(embeddings)\n        else:\n            print(f\"предупреждение: no_components ({no_components}) >= исходной размерности ({embeddings.shape[1]}). сжатие не выполняется.\")\n\n    # создаем датафрейм с эмбеддингами, готовый к объединению\n    col_names = [f'{output_prefix}_embed_{i}' for i in range(embeddings.shape[1])]\n    embedding_df = pd.DataFrame(embeddings, columns=col_names)\n    embedding_df[processing_col] = unique_texts[processing_col].values\n    \n    # присоединяем эмбеддинги к копии исходных данных по текстовому полю\n    data_with_features = data_copy.merge(embedding_df, on=processing_col, how='left')\n    \n    # удаляем временную колонку, если она создавалась\n    if temp_col_name:\n        data_with_features.drop(columns=[temp_col_name], inplace=True)\n    \n    # возвращаем всю таблицу с новыми признаками\n    return data_with_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:20:31.588383Z","iopub.execute_input":"2025-07-07T10:20:31.588738Z","iopub.status.idle":"2025-07-07T10:20:31.613227Z","shell.execute_reply.started":"2025-07-07T10:20:31.588714Z","shell.execute_reply":"2025-07-07T10:20:31.612327Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def preprocess_catnum_cols(all_data, cat_cols, num_cols, cat_num_cols):\n    \"\"\"\n    Обрабатывает DataFrame, кодируя категориальные признаки и создавая \n    числовые копии для указанных столбцов. cat_cols, num_cols и cat_num_cols\n    не должны пересекаться. Функцию следует использовать после заполнения NaN\n    значений.\n\n    Необходимые импорты:\n    from sklearn.preprocessing import LabelEncoder\n\n    :param all_data: pd.DataFrame, который является объединением обучающих данных и данных для сабмита.\n    :param cat_cols: список категориальных столбцов.\n    :param num_cols: список численных столбцов.\n    :param cat_num_cols: список столбцов, которые стоит добавить в данные как категориальные, так и численные.\n                         Изначально столбцы в cat_num_cols должны являться численными.\n\n    :return: обновлённый all_data и новые cat_cols и num_cols.\n    \"\"\"\n\n    # создание копий\n    data = all_data.copy()\n    final_cat_cols = cat_cols.copy()\n    final_num_cols = num_cols.copy()\n\n    le = LabelEncoder()  # создание кодировщника\n\n    # кодирование столбцов, которые должны быть и категориальными, и численными\n    for col in cat_num_cols:\n        data[col] = data[col].astype('float32')  # изменение типа данных на числовой, чтобы он точно не был категориальным\n        new_cat_col_name = f'{col}_cat'  # имя для кодированной копии столбца\n\n        # создание кодированной копии столбца\n        data[new_cat_col_name] = le.fit_transform(data[col].astype(str))\n        data[new_cat_col_name] = data[new_cat_col_name].astype('category')\n\n        final_num_cols.append(col)  # добавляем исходный столбец в список числовых столбцов\n        final_cat_cols.append(new_cat_col_name)  # добавляем новый столбец в список категориальных столбцов\n\n    # кодирование <<чисто>> категориальных столбцов\n    for col in cat_cols:\n        data[col] = le.fit_transform(data[col].astype(str))\n        data[col] = data[col].astype('category')\n\n    return data, final_cat_cols, final_num_cols","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\"\n    проходит по всем столбцам датафрейма и изменяет тип данных\n    числовых столбцов на минимально возможный для уменьшения использования памяти.\n    \n    :param df: pd.DataFrame для оптимизации.\n    :return: pd.DataFrame с оптимизированными типами данных.\n    \"\"\"\n\n    start_mem = df.memory_usage().sum() / 1024**2\n    print(f'Использование памяти до: {start_mem:.2f} mb')\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object and col_type.name != 'category' and 'datetime' not in str(col_type):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            # для категориальных можно дополнительно использовать df[col].astype('category')\n            pass\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f'Использование памяти после: {end_mem:.2f} mb')\n    print(f'Сжато на {100 * (start_mem - end_mem) / start_mem:.1f}%')\n    \n    return df","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}