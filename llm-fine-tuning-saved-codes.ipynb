{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ReRanker с catboostranker (лучше подбор весов работает, но можно попробовать это при больших данных)","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pickle\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom collections import Counter\nimport torch\nfrom rouge_score import rouge_scorer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\nfrom catboost import CatBoostRanker, Pool\nfrom scipy.spatial.distance import cosine\n\n\nclass FeaturesExtractor:\n    \"\"\"извлечение признаков для ранжирования\"\"\"\n    \n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.device = device\n        \n        # ленивая инициализация моделей\n        self._fluency_model = None\n        self._fluency_tokenizer = None\n        self._embedding_model = None\n        self._embedding_tokenizer = None\n        self._rouge_scorer = None\n        \n        # регистр признаков (для гибкости)\n        self.enabled_features = set()\n    \n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state['_fluency_model'] = None\n        state['_fluency_tokenizer'] = None\n        state['_embedding_model'] = None\n        state['_embedding_tokenizer'] = None\n        state['_rouge_scorer'] = None\n        return state\n    \n    def __setstate__(self, state):\n        self.__dict__.update(state)\n    \n    def enable_features(self, *feature_names):\n        \"\"\"включить признаки: extractor.enable_features('coverage', 'length', 'embeddings')\"\"\"\n        self.enabled_features.update(feature_names)\n    \n    def disable_features(self, *feature_names):\n        \"\"\"выключить признаки\"\"\"\n        self.enabled_features.difference_update(feature_names)\n    \n    # =============== ЗАГРУЗКА МОДЕЛЕЙ ===============\n    \n    def _load_rouge_scorer(self):\n        if self._rouge_scorer is None:\n            self._rouge_scorer = rouge_scorer.RougeScorer(['rouge2', 'rougeL'], use_stemmer=True)\n        return self._rouge_scorer\n    \n    def _load_fluency_model(self):\n        if self._fluency_model is None:\n            print(\"загрузка модели для fluency (gpt-2)...\")\n            self._fluency_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n            self._fluency_model = AutoModelForCausalLM.from_pretrained('gpt2').to(self.device)\n            self._fluency_model.eval()\n        return self._fluency_model, self._fluency_tokenizer\n    \n    def _load_embedding_model(self):\n        if self._embedding_model is None:\n            print(\"загрузка модели для embeddings (sentence-transformers)...\")\n            model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n            self._embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self._embedding_model = AutoModel.from_pretrained(model_name).to(self.device)\n            self._embedding_model.eval()\n        return self._embedding_model, self._embedding_tokenizer\n    \n    # =============== БАЗОВЫЕ МЕТРИКИ (из оригинального класса) ===============\n    \n    @staticmethod\n    def _get_ngrams(text, n):\n        words = text.lower().split()\n        return Counter([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])\n    \n    def compute_coverage(self, candidate, context):\n        \"\"\"покрытие слов из контекста\"\"\"\n        if context is None:\n            return 0.0\n        words = context.replace(',', '').lower().split()\n        if not words:\n            return 0.0\n        covered = sum(1 for word in words if word in candidate.lower())\n        return covered / len(words)\n    \n    def compute_fluency(self, candidate):\n        \"\"\"perplexity-based fluency\"\"\"\n        model, tokenizer = self._load_fluency_model()\n        inputs = tokenizer(candidate, return_tensors='pt', truncation=True, max_length=512)\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model(**inputs, labels=inputs['input_ids'])\n            loss = outputs.loss\n        \n        perplexity = torch.exp(loss).item()\n        return 1.0 / (1.0 + perplexity / 30.0)\n    \n    def compute_grammar(self, candidate):\n        \"\"\"эвристическая грамматика\"\"\"\n        score = 1.0\n        if candidate and not candidate[0].isupper():\n            score -= 0.2\n        if candidate and not candidate.rstrip()[-1] in '.!?':\n            score -= 0.2\n        if '  ' in candidate:\n            score -= 0.1\n        if candidate.count('\"') % 2 != 0:\n            score -= 0.1\n        if len(candidate.split()) < 3:\n            score -= 0.3\n        return max(0.0, score)\n    \n    def compute_length_simple(self, candidate, target_min=10, target_max=20):\n        \"\"\"соответствие целевой длине\"\"\"\n        words = len(candidate.split())\n        if target_min <= words <= target_max:\n            return 1.0\n        elif words < target_min:\n            return words / target_min\n        else:\n            return max(0, 1 - (words - target_max) / (target_max * 0.5))\n    \n    def compute_repetition_penalty(self, candidate):\n        \"\"\"штраф за повторы\"\"\"\n        words = candidate.lower().split()\n        if len(words) < 3:\n            return 1.0\n        \n        unique_words = len(set(words))\n        total_words = len(words)\n        unigram_diversity = unique_words / total_words if total_words > 0 else 0\n        \n        bigrams = self._get_ngrams(candidate, 2)\n        unique_bigrams = len(bigrams)\n        total_bigrams = sum(bigrams.values())\n        bigram_diversity = unique_bigrams / total_bigrams if total_bigrams > 0 else 0\n        \n        return 0.5 * unigram_diversity + 0.5 * bigram_diversity\n    \n    def compute_lexical_diversity(self, candidate):\n        \"\"\"лексическое разнообразие\"\"\"\n        words = candidate.lower().split()\n        if len(words) < 5:\n            return 0.5\n        \n        unique_words = len(set(words))\n        total_words = len(words)\n        ttr = unique_words / total_words\n        return min(1.0, ttr * (1 + np.log(total_words) / 5))\n    \n    def compute_extractive_coverage(self, candidate, context):\n        \"\"\"покрытие n-грамм\"\"\"\n        if context is None:\n            return 0.0\n        \n        source_ngrams = {n: self._get_ngrams(context, n) for n in [1, 2, 3, 4]}\n        \n        ngram_scores = []\n        for n in [1, 2, 3, 4]:\n            cand_ngrams = self._get_ngrams(candidate, n)\n            if not cand_ngrams:\n                continue\n            overlap = sum((cand_ngrams & source_ngrams[n]).values())\n            total = sum(cand_ngrams.values())\n            ngram_scores.append(overlap / total)\n        \n        return np.mean(ngram_scores) if ngram_scores else 0.0\n    \n    # =============== НОВЫЕ ПРИЗНАКИ ===============\n    \n    def compute_length_candidate(self, candidate):\n        \"\"\"длина кандидата (в словах)\"\"\"\n        return len(candidate.split())\n    \n    def compute_length_context(self, context):\n        \"\"\"длина контекста (в словах)\"\"\"\n        if context is None:\n            return 0\n        return len(context.split())\n    \n    def compute_length_ratio(self, candidate, context):\n        \"\"\"отношение длин candidate / context\"\"\"\n        if context is None or len(context.split()) == 0:\n            return 0.0\n        return len(candidate.split()) / len(context.split())\n    \n    def compute_char_length_candidate(self, candidate):\n        \"\"\"длина в символах\"\"\"\n        return len(candidate)\n    \n    def compute_embedding_similarity(self, candidate, context):\n        \"\"\"косинусная близость embeddings\"\"\"\n        if context is None:\n            return 0.0\n        \n        model, tokenizer = self._load_embedding_model()\n        \n        # mean pooling\n        def get_embedding(text):\n            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True)\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = model(**inputs)\n                # mean pooling\n                embeddings = outputs.last_hidden_state\n                attention_mask = inputs['attention_mask']\n                mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n                sum_embeddings = torch.sum(embeddings * mask_expanded, 1)\n                sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n                mean_pooled = sum_embeddings / sum_mask\n            \n            return mean_pooled.cpu().numpy()[0]\n        \n        emb_candidate = get_embedding(candidate)\n        emb_context = get_embedding(context)\n        \n        # косинусная близость (1 - distance)\n        return 1 - cosine(emb_candidate, emb_context)\n    \n    def compute_rouge_l(self, candidate, context):\n        \"\"\"rouge-l с контекстом\"\"\"\n        if context is None:\n            return 0.0\n        scorer = self._load_rouge_scorer()\n        result = scorer.score(context, candidate)\n        return result['rougeL'].fmeasure\n    \n    def compute_rouge_2(self, candidate, context):\n        \"\"\"rouge-2 с контекстом\"\"\"\n        if context is None:\n            return 0.0\n        scorer = self._load_rouge_scorer()\n        result = scorer.score(context, candidate)\n        return result['rouge2'].fmeasure\n    \n    def compute_word_overlap(self, candidate, context):\n        \"\"\"процент слов из candidate, которые есть в context\"\"\"\n        if context is None:\n            return 0.0\n        \n        candidate_words = set(candidate.lower().split())\n        context_words = set(context.lower().split())\n        \n        if not candidate_words:\n            return 0.0\n        \n        overlap = len(candidate_words & context_words)\n        return overlap / len(candidate_words)\n    \n    def compute_unique_words_ratio(self, candidate):\n        \"\"\"отношение уникальных слов к общему количеству\"\"\"\n        words = candidate.lower().split()\n        if not words:\n            return 0.0\n        return len(set(words)) / len(words)\n    \n    def compute_avg_word_length(self, candidate):\n        \"\"\"средняя длина слова\"\"\"\n        words = candidate.split()\n        if not words:\n            return 0.0\n        return np.mean([len(w) for w in words])\n    \n    def compute_sentence_count(self, candidate):\n        \"\"\"количество предложений\"\"\"\n        sentences = re.split(r'[.!?]+', candidate)\n        sentences = [s.strip() for s in sentences if s.strip()]\n        return len(sentences)\n    \n    # =============== ОСНОВНОЙ МЕТОД ИЗВЛЕЧЕНИЯ ===============\n    \n    def extract_features(self, candidate, context=None, feature_params=None):\n        \"\"\"\n        извлечь все включенные признаки\n        \n        args:\n            candidate: str - текст кандидата\n            context: str - контекст (опционально)\n            feature_params: dict - параметры для признаков\n        \n        returns:\n            dict: {feature_name: value}\n        \"\"\"\n        feature_params = feature_params or {}\n        features = {}\n        \n        # базовые метрики\n        if 'coverage' in self.enabled_features:\n            features['coverage'] = self.compute_coverage(candidate, context)\n        \n        if 'fluency' in self.enabled_features:\n            features['fluency'] = self.compute_fluency(candidate)\n        \n        if 'grammar' in self.enabled_features:\n            features['grammar'] = self.compute_grammar(candidate)\n        \n        if 'length_simple' in self.enabled_features:\n            params = feature_params.get('length_simple', {'target_min': 10, 'target_max': 20})\n            features['length_simple'] = self.compute_length_simple(\n                candidate, params['target_min'], params['target_max']\n            )\n        \n        if 'repetition_penalty' in self.enabled_features:\n            features['repetition_penalty'] = self.compute_repetition_penalty(candidate)\n        \n        if 'lexical_diversity' in self.enabled_features:\n            features['lexical_diversity'] = self.compute_lexical_diversity(candidate)\n        \n        if 'extractive_coverage' in self.enabled_features:\n            features['extractive_coverage'] = self.compute_extractive_coverage(candidate, context)\n        \n        # новые признаки\n        if 'length_candidate' in self.enabled_features:\n            features['length_candidate'] = self.compute_length_candidate(candidate)\n        \n        if 'length_context' in self.enabled_features:\n            features['length_context'] = self.compute_length_context(context)\n        \n        if 'length_ratio' in self.enabled_features:\n            features['length_ratio'] = self.compute_length_ratio(candidate, context)\n        \n        if 'char_length' in self.enabled_features:\n            features['char_length'] = self.compute_char_length_candidate(candidate)\n        \n        if 'embedding_similarity' in self.enabled_features:\n            features['embedding_similarity'] = self.compute_embedding_similarity(candidate, context)\n        \n        if 'rouge_l' in self.enabled_features:\n            features['rouge_l'] = self.compute_rouge_l(candidate, context)\n        \n        if 'rouge_2' in self.enabled_features:\n            features['rouge_2'] = self.compute_rouge_2(candidate, context)\n        \n        if 'word_overlap' in self.enabled_features:\n            features['word_overlap'] = self.compute_word_overlap(candidate, context)\n        \n        if 'unique_words_ratio' in self.enabled_features:\n            features['unique_words_ratio'] = self.compute_unique_words_ratio(candidate)\n        \n        if 'avg_word_length' in self.enabled_features:\n            features['avg_word_length'] = self.compute_avg_word_length(candidate)\n        \n        if 'sentence_count' in self.enabled_features:\n            features['sentence_count'] = self.compute_sentence_count(candidate)\n        \n        return features\n    \n    def extract_features_batch(self, candidates, context=None, feature_params=None):\n        \"\"\"извлечь признаки для батча кандидатов\"\"\"\n        return [\n            self.extract_features(candidate, context, feature_params)\n            for candidate in candidates\n        ]\n\n\nclass CatBoostReRanker:\n    \"\"\"re-ranker на основе CatBoost\"\"\"\n    \n    # предустановленные наборы признаков\n    FEATURE_SETS = {\n        'minimal': ['length_simple', 'repetition_penalty'],\n        \n        'basic': [\n            'coverage', 'length_simple', 'repetition_penalty', \n            'grammar', 'lexical_diversity'\n        ],\n        \n        'extended': [\n            'coverage', 'fluency', 'grammar', 'length_simple',\n            'repetition_penalty', 'lexical_diversity',\n            'length_candidate', 'length_ratio', 'word_overlap',\n            'unique_words_ratio'\n        ],\n        \n        'full': [\n            'coverage', 'fluency', 'grammar', 'length_simple',\n            'repetition_penalty', 'lexical_diversity', 'extractive_coverage',\n            'length_candidate', 'length_context', 'length_ratio', \n            'char_length', 'embedding_similarity', 'rouge_l', 'rouge_2',\n            'word_overlap', 'unique_words_ratio', 'avg_word_length',\n            'sentence_count'\n        ]\n    }\n    \n    def __init__(\n        self,\n        features='basic',  # 'minimal', 'basic', 'extended', 'full' или список признаков\n        feature_params=None,\n        catboost_params=None,\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        cache_dir='./reranker_cache'\n    ):\n        \"\"\"\n        args:\n            features: str или list\n                - str: название preset'а ('minimal', 'basic', 'extended', 'full')\n                - list: список названий признаков\n            feature_params: dict - параметры для признаков\n            catboost_params: dict - параметры для CatBoost\n            device: str - устройство для вычислений\n            cache_dir: str - директория для кэша\n        \"\"\"\n        self.device = device\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n        \n        # инициализация extractor\n        self.extractor = FeaturesExtractor(device=device)\n        \n        # определяем признаки\n        if isinstance(features, str):\n            if features not in self.FEATURE_SETS:\n                raise ValueError(f\"unknown feature set: {features}. available: {list(self.FEATURE_SETS.keys())}\")\n            self.feature_names = self.FEATURE_SETS[features]\n        elif isinstance(features, list):\n            self.feature_names = features\n        else:\n            raise ValueError(\"features must be str or list\")\n        \n        # включаем признаки в extractor\n        self.extractor.enable_features(*self.feature_names)\n        \n        self.feature_params = feature_params or {}\n        \n        # параметры CatBoost по умолчанию\n        default_catboost_params = {\n            'iterations': 500,\n            'depth': 6,\n            'learning_rate': 0.03,\n            'loss_function': 'YetiRank',\n            'verbose': False,\n            'random_seed': 42,\n            'task_type': 'GPU' if device == 'cuda' else 'CPU',\n        }\n        \n        if catboost_params:\n            default_catboost_params.update(catboost_params)\n        \n        self.catboost_params = default_catboost_params\n        self.model = None\n    \n    def _prepare_data(self, candidates_list, contexts, y_texts, target_metric='rouge2'):\n        \"\"\"подготовить данные для CatBoost\"\"\"\n        print(f\"извлечение признаков для {len(candidates_list)} примеров...\")\n        \n        all_features = []\n        all_labels = []\n        all_groups = []\n        \n        # для вычисления целевой метрики\n        scorer = rouge_scorer.RougeScorer([target_metric], use_stemmer=True)\n        \n        for group_id, (candidates, context, y_text) in enumerate(tqdm(\n            zip(candidates_list, contexts, y_texts),\n            total=len(candidates_list),\n            desc=\"preparing data\"\n        )):\n            for candidate in candidates:\n                # извлекаем признаки\n                features = self.extractor.extract_features(candidate, context, self.feature_params)\n                \n                # преобразуем в список (сохраняем порядок)\n                feature_vector = [features.get(name, 0.0) for name in self.feature_names]\n                \n                # целевая метрика\n                target_score = scorer.score(y_text, candidate)[target_metric].fmeasure\n                \n                all_features.append(feature_vector)\n                all_labels.append(target_score)\n                all_groups.append(group_id)\n        \n        return np.array(all_features), np.array(all_labels), np.array(all_groups)\n    \n    def fit(\n        self,\n        candidates_list,\n        contexts,\n        y_texts,\n        val_candidates_list=None,\n        val_contexts=None,\n        val_y_texts=None,\n        target_metric='rouge2',\n        cache_name=None,\n        use_cache=True,\n        early_stopping_rounds=50\n    ):\n        \"\"\"\n        обучить модель\n        \n        args:\n            candidates_list: list of list of str - train кандидаты\n            contexts: list of str - train контексты\n            y_texts: list of str - train референсы\n            val_candidates_list: list of list of str - val кандидаты (опционально)\n            val_contexts: list of str - val контексты (опционально)\n            val_y_texts: list of str - val референсы (опционально)\n            target_metric: str - целевая метрика ('rouge2', 'rougeL')\n            cache_name: str - имя кэша\n            use_cache: bool - использовать кэш\n            early_stopping_rounds: int - ранняя остановка\n        \n        returns:\n            dict: метрики обучения\n        \"\"\"\n        # кэширование\n        cache_path = None\n        cache_loaded = False\n        \n        if cache_name and use_cache:\n            feature_hash = hash(tuple(sorted(self.feature_names)))\n            cache_path = self.cache_dir / f\"{cache_name}_catboost_{feature_hash}.pkl\"\n            \n            if cache_path.exists():\n                print(f\"загрузка данных из кэша: {cache_path}\")\n                with open(cache_path, 'rb') as f:\n                    cached_data = pickle.load(f)\n                X_train, y_train, groups_train = cached_data['train']\n                if 'val' in cached_data:\n                    X_val, y_val, groups_val = cached_data['val']\n                else:\n                    X_val = y_val = groups_val = None\n                cache_loaded = True\n        \n        if not cache_loaded:\n            print(\"\\nподготовка train данных...\")\n            X_train, y_train, groups_train = self._prepare_data(\n                candidates_list, contexts, y_texts, target_metric\n            )\n            \n            if val_candidates_list is not None:\n                print(\"подготовка val данных...\")\n                X_val, y_val, groups_val = self._prepare_data(\n                    val_candidates_list, val_contexts, val_y_texts, target_metric\n                )\n            else:\n                X_val = y_val = groups_val = None\n            \n            if cache_path:\n                cache_data = {'train': (X_train, y_train, groups_train)}\n                if X_val is not None:\n                    cache_data['val'] = (X_val, y_val, groups_val)\n                \n                with open(cache_path, 'wb') as f:\n                    pickle.dump(cache_data, f)\n                print(f\"данные сохранены в кэш: {cache_path}\")\n        \n        # Pool\n        train_pool = Pool(\n            data=X_train,\n            label=y_train,\n            group_id=groups_train,\n            feature_names=self.feature_names\n        )\n        \n        eval_pool = None\n        if X_val is not None:\n            eval_pool = Pool(\n                data=X_val,\n                label=y_val,\n                group_id=groups_val,\n                feature_names=self.feature_names\n            )\n        \n        # параметры для fit\n        fit_params = {}\n        if eval_pool is not None:\n            fit_params['verbose_eval'] = 50\n            if early_stopping_rounds:\n                fit_params['early_stopping_rounds'] = early_stopping_rounds\n                print(f\"\\nранняя остановка: {early_stopping_rounds} итераций без улучшения\")\n        \n        # обучение\n        print(f\"\\nобучение CatBoost с {len(self.feature_names)} признаками...\")\n        print(f\"признаки: {self.feature_names}\")\n        print(f\"train: {len(set(groups_train))} примеров\")\n        if eval_pool:\n            print(f\"val:   {len(set(groups_val))} примеров\")\n        \n        self.model = CatBoostRanker(**self.catboost_params)\n        \n        self.model.fit(\n            train_pool,\n            eval_set=eval_pool,\n            **fit_params\n        )\n        \n        # feature importance\n        try:\n            feature_importance = self.model.get_feature_importance(type='PredictionValuesChange')\n        except:\n            feature_importance = self.model.get_feature_importance(\n                data=train_pool,\n                type='LossFunctionChange'\n            )\n        \n        sorted_importance = sorted(\n            zip(self.feature_names, feature_importance),\n            key=lambda x: x[1],\n            reverse=True\n        )\n        \n        # НОВОЕ: валидация на val по целевой метрике (rouge2)\n        val_rouge_score = None\n        if val_candidates_list is not None:\n            print(f\"\\nвалидация на val set по {target_metric}...\")\n            \n            from rouge_score import rouge_scorer\n            scorer = rouge_scorer.RougeScorer([target_metric], use_stemmer=True)\n            \n            val_predictions = []\n            for candidates, context, y_text in tqdm(\n                zip(val_candidates_list, val_contexts, val_y_texts),\n                total=len(val_candidates_list),\n                desc=\"validation\"\n            ):\n                best = self.get_best_candidate(candidates, context)\n                val_predictions.append(best)\n            \n            # вычисляем rouge\n            rouge_scores = []\n            for pred, ref in zip(val_predictions, val_y_texts):\n                score = scorer.score(ref, pred)[target_metric].fmeasure\n                rouge_scores.append(score)\n            \n            val_rouge_score = np.mean(rouge_scores)\n            print(f\"val {target_metric}: {val_rouge_score:.4f}\")\n        \n        # итоги\n        print(\"\\n\" + \"=\"*60)\n        print(\"РЕЗУЛЬТАТЫ ОБУЧЕНИЯ\")\n        print(\"=\"*60)\n        print(f\"количество признаков: {len(self.feature_names)}\")\n        print(f\"количество итераций: {self.model.tree_count_}\")\n        if val_rouge_score is not None:\n            print(f\"val {target_metric}: {val_rouge_score:.4f}\")\n        print(f\"\\nтоп-10 важных признаков:\")\n        for name, importance in sorted_importance[:10]:\n            print(f\"  {name:25s}: {importance:.4f}\")\n        print(\"=\"*60)\n        \n        return {\n            'feature_importance': dict(sorted_importance),\n            'train_size': len(set(groups_train)),\n            'val_size': len(set(groups_val)) if groups_val is not None else 0,\n            'iterations': self.model.tree_count_,\n            f'val_{target_metric}': val_rouge_score\n        }\n\n    def rank_candidates(self, candidates, context=None):\n        \"\"\"\n        ранжировать кандидатов\n        \n        returns:\n            list of tuple: [(idx, score, features), ...]\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"model not trained. call fit() first\")\n        \n        if not candidates:\n            return []\n        \n        # извлечение признаков\n        features_list = self.extractor.extract_features_batch(\n            candidates, context, self.feature_params\n        )\n        \n        # преобразование в матрицу\n        X = np.array([\n            [f.get(name, 0.0) for name in self.feature_names]\n            for f in features_list\n        ])\n        \n        # предсказание\n        scores = self.model.predict(X)\n        \n        # сортировка\n        results = []\n        for idx, (score, features) in enumerate(zip(scores, features_list)):\n            results.append((idx, score, features))\n        \n        results.sort(key=lambda x: x[1], reverse=True)\n        return results\n    \n    def get_best_candidate(self, candidates, context=None):\n        \"\"\"получить лучшего кандидата\"\"\"\n        results = self.rank_candidates(candidates, context)\n        if not results:\n            return None\n        best_idx = results[0][0]\n        return candidates[best_idx]\n    \n    def save(self, path):\n        \"\"\"сохранить ranker\"\"\"\n        save_data = {\n            'feature_names': self.feature_names,\n            'feature_params': self.feature_params,\n            'catboost_params': self.catboost_params,\n            'device': self.device,\n        }\n        \n        # сохраняем модель отдельно\n        model_path = Path(path).with_suffix('.cbm')\n        if self.model is not None:\n            self.model.save_model(str(model_path))\n            save_data['model_path'] = str(model_path)\n        \n        with open(path, 'wb') as f:\n            pickle.dump(save_data, f)\n        \n        print(f\"ranker сохранен: {path}\")\n        if self.model is not None:\n            print(f\"модель сохранена: {model_path}\")\n    \n    @staticmethod\n    def load(path):\n        \"\"\"загрузить ranker\"\"\"\n        with open(path, 'rb') as f:\n            save_data = pickle.load(f)\n        \n        ranker = CatBoostReRanker(\n            features=save_data['feature_names'],\n            feature_params=save_data['feature_params'],\n            catboost_params=save_data['catboost_params'],\n            device=save_data['device']\n        )\n        \n        if 'model_path' in save_data:\n            ranker.model = CatBoostRanker()\n            ranker.model.load_model(save_data['model_path'])\n        \n        print(f\"ranker загружен: {path}\")\n        return ranker","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# инициализация для QA\nranker_qa = CatBoostReRanker(\n    features=[\n        # кастомный набор признаков для QA\n        'length_simple',          # ответ должен быть коротким\n        'embedding_similarity',   # ответ должен быть семантически близок к вопросу\n        'extractive_coverage',    # хороший ответ часто является цитатой из контекста\n        'repetition_penalty',     # избегаем повторов\n        'unique_words_ratio',     # поощряем лаконичные ответы\n        'grammar'                 # базовый контроль грамматики\n    ],\n    feature_params={\n        # устанавливаем целевую длину для коротких ответов\n        'length_simple': {'target_min': 1, 'target_max': 15}\n    },\n    catboost_params={\n        'iterations': 1000,\n        'depth': 6,\n        'learning_rate': 0.03,\n        'loss_function': 'YetiRank',\n        'eval_metric': 'NDCG:top=5',  # понятная метрика для логов\n        'verbose': 100,\n        'random_seed': 42\n    },\n    device='cuda'\n)\n\n# обучение (dev_contexts - это исходные параграфы из SQuAD)\nresults_qa = ranker_qa.fit(\n    candidates_list=dev_candidates,\n    contexts=dev_contexts,\n    y_texts=dev_y_texts,\n    val_candidates_list=val_candidates,  # валидационный сет\n    val_contexts=val_contexts,\n    val_y_texts=val_y_texts,\n    target_metric='rougeL',  # rougeL лучше для коротких ответов\n    cache_name='squad_custom',\n    early_stopping_rounds=50\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# инициализация для CommonGen\nranker_commongen = CatBoostReRanker(\n    features='extended',  # preset 'extended' хорошо подходит\n    feature_params={\n        # устанавливаем целевую длину для предложений\n        'length_simple': {'target_min': 8, 'target_max': 25}\n    },\n    catboost_params={\n        'iterations': 800,\n        'depth': 8,  # можно глубже, т.к. больше признаков\n        'learning_rate': 0.02,\n        'loss_function': 'YetiRank',\n        'eval_metric': 'NDCG:top=10',\n        'verbose': 100,\n        'random_seed': 42\n    },\n    device='cuda'\n)\n\n# обучение (dev_contexts - это строки со словами через запятую)\nresults_commongen = ranker_commongen.fit(\n    candidates_list=dev_candidates,\n    contexts=dev_contexts,\n    y_texts=dev_y_texts,\n    val_candidates_list=val_candidates,\n    val_contexts=val_contexts,\n    val_y_texts=val_y_texts,\n    target_metric='rouge2',  # rouge2 хорошо ловит n-граммы\n    cache_name='commongen_extended',\n    early_stopping_rounds=50\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# инициализация для CommonGen\nranker_commongen = CatBoostReRanker(\n    features='extended',  # preset 'extended' хорошо подходит\n    feature_params={\n        # устанавливаем целевую длину для предложений\n        'length_simple': {'target_min': 8, 'target_max': 25}\n    },\n    catboost_params={\n        'iterations': 800,\n        'depth': 8,  # можно глубже, т.к. больше признаков\n        'learning_rate': 0.02,\n        'loss_function': 'YetiRank',\n        'eval_metric': 'NDCG:top=10',\n        'verbose': 100,\n        'random_seed': 42\n    },\n    device='cuda'\n)\n\n# обучение (dev_contexts - это строки со словами через запятую)\nresults_commongen = ranker_commongen.fit(\n    candidates_list=dev_candidates,\n    contexts=dev_contexts,\n    y_texts=dev_y_texts,\n    val_candidates_list=val_candidates,\n    val_contexts=val_contexts,\n    val_y_texts=val_y_texts,\n    target_metric='rouge2',  # rouge2 хорошо ловит n-граммы\n    cache_name='commongen_extended',\n    early_stopping_rounds=50\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# инициализация для Creative Writing\nranker_creative = CatBoostReRanker(\n    features=[\n        # кастомный набор для стиля\n        'lexical_diversity',      # богатство словаря\n        'fluency',                # естественность языка\n        'repetition_penalty',     # избегаем банальностей\n        'grammar',                # базовый стиль\n        'length_simple',          # контроль длины (например, для твита)\n        'avg_word_length',        # более длинные слова могут указывать на более сложный стиль\n        'semantic_coherence'      # логичность (для длинных ответов)\n    ],\n    feature_params={\n        'length_simple': {'target_min': 15, 'target_max': 40}  # например, для поста\n    },\n    catboost_params={\n        'iterations': 500,\n        'depth': 4, # неглубокие деревья, т.к. сигналы могут быть слабыми\n        'learning_rate': 0.05,\n        'loss_function': 'YetiRank',\n        'verbose': 100,\n        'random_seed': 42\n    },\n    device='cuda'\n)\n\n# обучение (dev_contexts - это промпты, например, \"Напиши твит о...\")\nresults_creative = ranker_creative.fit(\n    candidates_list=dev_candidates,\n    contexts=dev_contexts,\n    y_texts=dev_y_texts,\n    val_candidates_list=val_candidates,\n    val_contexts=val_contexts,\n    val_y_texts=val_y_texts,\n    target_metric='rougeL', # здесь rouge - это прокси, лучше использовать human evaluation\n    cache_name='creative_style',\n    early_stopping_rounds=30\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ReRanker с подбором весов","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nfrom scipy.optimize import differential_evolution\nfrom scipy.stats import spearmanr\nimport pickle\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom collections import Counter\nimport torch\nfrom rouge_score import rouge_scorer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nclass MetricsComputer:\n    \"\"\"класс для вычисления метрик качества текста\"\"\"\n    \n    ALL_METRICS = [\n        'coverage',\n        'compression_ratio',\n        'extractive_coverage',\n        'fluency',\n        'grammar',\n        'length_simple',\n        'lexical_diversity',\n        'repetition_penalty',\n        'rouge_with_source',\n        'semantic_coherence'\n    ]\n    \n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.device = device\n        self._fluency_model = None\n        self._fluency_tokenizer = None\n        self._rouge_scorer = None\n    \n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state['_fluency_model'] = None\n        state['_fluency_tokenizer'] = None\n        state['_rouge_scorer'] = None\n        return state\n    \n    def __setstate__(self, state):\n        self.__dict__.update(state)\n    \n    def _load_rouge_scorer(self):\n        \"\"\"загрузка rouge scorer\"\"\"\n        if self._rouge_scorer is None:\n            self._rouge_scorer = rouge_scorer.RougeScorer(\n                ['rouge2', 'rougeL'], \n                use_stemmer=True\n            )\n        return self._rouge_scorer\n    \n    def _load_fluency_model(self):\n        \"\"\"загрузка модели для fluency\"\"\"\n        if self._fluency_model is None:\n            print(\"загрузка модели для fluency (gpt-2)...\")\n            self._fluency_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n            self._fluency_model = AutoModelForCausalLM.from_pretrained('gpt2').to(self.device)\n            self._fluency_model.eval()\n        return self._fluency_model, self._fluency_tokenizer\n    \n    @staticmethod\n    def _get_ngrams(text, n):\n        \"\"\"извлечение n-грамм\"\"\"\n        words = text.lower().split()\n        return Counter([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])\n    \n    def compute_coverage(self, candidates, context, params=None):\n        \"\"\"\n        покрытие обязательных элементов\n        \n        применение:\n        - commongen: все ли слова использованы\n        - keyword-to-text: присутствуют ли ключевые слова\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        if context is None:\n            return [0.0] * len(candidates)\n        \n        words = context.replace(',', '').lower().split()\n        if not words:\n            return [0.0] * len(candidates)\n        \n        scores = []\n        for candidate in candidates:\n            candidate_lower = candidate.lower()\n            covered = sum(1 for word in words if word in candidate_lower)\n            scores.append(covered / len(words))\n        \n        return scores\n    \n    def compute_compression_ratio(self, candidates, source_text, params=None):\n        \"\"\"\n        соответствие целевому коэффициенту сжатия\n        \n        применение:\n        - суммаризация: контроль длины\n        - compression tasks\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        if source_text is None:\n            return [0.0] * len(candidates)\n        \n        params = params or {'optimal_ratio': 0.15, 'sigma': 0.05}\n        source_len = len(source_text.split())\n        optimal = params['optimal_ratio']\n        sigma = params['sigma']\n        \n        scores = []\n        for candidate in candidates:\n            cand_len = len(candidate.split())\n            ratio = cand_len / source_len if source_len > 0 else 0\n            score = np.exp(-((ratio - optimal) ** 2) / (2 * sigma ** 2))\n            scores.append(score)\n        \n        return scores\n    \n    def compute_extractive_coverage(self, candidates, source_text, params=None):\n        \"\"\"\n        покрытие n-грамм из исходного текста\n        \n        применение:\n        - extractive summarization\n        - faithful generation\n        - qa: ответ из контекста\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        if source_text is None:\n            return [0.0] * len(candidates)\n        \n        source_ngrams = {\n            n: self._get_ngrams(source_text, n)\n            for n in [1, 2, 3, 4]\n        }\n        \n        scores = []\n        for candidate in candidates:\n            ngram_scores = []\n            for n in [1, 2, 3, 4]:\n                cand_ngrams = self._get_ngrams(candidate, n)\n                if not cand_ngrams:\n                    continue\n                overlap = sum((cand_ngrams & source_ngrams[n]).values())\n                total = sum(cand_ngrams.values())\n                ngram_scores.append(overlap / total)\n            \n            scores.append(np.mean(ngram_scores) if ngram_scores else 0.0)\n        \n        return scores\n    \n    def compute_fluency(self, candidates, params=None):\n        \"\"\"\n        беглость текста на основе perplexity\n        \n        применение:\n        - все задачи генерации\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        model, tokenizer = self._load_fluency_model()\n        \n        scores = []\n        for candidate in candidates:\n            inputs = tokenizer(candidate, return_tensors='pt', truncation=True, max_length=512)\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = model(**inputs, labels=inputs['input_ids'])\n                loss = outputs.loss\n            \n            perplexity = torch.exp(loss).item()\n            fluency_score = 1.0 / (1.0 + perplexity / 30.0)\n            scores.append(fluency_score)\n        \n        return scores\n    \n    def compute_grammar(self, candidates, params=None):\n        \"\"\"\n        грамматическая корректность (эвристика)\n        \n        применение:\n        - все задачи генерации\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        scores = []\n        for candidate in candidates:\n            score = 1.0\n            \n            if candidate and not candidate[0].isupper():\n                score -= 0.2\n            \n            if candidate and not candidate.rstrip()[-1] in '.!?':\n                score -= 0.2\n            \n            if '  ' in candidate:\n                score -= 0.1\n            \n            if candidate.count('\"') % 2 != 0:\n                score -= 0.1\n            \n            if len(candidate.split()) < 3:\n                score -= 0.3\n            \n            scores.append(max(0.0, score))\n        \n        return scores\n    \n    def compute_length_simple(self, candidates, params=None):\n        \"\"\"\n        соответствие целевой длине\n        \n        применение:\n        - commongen: 10-20 слов\n        - заголовки: 5-10 слов\n        - qa: 1-10 слов\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        params = params or {'target_min': 10, 'target_max': 20}\n        target_min = params['target_min']\n        target_max = params['target_max']\n        \n        scores = []\n        for candidate in candidates:\n            words = len(candidate.split())\n            \n            if target_min <= words <= target_max:\n                score = 1.0\n            elif words < target_min:\n                score = words / target_min\n            else:\n                score = max(0, 1 - (words - target_max) / (target_max * 0.5))\n            \n            scores.append(score)\n        \n        return scores\n    \n    def compute_lexical_diversity(self, candidates, params=None):\n        \"\"\"\n        лексическое разнообразие\n        \n        применение:\n        - creative writing\n        - диалоги\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        scores = []\n        for candidate in candidates:\n            words = candidate.lower().split()\n            \n            if len(words) < 5:\n                scores.append(0.5)\n                continue\n            \n            unique_words = len(set(words))\n            total_words = len(words)\n            ttr = unique_words / total_words\n            \n            normalized_diversity = min(1.0, ttr * (1 + np.log(total_words) / 5))\n            scores.append(normalized_diversity)\n        \n        return scores\n    \n    def compute_repetition_penalty(self, candidates, params=None):\n        \"\"\"\n        штраф за повторы\n        \n        применение:\n        - все задачи генерации\n        - борьба с вырожденными генерациями\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        scores = []\n        for candidate in candidates:\n            words = candidate.lower().split()\n            \n            if len(words) < 3:\n                scores.append(1.0)\n                continue\n            \n            unique_words = len(set(words))\n            total_words = len(words)\n            unigram_diversity = unique_words / total_words if total_words > 0 else 0\n            \n            bigrams = self._get_ngrams(candidate, 2)\n            unique_bigrams = len(bigrams)\n            total_bigrams = sum(bigrams.values())\n            bigram_diversity = unique_bigrams / total_bigrams if total_bigrams > 0 else 0\n            \n            score = 0.5 * unigram_diversity + 0.5 * bigram_diversity\n            scores.append(score)\n        \n        return scores\n    \n    def compute_rouge_with_source(self, candidates, source_text, params=None):\n        \"\"\"\n        rouge overlap с исходным текстом\n        \n        применение:\n        - extractive summarization\n        - перефразирование\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        if source_text is None:\n            return [0.0] * len(candidates)\n        \n        scorer = self._load_rouge_scorer()\n        scores = []\n        \n        for candidate in candidates:\n            result = scorer.score(source_text, candidate)\n            scores.append(result['rougeL'].fmeasure)\n        \n        return scores\n    \n    def compute_semantic_coherence(self, candidates, params=None):\n        \"\"\"\n        семантическая связность (эвристика)\n        \n        применение:\n        - длинные генерации\n        - диалоги\n        \n        значения: 0.0 - 1.0\n        \"\"\"\n        scores = []\n        for candidate in candidates:\n            sentences = re.split(r'[.!?]+', candidate)\n            sentences = [s.strip() for s in sentences if s.strip()]\n            \n            if len(sentences) <= 1:\n                scores.append(1.0)\n                continue\n            \n            score = 1.0\n            \n            connectives = [\n                'however', 'therefore', 'moreover', 'furthermore',\n                'additionally', 'consequently', 'thus', 'hence',\n                'because', 'since', 'although', 'while', 'but', 'and'\n            ]\n            \n            candidate_lower = candidate.lower()\n            has_connectives = any(conn in candidate_lower for conn in connectives)\n            \n            if len(sentences) > 2 and not has_connectives:\n                score -= 0.2\n            \n            lengths = [len(s.split()) for s in sentences]\n            if len(lengths) > 1:\n                length_variance = np.std(lengths) / (np.mean(lengths) + 1e-6)\n                if length_variance > 1.5:\n                    score -= 0.2\n            \n            scores.append(max(0.0, score))\n        \n        return scores\n    \n    def compute_metric(self, metric_name, candidates, context, params=None):\n        \"\"\"вычислить одну метрику по имени\"\"\"\n        method_name = f'compute_{metric_name}'\n        if not hasattr(self, method_name):\n            raise ValueError(f\"unknown metric: {metric_name}\")\n        \n        method = getattr(self, method_name)\n        \n        # метрики требующие context\n        if metric_name in ['coverage', 'compression_ratio', 'extractive_coverage', 'rouge_with_source']:\n            return method(candidates, context, params)\n        else:\n            return method(candidates, params)\n\n\nclass ReRankingModel:\n    \"\"\"линейная модель для ранжирования\"\"\"\n    \n    def __init__(self, metric_names, weights=None):\n        self.metric_names = metric_names\n        self.weights = weights or self._init_uniform_weights()\n    \n    def _init_uniform_weights(self):\n        \"\"\"равномерные веса\"\"\"\n        n = len(self.metric_names)\n        return {name: 1.0 / n for name in self.metric_names}\n    \n    def predict(self, metrics_dict):\n        \"\"\"вычислить взвешенный скор\"\"\"\n        return sum(metrics_dict.get(m, 0) * self.weights.get(m, 0) for m in self.metric_names)\n    \n    def predict_batch(self, metrics_list):\n        \"\"\"вычислить скоры для батча\"\"\"\n        return [self.predict(m) for m in metrics_list]\n    \n    def set_weights(self, weights):\n        \"\"\"установить веса\"\"\"\n        self.weights = weights\n\n\nclass UniversalTextReRanker:\n    \"\"\"универсальный re-ranker для генераций текста\"\"\"\n    \n    def __init__(\n        self,\n        use_coverage=False,\n        use_compression_ratio=False,\n        use_extractive_coverage=False,\n        use_fluency=False,\n        use_grammar=False,\n        use_length_simple=False,\n        use_lexical_diversity=False,\n        use_repetition_penalty=False,\n        use_rouge_with_source=False,\n        use_semantic_coherence=False,\n        coverage_params=None,\n        compression_params=None,\n        length_params=None,\n        weights=None,\n        metric_selection='manual',\n        min_correlation=0.15,\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        cache_dir='./reranker_cache'\n    ):\n        \"\"\"\n        args:\n            use_*: флаги для включения метрик\n            *_params: параметры для метрик\n            weights: предустановленные веса\n            metric_selection: 'manual', 'auto' или 'auto_all'\n                - 'manual': используем только метрики с use_*=True\n                - 'auto': из метрик с use_*=True отбираем по корреляции\n                - 'auto_all': тестируем все метрики, отбираем по корреляции\n            min_correlation: порог корреляции для auto режимов\n            device: устройство для вычислений\n            cache_dir: директория для кэша\n        \"\"\"\n        self.use_coverage = use_coverage\n        self.use_compression_ratio = use_compression_ratio\n        self.use_extractive_coverage = use_extractive_coverage\n        self.use_fluency = use_fluency\n        self.use_grammar = use_grammar\n        self.use_length_simple = use_length_simple\n        self.use_lexical_diversity = use_lexical_diversity\n        self.use_repetition_penalty = use_repetition_penalty\n        self.use_rouge_with_source = use_rouge_with_source\n        self.use_semantic_coherence = use_semantic_coherence\n        \n        self.coverage_params = coverage_params or {}\n        self.compression_params = compression_params or {'optimal_ratio': 0.15, 'sigma': 0.05}\n        self.length_params = length_params or {'target_min': 10, 'target_max': 20}\n        \n        if metric_selection not in ['manual', 'auto', 'auto_all']:\n            raise ValueError(f\"metric_selection must be 'manual', 'auto' or 'auto_all', got {metric_selection}\")\n        \n        self.metric_selection = metric_selection\n        self.min_correlation = min_correlation\n        self.device = device\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n        \n        self.metrics_computer = MetricsComputer(device=device)\n        \n        active_metrics = self._get_active_metrics()\n        self.model = ReRankingModel(active_metrics, weights)\n    \n    def __getstate__(self):\n        state = self.__dict__.copy()\n        return state\n    \n    def __setstate__(self, state):\n        self.__dict__.update(state)\n    \n    def _get_active_metrics(self):\n        \"\"\"получить список активных метрик\"\"\"\n        metrics = []\n        if self.use_coverage:\n            metrics.append('coverage')\n        if self.use_compression_ratio:\n            metrics.append('compression_ratio')\n        if self.use_extractive_coverage:\n            metrics.append('extractive_coverage')\n        if self.use_fluency:\n            metrics.append('fluency')\n        if self.use_grammar:\n            metrics.append('grammar')\n        if self.use_length_simple:\n            metrics.append('length_simple')\n        if self.use_lexical_diversity:\n            metrics.append('lexical_diversity')\n        if self.use_repetition_penalty:\n            metrics.append('repetition_penalty')\n        if self.use_rouge_with_source:\n            metrics.append('rouge_with_source')\n        if self.use_semantic_coherence:\n            metrics.append('semantic_coherence')\n        return metrics\n    \n    def _get_metric_params(self, metric_name):\n        \"\"\"получить параметры для метрики\"\"\"\n        if metric_name == 'coverage':\n            return self.coverage_params\n        elif metric_name == 'compression_ratio':\n            return self.compression_params\n        elif metric_name == 'length_simple':\n            return self.length_params\n        else:\n            return None\n    \n    def compute_batch(self, candidates, context=None):\n        \"\"\"\n        вычислить все метрики для батча кандидатов\n        \n        args:\n            candidates: list of str\n            context: str или None\n        \n        returns:\n            list of dict: [{metric_name: score}, ...]\n        \"\"\"\n        if not candidates:\n            return []\n        \n        all_scores = {}\n        \n        for metric_name in self.model.metric_names:\n            params = self._get_metric_params(metric_name)\n            all_scores[metric_name] = self.metrics_computer.compute_metric(\n                metric_name, candidates, context, params\n            )\n        \n        results = []\n        for i in range(len(candidates)):\n            candidate_scores = {\n                metric: all_scores[metric][i] \n                for metric in all_scores\n            }\n            results.append(candidate_scores)\n        \n        return results\n    \n    def compute(self, text, context=None, return_individual=True, return_weighted=True):\n        \"\"\"вычислить метрики для одного текста\"\"\"\n        batch_results = self.compute_batch([text], context)\n        scores = batch_results[0]\n        \n        result = {}\n        if return_individual:\n            result['scores'] = scores\n        \n        if return_weighted:\n            result['weighted_score'] = self.model.predict(scores)\n        \n        return result\n    \n    def rank_candidates(self, candidates, context=None):\n        \"\"\"\n        ранжировать кандидатов\n        \n        returns:\n            list of tuple: [(idx, weighted_score, individual_scores), ...]\n        \"\"\"\n        if not candidates:\n            return []\n        \n        batch_scores = self.compute_batch(candidates, context)\n        \n        results = []\n        for idx, scores in enumerate(batch_scores):\n            weighted_score = self.model.predict(scores)\n            results.append((idx, weighted_score, scores))\n        \n        results.sort(key=lambda x: x[1], reverse=True)\n        return results\n    \n    def get_best_candidate(self, candidates, context=None):\n        \"\"\"получить лучшего кандидата\"\"\"\n        results = self.rank_candidates(candidates, context)\n        if not results:\n            return None\n        best_idx = results[0][0]\n        return candidates[best_idx]\n    \n    def fit(\n        self,\n        candidates_list,\n        contexts,\n        y_texts,\n        metric='rouge2',\n        cache_name=None,\n        use_cache=True,\n        max_iter=50,\n        popsize=15,\n        seed=42,\n        n_workers=1,\n        print_correlations=True\n    ):\n        \"\"\"\n        оптимизировать веса метрик\n        \n        args:\n            candidates_list: list of list of str\n            contexts: list\n            y_texts: list of str\n            metric: str - целевая метрика ('rouge2', 'rougeL')\n        \n        returns:\n            dict, float: оптимальные веса и достигнутый скор\n        \"\"\"\n        if len(candidates_list) != len(contexts) or len(candidates_list) != len(y_texts):\n            raise ValueError(\n                f\"length mismatch: candidates_list={len(candidates_list)}, \"\n                f\"contexts={len(contexts)}, y_texts={len(y_texts)}\"\n            )\n        \n        # определяем метрики для вычисления\n        if self.metric_selection == 'auto_all':\n            # вычисляем ВСЕ метрики\n            metrics_to_compute = MetricsComputer.ALL_METRICS\n            print(f\"\\nрежим auto_all: вычисляем все {len(metrics_to_compute)} метрик\")\n        else:\n            # вычисляем только активные (use_*=True)\n            metrics_to_compute = self._get_active_metrics()\n            if self.metric_selection == 'auto':\n                print(f\"\\nрежим auto: из {len(metrics_to_compute)} активных метрик отберём по корреляции >= {self.min_correlation}\")\n            else:\n                print(f\"\\nрежим manual: используем {len(metrics_to_compute)} метрик\")\n        \n        # предвычисление метрик\n        precomputed_metrics = self._load_or_compute_metrics(\n            candidates_list, contexts, cache_name, use_cache, metrics_to_compute\n        )\n        \n        # предвычисление целевой метрики\n        print(f\"предвычисление целевой метрики ({metric})...\")\n        target_scores_cache = self._precompute_target_scores(\n            candidates_list, y_texts, metric\n        )\n        \n        metric_names = list(precomputed_metrics[0][0].keys())\n        \n        print(f\"целевая метрика: {metric}\")\n        \n        # вычисление корреляций\n        correlations_dict = {}\n        if print_correlations or self.metric_selection in ['auto', 'auto_all']:\n            correlations_dict = self._compute_correlations(\n                precomputed_metrics, target_scores_cache, metric_names\n            )\n        \n        if print_correlations:\n            self._print_correlations(correlations_dict)\n        \n        # автоматический отбор метрик\n        if self.metric_selection in ['auto', 'auto_all']:\n            selected_metrics = self._select_metrics_by_correlation(\n                correlations_dict, self.min_correlation\n            )\n            \n            if not selected_metrics:\n                print(f\"warning: ни одна метрика не прошла порог {self.min_correlation}\")\n                print(f\"используем все {len(metric_names)} метрик\")\n                selected_metrics = metric_names\n            else:\n                dropped = set(metric_names) - set(selected_metrics)\n                if dropped:\n                    print(f\"\\nотброшены ({len(dropped)}):\")\n                    for m in sorted(dropped):\n                        corr = correlations_dict[m]['mean']\n                        print(f\"  {m:25s}: корреляция {corr:+.3f}\")\n                \n                print(f\"\\nотобрано: {len(selected_metrics)}/{len(metric_names)} метрик\")\n                print(f\"метрики: {selected_metrics}\")\n            \n            # фильтруем precomputed_metrics\n            precomputed_metrics = [\n                [{k: v for k, v in m.items() if k in selected_metrics} for m in batch]\n                for batch in precomputed_metrics\n            ]\n            metric_names = selected_metrics\n            \n            # обновляем активные метрики в объекте (для auto_all)\n            if self.metric_selection == 'auto_all':\n                self._update_active_metrics(selected_metrics)\n        \n        n_metrics = len(metric_names)\n        \n        # оптимизация весов\n        best_weights, best_score = self._optimize_weights(\n            precomputed_metrics, target_scores_cache,\n            metric_names, max_iter, popsize, seed, n_workers\n        )\n        \n        # обновление модели\n        weights_dict = {\n            metric_names[i]: best_weights[i]\n            for i in range(n_metrics)\n        }\n        \n        # добавляем нулевые веса для неиспользуемых метрик\n        for metric in MetricsComputer.ALL_METRICS:\n            if metric not in weights_dict:\n                weights_dict[metric] = 0.0\n        \n        # обновляем модель с новыми метриками\n        self.model = ReRankingModel(metric_names, weights_dict)\n        \n        self._print_results(metric, best_score, weights_dict)\n        \n        return weights_dict, best_score\n    \n    def _update_active_metrics(self, selected_metrics):\n        \"\"\"обновить флаги use_* (для auto_all режима)\"\"\"\n        for metric in MetricsComputer.ALL_METRICS:\n            attr_name = f'use_{metric}'\n            setattr(self, attr_name, metric in selected_metrics)\n    \n    def _compute_correlations(self, precomputed_metrics, target_scores_cache, metric_names):\n        \"\"\"вычислить корреляции метрик с целевой метрикой\"\"\"\n        metric_correlations = {name: [] for name in metric_names}\n        \n        for i in range(len(precomputed_metrics)):\n            target_values = target_scores_cache[i]\n            \n            for metric_name in metric_names:\n                metric_values = [\n                    precomputed_metrics[i][j][metric_name]\n                    for j in range(len(precomputed_metrics[i]))\n                ]\n                \n                if np.std(metric_values) > 1e-10 and np.std(target_values) > 1e-10:\n                    corr, _ = spearmanr(metric_values, target_values)\n                    metric_correlations[metric_name].append(corr)\n        \n        # усреднение\n        avg_correlations = {}\n        for metric_name in metric_names:\n            if metric_correlations[metric_name]:\n                avg_correlations[metric_name] = {\n                    'mean': np.mean(metric_correlations[metric_name]),\n                    'std': np.std(metric_correlations[metric_name])\n                }\n        \n        return avg_correlations\n    \n    def _select_metrics_by_correlation(self, correlations_dict, min_correlation):\n        \"\"\"отобрать метрики по корреляции\"\"\"\n        selected = []\n        for metric_name, stats in correlations_dict.items():\n            if abs(stats['mean']) >= min_correlation:\n                selected.append(metric_name)\n        return selected\n    \n    def _print_correlations(self, correlations_dict):\n        \"\"\"вывести корреляции\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"корреляция метрик с целевой метрикой (spearman)\")\n        print(\"=\"*60)\n        \n        correlations = [\n            (name, stats['mean'], stats['std'])\n            for name, stats in correlations_dict.items()\n        ]\n        correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n        \n        for metric_name, avg_corr, std_corr in correlations:\n            if abs(avg_corr) > 0.4:\n                status = \"сильная\"\n            elif abs(avg_corr) > 0.25:\n                status = \"средняя\"\n            elif abs(avg_corr) > 0.15:\n                status = \"слабая\"\n            else:\n                status = \"очень слабая\"\n            \n            print(f\"{status:15s} {metric_name:25s}: {avg_corr:+.3f} (±{std_corr:.3f})\")\n        \n        print(\"=\"*60)\n    \n    def _print_results(self, metric, best_score, weights_dict):\n        \"\"\"вывести результаты калибровки\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"результаты калибровки\")\n        print(\"=\"*60)\n        print(f\"best {metric}: {best_score:.4f}\")\n        print(f\"\\nоптимальные веса:\")\n        \n        sorted_weights = sorted(\n            weights_dict.items(), \n            key=lambda x: abs(x[1]), \n            reverse=True\n        )\n        \n        for name, weight in sorted_weights:\n            if abs(weight) > 1e-6:\n                print(f\"  {name:25s}: {weight:+.4f}\")\n        \n        print(\"=\"*60)\n    \n    def _load_or_compute_metrics(self, candidates_list, contexts, cache_name, use_cache, metrics_to_compute):\n        \"\"\"загрузить или вычислить метрики\"\"\"\n        cache_path = None\n        if cache_name and use_cache:\n            # добавляем метрики в имя кэша для различения\n            metrics_hash = hash(tuple(sorted(metrics_to_compute)))\n            cache_path = self.cache_dir / f\"{cache_name}_metrics_{metrics_hash}.pkl\"\n        \n        if cache_path and cache_path.exists():\n            print(f\"загрузка метрик из кэша: {cache_path}\")\n            with open(cache_path, 'rb') as f:\n                return pickle.load(f)\n        \n        print(f\"предвычисление {len(metrics_to_compute)} метрик...\")\n        precomputed = []\n        \n        for candidates, context in tqdm(\n            list(zip(candidates_list, contexts)), \n            desc=\"computing metrics\"\n        ):\n            candidate_metrics = []\n            for candidate in candidates:\n                metrics_dict = {}\n                for metric_name in metrics_to_compute:\n                    params = self._get_metric_params(metric_name)\n                    scores = self.metrics_computer.compute_metric(\n                        metric_name, [candidate], context, params\n                    )\n                    metrics_dict[metric_name] = scores[0]\n                candidate_metrics.append(metrics_dict)\n            \n            precomputed.append(candidate_metrics)\n        \n        if cache_path:\n            with open(cache_path, 'wb') as f:\n                pickle.dump(precomputed, f)\n            print(f\"метрики сохранены в кэш: {cache_path}\")\n        \n        return precomputed\n    \n    def _precompute_target_scores(self, candidates_list, y_texts, metric='rouge2'):\n        \"\"\"предвычислить целевую метрику\"\"\"\n        scorer = self.metrics_computer._load_rouge_scorer()\n        target_scores = []\n        \n        for candidates, y_text in tqdm(\n            zip(candidates_list, y_texts),\n            total=len(candidates_list),\n            desc=\"target metric\"\n        ):\n            candidate_scores = []\n            for candidate in candidates:\n                scores = scorer.score(y_text, candidate)\n                candidate_scores.append(scores[metric].fmeasure)\n            \n            target_scores.append(candidate_scores)\n        \n        return target_scores\n    \n    def _optimize_weights(\n        self, \n        precomputed_metrics, \n        target_scores_cache, \n        metric_names,\n        max_iter, \n        popsize, \n        seed, \n        n_workers\n    ):\n        \"\"\"оптимизация весов\"\"\"\n        n_metrics = len(metric_names)\n        \n        # сохраняем для objective function\n        self._opt_precomputed_metrics = precomputed_metrics\n        self._opt_target_scores_cache = target_scores_cache\n        self._opt_metric_names = metric_names\n        self._opt_n_metrics = n_metrics\n        \n        print(f\"\\nоптимизация весов (max_iter={max_iter}, popsize={popsize}, workers={n_workers})...\")\n        \n        result = differential_evolution(\n            self._objective_function,\n            bounds=[(-1, 1)] * n_metrics,\n            strategy='best1bin',\n            maxiter=max_iter,\n            popsize=popsize,\n            tol=0.001,\n            mutation=(0.5, 1.5),\n            recombination=0.7,\n            seed=seed,\n            workers=n_workers,\n            updating='deferred' if n_workers > 1 else 'immediate',\n            polish=True,\n            disp=True\n        )\n        \n        # очистка\n        del self._opt_precomputed_metrics\n        del self._opt_target_scores_cache\n        del self._opt_metric_names\n        del self._opt_n_metrics\n        \n        weights_raw = result.x\n        weights_sum = np.sum(np.abs(weights_raw))\n        \n        if weights_sum < 1e-10:\n            print(f\"warning: все веса близки к нулю\")\n            weights_normalized = np.ones(n_metrics) / n_metrics\n        else:\n            weights_normalized = weights_raw / weights_sum\n        \n        best_score = -result.fun\n        return weights_normalized, best_score\n    \n    def _objective_function(self, weights):\n        \"\"\"функция для оптимизации\"\"\"\n        total_score = 0.0\n        \n        for metrics_list, target_scores in zip(\n            self._opt_precomputed_metrics,\n            self._opt_target_scores_cache\n        ):\n            # вычисляем взвешенный скор\n            weighted_scores = [\n                sum(\n                    weights[i] * metrics_dict.get(self._opt_metric_names[i], 0)\n                    for i in range(self._opt_n_metrics)\n                )\n                for metrics_dict in metrics_list\n            ]\n            \n            # выбираем лучшего\n            best_idx = np.argmax(weighted_scores)\n            total_score += target_scores[best_idx]\n        \n        avg_score = total_score / len(self._opt_precomputed_metrics)\n        return -avg_score\n    \n    def save(self, path):\n        \"\"\"сохранить re-ranker\"\"\"\n        with open(path, 'wb') as f:\n            pickle.dump(self, f)\n        print(f\"re-ranker сохранен: {path}\")\n    \n    @staticmethod\n    def load(path):\n        \"\"\"загрузить re-ranker\"\"\"\n        with open(path, 'rb') as f:\n            ranker = pickle.load(f)\n        print(f\"re-ranker загружен: {path}\")\n        return ranker","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# инициализация: вручную выбираем метрики\nranker = UniversalTextReRanker(\n    use_coverage=True,\n    use_length_simple=True,\n    use_repetition_penalty=True,\n    use_grammar=True,\n    length_params={'target_min': 8, 'target_max': 20},\n    metric_selection='manual',\n    device='cuda',\n    cache_dir='./cache_commongen'\n)\n\n# обучение\noptimal_weights, best_score = ranker.fit(\n    candidates_list=dev_candidates,     # list of list of str\n    contexts=dev_contexts,              # list of str\n    y_texts=dev_y_texts,                # list of str\n    metric='rouge2',\n    cache_name='commongen_dev',\n    use_cache=True,\n    max_iter=50,\n    popsize=30,\n    print_correlations=True\n)\n\n# использование\nbest_candidate = ranker.get_best_candidate(candidates, context)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# инициализация: указываем потенциально полезные метрики\n# auto отберёт только сильно коррелирующие\nranker = UniversalTextReRanker(\n    use_coverage=True,\n    use_fluency=True,\n    use_grammar=True,\n    use_length_simple=True,\n    use_repetition_penalty=True,\n    use_lexical_diversity=True,\n    length_params={'target_min': 1, 'target_max': 15},\n    metric_selection='auto',            # автоотбор из указанных\n    min_correlation=0.15,               # порог отбора\n    device='cuda',\n    cache_dir='./cache_qa'\n)\n\n# обучение (автоматически отберёт метрики с |корреляцией| >= 0.15)\noptimal_weights, best_score = ranker.fit(\n    candidates_list=dev_candidates,\n    contexts=dev_contexts,\n    y_texts=dev_y_texts,\n    metric='rougeL',\n    cache_name='qa_dev',\n    max_iter=50,\n    popsize=30,\n    print_correlations=True\n)\n\n# использование\nbest_candidate = ranker.get_best_candidate(candidates, context)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# инициализация: тестирует ВСЕ 10 метрик автоматически\nranker = UniversalTextReRanker(\n    length_params={'target_min': 10, 'target_max': 30},\n    compression_params={'optimal_ratio': 0.15, 'sigma': 0.05},\n    metric_selection='auto_all',       # тестирует все метрики\n    min_correlation=0.1,               # низкий порог → больше метрик\n    device='cuda',\n    cache_dir='./cache_xsum'\n)\n\n# обучение (вычислит все 10 метрик, отберёт лучшие)\noptimal_weights, best_score = ranker.fit(\n    candidates_list=dev_candidates,\n    contexts=dev_contexts,\n    y_texts=dev_y_texts,\n    metric='rougeL',\n    cache_name='xsum_dev',\n    max_iter=50,\n    popsize=30,\n    print_correlations=True\n)\n\n# использование\nbest_candidate = ranker.get_best_candidate(candidates, context)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Метрики","metadata":{}},{"cell_type":"code","source":"!pip -q install bert_score rouge_score\n\nfrom collections import Counter\nimport numpy as np\nfrom rouge_score import rouge_scorer\nfrom bert_score import score as bert_score\nimport torch\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndef exact_match(prediction, reference):\n    return prediction.strip().lower() == reference.strip().lower()\n# Higher is better (0-1)\n\ndef token_f1(prediction, reference):\n    pred_tokens = prediction.lower().split()\n    ref_tokens = reference.lower().split()\n    \n    common = Counter(pred_tokens) & Counter(ref_tokens)\n    num_same = sum(common.values())\n    \n    if num_same == 0:\n        return 0.0\n    \n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(ref_tokens)\n    f1 = 2 * precision * recall / (precision + recall)\n    \n    return f1\n# Higher is better (0-1)\n\ndef compute_bleu(prediction, reference):\n    pred_tokens = prediction.lower().split()\n    ref_tokens = reference.lower().split()\n    \n    smoothing = SmoothingFunction().method1\n    return sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing)\n# Higher is better (0-1)\n\ndef compute_rouge(prediction, reference):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = scorer.score(reference, prediction)\n    \n    return {\n        'rouge1': scores['rouge1'].fmeasure,\n        'rouge2': scores['rouge2'].fmeasure,\n        'rougeL': scores['rougeL'].fmeasure\n    }\n# Higher is better (0-1)\n\ndef compute_bertscore(predictions, references):\n    P, R, F1 = bert_score(predictions, references, lang='en', verbose=False)\n    return {\n        'precision': P.mean().item(),\n        'recall': R.mean().item(),\n        'f1': F1.mean().item()\n    }\n# Higher is better (0-1)\n\ndef compute_meteor(prediction, reference):\n    pred_tokens = prediction.lower().split()\n    ref_tokens = reference.lower().split()\n    \n    return meteor_score([ref_tokens], pred_tokens)\n# Higher is better (0-1)\n\ndef perplexity(model, tokenizer, texts):\n    total_loss = 0\n    total_tokens = 0\n    \n    for text in texts:\n        inputs = tokenizer(text, return_tensors='pt').to(model.device)\n        \n        with torch.no_grad():\n            outputs = model(**inputs, labels=inputs.input_ids)\n            loss = outputs.loss\n        \n        total_loss += loss.item() * inputs.input_ids.size(1)\n        total_tokens += inputs.input_ids.size(1)\n    \n    return np.exp(total_loss / total_tokens)\n# Lower is better\n\ndef distinct_n(texts, n=2):\n    all_ngrams = []\n    \n    for text in texts:\n        tokens = text.lower().split()\n        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n        all_ngrams.extend(ngrams)\n    \n    if not all_ngrams:\n        return 0.0\n    \n    return len(set(all_ngrams)) / len(all_ngrams)\n# Higher is better (0-1, measures diversity)\n\ndef self_bleu(texts):\n    if len(texts) < 2:\n        return 0.0\n    \n    scores = []\n    smoothing = SmoothingFunction().method1\n    \n    for i, text in enumerate(texts):\n        others = texts[:i] + texts[i+1:]\n        if not others:\n            continue\n        \n        text_tokens = text.lower().split()\n        if not text_tokens:\n            continue\n        \n        refs_tokens = [other.lower().split() for other in others if other.strip()]\n        refs_tokens = [ref for ref in refs_tokens if ref]\n        \n        if not refs_tokens:\n            continue\n        \n        try:\n            score = sentence_bleu(refs_tokens, text_tokens, smoothing_function=smoothing)\n            scores.append(score)\n        except:\n            continue\n    \n    return np.mean(scores) if scores else 0.0\n# Lower is better (0-1, measures diversity - lower means more diverse)\n\npredictions = [\n    \"The cat sat on the mat\",\n    \"A dog runs in the park\",\n    \"She loves reading books\"\n]\n\nreferences = [\n    \"A cat was sitting on the mat\",\n    \"The dog is running\",\n    \"She loves reading books\"\n]\n\nem_scores = [exact_match(p, r) for p, r in zip(predictions, references)]\nf1_scores = [token_f1(p, r) for p, r in zip(predictions, references)]\nbleu_scores = [compute_bleu(p, r) for p, r in zip(predictions, references)]\n\nrouge_scores = [compute_rouge(p, r) for p, r in zip(predictions, references)]\nrouge_avg = {\n    'rouge1': np.mean([s['rouge1'] for s in rouge_scores]),\n    'rouge2': np.mean([s['rouge2'] for s in rouge_scores]),\n    'rougeL': np.mean([s['rougeL'] for s in rouge_scores])\n}\n\nbertscore = compute_bertscore(predictions, references)\nmeteor_scores = [compute_meteor(p, r) for p, r in zip(predictions, references)]\n\ndistinct = distinct_n(predictions, n=2)\nsbleu = self_bleu(predictions)\n\nprint(f\"EM: {np.mean(em_scores):.3f}\")\nprint(f\"F1: {np.mean(f1_scores):.3f}\")\nprint(f\"BLEU: {np.mean(bleu_scores):.3f}\")\nprint(f\"ROUGE-1: {rouge_avg['rouge1']:.3f}\")\nprint(f\"ROUGE-2: {rouge_avg['rouge2']:.3f}\")\nprint(f\"ROUGE-L: {rouge_avg['rougeL']:.3f}\")\nprint(f\"BERTScore F1: {bertscore['f1']:.3f}\")\nprint(f\"METEOR: {np.mean(meteor_scores):.3f}\")\nprint(f\"Distinct-2: {distinct:.3f}\")\nprint(f\"Self-BLEU: {sbleu:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T10:31:12.901307Z","iopub.execute_input":"2025-11-15T10:31:12.901606Z","iopub.status.idle":"2025-11-15T10:31:17.308690Z","shell.execute_reply.started":"2025-11-15T10:31:12.901577Z","shell.execute_reply":"2025-11-15T10:31:17.307832Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"EM: 0.333\nF1: 0.672\nBLEU: 0.411\nROUGE-1: 0.738\nROUGE-2: 0.455\nROUGE-L: 0.672\nBERTScore F1: 0.966\nMETEOR: 0.684\nDistinct-2: 1.000\nSelf-BLEU: 0.027\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# работа с A100","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,  # BF16\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    'Qwen/Qwen2.5-7B-Instruct',\n    quantization_config=bnb_config,\n    device_map=\"auto\",  # одна GPU\n    attn_implementation=\"flash_attention_2\"  # Flash Attention, ускорение модели, качество не ухудшается\n)\n\nargs = TrainingArguments(\n    output_dir='./output',\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    fp16=False,\n    bf16=True,  # BF16\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RAG","metadata":{}},{"cell_type":"code","source":"class RAG:\n    def __init__(self, checkpoint='BAAI/bge-base-en-v1.5', device='cuda'):\n        self.model = AutoModel.from_pretrained(checkpoint).to(device)\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.device = device\n        \n        self.x_texts = None\n        self.y_texts = None\n        self.embeddings = None\n\n    def fit(self, x_texts, y_texts, batch_size=32):\n        self.x_texts = x_texts\n        self.y_texts = y_texts\n        all_embeddings = []\n\n        for i in tqdm(range(0, len(x_texts), batch_size), desc='RAG fitting'):\n            batch = x_texts[i:i + batch_size]\n            \n            inputs = self.tokenizer(\n                batch,\n                max_length=512,\n                truncation=True,\n                padding='longest',\n                return_tensors='pt'\n            ).to(self.device)\n            \n            with torch.no_grad():\n                embeddings = self.model(**inputs).last_hidden_state[:, 0]\n            \n            all_embeddings.append(embeddings.cpu())\n\n        self.embeddings = torch.cat(all_embeddings, dim=0).numpy()\n        self.embeddings = self.embeddings / np.linalg.norm(\n            self.embeddings, axis=1, keepdims=True\n        )\n\n    def predict(self, x_texts, k=3, batch_size=32):\n        if isinstance(x_texts, str):\n            x_texts = [x_texts]\n            single = True\n        else:\n            single = False\n\n        all_results = []\n        \n        for i in range(0, len(x_texts), batch_size):\n            batch = x_texts[i:i + batch_size]\n            \n            inputs = self.tokenizer(\n                batch,\n                max_length=512,\n                truncation=True,\n                padding='longest',\n                return_tensors='pt'\n            ).to(self.device)\n            \n            with torch.no_grad():\n                query_embs = self.model(**inputs).last_hidden_state[:, 0]\n            \n            query_embs = query_embs.cpu().numpy()\n            query_embs = query_embs / np.linalg.norm(query_embs, axis=1, keepdims=True)\n            \n            similarities = np.dot(query_embs, self.embeddings.T)\n            \n            for j, sims in enumerate(similarities):\n                top_k = np.argsort(sims)[-k - len(x_texts):][::-1]\n                \n                results = []\n                for idx in top_k:\n                    if self.x_texts[idx] == batch[j]:\n                        continue\n\n                    results.append({\n                        'x': self.x_texts[idx],\n                        'y': self.y_texts[idx],\n                        'similarity': float(sims[idx]),\n                        'index': int(idx)\n                    })\n                \n                all_results.append(results[:k])\n        \n        return all_results[0] if single else all_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'] для LLaMA / Llama-2 / Llama-3 / Mistral / Qwen / Yi","metadata":{}},{"cell_type":"markdown","source":"# Форматы данных для LLM","metadata":{}},{"cell_type":"markdown","source":"1. Instruction-following (самый популярный)","metadata":{}},{"cell_type":"code","source":"# Формат Alpaca\ntemplate = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n{response}\"\"\"\n\n# Формат ChatML (для chat-моделей)\ntemplate = \"\"\"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n{assistant_response}<|im_end|>\"\"\"\n\n# Формат Llama/Mistral Instruct\ntemplate = \"\"\"<s>[INST] {instruction} [/INST] {response}</s>\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Структура датасета","metadata":{}},{"cell_type":"code","source":"# Вариант 1: Простой (для SFTTrainer)\ndataset = [\n    {\"text\": \"### Вопрос: Столица России?\\n### Ответ: Москва\"},\n    {\"text\": \"### Вопрос: 2+2=?\\n### Ответ: 4\"},\n]\n\n# Вариант 2: Разделенный (лучше для контроля)\ndataset = [\n    {\n        \"instruction\": \"Столица России?\",\n        \"response\": \"Москва\"\n    },\n    {\n        \"instruction\": \"2+2=?\",\n        \"response\": \"4\"\n    },\n]\n\n# Вариант 3: С контекстом\ndataset = [\n    {\n        \"instruction\": \"Суммаризируй текст\",\n        \"input\": \"Длинный текст...\",\n        \"output\": \"Краткое содержание\"\n    }\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Маскирование промпта","metadata":{}},{"cell_type":"markdown","source":"Способ 1: Автоматический (SFTTrainer)","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom datasets import Dataset\n\n# 1. Подготовка данных\ndata = [\n    {\"instruction\": \"Столица России?\", \"response\": \"Москва\"},\n    {\"instruction\": \"Автор 'Война и мир'?\", \"response\": \"Лев Толстой\"},\n]\n\ndataset = Dataset.from_list(data)\n\n# 2. Функция форматирования\ndef formatting_func(example):\n    return f\"### Инструкция:\\n{example['instruction']}\\n\\n### Ответ:\\n{example['response']}\"\n\n# 3. Обучение\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,  # эффективный batch = 4*4 = 16\n    learning_rate=2e-4,\n    fp16=True,  # или bf16=True для новых GPU\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    optim=\"paged_adamw_8bit\",  # экономия памяти\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    formatting_func=formatting_func,\n    max_seq_length=512,  # максимальная длина последовательности\n    peft_config=lora_config,  # из прошлого блока\n    args=training_args,\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Способ 2: Продвинутый (кастомный Data Collator)","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List\nfrom transformers import DataCollatorForLanguageModeling\n\n@dataclass\nclass DataCollatorForCompletionOnlyLM:\n    tokenizer: any\n    response_template: str = \"### Ответ:\\n\"\n    mlm: bool = False\n    \n    def __call__(self, examples: List[Dict[str, List[int]]]) -> Dict[str, any]:\n        batch = {\n            \"input_ids\": [],\n            \"attention_mask\": [],\n            \"labels\": []\n        }\n        \n        for example in examples:\n            # Токенизируем полный текст\n            full_text = example[\"text\"]\n            tokenized = self.tokenizer(\n                full_text,\n                truncation=True,\n                max_length=512,\n                padding=False,\n            )\n            \n            input_ids = tokenized[\"input_ids\"]\n            \n            # Находим где начинается ответ\n            response_token_ids = self.tokenizer.encode(\n                self.response_template, \n                add_special_tokens=False\n            )\n            \n            # Ищем шаблон в input_ids\n            labels = [-100] * len(input_ids)\n            \n            for i in range(len(input_ids) - len(response_token_ids)):\n                if input_ids[i:i+len(response_token_ids)] == response_token_ids:\n                    # Нашли начало ответа\n                    response_start = i + len(response_token_ids)\n                    labels[response_start:] = input_ids[response_start:]\n                    break\n            \n            batch[\"input_ids\"].append(input_ids)\n            batch[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n            batch[\"labels\"].append(labels)\n        \n        # Padding\n        from torch.nn.utils.rnn import pad_sequence\n        import torch\n        \n        batch[\"input_ids\"] = pad_sequence(\n            [torch.tensor(x) for x in batch[\"input_ids\"]], \n            batch_first=True, \n            padding_value=self.tokenizer.pad_token_id\n        )\n        batch[\"attention_mask\"] = pad_sequence(\n            [torch.tensor(x) for x in batch[\"attention_mask\"]], \n            batch_first=True, \n            padding_value=0\n        )\n        batch[\"labels\"] = pad_sequence(\n            [torch.tensor(x) for x in batch[\"labels\"]], \n            batch_first=True, \n            padding_value=-100\n        )\n        \n        return batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Способ 3: Готовый инструмент из trl","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM\n\n# Указываем шаблон ответа\nresponse_template = \"### Ответ:\\n\"\n\ncollator = DataCollatorForCompletionOnlyLM(\n    response_template=response_template,\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# Используем в Trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    data_collator=collator,\n    # ...\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TrainingArguments: Что важно","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    # === Основное ===\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    \n    # === Batch size ===\n    per_device_train_batch_size=4,  # на 1 GPU\n    gradient_accumulation_steps=4,   # накапливаем градиенты\n    # Реальный batch = 4 * 4 = 16\n    \n    # === Learning rate ===\n    learning_rate=2e-4,  # для LoRA обычно выше: 1e-4 до 5e-4\n    lr_scheduler_type=\"cosine\",  # или \"linear\"\n    warmup_steps=100,  # или warmup_ratio=0.1\n    \n    # === Оптимизация памяти ===\n    fp16=True,  # для старых GPU (V100, RTX 2080)\n    # bf16=True,  # для новых GPU (A100, RTX 3090+) - лучше чем fp16\n    gradient_checkpointing=True,  # экономия памяти за счет скорости\n    optim=\"paged_adamw_8bit\",  # 8-bit optimizer от bitsandbytes\n    \n    # === Логирование ===\n    logging_steps=10,\n    logging_dir=\"./logs\",\n    report_to=\"tensorboard\",  # или \"wandb\"\n    \n    # === Сохранение ===\n    save_strategy=\"epoch\",  # \"steps\", \"epoch\", \"no\"\n    save_total_limit=2,  # храним только 2 последних чекпоинта\n    \n    # === Eval (если есть val set) ===\n    evaluation_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"loss\",\n    \n    # === Прочее ===\n    remove_unused_columns=False,  # важно для SFTTrainer\n    dataloader_num_workers=4,  # параллельная загрузка данных\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Полный пример: От данных до обученной модели","metadata":{}},{"cell_type":"code","source":"!pip -q install transformers>=4.38.0 trl>=0.8.0 peft>=0.9.0 bitsandbytes\n\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\n\ncheckpoint = \"mistralai/Mistral-7B-v0.1\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_type=torch.float16,\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\n    quantization_config=bnb_config,\n    device_map='auto'\n)\n\nprepare_model_for_kbit_training(model)\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout=0.05,\n    bias='none',\n    task_type='CAUSAL_LM'\n)\n\ntrain_data = [\n    {\n        \"question\": \"Переведи на английский: Привет, как дела?\",\n        \"answer\": \"Hello, how are you?\"\n    },\n    {\n        \"question\": \"Реши: 15 * 8\",\n        \"answer\": \"120\"\n    }\n]\ndataset = Dataset.from_list([\n    {\"text\": f\"<s>[INST] {item['question']} [/INST] {item['answer']}</s>\"}\n    for item in train_data\n])\n\nargs = TrainingArguments(\n    optim='paged_adamw_8bit',\n    report_to='none',\n    output_dir='./result',\n    fp16=torch.cuda.is_available()\n)\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    args=args,\n    peft_config=lora_config\n)\ntrainer.train()\ntrainer.save_model(\"./qa_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:05:05.912300Z","iopub.execute_input":"2025-11-09T07:05:05.912944Z","iopub.status.idle":"2025-11-09T07:08:47.322783Z","shell.execute_reply.started":"2025-11-09T07:05:05.912910Z","shell.execute_reply":"2025-11-09T07:08:47.321847Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-09 07:06:42.452891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762672002.680153      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762672002.744361      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10bf71b90a6f476c8c2ec52233e49cce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27834fcd3de406fac4238660e925c70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b26667f5d7504ef2b4db80e80e0f5f87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001b1b79213e43d186be13a9a140e538"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd8eeafb43054d268bf38b5e9349a9ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ad1627cdd734e91b275c18015de233a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1035e9c9ee384c81a9c8fef14ffb1edc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b02ae79b52244e92b00d485e55f8b95e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7016b85b47d24cbb9289c8973649c145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"933b96de1aee448da9af6de0383743cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc4b706f42542fe9c40ca3198f692ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8621a1e8d1cd409d9b1fb0f080189e37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72d0ea43281445d395c2ddd3e7beadcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6460fced3bbe40449a68cf84aa128563"}},"metadata":{}},{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from peft import PeftModel\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\nmodel = PeftModel.from_pretrained(model, \"./qa_model\")\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:08:47.323999Z","iopub.execute_input":"2025-11-09T07:08:47.324276Z","iopub.status.idle":"2025-11-09T07:09:12.059106Z","shell.execute_reply.started":"2025-11-09T07:08:47.324256Z","shell.execute_reply":"2025-11-09T07:09:12.058460Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"931c9fc61771459592a1d64c8f3e24e3"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): MistralRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Подготовка промпта\nprompt = \"<s>[INST] Привет, как дела? [/INST]\"\n\n# Токенизация\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(trainer.model.device)\n\n# Генерация\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=256,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    repetition_penalty=1.2\n)\n\n# Декодирование\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = tokenizer('<s>[INST] Приветики-пистолетики! [/INST]</s>', return_tensors='pt').to(trainer.model.device)\noutputs = model(\n    **inputs,\n    max_new_tokens=256,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    repetitiin_penalty=1.2\n)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Подготовка промпта\nprompt = \"<s>[INST] Привет, как дела? [/INST]\"\n\n# Токенизация\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(trainer.model.device)\n\n# Генерация\noutputs = model.generate(\n    **inputs,\n    \n    # === Длина ===\n    max_new_tokens=256,      # максимум новых токенов\n    min_new_tokens=10,       # минимум (опционально)\n    max_length=512,          # альтернатива: общая длина (prompt + generation)\n    \n    # === Стратегия декодирования ===\n    do_sample=True,          # False = greedy, True = sampling\n    \n    # === Temperature (креативность) ===\n    temperature=0.7,         # 0.1 = консервативно, 1.0 = нормально, 2.0 = креативно\n                             # <0.7: факты, код, переводы\n                             # 0.7-1.0: обычная генерация\n                             # >1.0: креативное письмо\n    \n    # === Top-p (nucleus sampling) ===\n    top_p=0.9,              # рассматриваем топ токенов с суммарной вероятностью 90%\n                            # 0.9-0.95: хороший баланс\n                            # 0.5: более консервативно\n                            # 0.99: почти все токены\n    \n    # === Top-k sampling ===\n    top_k=50,               # рассматриваем только топ-50 токенов\n                            # обычно 40-100\n                            # 0 = выключено\n    \n    # === Repetition penalty ===\n    repetition_penalty=1.2, # штраф за повторения\n                            # 1.0 = нет штрафа\n                            # 1.1-1.5: легкий штраф (обычно хорошо)\n                            # >1.5: сильный штраф\n    \n    # # === Stopping criteria ===\n    # eos_token_id=tokenizer.eos_token_id,\n    # pad_token_id=tokenizer.pad_token_id,\n    \n    # === Другое ===\n    num_return_sequences=1,  # сколько вариантов генерировать\n    num_beams=1,            # beam search (1 = выключен)\n)\n\n# Декодирование\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Если не хватает памяти","metadata":{}},{"cell_type":"code","source":"# Уменьшите:\nper_device_train_batch_size=2  # было 4\nmax_seq_length=256  # было 512\nr=8  # было 16 в LoRA\n\n# Добавьте:\ngradient_checkpointing=True\noptim=\"paged_adamw_8bit\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Блок 3: Inference, генерация и валидация","metadata":{}},{"cell_type":"markdown","source":"Часть 1: Загрузка обученной модели","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    checkpoint,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\nmodel = PeftModel.from_pretrained(model, \"./qa_model\")\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Часть 2: Генерация текста","metadata":{}},{"cell_type":"code","source":"# Подготовка промпта\nprompt = \"<s>[INST] Hello! [/INST]\"\n\n# Токенизация\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(trainer.model.device)\n\n# Генерация\noutputs = trainer.model.generate(\n    **inputs,\n    max_new_tokens=100,  # сколько токенов сгенерировать\n    do_sample=False,     # greedy decoding\n)\n\n# Декодирование\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:11:39.914349Z","iopub.execute_input":"2025-11-09T07:11:39.914643Z","iopub.status.idle":"2025-11-09T07:11:55.946817Z","shell.execute_reply.started":"2025-11-09T07:11:39.914622Z","shell.execute_reply":"2025-11-09T07:11:55.946118Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nBoth `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"[INST] Hello! [/INST]\n\nHi there, I'm new to this forum. My name is Kieran and I am from the United Kingdom (hence my accent). I have been playing TF2 for 5 years now, and have always loved it. It has never failed to give me fun times with friends or strangers alike.\nI've just recently got back into trading after a long break, so if you want to trade with someone who knows what they are doing then feel free to send me an offer. Also, any tips would be appreciated as I'm not very good at trading myself xD .\nI will also like to join your group, as I think that we could help each other out in trades.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Параметры генерации","metadata":{}},{"cell_type":"code","source":"outputs = model.generate(\n    **inputs,\n    \n    # === Длина ===\n    max_new_tokens=256,      # максимум новых токенов\n    min_new_tokens=10,       # минимум (опционально)\n    max_length=512,          # альтернатива: общая длина (prompt + generation)\n    \n    # === Стратегия декодирования ===\n    do_sample=True,          # False = greedy, True = sampling\n    \n    # === Temperature (креативность) ===\n    temperature=0.7,         # 0.1 = консервативно, 1.0 = нормально, 2.0 = креативно\n                             # <0.7: факты, код, переводы\n                             # 0.7-1.0: обычная генерация\n                             # >1.0: креативное письмо\n    \n    # === Top-p (nucleus sampling) ===\n    top_p=0.9,              # рассматриваем топ токенов с суммарной вероятностью 90%\n                            # 0.9-0.95: хороший баланс\n                            # 0.5: более консервативно\n                            # 0.99: почти все токены\n    \n    # === Top-k sampling ===\n    top_k=50,               # рассматриваем только топ-50 токенов\n                            # обычно 40-100\n                            # 0 = выключено\n    \n    # === Repetition penalty ===\n    repetition_penalty=1.2, # штраф за повторения\n                            # 1.0 = нет штрафа\n                            # 1.1-1.5: легкий штраф (обычно хорошо)\n                            # >1.5: сильный штраф\n    \n    # === Stopping criteria ===\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.pad_token_id,\n    \n    # === Другое ===\n    num_return_sequences=1,  # сколько вариантов генерировать\n    num_beams=1,            # beam search (1 = выключен)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T07:09:43.686036Z","iopub.execute_input":"2025-11-09T07:09:43.686576Z","iopub.status.idle":"2025-11-09T07:10:11.753967Z","shell.execute_reply.started":"2025-11-09T07:09:43.686551Z","shell.execute_reply":"2025-11-09T07:10:11.753347Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nBoth `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Комбинации параметров для разных задач","metadata":{}},{"cell_type":"code","source":"# 1. Факты, QA, перевод (нужна точность)\ngeneration_config_precise = {\n    \"max_new_tokens\": 100,\n    \"do_sample\": True,\n    \"temperature\": 0.3,\n    \"top_p\": 0.85,\n    \"repetition_penalty\": 1.1,\n}\n\n# 2. Обычная генерация (баланс)\ngeneration_config_balanced = {\n    \"max_new_tokens\": 200,\n    \"do_sample\": True,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"top_k\": 50,\n    \"repetition_penalty\": 1.2,\n}\n\n# 3. Креативное письмо\ngeneration_config_creative = {\n    \"max_new_tokens\": 300,\n    \"do_sample\": True,\n    \"temperature\": 1.0,\n    \"top_p\": 0.95,\n    \"repetition_penalty\": 1.15,\n}\n\n# 4. Детерминированная генерация (для debug)\ngeneration_config_greedy = {\n    \"max_new_tokens\": 100,\n    \"do_sample\": False,  # greedy decoding\n}\n\n# Использование\noutputs = model.generate(**inputs, **generation_config_balanced)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Batch генерация","metadata":{}},{"cell_type":"code","source":"prompts = [\n    \"<s>[INST] Столица России? [/INST]\",\n    \"<s>[INST] 2+2=? [/INST]\",\n    \"<s>[INST] Кто написал 'Евгений Онегин'? [/INST]\"\n]\n\n# Токенизация с padding\ninputs = tokenizer(\n    prompts, \n    return_tensors=\"pt\", \n    padding=True,  # важно!\n    truncation=True,\n    max_length=512\n).to(model.device)\n\n# Генерация\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=50,\n    temperature=0.7,\n    top_p=0.9,\n    pad_token_id=tokenizer.pad_token_id\n)\n\n# Декодирование\nfor i, output in enumerate(outputs):\n    text = tokenizer.decode(output, skip_special_tokens=True)\n    print(f\"Prompt {i}: {text}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_response(model, tokenizer, promts):\n    inputs = tokenizer(\n        promts,\n        return_tensors='pt',\n        padding='longest',\n        truncation=True,\n        max_length=512\n    ).to(model.device)\n    outputs = model.generate(\n        **inputs,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.2,\n        max_new_tokens=256\n    )\n\n    texts = []\n    for output in outputs:\n        output = output[inputs.input_ids.shape[1]:]\n        text = tokenizer.decode(output, skip_special_tokens=True)\n        texts.append(text)\n\n    return texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def template_processing(question, answer=None):\n    if answer is None:\n        return tokenizer.apply_chat_template(\n            [{'role': 'user', 'content': f'здесь задача модели:\\n\\n{question}'}],\n            tokenize=False,\n            add_generation_promt=True\n        )\n    else:\n        return tokenizer.apply_chat_template(\n            [{'role': 'user', 'content': f'здесь задача модели:\\n\\n{question}'},\n             {'role': 'assistant', 'content': answer}],\n            tokenize=False\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Часть 3: Валидация в процессе обучения","metadata":{}},{"cell_type":"markdown","source":"Вариант 1: Простой callback для генерации примеров","metadata":{}},{"cell_type":"code","source":"from transformers import TrainerCallback\n\nclass GenerationCallback(TrainerCallback):\n    def __init__(self, tokenizer, test_prompts, every_n_steps=100):\n        self.tokenizer = tokenizer\n        self.test_prompts = test_prompts\n        self.every_n_steps = every_n_steps\n    \n    def on_step_end(self, args, state, control, model=None, **kwargs):\n        if state.global_step % self.every_n_steps == 0:\n            print(f\"\\n{'='*50}\")\n            print(f\"Generation at step {state.global_step}\")\n            print(f\"{'='*50}\")\n            \n            model.eval()\n            for prompt in self.test_prompts:\n                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n                \n                with torch.no_grad():\n                    outputs = model.generate(\n                        **inputs,\n                        max_new_tokens=50,\n                        temperature=0.7,\n                        top_p=0.9,\n                        do_sample=True\n                    )\n                \n                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n                print(f\"\\nPrompt: {prompt}\")\n                print(f\"Generated: {generated}\")\n            \n            model.train()\n            print(f\"{'='*50}\\n\")\n\n# Использование\ntest_prompts = [\n    \"<s>[INST] Столица России? [/INST]\",\n    \"<s>[INST] Что такое Python? [/INST]\",\n]\n\ntrainer = SFTTrainer(\n    model=model,\n    # ... остальные параметры\n    callbacks=[GenerationCallback(tokenizer, test_prompts, every_n_steps=50)]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Вариант 2: Validation set с метриками","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\n# Разделяем данные\ntrain_data = data[:800]\nval_data = data[800:]\n\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\n# В TrainingArguments добавляем\ntraining_args = TrainingArguments(\n    # ...\n    evaluation_strategy=\"steps\",  # или \"epoch\"\n    eval_steps=100,               # оценивать каждые 100 шагов\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n)\n\n# Trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,  # добавили validation set\n    # ...\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Вариант 3: Кастомные метрики (ROUGE, BLEU)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom evaluate import load\n\n# Загружаем метрики\nrouge = load('rouge')\nbleu = load('bleu')\n\n# Сам генерируй и считай метрики\ndef evaluate_model(model, val_data):\n    rouge = load('rouge')\n    predictions = []\n    references = []\n    \n    for item in val_data[:50]:\n        prompt = f\"<s>[INST] {item['question']} [/INST]\"\n        \n        # Генерируем\n        generated = generate_response(model, tokenizer, prompt)\n        \n        predictions.append(generated)\n        references.append(item['answer'])\n    \n    scores = rouge.compute(predictions=predictions, references=references)\n    return scores  # чем больше, тем лучше\n\n########################################## ИЛИ\n\nrouge = evaluate.load('rouge')\ndef evaluate_model(model, val_data):\n    predictions = generate_response(model, tokenizer, [f\"<s>[INST] {item['question']} [/INST]\" for item in val_data])\n    references = [item['answer'] for item in val_data]\n\n    return rouge.compute(predictions=predictions, references=references)\n\n########################################## ИЛИ\n\nrouge = evaluate.load('rouge')\ndef evaluate_model(model, tokenizer, val_data, batch_size=8):\n    predictions = []\n    for i in tqdm(range(0, len(val_data), batch_size)):\n        batch = [val_data[j] for j in range(i, min(i+batch_size, len(val_data)))]\n        batch_predictions = generate_response(\n            model, tokenizer,\n            [tokenizer.apply_chat_template(\n                [{'role': 'user', 'content': item['text']}],\n                add_generation_prompt=True, tokenize=False\n            ) for item in batch]\n        )\n        predictions.extend(batch_predictions)\n    references = [item['summary'] for item in val_data]\n    return rouge.compute(predictions=predictions, references=references)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Вариант 4: Полноценная валидация с генерацией (для олимпиады)","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, tokenizer, val_data, num_samples=50):\n    \"\"\"\n    Оценка модели на validation set с реальной генерацией\n    \"\"\"\n    model.eval()\n    results = {\n        \"rouge1\": [],\n        \"rouge2\": [],\n        \"rougeL\": [],\n        \"exact_match\": 0,\n    }\n    \n    rouge_metric = load('rouge')\n    \n    for i, example in enumerate(val_data[:num_samples]):\n        # Формируем промпт\n        prompt = f\"<s>[INST] {example['instruction']} [/INST]\"\n        true_response = example['response']\n        \n        # Генерируем\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=100,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True\n            )\n        \n        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Убираем промпт из сгенерированного текста\n        generated = generated.replace(prompt, \"\").strip()\n        \n        # Считаем метрики\n        rouge_scores = rouge_metric.compute(\n            predictions=[generated],\n            references=[true_response]\n        )\n        \n        results[\"rouge1\"].append(rouge_scores[\"rouge1\"])\n        results[\"rouge2\"].append(rouge_scores[\"rouge2\"])\n        results[\"rougeL\"].append(rouge_scores[\"rougeL\"])\n        \n        # Exact match (для простых задач типа QA)\n        if generated.strip().lower() == true_response.strip().lower():\n            results[\"exact_match\"] += 1\n    \n    # Усредняем\n    final_results = {\n        \"rouge1\": np.mean(results[\"rouge1\"]),\n        \"rouge2\": np.mean(results[\"rouge2\"]),\n        \"rougeL\": np.mean(results[\"rougeL\"]),\n        \"exact_match\": results[\"exact_match\"] / num_samples,\n    }\n    \n    model.train()\n    return final_results\n\n# Использование\nval_results = evaluate_model(model, tokenizer, val_data)\nprint(val_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Часть 4: Debugging и типичные проблемы","metadata":{}},{"cell_type":"markdown","source":"Проблема 1: Модель повторяет промпт","metadata":{}},{"cell_type":"code","source":"# Проблема:\nprompt = \"Вопрос: Столица России?\"\n# Генерация: \"Вопрос: Столица России? Вопрос: Столица России? Вопрос...\"\n\n# Решение 1: Увеличить repetition_penalty\noutputs = model.generate(\n    **inputs,\n    repetition_penalty=1.5,  # было 1.2\n)\n\n# Решение 2: Правильно форматировать промпт (использовать тот же формат что при обучении)\nprompt = \"<s>[INST] Столица России? [/INST]\"  # как в обучении!\n\n# Решение 3: Убрать промпт из вывода\ngenerated = tokenizer.decode(outputs[0], skip_special_tokens=True)\nresponse = generated.replace(prompt, \"\").strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Проблема 2: Модель генерирует бессмыслицу","metadata":{}},{"cell_type":"code","source":"# Причины:\n# 1. Слишком высокий temperature\ntemperature=0.5  # вместо 1.5\n\n# 2. Модель недообучена\n# Проверьте loss, обучите дольше\n\n# 3. Слишком мало данных\n# Нужно минимум 100-500 качественных примеров\n\n# 4. Неправильный формат промпта\n# Используйте ТОТ ЖЕ формат что и при обучении!","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Проблема 3: Модель обрывается на середине","metadata":{}},{"cell_type":"code","source":"# Проблема: генерация заканчивается слишком рано\n\n# Решение 1: Увеличить max_new_tokens\nmax_new_tokens=256  # было 50\n\n# Решение 2: Проверить eos_token\nprint(f\"EOS token: {tokenizer.eos_token}\")\nprint(f\"EOS token ID: {tokenizer.eos_token_id}\")\n\n# Решение 3: Добавить min_new_tokens\nmin_new_tokens=20","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Проблема 4: Медленная генерация","metadata":{}},{"cell_type":"code","source":"# Решение 1: Использовать квантизацию\n# (уже покрыто выше)\n\n# Решение 2: Уменьшить max_new_tokens\nmax_new_tokens=100  # было 512\n\n# Решение 3: Использовать greedy вместо sampling\ndo_sample=False  # быстрее, но менее разнообразно\n\n# Решение 4: Batch inference\n# (покрыто выше)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Часть 6: Быстрый inference для олимпиады","metadata":{}},{"cell_type":"code","source":"class LLMInference:\n    def __init__(self, base_model_name, adapter_path, use_4bit=True):\n        \"\"\"Класс для быстрого inference\"\"\"\n        \n        if use_4bit:\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_compute_dtype=torch.float16\n            )\n            self.model = AutoModelForCausalLM.from_pretrained(\n                base_model_name,\n                quantization_config=bnb_config,\n                device_map=\"auto\"\n            )\n        else:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                base_model_name,\n                torch_dtype=torch.float16,\n                device_map=\"auto\"\n            )\n        \n        self.model = PeftModel.from_pretrained(self.model, adapter_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        self.model.eval()\n    \n    def generate(self, prompt, max_new_tokens=100, temperature=0.7, **kwargs):\n        \"\"\"Генерация одного ответа\"\"\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                top_p=kwargs.get('top_p', 0.9),\n                do_sample=True,\n                repetition_penalty=kwargs.get('repetition_penalty', 1.2),\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        \n        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Убираем промпт\n        response = generated.replace(prompt, \"\").strip()\n        return response\n    \n    def batch_generate(self, prompts, **kwargs):\n        \"\"\"Batch генерация\"\"\"\n        inputs = self.tokenizer(\n            prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=512\n        ).to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=kwargs.get('max_new_tokens', 100),\n                temperature=kwargs.get('temperature', 0.7),\n                top_p=kwargs.get('top_p', 0.9),\n                do_sample=True,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        \n        results = []\n        for i, output in enumerate(outputs):\n            generated = self.tokenizer.decode(output, skip_special_tokens=True)\n            response = generated.replace(prompts[i], \"\").strip()\n            results.append(response)\n        \n        return results\n\n# Использование\ninferencer = LLMInference(\n    base_model_name=\"mistralai/Mistral-7B-v0.1\",\n    adapter_path=\"./final_model\"\n)\n\nresponse = inferencer.generate(\n    \"<s>[INST] Столица России? [/INST]\",\n    max_new_tokens=50,\n    temperature=0.5\n)\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Часть 7: Сохранение и загрузка для submission","metadata":{}},{"cell_type":"code","source":"# === После обучения ===\n\n# Вариант 1: Сохранить только LoRA адаптер (маленький размер)\ntrainer.save_model(\"./lora_adapter\")\n# Размер: ~10-50 MB\n\n# Вариант 2: Объединить и сохранить полную модель\nmodel = model.merge_and_unload()\nmodel.save_pretrained(\"./full_model\")\ntokenizer.save_pretrained(\"./full_model\")\n# Размер: ~13 GB для 7B модели\n\n# === Для загрузки ===\n\n# Вариант 1: Загрузить LoRA адаптер\nbase_model = AutoModelForCausalLM.from_pretrained(...)\nmodel = PeftModel.from_pretrained(base_model, \"./lora_adapter\")\n\n# Вариант 2: Загрузить полную модель\nmodel = AutoModelForCausalLM.from_pretrained(\"./full_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"./full_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}